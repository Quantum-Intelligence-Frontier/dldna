{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 制造小巧但强大的语言模型：探索DeepSeek和Phi-3的秘密\n",
    "\n",
    "尽管大型语言模型（LLM）取得了耀眼的发展，但在其背后存在着巨大的计算资源消耗和环境问题。训练和运行拥有数千亿参数的LLM需要大量的GPU，这将导致碳排放量增加，加速全球变暖。此外，高昂的成本使得只有少数巨型公司主导LLM开发，阻碍了AI技术的民主化，并加深了对特定企业的依赖。\n",
    "\n",
    "在这种情况下，“小巧但强大”的小型语言模型（SLM）正成为可持续AI发展的新选择。SLM能够在有限的计算资源下表现出足够的性能，为个人开发者或小型研究团队参与AI技术研发开辟了道路。此外，减少能源消耗可以缓解环境负担，并降低对特定硬件或平台的依赖性，从而促进AI技术的多样性。\n",
    "\n",
    "在这里，我们将深入分析最近受到关注的小型语言模型（SLM）DeepSeek和Phi-3，基于它们的设计理念和训练技巧，提出构建自己高效语言模型的方法。\n",
    "\n",
    "**将涉及以下内容**\n",
    "\n",
    "*   **小巨人，DeepSeek与Phi-3：**\n",
    "\n",
    "    *   DeepSeek和Phi-3是如何在较小的规模下实现卓越性能的？\n",
    "    *   它们的架构与现有的LLM有何不同？\n",
    "    *   什么是数据驱动训练（Data-Centric Training），为什么重要？\n",
    "    *   持续预训练（Continual Pre-training）会带来什么效果？\n",
    "\n",
    "*   **创建自己的小型语言模型**\n",
    "\n",
    "    *   模型架构设计：分析DeepSeek和Phi-3的核心组成部分，获得可以应用于自己模型的想法。\n",
    "    *   数据集构建及预处理：学习获取高质量的训练数据，并将其加工成适合模型的形式的方法。\n",
    "    *   高效的训练技术：了解如何在有限资源下实现最大效果的训练策略。（知识蒸馏（Knowledge Distillation）、量化（Quantization）、剪枝（Pruning）等）\n",
    "    *   模型评估及微调：学习客观评估训练模型性能，并针对特定任务进行优化的方法。\n",
    "\n",
    "**通过这些，您将能够**\n",
    "\n",
    "*   掌握最新小型语言模型的核心技术和趋势。\n",
    "*   在资源有限的环境中具备开发高效语言模型的能力。\n",
    "*   利用自己开发的语言模型构建各种自然语言处理（NLP）应用程序。\n",
    "*   减少对大型语言模型的依赖，并探索可持续AI发展的可能性。\n",
    "\n",
    "**大型模型并不总是有利。通过DeepSeek和Phi-3的创新方法，我们邀请您进入小巧但强大的语言模型的世界！**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
