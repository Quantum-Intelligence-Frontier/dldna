<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>quarto-inputacb4930a8386a86e – Deep Learning DNA: Surviving Architectures and Essential Principles</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap-f507c7d0488cb7630e20aad62ad8c2aa.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script>window.MathJax = {loader: {load: ['[tex]/boldsymbol']},tex: {packages: {'[+]': ['boldsymbol']}}};</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../../notebooks/zh/part_1/01_深度学习的开始.html">part_1</a></li><li class="breadcrumb-item"><a href="../../../notebooks/zh/part_1/08_变压器的诞生.html">8. 变压器的诞生</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../../../">中文</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Language</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index_de.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Deutsch</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index_en.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">English</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index_es.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Español</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">한국어</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index_zh.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">中文</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/zh/part_1/00_引言.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">引言</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">part_1</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/zh/part_1/01_深度学习的开始.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">1. 深度学习的开始</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/zh/part_1/02_深度学习的数学.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">2. 深度学习的数学</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/zh/part_1/03_深度学习框架.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">3. 深度学习框架</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/zh/part_1/04_激活函数.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">4. 激活函数</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/zh/part_1/05_优化和可视化.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">5. 优化和可视化</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/zh/part_1/06_过拟合和解决技术的发展.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">6. 过拟合和解决技术的发展</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/zh/part_1/07_卷积神经网络的进化.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">7. 卷积神经网络的进化</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/zh/part_1/08_变压器的诞生.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">8. 变压器的诞生</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/zh/part_1/09_变压器的进化.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">9. 变压器的进化</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/zh/part_1/10_多模态深度学习：多感官融合的开始.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">10. 多模态深度学习：多感官融合的开始</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/zh/part_1/11_多模态深度学习：超越智能的极限.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">11. 多模态深度学习：超越智能的极限</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">深度学习的最前沿</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/zh/深度学习的最前沿/01_SLM: 小但强大的语言模型.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">1. SLM: 小但强大的语言模型</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/zh/深度学习的最前沿/02_自动驾驶.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">2. 自动驾驶</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#第8章-变压器的诞生" id="toc-第8章-变压器的诞生" class="nav-link active" data-scroll-target="#第8章-变压器的诞生">第8章 变压器的诞生</a>
  <ul class="collapse">
  <li><a href="#变压器---序列处理的革命" id="toc-变压器---序列处理的革命" class="nav-link" data-scroll-target="#变压器---序列处理的革命">8.1 变压器 - 序列处理的革命</a></li>
  <li><a href="#变压器的进化过程" id="toc-变压器的进化过程" class="nav-link" data-scroll-target="#变压器的进化过程">8.2 变压器的进化过程</a>
  <ul class="collapse">
  <li><a href="#rnn的局限性和注意力的诞生" id="toc-rnn的局限性和注意力的诞生" class="nav-link" data-scroll-target="#rnn的局限性和注意力的诞生">8.2.1 RNN的局限性和注意力的诞生</a></li>
  <li><a href="#注意的基本概念" id="toc-注意的基本概念" class="nav-link" data-scroll-target="#注意的基本概念">8.2.2 注意的基本概念</a></li>
  <li><a href="#向自注意力的演变" id="toc-向自注意力的演变" class="nav-link" data-scroll-target="#向自注意力的演变">8.2.3 向自注意力的演变</a></li>
  <li><a href="#多头注意力与并行处理" id="toc-多头注意力与并行处理" class="nav-link" data-scroll-target="#多头注意力与并行处理">8.2.4 多头注意力与并行处理</a></li>
  <li><a href="#多头注意力multi-head-attention详细分析" id="toc-多头注意力multi-head-attention详细分析" class="nav-link" data-scroll-target="#多头注意力multi-head-attention详细分析">多头注意力（Multi-Head Attention）详细分析</a></li>
  <li><a href="#并行学习的掩码策略" id="toc-并行学习的掩码策略" class="nav-link" data-scroll-target="#并行学习的掩码策略">8.2.5 并行学习的掩码策略</a></li>
  <li><a href="#头概念的演变从头部到脑" id="toc-头概念的演变从头部到脑" class="nav-link" data-scroll-target="#头概念的演变从头部到脑">8.2.6 头概念的演变：从“头部”到“脑”</a></li>
  </ul></li>
  <li><a href="#位置信息的处理" id="toc-位置信息的处理" class="nav-link" data-scroll-target="#位置信息的处理">8.3 位置信息的处理</a>
  <ul class="collapse">
  <li><a href="#序列信息的重要性" id="toc-序列信息的重要性" class="nav-link" data-scroll-target="#序列信息的重要性">8.3.1 序列信息的重要性</a></li>
  <li><a href="#位置编码的设计" id="toc-位置编码的设计" class="nav-link" data-scroll-target="#位置编码的设计">8.3.2 位置编码的设计</a></li>
  </ul></li>
  <li><a href="#变压器的完整架构" id="toc-变压器的完整架构" class="nav-link" data-scroll-target="#变压器的完整架构">8.4 变压器的完整架构</a>
  <ul class="collapse">
  <li><a href="#基本组件的整合" id="toc-基本组件的整合" class="nav-link" data-scroll-target="#基本组件的整合">8.4.1 基本组件的整合</a></li>
  <li><a href="#编码器的构成" id="toc-编码器的构成" class="nav-link" data-scroll-target="#编码器的构成">8.4.2 编码器的构成</a></li>
  <li><a href="#解码器的构成" id="toc-解码器的构成" class="nav-link" data-scroll-target="#解码器的构成">8.4.3 解码器的构成</a></li>
  <li><a href="#整体结构的说明" id="toc-整体结构的说明" class="nav-link" data-scroll-target="#整体结构的说明">8.4.4 整体结构的说明</a></li>
  </ul></li>
  <li><a href="#变压器示例" id="toc-变压器示例" class="nav-link" data-scroll-target="#变压器示例">8.5 变压器示例</a>
  <ul class="collapse">
  <li><a href="#简单复制任务" id="toc-简单复制任务" class="nav-link" data-scroll-target="#简单复制任务">8.5.1 简单复制任务</a></li>
  <li><a href="#位数加法问题" id="toc-位数加法问题" class="nav-link" data-scroll-target="#位数加法问题">8.5.2 位数加法问题</a></li>
  <li><a href="#解析器任务" id="toc-解析器任务" class="nav-link" data-scroll-target="#解析器任务">8.5.3 解析器任务</a></li>
  </ul></li>
  <li><a href="#结语" id="toc-结语" class="nav-link" data-scroll-target="#结语">结语</a></li>
  <li><a href="#练习题" id="toc-练习题" class="nav-link" data-scroll-target="#练习题">练习题</a>
  <ul class="collapse">
  <li><a href="#基本问题" id="toc-基本问题" class="nav-link" data-scroll-target="#基本问题">基本问题</a></li>
  <li><a href="#应用问题" id="toc-应用问题" class="nav-link" data-scroll-target="#应用问题">应用问题</a></li>
  <li><a href="#深化问题" id="toc-深化问题" class="nav-link" data-scroll-target="#深化问题">深化问题</a></li>
  </ul></li>
  <li><a href="#参考资料" id="toc-参考资料" class="nav-link" data-scroll-target="#参考资料">参考资料</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../../notebooks/zh/part_1/01_深度学习的开始.html">part_1</a></li><li class="breadcrumb-item"><a href="../../../notebooks/zh/part_1/08_变压器的诞生.html">8. 变压器的诞生</a></li></ol></nav></header>




<p><a href="https://colab.research.google.com/github/Quantum-Intelligence-Frontier/dldna/blob/main/notebooks/zh/part_1/08_变压器的诞生.ipynb" target="_parent"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="在 Colab 中打开"> </a></p>
<section id="第8章-变压器的诞生" class="level1">
<h1>第8章 变压器的诞生</h1>
<blockquote class="blockquote">
<p>“Attention is all you need.” - Ashish Vaswani et al., NeurIPS 2017.</p>
</blockquote>
<p>在自然语言处理的历史中，2017年是特别的一年。因为谷歌在其论文“Attention is All You Need”中发布了变压器（Transformer）。这可以与2012年AlexNet为计算机视觉带来的革命相提并论。变压器的出现使自然语言处理（NLP）进入了新时代。此后，基于变压器的强大语言模型如BERT和GPT相继问世，开启了人工智能的新篇章。</p>
<p><strong>注意事项</strong></p>
<p>第8章以<em>戏剧化的方式</em>重构了谷歌研究团队开发变压器的过程。基于原论文、研究博客、学术演讲资料等多种资料，本章节力图生动地描述研究人员可能面临的困惑和解决问题的过程。在此过程中，部分内容是基于合理的推理和想象力进行重构的。</p>
<section id="变压器---序列处理的革命" class="level2">
<h2 class="anchored" data-anchor-id="变压器---序列处理的革命">8.1 变压器 - 序列处理的革命</h2>
<blockquote class="blockquote">
<p><strong>挑战:</strong> 如何克服现有循环神经网络（RNN）基础模型的根本局限？</p>
<p><strong>研究者的苦恼:</strong> 当时自然语言处理领域主要使用的是以RNN、LSTM、GRU等循环神经网络为基础的模型。然而，这些模型需要按顺序处理输入序列，因此无法并行化，并且在处理长句时会出现长期依赖性问题。研究人员必须克服这些根本局限，开发出更快、更高效并且能够更好地理解长上下文的新架构。</p>
</blockquote>
<p>自然语言处理长期以来一直受制于顺序处理的局限。所谓顺序处理，是指按单词或标记单元依次处理句子。就像人们逐字阅读文章一样，RNN和LSTM也必须按顺序处理输入。这种顺序处理存在两个严重的问题：1. 无法有效利用GPU等并行处理硬件；2. 处理长句时，前面部分的信息（词语）不能充分传递到后面的部分，即所谓的“长期依赖性问题(long-range dependency problem)”，换句话说，在句子内关系相关的元素（如单词等）相距较远时无法妥善处理。</p>
<p>2014年出现的注意力机制部分解决了这些问题。传统的RNN在解码器生成输出时仅参考编码器的最后一个隐藏状态。而注意力机制使解码器能够直接参考编码器的所有中间隐藏状态。然而，仍然存在根本局限。由于RNN本身的结构基于顺序处理，因此仍需按顺序逐词处理输入。因此，无法利用GPU进行并行处理，并且在处理长序列时需要更多时间。</p>
<p>2017年，谷歌研究团队为大幅提高机器翻译的性能而开发了变压器。变压器从根本上解决了这些问题。它完全去除了RNN，仅使用自注意力（self-attention）来处理序列。</p>
<p>变压器具有以下三个核心优势： 1. 并行处理：可以同时处理序列的所有位置，从而最大限度地利用GPU。 2. 全局依赖性：所有标记都可以直接定义与其他所有标记的关联强度。 3. 位置信息的灵活处理：通过位置编码有效地表达顺序信息，同时能够灵活应对各种长度的序列。 变压器很快成为了像BERT、GPT这样的强大语言模型的基础，并扩展到了其他领域，如视觉变压器（Vision Transformer）。变压器不仅仅是一个新的架构，它还带来了对深度学习信息处理方式的根本性重新思考。特别是在计算机视觉领域，由于ViT（Vision Transformer）的成功，它已成为威胁CNN的强大竞争者。</p>
</section>
<section id="变压器的进化过程" class="level2">
<h2 class="anchored" data-anchor-id="变压器的进化过程">8.2 变压器的进化过程</h2>
<p>2017年初，谷歌研究团队在机器翻译领域遇到了难题。当时主流的基于RNN的序列到序列(seq-to-seq)模型在处理长句子时存在性能显著下降的问题。研究团队尝试从多个角度改进RNN结构，但这些只是临时解决方案，并不能从根本上解决问题。在此期间，一位研究员注意到了2014年发布的注意力机制(Bahdanau et al., 2014)。“如果注意力可以缓解长距离依赖问题，那么是否只用注意力而不用RNN也能处理序列呢？”</p>
<p>许多人在第一次接触注意力机制时都会对Q、K、V概念感到困惑。事实上，注意力的最初形式是2014年Bahdanau论文中出现的“alignment score”概念。这是在解码器生成输出词时，表示编码器应关注哪一部分的分数，本质上是<strong>两个向量之间的相关性</strong>数值。</p>
<p>研究团队可能从一个实用的问题出发：“如何量化词语间的关系？”他们从计算向量间的相似度，并将其作为权重来整合上下文信息这一相对简单的想法开始。实际上，在谷歌研究团队早期的设计文档(“Transformers: Iterative Self-Attention and Processing for Various Tasks”)中，使用了类似于“alignment score”的方式来表示词与词之间的关系，而不是Q、K、V这样的术语。</p>
<p>现在，为了理解注意力机制，让我们跟随谷歌研究人员解决问题的过程。从计算向量间相似度这一基本想法开始，逐步解释他们最终是如何完成变压器架构的。</p>
<section id="rnn的局限性和注意力的诞生" class="level3">
<h3 class="anchored" data-anchor-id="rnn的局限性和注意力的诞生">8.2.1 RNN的局限性和注意力的诞生</h3>
<p>研究团队首先试图明确RNN的局限性。通过实验，他们发现随着句子长度的增加，尤其是超过50个词时，BLEU分数急剧下降。更大的问题是由于RNN的顺序处理方式，即使使用GPU也难以实现根本性的速度提升。为克服这些限制，研究团队深入分析了Bahdanau et al.&nbsp;(2014)提出的注意力机制。注意力使解码器能够参考编码器的所有状态，从而有效缓解长距离依赖问题。以下是基本注意力机制的实现。</p>
<div id="cell-3" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip install dldna[colab] <span class="co"># in Colab</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="co"># !pip install dldna[all] # in your local</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>load_ext autoreload</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>autoreload <span class="dv">2</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="cell-4" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Example word vectors (3-dimensional)</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>word_vectors <span class="op">=</span> {</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">'time'</span>: np.array([<span class="fl">0.2</span>, <span class="fl">0.8</span>, <span class="fl">0.3</span>]),   <span class="co"># In reality, these would be hundreds of dimensions</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">'flies'</span>: np.array([<span class="fl">0.7</span>, <span class="fl">0.2</span>, <span class="fl">0.9</span>]),</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>    <span class="st">'like'</span>: np.array([<span class="fl">0.3</span>, <span class="fl">0.5</span>, <span class="fl">0.2</span>]),</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>    <span class="st">'an'</span>: np.array([<span class="fl">0.1</span>, <span class="fl">0.3</span>, <span class="fl">0.4</span>]),</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>    <span class="st">'arrow'</span>: np.array([<span class="fl">0.8</span>, <span class="fl">0.1</span>, <span class="fl">0.6</span>])</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> calculate_similarity_matrix(word_vectors):</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Calculates the similarity matrix between word vectors."""</span></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> np.vstack(<span class="bu">list</span>(word_vectors.values()))</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.dot(X, X.T)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>The autoreload extension is already loaded. To reload it, use:
  %reload_ext autoreload</code></pre>
</div>
</div>
</section>
<section id="注意的基本概念" class="level3">
<h3 class="anchored" data-anchor-id="注意的基本概念">8.2.2 注意的基本概念</h3>
<p>本节中介绍的内容来源于早期设计文档 “Transformers: Iterative Self-Attention and Processing for Various Tasks”。下面我们将逐步查看用于解释基本注意机制的代码。首先，我们只看相似度矩阵（源代码的第1、2步）。通常情况下，单词具有数百个维度。这里为了示例，使用3维向量表示。将这些向量组成一个矩阵，每个列即为一个词向量的列向量。对这个矩阵进行转置（transpose）后，就得到了一个行向量为词向量的矩阵。计算这两个矩阵的乘积时，每个元素 (i, j) 表示第 i 个单词与第 j 个单词之间的向量内积值，因此表示了两个单词之间的距离（相似度）。</p>
<div id="cell-6" class="cell" data-execution_count="8">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> visualize_similarity_matrix(words, similarity_matrix):</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Visualizes the similarity matrix in ASCII art format."""</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>    max_word_len <span class="op">=</span> <span class="bu">max</span>(<span class="bu">len</span>(word) <span class="cf">for</span> word <span class="kw">in</span> words)</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>    col_width <span class="op">=</span> max_word_len <span class="op">+</span> <span class="dv">4</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>    header <span class="op">=</span> <span class="st">" "</span> <span class="op">*</span> (col_width) <span class="op">+</span> <span class="st">""</span>.join(<span class="ss">f"</span><span class="sc">{</span>word<span class="sc">:</span><span class="op">&gt;</span>{col_width}<span class="sc">}</span><span class="ss">"</span> <span class="cf">for</span> word <span class="kw">in</span> words)</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(header)</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, word <span class="kw">in</span> <span class="bu">enumerate</span>(words):</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>        row_str <span class="op">=</span> <span class="ss">f"</span><span class="sc">{</span>word<span class="sc">:</span><span class="op">&lt;</span>{col_width}<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>        row_values <span class="op">=</span> [<span class="ss">f"</span><span class="sc">{</span>similarity_matrix[i, j]<span class="sc">:.2f}</span><span class="ss">"</span> <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(words))]</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>        row_str <span class="op">+=</span> <span class="st">""</span>.join(<span class="ss">f"[</span><span class="sc">{</span>value<span class="sc">:</span><span class="op">&gt;</span>{col_width<span class="op">-</span><span class="dv">2</span>}<span class="sc">}</span><span class="ss">]"</span> <span class="cf">for</span> value <span class="kw">in</span> row_values)</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(row_str)</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Example word vectors (in practice, these would have hundreds of dimensions)</span></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>word_vectors <span class="op">=</span> {</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>    <span class="st">'time'</span>: np.array([<span class="fl">0.2</span>, <span class="fl">0.8</span>, <span class="fl">0.3</span>]),</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>    <span class="st">'flies'</span>: np.array([<span class="fl">0.7</span>, <span class="fl">0.2</span>, <span class="fl">0.9</span>]),</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>    <span class="st">'like'</span>: np.array([<span class="fl">0.3</span>, <span class="fl">0.5</span>, <span class="fl">0.2</span>]),</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>    <span class="st">'an'</span>: np.array([<span class="fl">0.1</span>, <span class="fl">0.3</span>, <span class="fl">0.4</span>]),</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>    <span class="st">'arrow'</span>: np.array([<span class="fl">0.8</span>, <span class="fl">0.1</span>, <span class="fl">0.6</span>])</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>words <span class="op">=</span> <span class="bu">list</span>(word_vectors.keys()) <span class="co"># Preserve order</span></span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Convert word vectors into a matrix</span></span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.vstack([word_vectors[word] <span class="cf">for</span> word <span class="kw">in</span> words])</span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Calculate the similarity matrix (dot product)</span></span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>similarity_matrix <span class="op">=</span> calculate_similarity_matrix(word_vectors)</span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a><span class="co"># Print results</span></span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Input matrix shape:"</span>, X.shape)</span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Input matrix:</span><span class="ch">\n</span><span class="st">"</span>, X)</span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Input matrix transpose:</span><span class="ch">\n</span><span class="st">"</span>, X.T)</span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Similarity matrix shape:"</span>, similarity_matrix.shape)</span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Similarity matrix:"</span>) <span class="co"># Output from visualize_similarity_matrix</span></span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a>visualize_similarity_matrix(words, similarity_matrix)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Input matrix shape: (5, 3)
Input matrix:
 [[0.2 0.8 0.3]
 [0.7 0.2 0.9]
 [0.3 0.5 0.2]
 [0.1 0.3 0.4]
 [0.8 0.1 0.6]]

Input matrix transpose:
 [[0.2 0.7 0.3 0.1 0.8]
 [0.8 0.2 0.5 0.3 0.1]
 [0.3 0.9 0.2 0.4 0.6]]

Similarity matrix shape: (5, 5)
Similarity matrix:
              time    flies     like       an    arrow
time     [   0.77][   0.57][   0.52][   0.38][   0.42]
flies    [   0.57][   1.34][   0.49][   0.49][   1.12]
like     [   0.52][   0.49][   0.38][   0.26][   0.41]
an       [   0.38][   0.49][   0.26][   0.26][   0.35]
arrow    [   0.42][   1.12][   0.41][   0.35][   1.01]</code></pre>
</div>
</div>
<p>例如，<strong>相似矩阵的 (1,2) 元素值 0.57 表示行轴 times 和列轴 flies 的向量距离（相似度）</strong>。用数学表示如下。</p>
<ul>
<li>句子中词向量的矩阵 X</li>
</ul>
<p><span class="math inline">\(\mathbf{X} = \begin{bmatrix}
\mathbf{x_1} \\
\mathbf{x_2} \\
\vdots \\
\mathbf{x_n}
\end{bmatrix}\)</span></p>
<ul>
<li>X 的转置矩阵</li>
</ul>
<p><span class="math inline">\(\mathbf{X}^T = \begin{bmatrix}
\mathbf{x_1}^T &amp; \mathbf{x_2}^T &amp; \cdots &amp; \mathbf{x_n}^T
\end{bmatrix}\)</span></p>
<ul>
<li><span class="math inline">\(\mathbf{X}\mathbf{X}^T\)</span> 运算</li>
</ul>
<p><span class="math inline">\(\mathbf{X}\mathbf{X}^T = \begin{bmatrix}
\mathbf{x_1} \cdot \mathbf{x_1} &amp; \mathbf{x_1} \cdot \mathbf{x_2} &amp; \cdots &amp; \mathbf{x_1} \cdot \mathbf{x_n} \\
\mathbf{x_2} \cdot \mathbf{x_1} &amp; \mathbf{x_2} \cdot \mathbf{x_2} &amp; \cdots &amp; \mathbf{x_2} \cdot \mathbf{x_n} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\mathbf{x_n} \cdot \mathbf{x_1} &amp; \mathbf{x_n} \cdot \mathbf{x_2} &amp; \cdots &amp; \mathbf{x_n} \cdot \mathbf{x_n}
\end{bmatrix}\)</span></p>
<ul>
<li>每个元素 (i,j)</li>
</ul>
<p><span class="math inline">\((\mathbf{X}\mathbf{X}^T)_{ij} = \mathbf{x_i} \cdot \mathbf{x_j} = \sum_{k=1}^d x_{ik}x_{jk}\)</span></p>
<p>这个 n×n 矩阵的每个元素是两个词向量之间的点积，因此表示两词的距离（相似度）。这就是“注意力得分”。</p>
<p>以下是将相似度矩阵通过软最大值函数转换为权重矩阵的三个步骤。</p>
<div id="cell-9" class="cell" data-execution_count="9">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. Convert similarities to weights (probability distribution) (softmax)</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> softmax(x):</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>    exp_x <span class="op">=</span> np.exp(x <span class="op">-</span> np.<span class="bu">max</span>(x, axis<span class="op">=-</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>))  <span class="co"># trick for stability</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> exp_x <span class="op">/</span> exp_x.<span class="bu">sum</span>(axis<span class="op">=-</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>attention_weights <span class="op">=</span> softmax(similarity_matrix)</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Attention weights shape:"</span>, attention_weights.shape)</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Attention weights:</span><span class="ch">\n</span><span class="st">"</span>, attention_weights)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Attention weights shape: (5, 5)
Attention weights:
 [[0.25130196 0.20574865 0.19571417 0.17014572 0.1770895 ]
 [0.14838442 0.32047566 0.13697608 0.13697608 0.25718775]
 [0.22189237 0.21533446 0.19290396 0.17109046 0.19877876]
 [0.20573742 0.22966017 0.18247272 0.18247272 0.19965696]
 [0.14836389 0.29876818 0.14688764 0.13833357 0.26764673]]</code></pre>
</div>
</div>
<p>注意力权重应用了软最大值(softmax)函数。它执行两种关键转换：</p>
<ol type="1">
<li>将相似度分数转换为0到1之间的值。</li>
<li>使每行的和为1，从而转换为概率分布。</li>
</ol>
<p>将相似度矩阵转换为权重后，可以以概率形式表示单词与其他单词的相关性。由于行、列轴都代表句子中词的顺序，因此权重的第一行是’时间’这个词所在的行，而列则是所有句子中的词。因此，</p>
<ol type="1">
<li>所有其他词(‘time’, ‘flies’, ‘like’, ‘an’, ‘arrow’)之间的关系以概率值表示。</li>
<li>这些概率值的总和为1。</li>
<li>较高的概率值意味着更强的相关性。</li>
</ol>
<p>这样转换后的权重在下一步中用作乘以句子的比例。应用这个比例后，每个词都显示了其反映信息的程度。这相当于决定了每个词在“参考”其他词的信息时应给予多少关注。</p>
<div id="cell-11" class="cell" data-execution_count="17">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 4. Generate contextualized representations using the weights</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>contextualized_vectors <span class="op">=</span> np.dot(attention_weights, X)</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Contextualized vectors shape:"</span>, contextualized_vectors.shape)</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Contextualized vectors:</span><span class="ch">\n</span><span class="st">"</span>, contextualized_vectors)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
Contextualized vectors shape: (5, 3)
Contextualized vectors:
 [[0.41168487 0.40880105 0.47401919]
 [0.51455048 0.31810231 0.56944172]
 [0.42911583 0.38823778 0.48665295]
 [0.43462426 0.37646585 0.49769319]
 [0.51082753 0.32015331 0.55869952]]</code></pre>
</div>
</div>
<p>加权矩阵和词矩阵（由词行向量组成）的点积需要解释。假设 <code>attention_weights</code> 的第一行为 [0.5, 0.2, 0.1, 0.1, 0.1]，则每个值表示 ‘time’ 与其他单词之间的相关性的概率。如果将第一个加权行表示为 <span class="math inline">\(\begin{bmatrix} \alpha_{11} &amp; \alpha_{12} &amp; \alpha_{13} &amp; \alpha_{14} &amp; \alpha_{15} \end{bmatrix}\)</span>，那么对于这个加权第一行的词矩阵运算可以表示如下。</p>
<p><span class="math inline">\(\begin{bmatrix} \alpha_{11} &amp; \alpha_{12} &amp; \alpha_{13} &amp; \alpha_{14} &amp; \alpha_{15} \end{bmatrix} \begin{bmatrix} \vec{v}_{\text{time}} \ \vec{v}_{\text{flies}} \ \vec{v}_{\text{like}} \ \vec{v}_{\text{an}} \ \vec{v}_{\text{arrow}} \end{bmatrix}\)</span></p>
<p>这在 Python 代码中表示如下。</p>
<div id="cell-13" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>time_contextualized <span class="op">=</span> <span class="fl">0.5</span><span class="op">*</span>time_vector <span class="op">+</span> <span class="fl">0.2</span><span class="op">*</span>flies_vector <span class="op">+</span> <span class="fl">0.1</span><span class="op">*</span>like_vector <span class="op">+</span> <span class="fl">0.1</span><span class="op">*</span>an_vector <span class="op">+</span> <span class="fl">0.1</span><span class="op">*</span>arrow_vector</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="co"># 0.5는 time과 time의 관련도 확률값</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="co"># 0.2는 time과 files의 관련도 확률값</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>运算将这些概率（时间与每个单词相关联的概率值）乘以每个单词的原始向量并全部相加。结果，<strong>’time’的新向量是其他单词意义的加权平均，反映了它们的相关程度</strong>。关键是求加权平均值。因此，为了获得加权平均值，需要先计算权重矩阵。</p>
<p>最终上下文化后的向量的形状为 (5, 3)，这是因为 (5,5) 大小的注意力权重矩阵与 (5,3) 的单词向量矩阵 X 相乘的结果 (5,5) @ (5,3) = (5,3)。</p>
<p>翻译后的文本：</p>
</section>
<section id="向自注意力的演变" class="level3">
<h3 class="anchored">8.2.3 向自注意力的演变</h3>
<p>谷歌研究团队在分析基本注意力机制（第8.2.2节）时发现了一些<strong>局限性</strong>。最大的问题是，词向量同时承担着<strong>相似度计算</strong>和<strong>信息传递</strong>等多种角色是低效的。例如，“bank”这个词根据上下文可以表示“银行”或“河岸”等<em>不同含义</em>，因此与<em>其他词语的关系</em>也应有所不同。但是，用<strong>一个向量</strong>很难表达这些不同的含义和关系。</p>
<p>研究团队寻求了使每个角色能够<strong>独立优化</strong>的方法。这类似于CNN中滤波器以<em>可学习的形式发展</em>来提取图像特征，在注意力机制中则是设计使其可以<em>学习</em>针对每个角色的专门表示。这一想法从将词向量转换为用于不同角色的空间开始。</p>
<p><strong>基本概念的局限性（代码示例）</strong></p>
<div id="cell-17" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> basic_self_attention(word_vectors):</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>    similarity_matrix <span class="op">=</span> np.dot(word_vectors, word_vectors.T)</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>    attention_weights <span class="op">=</span> softmax(similarity_matrix)</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>    contextualized_vectors <span class="op">=</span> np.dot(attention_weights, word_vectors)</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> contextualized_vectors</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>在上面的代码中，<code>word_vectors</code>同时执行了三种角色。</p>
<ol type="1">
<li><strong>相似度计算的主体：</strong> 用于计算与其他词的相似度。</li>
<li><strong>相似度计算的对象：</strong> 被其他词用来计算相似度。</li>
<li><strong>信息传递：</strong> 在生成最终上下文向量时用作加权平均。</li>
</ol>
<p><strong>第一次改进：分离信息传递角色</strong></p>
<p>研究团队首先分离了<strong>信息传递角色</strong>。在线性代数中，分离向量角色的最简单方法是使用<em>单独的学习矩阵</em>将向量进行<em>线性变换(linear transformation)</em>到新的空间。</p>
<div id="cell-19" class="cell" data-execution_count="10">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> improved_self_attention(word_vectors, W_similarity, W_content):</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>    similarity_vectors <span class="op">=</span> np.dot(word_vectors, W_similarity)</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>    content_vectors <span class="op">=</span> np.dot(word_vectors, W_content)</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate similarity by taking the dot product between similarity_vectors</span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>    attention_scores <span class="op">=</span> np.dot(similarity_vectors, similarity_vectors.T)</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Convert to probability distribution using softmax</span></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>    attention_weights <span class="op">=</span> softmax(attention_scores)</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Generate the final contextualized representation by multiplying weights and content_vectors</span></span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>    contextualized_vectors <span class="op">=</span> np.dot(attention_weights, content_vectors)</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> contextualized_vectors</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<ul>
<li><code>W_similarity</code>: 将词向量投影到最适合相似度计算的空间的<em>可学习</em>矩阵。</li>
<li><code>W_content</code>: 将词向量投影到最适合信息传递的空间的<em>可学习</em>矩阵。</li>
</ul>
<p>通过这一改进，<code>similarity_vectors</code>可以专门用于相似度计算，而<code>content_vectors</code>则专注于信息传递。这成为了通过Value进行信息聚合概念的前身。</p>
<p><strong>第二次改进：完全分离相似度角色（Q、K的诞生）</strong></p>
<p>下一步是将相似度计算过程本身分为两个角色。不再让<code>similarity_vectors</code>同时承担“提问角色”(Query)和“回答角色”(Key)，而是向<em>完全分离</em>这两个角色的方向发展。</p>
<div id="cell-21" class="cell" data-execution_count="11">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SelfAttention(nn.Module):</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, embed_dim):</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 각각의 역할을 위한 독립적인 선형 변환</span></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.q <span class="op">=</span> nn.Linear(embed_dim, embed_dim)  <span class="co"># 질문(Query)을 위한 변환</span></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.k <span class="op">=</span> nn.Linear(embed_dim, embed_dim)  <span class="co"># 답변(Key)을 위한 변환</span></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.v <span class="op">=</span> nn.Linear(embed_dim, embed_dim)  <span class="co"># 정보 전달(Value)을 위한 변환</span></span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>        Q <span class="op">=</span> <span class="va">self</span>.q(x)  <span class="co"># 질문자로서의 표현</span></span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>        K <span class="op">=</span> <span class="va">self</span>.k(x)  <span class="co"># 응답자로서의 표현</span></span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>        V <span class="op">=</span> <span class="va">self</span>.v(x)  <span class="co"># 전달할 정보의 표현</span></span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 질문과 답변 간의 관련성(유사도) 계산</span></span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a>        scores <span class="op">=</span> torch.matmul(Q, K.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>))</span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a>        weights <span class="op">=</span> F.softmax(scores, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 관련성에 따른 정보 집계 (가중 평균)</span></span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.matmul(weights, V)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><strong>Q, K, V 空间分离的意义</strong></p>
<p>交换 Q 和 K 的顺序（使用 <span class="math inline">\(QK^T\)</span> 而不是 <span class="math inline">\(KQ^T\)</span>）在数学上会产生相同的相似度矩阵。从纯数学角度看，尽管这两者是相同的，但为什么将 Q、K 分别命名为“查询(Query)”和“键(Key)”呢？关键在于<em>为了更好地计算相似度而优化为独立的空间</em>。这种命名似乎是因为变压器模型的注意力机制受到了信息检索(Information Retrieval)系统的启发。在检索系统中，“查询(Query)”代表用户想要的信息，而“键(Key)”则类似于每个文档的索引词。注意力通过计算查询和键之间的相似度来模仿查找相关信息的过程。</p>
<p>例如</p>
<ul>
<li>“I need to deposit money in the bank”（银行）</li>
<li>“The river bank is covered with flowers”（河岸）</li>
</ul>
<p>上述两个句子中的 “bank” 根据上下文有不同的含义。通过 Q, K 空间分离，</p>
<ul>
<li>“bank” 和其他词语在 Q、K 空间中以 <em>不同的方式</em> 排布，从而优化了相似度计算。</li>
<li>在金融相关背景下，空间中的向量被排列得使 ‘money’, ‘deposit’ 等词的相似度更高。</li>
<li>在地形相关的背景下，则使 ‘river’, ‘covered’ 等词的相似度更高。</li>
</ul>
<p>也就是说，Q-K 对在 <em>两个优化的空间</em> 中进行点积计算以确定相似度。重要的是 Q, K 空间是 <em>通过学习</em> 得到优化的。谷歌研究团队很可能发现了在学习过程中 Q 和 K 矩阵确实像查询和键那样运作并得到优化的现象。</p>
<p><strong>Q, K 空间分离的重要性</strong></p>
<p>另一个通过分离 Q 和 K 获得的优势是 <em>灵活性增强</em>。如果将 Q 和 K 放在同一空间中，相似度计算方式可能会受到限制（例如：对称性相似度）。但是，如果分离 Q、K，则可以学习更复杂和非对称的关系（例如：“A 是 B 的原因”）。此外，通过不同的变换 (<span class="math inline">\(W^Q\)</span>, <span class="math inline">\(W^K\)</span>)，Q 和 K 可以更细致地表达每个词的作用，从而增强模型的表达能力。最后，通过分离 Q 和 K 空间，各空间的优化目标更加明确，使得 Q 空间适合于学习适合提问的表示，K 空间则负责学习适合回答的表示。</p>
<p><strong>Value 的作用</strong></p>
<p>如果说 Q, K 是用于相似度计算的空间，那么 V 则是 <em>实际传递信息</em> 的空间。到 V 空间的变换被优化为最能表达词语意义的方向。Q, K 决定了“反映哪些词的信息程度”，而 V 负责决定“实际上要传递什么样的信息”。以上 “bank” 为例，</p>
<ul>
<li>Q, K 根据上下文计算与金融相关词汇的相似度，</li>
<li>V 则表达了作为金融机构的 ‘bank’ 的实际意义。</li>
</ul>
<p>这三种空间的分离使得“如何查找信息 (Q, K)”和“要传递的信息内容 (V)”可以独立地进行优化，类似于在 CNN 中将 “学习识别哪些模式（滤波器学习）”与“如何表达找到的模式（通道学习）”分开来处理。</p>
<p><strong>注意力的数学表示</strong></p>
<p>最终的注意力机制可以用以下公式表示：</p>
<p><span class="math display">\[\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V\]</span> * <span class="math inline">\(Q \in \mathbb{R}^{n \times d_k}\)</span>: 查询矩阵 * <span class="math inline">\(K \in \mathbb{R}^{n \times d_k}\)</span>: 键矩阵 * <span class="math inline">\(V \in \mathbb{R}^{n \times d_v}\)</span>: 值矩阵 (<span class="math inline">\(d_v\)</span>通常与<span class="math inline">\(d_k\)</span>相同) * <span class="math inline">\(n\)</span>: 序列长度 * <span class="math inline">\(d_k\)</span>: 查询，键向量的维度 * <span class="math inline">\(d_v\)</span>: 值向量的维度 * <span class="math inline">\(\frac{QK^T}{\sqrt{d_k}}\)</span>: 缩放点积注意力。随着维度增大，内积值也会增大，通过softmax函数时会防止梯度消失。</p>
<p>这种进化的结构成为变压器的核心要素，并成为了之后BERT、GPT等现代语言模型的基础。</p>
<div class="callout callout-style-default callout-note callout-titled" title="点击查看内容（理论深入探讨：自注意力机制的综合理解与最新理论）">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
点击查看内容（理论深入探讨：自注意力机制的综合理解与最新理论）
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<section id="自注意力机制的综合理解和最新理论" class="level2 callout-body-container callout-body">
<h2 class="anchored" data-anchor-id="自注意力机制的综合理解和最新理论">自注意力机制的综合理解和最新理论</h2>
<section id="数学原理及计算复杂度" class="level3">
<h3 class="anchored" data-anchor-id="数学原理及计算复杂度">1. 数学原理及计算复杂度</h3>
<p>自注意力通过计算输入序列中每个词与包括自身在内的其他所有词的关系，生成反映上下文的新表示。这个过程大致分为三个阶段。</p>
<ol type="1">
<li><p><strong>Query, Key, Value 生成:</strong></p>
<p>对于输入序列中的每个词嵌入向量(<span class="math inline">\(x_i\)</span>)，应用三种线性变换来生成 Query (<span class="math inline">\(q_i\)</span>), Key (<span class="math inline">\(k_i\)</span>), Value (<span class="math inline">\(v_i\)</span>) 向量。这些变换使用可学习的权重矩阵(<span class="math inline">\(W^Q\)</span>,<span class="math inline">\(W^K\)</span>,<span class="math inline">\(W^V\)</span>)执行。</p>
<p><span class="math inline">\(q_i = x_i W^Q\)</span></p>
<p><span class="math inline">\(k_i = x_i W^K\)</span></p>
<p><span class="math inline">\(v_i = x_i W^V\)</span></p>
<p><span class="math inline">\(W^Q, W^K, W^V \in \mathbb{R}^{d_{model} \times d_k}\)</span> : 可学习的权重矩阵。(<span class="math inline">\(d_{model}\)</span>: 嵌入维度,<span class="math inline">\(d_k\)</span>: query, key, value 向量的维度)</p></li>
<li><p><strong>注意力分数计算及归一化</strong></p>
<p>对于每对词，计算 Query 和 Key 向量的点积(dot product)来获得注意力分数(attention score)。</p>
<p><span class="math display">\[\text{score}(q_i, k_j) = q_i \cdot k_j^T\]</span></p>
<p>这个分数表示两个词的相关程度。在进行点积运算后，会执行缩放(scaling)，以防止点积值变得过大而缓解梯度消失(gradient vanishing)问题。缩放是通过除以 Key 向量维度(<span class="math inline">\(d_k\)</span>)的平方根来实现。</p>
<p><span class="math display">\[\text{scaled score}(q_i, k_j) = \frac{q_i \cdot k_j^T}{\sqrt{d_k}}\]</span></p>
<p>最后，应用 Softmax 函数对注意力分数进行归一化，并获得每个词的注意力权重(attention weight)。</p>
<p><span class="math display">\[\alpha_{ij} = \text{softmax}(\text{scaled score}(q_i, k_j)) = \frac{\exp(\text{scaled score}(q_i, k_j))}{\sum_{l=1}^{n} \exp(\text{scaled score}(q_i, k_l))}\]</span></p>
<p>其中<span class="math inline">\(\alpha_{ij}\)</span>是第<span class="math inline">\(i\)</span>个词对第<span class="math inline">\(j\)</span>个词的注意力权重，<span class="math inline">\(n\)</span>是序列长度。</p></li>
<li><p><strong>加权平均计算</strong></p>
<p>使用注意力权重(<span class="math inline">\(\alpha_{ij}\)</span>)来计算 Value 向量(<span class="math inline">\(v_j\)</span>)的加权平均(weighted average)。这个加权平均成为综合了输入序列所有词信息的上下文向量(context vector)<span class="math inline">\(c_i\)</span>。</p></li>
</ol>
<p><span class="math display">\[c_i = \sum_{j=1}^{n} \alpha_{ij} v_j\]</span></p>
<p><strong>整个过程以矩阵形式表示</strong></p>
<p>设输入嵌入矩阵为<span class="math inline">\(X \in \mathbb{R}^{n \times d_{model}}\)</span>，则整个自注意力过程可以如下表示。</p>
<p><span class="math display">\[\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V\]</span></p>
<p>其中<span class="math inline">\(Q = XW^Q\)</span>,<span class="math inline">\(K = XW^K\)</span>,<span class="math inline">\(V = XW^V\)</span>。</p>
<p><strong>计算复杂度</strong></p>
<p>自注意力的计算复杂度为输入序列长度(<span class="math inline">\(n\)</span>)的<span class="math inline">\(O(n^2)\)</span>。这是因为每个词都要与其他所有词的关系进行计算。 * <strong><span class="math inline">\(QK^T\)</span>计算:</strong> 对<span class="math inline">\(n\)</span>个查询向量和<span class="math inline">\(n\)</span>个键向量进行点积运算，因此需要<span class="math inline">\(O(n^2d_k)\)</span>的计算。 * <strong>softmax运算:</strong> 为了计算每个查询的注意力权重，对<span class="math inline">\(n\)</span>个键执行softmax运算，因此具有<span class="math inline">\(O(n^2)\)</span>的计算复杂度。 * <strong>与<span class="math inline">\(V\)</span>的加权平均:</strong> 需要将<span class="math inline">\(n\)</span>个值向量和<span class="math inline">\(n\)</span>个注意力权重相乘，因此具有<span class="math inline">\(O(n^2d_k)\)</span>的计算复杂度。</p>
</section>
<section id="核方法视角下的扩展" class="level3">
<h3 class="anchored" data-anchor-id="核方法视角下的扩展">2. 核方法视角下的扩展</h3>
<section id="非对称核函数" class="level4">
<h4 class="anchored" data-anchor-id="非对称核函数">2.1 非对称核函数</h4>
<p>将注意力解释为非对称核函数 <span class="math inline">\(K(Q_i, K_j) = \exp\left(\frac{Q_i \cdot K_j}{\sqrt{d_k}}\right)\)</span></p>
<p>该内核学习了重构输入空间的特征映射。</p>
</section>
<section id="特征值分解svd分析" class="level4">
<h4 class="anchored" data-anchor-id="特征值分解svd分析">2.2 特征值分解(SVD)分析</h4>
<p>注意力矩阵的非对称KSVD</p>
<p><span class="math inline">\(A = U\Sigma V^T \quad \text{where } \Sigma = \text{diag}(\sigma_1, \sigma_2, ...)\)</span></p>
<p>-<span class="math inline">\(U\)</span>: 查询空间的主要方向 (上下文请求模式) -<span class="math inline">\(V\)</span>: 键空间的主要方向 (信息提供模式) -<span class="math inline">\(\sigma_i\)</span>: 交互强度 (观察到≥0.9的解释力集中现象)</p>
</section>
</section>
<section id="基于能量模型和动力学" class="level3">
<h3 class="anchored" data-anchor-id="基于能量模型和动力学">3. 基于能量模型和动力学</h3>
<section id="能量函数公式化" class="level4">
<h4 class="anchored" data-anchor-id="能量函数公式化">3.1 能量函数公式化</h4>
<p><span class="math inline">\(E(Q,K,V) = -\sum_{i,j} \frac{Q_i \cdot K_j}{\sqrt{d_k}}V_j + \text{log-partition function}\)</span></p>
<p>输出可以解释为能量最小化过程</p>
<p><span class="math inline">\(\text{Output} = \arg\min_V E(Q,K,V)\)</span></p>
</section>
<section id="与hopfield网络的等价性" class="level4">
<h4 class="anchored" data-anchor-id="与hopfield网络的等价性">3.2 与Hopfield网络的等价性</h4>
<p>连续型Hopfield网络方程 <span class="math inline">\(\tau\frac{dX}{dt} = -X + \text{softmax}(XWX^T)XW\)</span></p>
<p>其中，<span class="math inline">\(\tau\)</span>是时间常数，<span class="math inline">\(W\)</span>是学习到的连接强度矩阵。</p>
</section>
</section>
<section id="低维结构和优化" class="level3">
<h3 class="anchored" data-anchor-id="低维结构和优化">4. 低维结构和优化</h3>
<section id="秩崩溃现象" class="level4">
<h4 class="anchored" data-anchor-id="秩崩溃现象">4.1 秩崩溃现象</h4>
<p>在深层中 <span class="math inline">\(\text{rank}(A) \leq \lfloor0.1n\rfloor\)</span>(实验观察)</p>
<p>这表示信息的有效压缩。</p>
</section>
<section id="高效注意力技术比较" class="level4">
<h4 class="anchored" data-anchor-id="高效注意力技术比较">4.2 高效注意力技术比较</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>技术</th>
<th>原理</th>
<th>复杂度</th>
<th>应用实例</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Linformer</td>
<td>低秩近似</td>
<td><span class="math inline">\(O(n d)\)</span></td>
<td>文本处理</td>
</tr>
<tr class="even">
<td>Performer</td>
<td>随机特征映射</td>
<td><span class="math inline">\(O(n d)\)</span></td>
<td>自然语言理解</td>
</tr>
<tr class="odd">
<td>Reformer</td>
<td>基于哈希的注意力机制</td>
<td><span class="math inline">\(O(n \log n)\)</span></td>
<td>大规模序列模型</td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="动力学分析" class="level3">
<h3 class="anchored" data-anchor-id="动力学分析">5. 动力学分析</h3>
<section id="李雅普诺夫稳定性" class="level4">
<h4 class="anchored" data-anchor-id="李雅普诺夫稳定性">5.1 李雅普诺夫稳定性</h4>
<p><span class="math inline">\(V(X) = \|X - X^*\|^2\)</span>下降函数</p>
<p>注意力更新保证了渐近稳定性。</p>
</section>
<section id="频域解释" class="level4">
<h4 class="anchored" data-anchor-id="频域解释">5.2 频域解释</h4>
<p>应用傅里叶变换后的注意力谱</p>
<p><span class="math inline">\(\mathcal{F}(A)_{kl} = \sum_{m,n} A_{mn}e^{-i2\pi(mk/M+nl/N)}\)</span></p>
<p>低频分量捕获了信息的80%以上。</p>
</section>
</section>
<section id="信息论解释" class="level3">
<h3 class="anchored" data-anchor-id="信息论解释">6. 信息论解释</h3>
<section id="互信息最大化" class="level4">
<h4 class="anchored" data-anchor-id="互信息最大化">6.1 互信息最大化</h4>
<p><span class="math inline">\(\max I(X;Y) = H(Y) - H(Y|X) \quad \text{s.t. } Y = \text{Attention}(X)\)</span></p>
<p>softmax生成使熵<span class="math inline">\(H(Y)\)</span>最大化的最优分布。</p>
</section>
<section id="信噪比snr分析" class="level4">
<h4 class="anchored" data-anchor-id="信噪比snr分析">6.2 信噪比(SNR)分析</h4>
<p>随层深<span class="math inline">\(l\)</span>的SNR衰减</p>
<p><span class="math inline">\(\text{SNR}^{(l)} \propto e^{-0.2l} \quad \text{(以ResNet-50为例)}\)</span></p>
</section>
</section>
<section id="神经科学启发" class="level3">
<h3 class="anchored" data-anchor-id="神经科学启发">7. 神经科学启发</h3>
<section id="视觉皮层v4区域" class="level4">
<h4 class="anchored" data-anchor-id="视觉皮层v4区域">7.1 视觉皮层V4区域</h4>
<ul>
<li>方向选择性神经元 ≈ 对特定模式作出反应的注意力头</li>
<li>接受野层次结构 ≈ 多尺度注意力机制</li>
</ul>
</section>
<section id="前额叶工作记忆" class="level4">
<h4 class="anchored" data-anchor-id="前额叶工作记忆">7.2 前额叶工作记忆</h4>
<ul>
<li>持续神经元激活 ≈ 注意力处理长期依赖性的方法</li>
<li>上下文保持机制 ≈ 解码器中的掩码技术</li>
</ul>
</section>
</section>
<section id="高级数学建模" class="level3">
<h3 class="anchored" data-anchor-id="高级数学建模">8. 高级数学建模</h3>
<section id="张量网络扩展" class="level4">
<h4 class="anchored" data-anchor-id="张量网络扩展">8.1 张量网络扩展</h4>
<p>MPO（矩阵乘积算子）表示</p>
<p><span class="math inline">\(A_{ij} = \sum_{\alpha=1}^r Q_{i\alpha}K_{j\alpha}\)</span> 其中<span class="math inline">\(r\)</span>是张量网络的键维度</p>
</section>
<section id="微分几何解析" class="level4">
<h4 class="anchored" data-anchor-id="微分几何解析">8.2 微分几何解析</h4>
<p>注意力流形的黎曼曲率 <span class="math inline">\(R_{ijkl} = \partial_i\Gamma_{jk}^m - \partial_j\Gamma_{ik}^m + \Gamma_{il}^m\Gamma_{jk}^l - \Gamma_{jl}^m\Gamma_{ik}^l\)</span></p>
<p>通过曲率分析可以估计模型的表达能力限制</p>
</section>
</section>
<section id="最新研究趋势2025" class="level3">
<h3 class="anchored" data-anchor-id="最新研究趋势2025">9. 最新研究趋势（2025）</h3>
<ol type="1">
<li><p><strong>量子注意力</strong></p>
<ul>
<li>将查询/键表示为量子叠加态：<span class="math inline">\(|\psi_Q\rangle = \sum c_i|i\rangle\)</span></li>
<li>加速量子内积运算</li>
</ul></li>
<li><p><strong>仿生优化</strong></p>
<ul>
<li>应用尖峰时间依赖性可塑性（STDP）</li>
</ul>
<p><span class="math inline">\(\Delta W_{ij} \propto x_i x_j - \beta W_{ij}\)</span></p></li>
<li><p><strong>动态能量调整</strong></p>
<ul>
<li>基于元学习的实时能量函数调优<br>
</li>
<li>与物理引擎联动模拟</li>
</ul></li>
</ol>
<hr>
</section>
<section id="参考文献" class="level3">
<h3 class="anchored" data-anchor-id="参考文献">参考文献</h3>
<ol type="1">
<li>Vaswani et al., “Attention Is All You Need”, NeurIPS 2017<br>
</li>
<li>Choromanski et al., “Rethinking Attention with Performers”, ICLR 2021<br>
</li>
<li>Ramsauer et al., “Hopfield Networks is All You Need”, ICLR 2021<br>
</li>
<li>Wang et al., “Linformer: Self-Attention with Linear Complexity”, arXiv 2020<br>
</li>
<li>Chen et al., “Theoretical Analysis of Self-Attention via Signal Propagation”, NeurIPS 2023</li>
</ol>
</section>
</section>
</div>
</div>
</section>
<section id="多头注意力与并行处理" class="level3">
<h3 class="anchored" data-anchor-id="多头注意力与并行处理">8.2.4 多头注意力与并行处理</h3>
<p>谷歌研究团队为了进一步提高自注意力的性能，提出了一个想法：“与其使用一个大的注意力空间，不如在<em>多个较小的注意力空间</em>中捕捉不同类型的关联会怎样？”他们认为，就像多名专家从各自的视角分析问题一样，如果能够同时考虑输入序列的不同方面，则可以获得更丰富的上下文信息。</p>
<p>基于这一想法，研究团队设计了将Q、K、V向量分割成多个小空间并行计算注意力的<strong>多头注意力（Multi-Head Attention）</strong>。在原论文“Attention is All You Need”中，512维的嵌入被分成8个64维的头部（head）进行处理。之后，像BERT这样的模型进一步扩展了这一结构（例如：BERT-base将768维分割成12个64维的头部）。</p>
<p><strong>多头注意力的工作原理</strong></p>
<div id="cell-25" class="cell" data-execution_count="12">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> math</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MultiHeadAttention(nn.Module):</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config):</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span> config.hidden_size <span class="op">%</span> config.num_attention_heads <span class="op">==</span> <span class="dv">0</span></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.d_k <span class="op">=</span> config.hidden_size <span class="op">//</span> config.num_attention_heads  <span class="co"># Dimension of each head</span></span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.h <span class="op">=</span> config.num_attention_heads  <span class="co"># Number of heads</span></span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Linear transformation layers for Q, K, V, and output</span></span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.linear_layers <span class="op">=</span> nn.ModuleList([</span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>            nn.Linear(config.hidden_size, config.hidden_size)</span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">4</span>)  <span class="co"># For Q, K, V, and output</span></span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a>        ])</span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(config.attention_probs_dropout_prob) <span class="co"># added</span></span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.attention_weights <span class="op">=</span> <span class="va">None</span> <span class="co"># added</span></span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> attention(<span class="va">self</span>, query, key, value, mask<span class="op">=</span><span class="va">None</span>): <span class="co"># separate function</span></span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a>        scores <span class="op">=</span> torch.matmul(query, key.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>)) <span class="op">/</span> math.sqrt(<span class="va">self</span>.d_k) <span class="co"># scaled dot product</span></span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-25"><a href="#cb14-25" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> mask <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb14-26"><a href="#cb14-26" aria-hidden="true" tabindex="-1"></a>            scores <span class="op">=</span> scores.masked_fill(mask <span class="op">==</span> <span class="dv">0</span>, <span class="op">-</span><span class="fl">1e9</span>)</span>
<span id="cb14-27"><a href="#cb14-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-28"><a href="#cb14-28" aria-hidden="true" tabindex="-1"></a>        p_attn <span class="op">=</span> scores.softmax(dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb14-29"><a href="#cb14-29" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.attention_weights <span class="op">=</span> p_attn.detach()  <span class="co"># Store attention weights</span></span>
<span id="cb14-30"><a href="#cb14-30" aria-hidden="true" tabindex="-1"></a>        p_attn <span class="op">=</span> <span class="va">self</span>.dropout(p_attn)</span>
<span id="cb14-31"><a href="#cb14-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-32"><a href="#cb14-32" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.matmul(p_attn, value), p_attn</span>
<span id="cb14-33"><a href="#cb14-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-34"><a href="#cb14-34" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, query, key, value, mask<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb14-35"><a href="#cb14-35" aria-hidden="true" tabindex="-1"></a>        batch_size <span class="op">=</span> query.size(<span class="dv">0</span>)</span>
<span id="cb14-36"><a href="#cb14-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-37"><a href="#cb14-37" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 1) Linear projections in batch from d_model =&gt; h x d_k</span></span>
<span id="cb14-38"><a href="#cb14-38" aria-hidden="true" tabindex="-1"></a>        query, key, value <span class="op">=</span> [l(x).view(batch_size, <span class="op">-</span><span class="dv">1</span>, <span class="va">self</span>.h, <span class="va">self</span>.d_k).transpose(<span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb14-39"><a href="#cb14-39" aria-hidden="true" tabindex="-1"></a>                             <span class="cf">for</span> l, x <span class="kw">in</span> <span class="bu">zip</span>(<span class="va">self</span>.linear_layers, (query, key, value))]</span>
<span id="cb14-40"><a href="#cb14-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-41"><a href="#cb14-41" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 2) Apply attention on all the projected vectors in batch.</span></span>
<span id="cb14-42"><a href="#cb14-42" aria-hidden="true" tabindex="-1"></a>        x, attn <span class="op">=</span> <span class="va">self</span>.attention(query, key, value, mask<span class="op">=</span>mask)</span>
<span id="cb14-43"><a href="#cb14-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-44"><a href="#cb14-44" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 3) "Concat" using a view and apply a final linear.</span></span>
<span id="cb14-45"><a href="#cb14-45" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.transpose(<span class="dv">1</span>, <span class="dv">2</span>).contiguous().view(batch_size, <span class="op">-</span><span class="dv">1</span>, <span class="va">self</span>.h <span class="op">*</span> <span class="va">self</span>.d_k)</span>
<span id="cb14-46"><a href="#cb14-46" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.linear_layers[<span class="op">-</span><span class="dv">1</span>](x)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="多头注意力multi-head-attention详细分析" class="level3">
<h3 class="anchored" data-anchor-id="多头注意力multi-head-attention详细分析">多头注意力（Multi-Head Attention）详细分析</h3>
<p><strong>代码结构 (<code>__init__</code> 和 <code>forward</code>)</strong></p>
<p>多头注意力的代码主要由初始化(<code>__init__</code>)和前向传播(<code>forward</code>)方法组成。我们将仔细研究每个方法的作用及其详细的运行方式。</p>
<ul>
<li><strong><code>__init__</code> 方法</strong>:
<ul>
<li><code>d_k</code>: 表示每个注意力头的维度。这个值是模型隐藏大小除以头数(num_attention_heads)的结果，决定了每个头处理的信息量。</li>
<li><code>h</code>: 设置注意力头的数量。这个值是一个超参数，决定了模型从多少个不同的视角看待输入。</li>
<li><code>linear_layers</code>: 生成用于查询(Q), 键(K), 值(V)和最终输出的四个线性变换层。这些层将输入转换为适合每个头的形式，并在最后整合各头的结果。</li>
</ul></li>
<li><strong><code>forward</code> 方法</strong>:
<ol type="1">
<li><strong>线性变换及分割</strong>:
<ul>
<li>使用 <code>self.linear_layers</code> 对接收到的 <code>query</code>, <code>key</code>, <code>value</code> 分别进行线性变换。此过程将输入转换为适合每个头的形式。</li>
<li>使用 <code>view</code> 函数将张量的形状从 (batch_size, sequence_length, hidden_size) 转换为 (batch_size, sequence_length, h, d_k)。这是将整个输入分割成 h 个头的过程。</li>
<li>使用 <code>transpose</code> 函数将张量的维度从 (batch_size, sequence_length, h, d_k) 变更为 (batch_size, h, sequence_length, d_k)。现在每个头都准备好了独立进行注意力计算。</li>
</ul></li>
<li><strong>应用注意力</strong>:
<ul>
<li>对每个头调用 <code>attention</code> 函数，即缩放点积注意力(Scaled Dot-Product Attention)，以计算注意力权重和每个头的结果。</li>
</ul></li>
<li><strong>整合及最终线性变换</strong>:
<ul>
<li>使用 <code>transpose</code> 和 <code>contiguous</code> 将各头的结果(<code>x</code>)重新转换为 (batch_size, sequence_length, h, d_k) 的形式。</li>
<li>使用 <code>view</code> 函数将其整合为 (batch_size, sequence_length, h * d_k)，即 (batch_size, sequence_length, hidden_size) 的形式。</li>
<li>最后，应用 <code>self.linear_layers[-1]</code> 生成最终输出。这个线性变换综合了各头的结果，并最终生成模型所需的输出形式。</li>
</ul></li>
</ol></li>
<li><strong><code>attention</code> 方法（缩放点积注意力）</strong>:
<ul>
<li>此函数是每个头实际执行注意力机制的地方，返回每个头的结果和注意力权重。</li>
<li><strong>核心:</strong> 计算 <code>scores</code> 时，通过 <span class="math inline">\(\sqrt{d_k}\)</span> 对 <code>key</code> 向量维度进行除法缩放的过程非常重要。
<ul>
<li><strong>目的:</strong> 随着内积值(<span class="math inline">\(QK^T\)</span>)的增大，防止softmax函数的输入值过度放大。这有助于缓解梯度消失(gradient vanishing)问题，使学习过程更加稳定，并提高模型性能。</li>
</ul></li>
</ul></li>
</ul>
<hr>
<p><strong>各头的作用和多头注意力的优点</strong></p>
<table class="caption-top table">
<tbody>
<tr class="odd">
<td>多头注意力机制可以比喻为使用多个“小镜头”从不同角度观察目标。每个头独立地转换查询(Q)、键(K)和值(V)，并执行注意力计算。通过这种方式，它可以在整个输入序列的不同部分空间（subspace）中集中提取信息。</td>
</tr>
<tr class="even">
<td>* <strong>捕捉多种关系</strong>：每个头可以专门学习不同类型的语言关系。例如，某个头可能专注于主语-动词关系，另一个头则可能关注形容词-名词关系，还有其他头可能关注代词与其先行词之间的关系等。 * <strong>计算效率</strong>：每个头在相对较小的维度(d_k)中进行注意力计算。这比在一个大的单一维度中进行注意力计算在计算成本上更为高效。 * <strong>并行处理</strong>：各头的计算是相互独立的。因此，可以利用GPU进行并行处理，从而显著提高计算速度。</td>
</tr>
<tr class="odd">
<td><strong>实际分析案例</strong></td>
</tr>
<tr class="even">
<td>研究表明多头注意力机制的每个头实际上确实捕捉到了不同的语言特征。例如，在<a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://arxiv.org/abs/1906.04341">“BERT在看什么？对BERT注意力机制的分析”</a>这篇论文中，通过对BERT模型的多头注意力进行分析发现，某些头更擅长于理解句子的句法(syntactic)结构，而其他头则在识别词汇间的语义(semantic)相似性方面发挥更重要的作用。</td>
</tr>
</tbody>
</table>
<p><strong>数学表达</strong></p>
<ul>
<li><strong>整体</strong>: <span class="math inline">\(\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \ldots, \text{head}_h)W^O\)</span></li>
<li><strong>每个头</strong>: <span class="math inline">\(\text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)\)</span></li>
<li><strong>注意力函数</strong>: <span class="math inline">\(\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V\)</span></li>
</ul>
<p><strong>符号说明</strong>:</p>
<ul>
<li><span class="math inline">\(h\)</span>: 头的数量</li>
<li><span class="math inline">\(W_i^Q \in \mathbb{R}^{d_{\text{model}} \times d_k}\)</span>: 第i个头的查询转换矩阵</li>
<li><span class="math inline">\(W_i^K \in \mathbb{R}^{d_{\text{model}} \times d_k}\)</span>: 第i个头的键转换矩阵</li>
<li><span class="math inline">\(W_i^V \in \mathbb{R}^{d_{\text{model}} \times d_v}\)</span>: 第i个头的值转换矩阵</li>
<li><span class="math inline">\(W^O \in \mathbb{R}^{hd_v \times d_{\text{model}}}\)</span>: 最终输出的线性转换矩阵</li>
</ul>
<p><strong>最终线性转换(<span class="math inline">\(W^O\)</span>)的重要性</strong>: 每个头的输出被简单连接(Concat)后，通过额外的线性变换(<span class="math inline">\(W^O\)</span>)将信息投影回原始嵌入维度(<span class="math inline">\(d_{\text{model}}\)</span>)，这一过程起着至关重要的作用。</p>
<ul>
<li><strong>信息整合</strong>：从不同头部提取的各种视角的信息可以平衡且稳定地集成起来，以丰富整体的上下文信息。</li>
<li><strong>最优组合</strong>：通过学习过程，系统自行学习如何最有效地组合每个头的信息。这类似于在集成模型中不是简单平均各个单独模型的预测结果，而是使用学习到的权重进行组合。</li>
</ul>
<hr>
<p><strong>结论</strong></p>
<p>多头注意力机制是变压器模型能够高效捕捉输入序列的上下文信息，并通过利用GPU并行处理提高计算速度的关键机制。正是这一机制使得变压器模型在各种自然语言处理任务中表现出色。</p>
</section>
<section id="并行学习的掩码策略" class="level3">
<h3 class="anchored" data-anchor-id="并行学习的掩码策略">8.2.5 并行学习的掩码策略</h3>
<p>实现多头注意力后，研究团队在实际训练过程中遇到了一个重要问题。即模型通过参考未来的词来预测当前词的 <strong>“信息泄露(information leakage)”</strong> 现象。例如，在句子 “The cat ___ on the mat” 中预测空白处时，模型可以通过提前看到后面的 “mat” 一词轻松预测出 “sits”。</p>
<p><strong>掩码的必要性：防止信息泄露</strong></p>
<p>这种信息泄露导致的结果是模型没有真正发展推理能力，而是简单地“窥视”正确答案。这会导致模型在训练数据上表现出高性能，但在实际的新数据（未来时间点的数据）上无法进行准确预测的问题。</p>
<p>研究团队为了解决这个问题，引入了精心设计的 <strong>掩码(masking)</strong> 策略。在变压器中使用了两种类型的掩码。</p>
<ol type="1">
<li><strong>因果关系掩码(Causal Mask, Look-Ahead Mask):</strong> 阻止自回归(autoregressive)模型参考未来时间点的信息。</li>
<li><strong>填充掩码(Padding Mask):</strong> 在处理可变长度序列时，消除无意义的填充标记(padding token)的影响。</li>
</ol>
<p><strong>1. 因果关系掩码 (Causal Mask)</strong></p>
<p>因果关系掩码的作用是遮挡未来的信息。运行下面的代码可以直观地看到注意力得分矩阵中对应未来信息的部分是如何被掩码的。</p>
<div id="cell-28" class="cell" data-execution_count="4">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_08.visualize_masking <span class="im">import</span> visualize_causal_mask</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>visualize_causal_mask()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>1. Original attention score matrix:
                       I        love        deep    learning
I           [      0.90][      0.70][      0.30][      0.20]
love        [      0.60][      0.80][      0.90][      0.40]
deep        [      0.20][      0.50][      0.70][      0.90]
learning    [      0.40][      0.30][      0.80][      0.60]

Each row represents the attention scores from the current position to all positions
--------------------------------------------------

2. Lower triangular mask (1: allowed, 0: blocked):
                       I        love        deep    learning
I           [      1.00][      0.00][      0.00][      0.00]
love        [      1.00][      1.00][      0.00][      0.00]
deep        [      1.00][      1.00][      1.00][      0.00]
learning    [      1.00][      1.00][      1.00][      1.00]

Only the diagonal and below are 1, the rest are 0
--------------------------------------------------

3. Mask converted to -inf:
                       I        love        deep    learning
I           [   1.0e+00][      -inf][      -inf][      -inf]
love        [   1.0e+00][   1.0e+00][      -inf][      -inf]
deep        [   1.0e+00][   1.0e+00][   1.0e+00][      -inf]
learning    [   1.0e+00][   1.0e+00][   1.0e+00][   1.0e+00]

Converting 0 to -inf so that it becomes 0 after softmax
--------------------------------------------------

4. Attention scores with mask applied:
                       I        love        deep    learning
I           [       1.9][      -inf][      -inf][      -inf]
love        [       1.6][       1.8][      -inf][      -inf]
deep        [       1.2][       1.5][       1.7][      -inf]
learning    [       1.4][       1.3][       1.8][       1.6]

Future information (upper triangle) is masked with -inf
--------------------------------------------------

5. Final attention weights (after softmax):
                       I        love        deep    learning
I           [      1.00][      0.00][      0.00][      0.00]
love        [      0.45][      0.55][      0.00][      0.00]
deep        [      0.25][      0.34][      0.41][      0.00]
learning    [      0.22][      0.20][      0.32][      0.26]

The sum of each row becomes 1, and future information is masked to 0</code></pre>
</div>
</div>
<p><strong>序列处理结构和矩阵</strong></p>
<p>为了说明为什么未来信息会形成上三角矩阵，我将以句子“I love deep learning”为例。单词顺序是[I(0), love(1), deep(2), learning(3)]。在注意力得分矩阵(<span class="math inline">\(QK^T\)</span>)中，行和列都遵循这个单词顺序。</p>
<div id="cell-30" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>attention_scores <span class="op">=</span> [</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>    [<span class="fl">0.9</span>, <span class="fl">0.7</span>, <span class="fl">0.3</span>, <span class="fl">0.2</span>],  <span class="co"># I -&gt; I, love, deep, learning</span></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>    [<span class="fl">0.6</span>, <span class="fl">0.8</span>, <span class="fl">0.9</span>, <span class="fl">0.4</span>],  <span class="co"># love -&gt; I, love, deep, learning</span></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>    [<span class="fl">0.2</span>, <span class="fl">0.5</span>, <span class="fl">0.7</span>, <span class="fl">0.9</span>],  <span class="co"># deep -&gt; I, love, deep, learning</span></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>    [<span class="fl">0.4</span>, <span class="fl">0.3</span>, <span class="fl">0.8</span>, <span class="fl">0.6</span>]   <span class="co"># learning -&gt; I, love, deep, learning</span></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<ul>
<li>Q的每一行是当前处理的单词的查询向量。</li>
<li>K的每一列（由于K被转置了）是将被引用的单词的键向量。</li>
</ul>
<p>解释上述矩阵：</p>
<ol type="1">
<li>第一行(I): [I] → 与[I, love, deep, learning]的关系</li>
<li>第二行(love): [love] → 与[I, love, deep, learning]的关系</li>
<li>第三行(deep): [deep] → 与[I, love, deep, learning]的关系</li>
<li>第四行(learning): [learning] → 与[I, love, deep, learning]的关系</li>
</ol>
<p>处理”deep”这个词时（第3行）</p>
<ul>
<li>可引用: [I, love, deep] （到目前为止出现的词）</li>
<li>不可引用: [learning] （未来还未出现的词）</li>
</ul>
<p>因此，以行为基准来看，该列单词的未来单词（即未来信息）位于该位置的右侧，即<strong>上三角(upper triangular)</strong>部分。相反，可引用的单词则位于<strong>下三角(lower triangular)</strong>。</p>
<p>因果关系掩码是将下三角部分设为1，上三角部分设为0，然后将上三角的0替换为<span class="math inline">\(-\infty\)</span>。2. <span class="math inline">\(-\infty\)</span>通过softmax函数后变为0。掩码矩阵简单地与注意力得分矩阵相加。结果，在应用了softmax的注意力得分矩阵中，未来信息被变为0从而阻断。</p>
<p><strong>2. 填充掩码 (Padding Mask)</strong></p>
<p>在自然语言处理中，句子的长度各不相同。为了批处理(batch)，需要将所有句子调整为相同的长度，此时较短句子的空缺部分用填充令牌(PAD)填充。但这些填充令牌没有实际意义，因此不应包含在注意力计算中。</p>
<div id="cell-32" class="cell" data-execution_count="14">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_08.visualize_masking <span class="im">import</span> visualize_padding_mask</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>visualize_padding_mask()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
2. Create padding mask (1: valid token, 0: padding token):
tensor([[[1., 1., 1., 1.]],

        [[1., 1., 1., 0.]],

        [[1., 1., 1., 1.]],

        [[1., 1., 1., 1.]]])

Positions that are not padding (0) are 1, padding positions are 0
--------------------------------------------------

3. Original attention scores (first sentence):
                       I        love        deep    learning
I           [      0.90][      0.70][      0.30][      0.20]
love        [      0.60][      0.80][      0.90][      0.40]
deep        [      0.20][      0.50][      0.70][      0.90]
learning    [      0.40][      0.30][      0.80][      0.60]

Attention scores at each position
--------------------------------------------------

4. Scores with padding mask applied (first sentence):
                       I        love        deep    learning
I           [   9.0e-01][   7.0e-01][   3.0e-01][   2.0e-01]
love        [   6.0e-01][   8.0e-01][   9.0e-01][   4.0e-01]
deep        [   2.0e-01][   5.0e-01][   7.0e-01][   9.0e-01]
learning    [   4.0e-01][   3.0e-01][   8.0e-01][   6.0e-01]

The scores at padding positions are masked with -inf
--------------------------------------------------

5. Final attention weights (first sentence):
                       I        love        deep    learning
I           [      0.35][      0.29][      0.19][      0.17]
love        [      0.23][      0.28][      0.31][      0.19]
deep        [      0.17][      0.22][      0.27][      0.33]
learning    [      0.22][      0.20][      0.32][      0.26]

The weights at padding positions become 0, and the sum of the weights at the remaining positions is 1</code></pre>
</div>
</div>
<p>我们将举例说明如下句子。</p>
<ul>
<li>“I love ML” → [I, love, ML, PAD]</li>
<li>“Deep learning is fun” → [Deep, learning, is, fun]</li>
</ul>
<p>这里，第一个句子只有3个单词，因此最后用PAD填充。padding mask用于消除这些PAD token的影响。生成的mask会将实际单词标记为1，将padding token标记为0，并且2. 将padding位置的注意力分数设置为<span class="math inline">\(-\infty\)</span>，使其在经过softmax后变为0。</p>
<p>最终我们得到以下效果。</p>
<ol type="1">
<li>实际单词可以自由地相互给予和接收注意力。</li>
<li>padding token完全从注意力计算中排除。</li>
<li>句子的实际有意义部分形成上下文。</li>
</ol>
<div id="cell-34" class="cell" data-execution_count="2">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_attention_mask(size):</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create a lower triangular matrix (including the diagonal)</span></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>    mask <span class="op">=</span> torch.tril(torch.ones(size, size))</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Mask with -inf (becomes 0 after softmax)</span></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>    mask <span class="op">=</span> mask.masked_fill(mask <span class="op">==</span> <span class="dv">0</span>, <span class="bu">float</span>(<span class="st">'-inf'</span>))</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> mask</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> masked_attention(Q, K, V, mask):</span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate attention scores</span></span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>    scores <span class="op">=</span> torch.matmul(Q, K.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>))</span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Apply mask</span></span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>    scores <span class="op">=</span> scores <span class="op">+</span> mask</span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Apply softmax</span></span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a>    weights <span class="op">=</span> F.softmax(scores, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate final attention output</span></span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> torch.matmul(weights, V)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><strong>遮罩策略的创新与影响</strong></p>
<p>研究团队开发的两种遮罩策略（填充遮罩，因果关系遮罩）使变压器的学习过程更加稳健，并成为GPT等自回归模型的基础。特别是因果关系遮罩引导语言模型以类似于实际人类语言理解过程的方式顺序地把握上下文。</p>
<p><strong>实现的效率</strong></p>
<p>遮罩在计算注意力分数后、应用softmax函数<em>前</em>执行。被遮罩为<span class="math inline">\(-\infty\)</span>的位置在通过softmax函数时变为0，从而完全阻断该位置的信息。这在计算效率和内存使用方面也是优化的方法。</p>
<p>引入这些遮罩策略使变压器能够实现真正意义上的并行学习，这对现代语言模型的发展产生了重大影响。</p>
</section>
<section id="头概念的演变从头部到脑" class="level3">
<h3 class="anchored" data-anchor-id="头概念的演变从头部到脑">8.2.6 头概念的演变：从“头部”到“脑”</h3>
<p>在深度学习中，“头（head）”一词的意义随着神经网络架构的发展逐渐且根本地发生了变化。最初，它主要被用作指代“接近输出层的部分”的相对简单的含义，但近年来，其意义扩展为承担模型特定功能的“独立模块”，这一概念更加抽象和复杂。</p>
<ol type="1">
<li><p><strong>初期：“靠近输出层”</strong></p>
<p>在早期深度学习模型（例如：简单的多层感知器（MLP））中，“头”通常指接收通过特征提取器（feature extractor, backbone）传递的特征向量，并执行最终预测（分类、回归等）的网络的最后一部分。在这种情况下，头部主要由全连接层（fully connected layer）和激活函数（activation function）组成。</p></li>
</ol>
<div id="cell-37" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SimpleModel(nn.Module):</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, num_classes):</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.backbone <span class="op">=</span> nn.Sequential( <span class="co"># Feature extractor</span></span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">784</span>, <span class="dv">128</span>),</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">128</span>, <span class="dv">64</span>),</span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>            nn.ReLU()</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.head <span class="op">=</span> nn.Linear(<span class="dv">64</span>, num_classes)  <span class="co"># Head (output layer)</span></span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a>        features <span class="op">=</span> <span class="va">self</span>.backbone(x)</span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> <span class="va">self</span>.head(features)</span>
<span id="cb21-15"><a href="#cb21-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> output</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<ol start="2" type="1">
<li><strong>多任务学习: “按任务分叉”</strong></li>
</ol>
<p>随着使用像ImageNet这样的大型数据集的深度学习模型的发展，从一个特征提取器中分支出多个头来执行不同任务的多任务学习（multi-task learning）出现了。例如，在对象检测（object detection）模型中，同时使用了对图像中的对象种类进行分类（classification）的头和预测表示对象位置的边界框（bounding box）的头。</p>
<div id="cell-39" class="cell" data-execution_count="3">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MultiTaskModel(nn.Module):</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, num_classes):</span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.backbone <span class="op">=</span> ResNet50()  <span class="co"># Feature extractor (ResNet)</span></span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.classification_head <span class="op">=</span> nn.Linear(<span class="dv">2048</span>, num_classes)  <span class="co"># Classification head</span></span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bbox_head <span class="op">=</span> nn.Linear(<span class="dv">2048</span>, <span class="dv">4</span>)  <span class="co"># Bounding box regression head</span></span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a>        features <span class="op">=</span> <span class="va">self</span>.backbone(x)</span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a>        class_output <span class="op">=</span> <span class="va">self</span>.classification_head(features)</span>
<span id="cb22-15"><a href="#cb22-15" aria-hidden="true" tabindex="-1"></a>        bbox_output <span class="op">=</span> <span class="va">self</span>.bbox_head(features)</span>
<span id="cb22-16"><a href="#cb22-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> class_output, bbox_output</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<ol start="3" type="1">
<li><p><strong>Attention is All You Need 论文（变压器）中的“头”概念</strong>:</p>
<p>变压器的多头注意力机制更进一步。在变压器中，不再遵循“头 = 接近输出的部分”的固定观念。</p></li>
</ol>
<div id="cell-41" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MultiHeadAttention(nn.Module):</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, num_heads):</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.heads <span class="op">=</span> nn.ModuleList([</span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>            AttentionHead() <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_heads)  <span class="co"># num_heads개의 독립적인 어텐션 헤드</span></span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>        ])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<ul>
<li><strong>独立的模块:</strong> 在这里，每个“头”都是一个接收输入并独立执行注意力机制的<em>单独模块</em>。每个头都有不同的权重，并关注输入序列的不同方面。</li>
<li><strong>并行处理:</strong> 多个头<em>并行地</em>工作，同时处理各种类型的信息。</li>
<li><strong>中间处理阶段:</strong> 头不再仅限于输出层。变压器的编码器和解码器由<em>多头注意力的多个层次</em>组成，每个层次的头学习输入序列的不同表示。</li>
</ul>
<ol start="4" type="1">
<li><p><strong>最近的趋势: “功能模块”</strong></p>
<p>在最近的深度学习模型中，“头”这个词的使用更加灵活。即使不在输出层附近，特定功能的独立模块也经常被称为“头”。</p>
<ul>
<li><strong>语言模型 (Language Models):</strong> 例如在BERT、GPT等大规模语言模型中，使用了多种类型的头，如“语言建模头”，“掩码语言建模头”，“下一句预测头”。</li>
<li><strong>视觉变压器 (Vision Transformers):</strong> 在ViT中，图像被分割成补丁（patch），并使用“补丁嵌入头”来处理每个补丁，就像处理令牌一样。</li>
</ul></li>
</ol>
<p><strong>结论</strong></p>
<p>在深度学习中，“头”的含义已经从“接近输出的部分”演变为“执行特定功能的独立模块（包括并行和中间处理）”。这种变化反映了随着深度学习架构变得更加复杂和精细，模型各个部分正变得越来越细分和专业化。变压器的多头注意力是这一意义演变的典型例子，“头”这个词不再仅仅指代“头部”，而是像多个“大脑”一样工作。</p>
</section>
</section>
<section id="位置信息的处理" class="level2">
<h2 class="anchored" data-anchor-id="位置信息的处理">8.3 位置信息的处理</h2>
<p><strong>挑战:</strong> 如何在没有RNN的情况下有效地表达词序信息？</p>
<p><strong>研究者的困惑:</strong> 由于变压器不像RNN那样顺序处理数据，因此必须显式地提供词语的位置信息。研究人员尝试了多种方法（如位置索引、可学习的嵌入等），但未能获得令人满意的结果。就像破解密码一样，他们需要找到一种有效表达位置信息的新方法。</p>
<p>与RNN不同，变压器不使用递归结构或卷积运算，因此必须单独提供序列的顺序信息。“dog bites man”和“man bites dog”的词相同，但由于顺序不同，意义完全不同。注意力操作(<span class="math inline">\(QK^T\)</span>)本身仅计算词向量之间的相似性，并不考虑词语的位置信息，因此研究团队不得不思考如何将位置信息注入模型中。这是一项<strong>挑战任务</strong>：在没有RNN的情况下，如何有效地表达词序信息？</p>
<section id="序列信息的重要性" class="level3">
<h3 class="anchored" data-anchor-id="序列信息的重要性">8.3.1 序列信息的重要性</h3>
<p>研究团队考虑了多种位置编码方法。</p>
<ol type="1">
<li><strong>直接使用位置索引:</strong> 最简单的处理方式是将每个词语的位置索引（0, 1, 2, …）加到嵌入向量中。</li>
</ol>
<div id="cell-44" class="cell" data-execution_count="15">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_08.visualize_positional_embedding <span class="im">import</span> visualize_position_embedding</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>visualize_position_embedding()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>1. Original embedding matrix:
                dim1      dim2      dim3      dim4
I         [    0.20][    0.30][    0.10][    0.40]
love      [    0.50][    0.20][    0.80][    0.10]
deep      [    0.30][    0.70][    0.20][    0.50]
learning  [    0.60][    0.40][    0.30][    0.20]

Each row is the embedding vector of a word
--------------------------------------------------

2. Position indices:
[0 1 2 3]

Indices representing the position of each word (starting from 0)
--------------------------------------------------

3. Embeddings with position information added:
                dim1      dim2      dim3      dim4
I         [    0.20][    0.30][    0.10][    0.40]
love      [    1.50][    1.20][    1.80][    1.10]
deep      [    2.30][    2.70][    2.20][    2.50]
learning  [    3.60][    3.40][    3.30][    3.20]

Result of adding position indices to each embedding vector (broadcasting)
--------------------------------------------------

4. Changes due to adding position information:

I (0):
  Original:     [0.2 0.3 0.1 0.4]
  Pos. Added: [0.2 0.3 0.1 0.4]
  Difference:     [0. 0. 0. 0.]

love (1):
  Original:     [0.5 0.2 0.8 0.1]
  Pos. Added: [1.5 1.2 1.8 1.1]
  Difference:     [1. 1. 1. 1.]

deep (2):
  Original:     [0.3 0.7 0.2 0.5]
  Pos. Added: [2.3 2.7 2.2 2.5]
  Difference:     [2. 2. 2. 2.]

learning (3):
  Original:     [0.6 0.4 0.3 0.2]
  Pos. Added: [3.6 3.4 3.3 3.2]
  Difference:     [3. 3. 3. 3.]</code></pre>
</div>
</div>
<p>但是这种方法存在两个问题。</p>
<ul>
<li><strong>无法处理比训练数据更长的序列：</strong> 如果输入中出现了训练时未见过的位置（例如，第100位），则无法找到合适的表示。</li>
<li><strong>难以表达相对距离信息：</strong> 难以表示位置2和4之间的距离与位置102和104之间的距离相同。</li>
</ul>
<ol start="2" type="1">
<li><strong>可学习的位置嵌入：</strong> 还考虑了使用每个位置的可学习嵌入向量的方法。</li>
</ol>
<div id="cell-46" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Conceptual code</span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>    positional_embeddings <span class="op">=</span> nn.Embedding(max_seq_length, embedding_dim)</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>    positions <span class="op">=</span> torch.arange(seq_length)</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>    positional_encoding <span class="op">=</span> positional_embeddings(positions)</span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>    final_embedding <span class="op">=</span> word_embedding <span class="op">+</span> positional_encoding</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>这种方式可以学习每个位置的唯一表达，但仍然存在无法处理比训练数据更长序列的根本限制。</p>
<p><strong>位置信息表示的关键条件</strong></p>
<p>研究团队通过上述试错过程，认识到位置信息表示必须满足以下三个关键条件。</p>
<ol type="1">
<li><strong>无序列长度限制：</strong> 必须能够适当表示在训练时未见过的位置（例如：第1000个位置）。</li>
<li><strong>相对距离关系表达：</strong> 位置2和4之间的距离应与位置102和104之间的距离相同地表示。即，必须保持位置间的相对距离。</li>
<li><strong>与注意力运算的兼容性：</strong> 位置信息不应妨碍注意力权重计算，同时有效地传达顺序信息。</li>
</ol>
</section>
<section id="位置编码的设计" class="level3">
<h3 class="anchored" data-anchor-id="位置编码的设计">8.3.2 位置编码的设计</h3>
<p>经过这样的思考，研究团队发现了一种独特的解决方案——利用正弦(sin)和余弦(cos)函数的周期特性进行的<strong>位置编码(Positional Encoding)</strong>。</p>
<p><strong>基于正弦-余弦函数的位置编码原理</strong></p>
<p>使用不同频率(frequency)的正弦和余弦函数对每个位置进行编码，则位置间的相对距离会自然地得到表示。</p>
<div id="cell-48" class="cell" data-execution_count="17">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_08.positional_encoding_utils <span class="im">import</span> visualize_sinusoidal_features</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>visualize_sinusoidal_features()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="08_变压器的诞生_files/figure-html/cell-21-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>3 是表示位置移动的可视化图。它展示了如何使用正弦函数来表达位置关系，满足了第二个条件“相对距离关系表达”。所有平移后的曲线都保持与原始曲线相同的形状，并且保持恒定的距离。这意味着如果两个位置之间的距离相同（例如：2→7 和 102→107），它们的关系也会以相同的方式表示。</p>
<p>4 是位置编码热图 (Positional Encoding Matrix)。它展示了每个位置（纵轴）具有独特的模式（横轴）。横轴的列代表不同周期的正弦/余弦函数，且越往右周期越长。每一行（位置）由红色（正值）和蓝色（负值）组成的独特图案构成。通过使用从短周期到长周期的各种频率，在每个位置生成独特的模式。这种方法满足了第一个条件“序列长度无限制”。通过组合不同周期的正弦/余弦函数，可以数学上无限地为每个位置生成唯一值。</p>
<p>利用这一数学特征，研究团队实现了如下位置编码算法。</p>
<p><strong>位置编码实现</strong></p>
<div id="cell-50" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> positional_encoding(seq_length, d_model):</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 1. 위치별 인코딩 행렬 생성</span></span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>    position <span class="op">=</span> np.arange(seq_length)[:, np.newaxis]  <span class="co"># [0, 1, 2, ..., seq_length-1]</span></span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 2. 각 차원별 주기 계산</span></span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a>    div_term <span class="op">=</span> np.exp(np.arange(<span class="dv">0</span>, d_model, <span class="dv">2</span>) <span class="op">*</span> <span class="op">-</span>(np.log(<span class="fl">10000.0</span>) <span class="op">/</span> d_model))</span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 예: d_model=512일 때</span></span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># div_term[0] ≈ 1.0        (가장 짧은 주기)</span></span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># div_term[256] ≈ 0.0001   (가장 긴 주기)</span></span>
<span id="cb28-10"><a href="#cb28-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb28-11"><a href="#cb28-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 3. 짝수/홀수 차원에 사인/코사인 적용</span></span>
<span id="cb28-12"><a href="#cb28-12" aria-hidden="true" tabindex="-1"></a>    pe <span class="op">=</span> np.zeros((seq_length, d_model))</span>
<span id="cb28-13"><a href="#cb28-13" aria-hidden="true" tabindex="-1"></a>    pe[:, <span class="dv">0</span>::<span class="dv">2</span>] <span class="op">=</span> np.sin(position <span class="op">*</span> div_term)  <span class="co"># 짝수 차원</span></span>
<span id="cb28-14"><a href="#cb28-14" aria-hidden="true" tabindex="-1"></a>    pe[:, <span class="dv">1</span>::<span class="dv">2</span>] <span class="op">=</span> np.cos(position <span class="op">*</span> div_term)  <span class="co"># 홀수 차원</span></span>
<span id="cb28-15"><a href="#cb28-15" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb28-16"><a href="#cb28-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> pe</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<ul>
<li><code>position</code>: <code>[0, 1, 2, ..., seq_length-1]</code> 形式的数组。表示每个词的位置索引。</li>
<li><code>div_term</code>: 确定每个维度周期的值。随着 <code>d_model</code> 的增加，周期变长。</li>
<li><code>pe[:, 0::2] = np.sin(position * div_term)</code>: 偶数索引维度应用正弦函数。</li>
<li><code>pe[:, 1::2] = np.cos(position * div_term)</code>: 奇数索引维度应用余弦函数。</li>
</ul>
<p><strong>数学表达</strong></p>
<p>位置编码的每个维度按以下公式计算。</p>
<ul>
<li><span class="math inline">\(PE_{(pos, 2i)} = \sin(pos / 10000^{2i/d_{\text{model}}})\)</span></li>
<li><span class="math inline">\(PE_{(pos, 2i+1)} = \cos(pos / 10000^{2i/d_{\text{model}}})\)</span></li>
</ul>
<p>其中</p>
<ul>
<li><span class="math inline">\(pos\)</span>: 单词的位置 (0, 1, 2, …)</li>
<li><span class="math inline">\(i\)</span>: 维度索引 (0, 1, 2, …, <span class="math inline">\(d_{model}\)</span>-1)</li>
<li><span class="math inline">\(d_{model}\)</span>: 嵌入维度（及位置编码维度）</li>
</ul>
<p><strong>周期变化检查</strong></p>
<div id="cell-52" class="cell" data-execution_count="18">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_08.positional_encoding_utils <span class="im">import</span> show_positional_periods</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>show_positional_periods()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>1. Periods of positional encoding:
First dimension (i=0): 1.00
Middle dimension (i=128): 100.00
Last dimension (i=255): 9646.62

2. Positional encoding formula values (10000^(2i/d_model)):
i=  0: 1.0000000000
i=128: 100.0000000000
i=255: 9646.6161991120

3. Actual div_term values (first/middle/last):
First (i=0): 1.0000000000
Middle (i=128): 0.0100000000
Last (i=255): 0.0001036633</code></pre>
</div>
</div>
<p>这里的关键是三个步骤。</p>
<div id="cell-54" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 3. 짝수/홀수 차원에 사인/코사인 적용</span></span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>    pe <span class="op">=</span> np.zeros((seq_length, d_model))</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>    pe[:, <span class="dv">0</span>::<span class="dv">2</span>] <span class="op">=</span> np.sin(position <span class="op">*</span> div_term)  <span class="co"># 짝수 차원</span></span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>    pe[:, <span class="dv">1</span>::<span class="dv">2</span>] <span class="op">=</span> np.cos(position <span class="op">*</span> div_term)  <span class="co"># 홀수 차원</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>上述结果展示了周期随维度的变化。</p>
<p><strong>最终嵌入</strong></p>
<p>生成的位置编码 <code>pe</code> 具有 (seq_length, d_model) 的形状，并与原始词嵌ding矩阵(sentence_embedding)相加以生成最终嵌入。</p>
<div id="cell-56" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>final_embedding <span class="op">=</span> sentence_embedding <span class="op">+</span> positional_encoding</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>这样相加的最终嵌入包含了词语的意义和位置信息。例如，“bank”这个词根据在句子中的位置会有不同的最终向量值，从而帮助区分“银行”和“河岸”的含义。</p>
<p>通过这种方式，变压器能够在没有RNN的情况下有效地处理顺序信息，并为最大限度地利用并行处理的优势奠定了基础。</p>
<div class="callout callout-style-default callout-note callout-titled" title="点击查看内容（深入探讨：位置编码的演变、最新技术及其数学基础）">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
点击查看内容（深入探讨：位置编码的演变、最新技术及其数学基础）
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<section id="位置编码的演变最新技术及数学基础" class="level3 callout-body-container callout-body">
<h3 class="anchored" data-anchor-id="位置编码的演变最新技术及数学基础">位置编码的演变、最新技术及数学基础</h3>
<p>8.3.2节中，我们探讨了基于正弦-余弦函数的位置编码，这是变压器模型的基础。然而，在“Attention is All You Need”论文发布之后，位置编码在多个方向上得到了发展。在这个深入探讨部分，我们将全面覆盖可学习的位置编码、相对位置编码以及最新的研究趋势，并对每种技术的数学表达和优缺点进行深入分析。</p>
<section id="可学习的位置编码-learnable-positional-encoding" class="level4">
<h4 class="anchored" data-anchor-id="可学习的位置编码-learnable-positional-encoding">1. 可学习的位置编码 (Learnable Positional Encoding)</h4>
<ul>
<li><p><strong>概念</strong>: 不是使用固定的函数，而是让模型通过学习直接获得表示位置信息的嵌入。</p></li>
<li><p><strong>1.1 数学表达</strong>: 可学习的位置嵌入可以表示为以下矩阵：</p>
<p><span class="math inline">\(P \in \mathbb{R}^{L_{max} \times d}\)</span></p>
<p>其中，<span class="math inline">\(L_{max}\)</span> 是最大序列长度，<span class="math inline">\(d\)</span> 是嵌入维度。位置 <span class="math inline">\(i\)</span> 的嵌入由矩阵 <span class="math inline">\(P\)</span> 的第 <span class="math inline">\(i\)</span> 行给出，即 <span class="math inline">\(P[i,:]\)</span>。</p></li>
<li><p><strong>1.2 解决外推(Extrapolation)问题的技术</strong>: 当处理比训练数据更长的序列时，会出现没有学习过的嵌入对应位置的信息的问题。为了克服这一问题，已经研究了多种技术。</p>
<ul>
<li><p><strong>位置插值 (Chen et al., 2023)</strong>: 通过在已学习的嵌入之间进行线性插值得到新的位置嵌入。 <span class="math inline">\(P_{ext}(i) = P[\lfloor \alpha i \rfloor] + (\alpha i - \lfloor \alpha i \rfloor)(P[\lfloor \alpha i \rfloor +1] - P[\lfloor \alpha i \rfloor])\)</span></p>
<p>其中，<span class="math inline">\(\alpha = \frac{\text{训练序列长度}}{\text{推理序列长度}}\)</span>。</p></li>
<li><p><strong>NTK-aware 缩放 (2023)</strong>: 基于神经切线核 (NTK) 理论，通过逐渐增加频率引入平滑效果的方法。</p></li>
</ul></li>
<li><p><strong>1.3 最新应用案例</strong>:</p>
<ul>
<li><strong>BERT</strong>: 初始时限制为512个token，但在RoBERTa中扩展到了1024个token。</li>
<li><strong>GPT-3</strong>: 有2048个token的限制，并在训练过程中使用了逐渐增加序列长度的技术。</li>
</ul></li>
<li><p><strong>优点</strong>:</p>
<ul>
<li><strong>灵活性</strong>: 可以学习到针对数据的位置信息。</li>
<li><strong>潜在性能提升</strong>: 在特定任务中，可能比固定的函数表现出更好的性能。</li>
</ul></li>
<li><p><strong>缺点</strong>:</p>
<ul>
<li><strong>过拟合风险</strong>: 对于训练数据之外长度的序列，泛化性能可能会降低。</li>
<li><strong>处理长序列的困难</strong>: 需要额外的技术来解决外推问题。</li>
</ul></li>
</ul>
</section>
<section id="相对位置编码-relative-positional-encoding" class="level4">
<h4 class="anchored" data-anchor-id="相对位置编码-relative-positional-encoding">2. 相对位置编码 (Relative Positional Encoding)</h4>
<ul>
<li><p><strong>核心思想</strong>: 关注词语之间的相对距离而非绝对位置信息。</p></li>
<li><p><strong>背景</strong>: 在自然语言中，单词的意义往往更多地受周围单词的相对关系影响，而不仅仅是绝对位置。此外，绝对位置编码难以有效地捕捉远距离单词间的关系。</p></li>
<li><p><strong>2.1 数学扩展</strong>:</p>
<ul>
<li><strong>Shaw et al.&nbsp;(2018) 公式</strong>: 在计算注意力机制中Query和Key向量之间的关系时，添加一个表示相对距离的可学习嵌入(<span class="math inline">\(a_{i-j}\)</span>)。 <span class="math inline">\(e_{ij} = \frac{x_iW^Q(x_jW^K + a_{i-j})^T}{\sqrt{d}}\)</span></li>
</ul></li>
</ul>
<p>这里 <span class="math inline">\(a_{i-j} \in \mathbb{R}^d\)</span> 是相对位置 <span class="math inline">\(i-j\)</span> 的可学习向量。</p>
<ul>
<li><p><strong>旋转位置编码 (RoPE)</strong>: 使用旋转矩阵对相对位置进行编码。</p>
<p><span class="math inline">\(\text{RoPE}(x, m) = x \odot e^{im\theta}\)</span></p>
<p>其中 <span class="math inline">\(\theta\)</span> 是控制频率的超参数，<span class="math inline">\(\odot\)</span> 表示复数乘法（或相应的旋转矩阵）。</p></li>
<li><p><strong>T5 的简化版本</strong>: 使用相对位置的可学习偏置 (<span class="math inline">\(b\)</span>)，并且当相对距离超过一定范围时进行裁剪 (clipping)。</p>
<p><span class="math inline">\(e_{ij} = \frac{x_iW^Q(x_jW^K)^T + b_{\text{clip}(i-j)}}{\sqrt{d}}\)</span></p>
<p><span class="math inline">\(b \in \mathbb{R}^{2k+1}\)</span> 是针对裁剪后的相对位置 [-k, k] 的偏置向量。</p></li>
<li><p><strong>优点</strong>:</p>
<ul>
<li><strong>泛化能力提高</strong>: 对于训练数据中不存在的序列长度，能够更好地进行泛化。</li>
<li><strong>长距离依赖捕捉能力增强</strong>: 更有效地建模远距离词语之间的关系。</li>
</ul></li>
<li><p><strong>缺点</strong>:</p>
<ul>
<li><strong>计算复杂度增加</strong>: 由于需要考虑相对距离，注意力计算可能会变得更加复杂。 (特别是当需要考虑所有词对的相对距离时)</li>
</ul></li>
</ul>
</section>
<section id="基于-cnn-的位置编码优化" class="level4">
<h4 class="anchored" data-anchor-id="基于-cnn-的位置编码优化">3. 基于 CNN 的位置编码优化</h4>
<ul>
<li><p><strong>3.1 深度可分离卷积的应用</strong>: 对每个通道独立执行卷积以减少参数数量并提高计算效率。 <span class="math inline">\(P(i) = \sum_{k=-K}^K w_k \cdot x_{i+k}\)</span></p>
<p>其中 <span class="math inline">\(K\)</span> 是内核大小，<span class="math inline">\(w_k\)</span> 是可学习的权重。</p></li>
<li><p><strong>3.2 多尺度卷积</strong>: 类似于 ResNet，利用并行卷积通道来捕获不同范围的位置信息。</p>
<p><span class="math inline">\(P(i) = \text{Concat}(\text{Conv}_{3x1}(x), \text{Conv}_{5x1}(x))\)</span></p></li>
</ul>
</section>
<section id="递归位置编码的动力学" class="level4">
<h4 class="anchored" data-anchor-id="递归位置编码的动力学">4. 递归位置编码的动力学</h4>
<ul>
<li><p><strong>4.1 基于 LSTM 的编码</strong>: 使用 LSTM 对顺序位置信息进行编码。</p>
<p><span class="math inline">\(h_t = \text{LSTM}(x_t, h_{t-1})\)</span> <span class="math inline">\(P(t) = W_ph_t\)</span></p></li>
<li><p><strong>4.2 最新变体: 神经 ODE</strong>: 建模连续时间动力学，以克服离散 (discrete) LSTM 的局限性。</p>
<p><span class="math inline">\(\frac{dh(t)}{dt} = f_\theta(h(t), t)\)</span> <span class="math inline">\(P(t) = \int_0^t f_\theta(h(\tau), \tau)d\tau\)</span></p></li>
</ul>
</section>
<section id="复数位置编码的量子力学解释" class="level4">
<h4 class="anchored" data-anchor-id="复数位置编码的量子力学解释">5. 复数位置编码的量子力学解释</h4>
<ul>
<li><p><strong>5.1 复数嵌入表示</strong>: 以复数形式表示位置信息。</p>
<p><span class="math inline">\(z(i) = r(i)e^{i\phi(i)}\)</span></p>
<p>其中 <span class="math inline">\(r\)</span> 表示位置的大小，<span class="math inline">\(\phi\)</span> 表示相位角。</p></li>
<li><p><strong>5.2 相移定理</strong>: 在复平面上用旋转来表示位置移动。</p>
<p><span class="math inline">\(z(i+j) = z(i) \cdot e^{i\omega j}\)</span></p>
<p>其中 <span class="math inline">\(\omega\)</span> 是可学习的频率参数。</p></li>
</ul>
</section>
<section id="混合方法" class="level4">
<h4 class="anchored" data-anchor-id="混合方法">6. 混合方法</h4>
<ul>
<li><strong>6.1 组合位置编码:</strong> <span class="math inline">\(P(i)=αP_{abs}(i)+βP_{rel}(i)\)</span> α, β = 学习权重</li>
</ul>
</section>
<section id="动态位置编码" class="level4">
<h4 class="anchored" data-anchor-id="动态位置编码">6.2 动态位置编码：</h4>
<p><span class="math inline">\(P(i) = \text{MLP}(i, \text{Context})\)</span> 上下文依赖的位置表示学习</p>
</section>
<section id="实验性能比较glue基准" class="level4">
<h4 class="anchored" data-anchor-id="实验性能比较glue基准">7. 实验性能比较（GLUE基准）</h4>
<p>以下是GLUE基准上各种位置编码方法的实验性能比较结果。（实际性能会因模型结构、数据、超参数设置等因素而有所不同。）</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: left;">方法</th>
<th style="text-align: left;">准确率</th>
<th style="text-align: left;">推理时间 (ms)</th>
<th style="text-align: left;">内存使用量 (GB)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">绝对（正弦波）</td>
<td style="text-align: left;">88.2</td>
<td style="text-align: left;">12.3</td>
<td style="text-align: left;">2.1</td>
</tr>
<tr class="even">
<td style="text-align: left;">相对（RoPE）</td>
<td style="text-align: left;">89.7</td>
<td style="text-align: left;">14.5</td>
<td style="text-align: left;">2.4</td>
</tr>
<tr class="odd">
<td style="text-align: left;">CNN多尺度</td>
<td style="text-align: left;">87.9</td>
<td style="text-align: left;">13.8</td>
<td style="text-align: left;">3.2</td>
</tr>
<tr class="even">
<td style="text-align: left;">复数（CLEX）</td>
<td style="text-align: left;">90.1</td>
<td style="text-align: left;">15.2</td>
<td style="text-align: left;">2.8</td>
</tr>
<tr class="odd">
<td style="text-align: left;">动态PE</td>
<td style="text-align: left;">90.3</td>
<td style="text-align: left;">17.1</td>
<td style="text-align: left;">3.5</td>
</tr>
</tbody>
</table>
</section>
<section id="最新研究趋势2024" class="level4">
<h4 class="anchored" data-anchor-id="最新研究趋势2024">8. 最新研究趋势（2024）</h4>
<p>最近，受量子计算、生物系统等启发的新位置编码技术正在被研究。</p>
<ul>
<li><strong>量子位置编码</strong>：
<ul>
<li>利用Qubit旋转门: <span class="math inline">\(R_z(\theta_i)|x\rangle\)</span></li>
<li>基于Grover算法的位置搜索</li>
</ul></li>
<li><strong>仿生编码</strong>：
<ul>
<li>应用突触可塑性的STDP(Spike-Timing-Dependent Plasticity)规则: <span class="math inline">\(\Delta w_{ij} \propto e^{-\frac{|i-j|}{\tau}}\)</span></li>
</ul></li>
<li><strong>图神经网络整合</strong>：
<ul>
<li>将位置表示为节点，将关系表示为边: <span class="math inline">\(P(i) = \sum_{j \in \mathcal{N}(i)} \alpha_{ij}Wx_j\)</span></li>
</ul></li>
</ul>
</section>
<section id="选择指南" class="level4">
<h4 class="anchored" data-anchor-id="选择指南">9. 选择指南</h4>
<ul>
<li><strong>固定长度序列</strong>：可学习的PE。过拟合风险低，优化容易。</li>
<li><strong>可变长度/需要外推</strong>：RoPE。旋转不变性使长度扩展性优秀。</li>
<li><strong>低延迟实时处理</strong>：基于CNN。并行处理优化，硬件加速方便。</li>
<li><strong>物理信号处理</strong>：复数PE。保持频率信息，与傅里叶变换兼容。</li>
<li><strong>多模态数据</strong>：动态PE。响应跨模式上下文的适应性。</li>
</ul>
</section>
<section id="数学附录" class="level4">
<h4 class="anchored" data-anchor-id="数学附录">数学附录</h4>
<ul>
<li><p><strong>RoPE的群论特性</strong>：</p>
<p>SO(2)旋转群的表示: <span class="math inline">\(R(\theta) = \begin{bmatrix} \cos\theta &amp; -\sin\theta \\ \sin\theta &amp; \cos\theta \end{bmatrix}\)</span></p>
<p>这一性质保证了注意力分数的相对位置保持。</p></li>
<li><p><strong>相对位置偏差的有效计算</strong>：</p>
<p>利用Toeplitz矩阵结构: <span class="math inline">\(B = [b_{i-j}]_{i,j}\)</span></p>
<p>可以通过FFT实现<span class="math inline">\(O(n\log n)\)</span>复杂度。</p></li>
<li><p><strong>复数PE的梯度流</strong>：</p>
<p>应用Wirtinger微分规则: <span class="math inline">\(\frac{\partial L}{\partial z} = \frac{1}{2}\left(\frac{\partial L}{\partial \text{Re}(z)} - i\frac{\partial L}{\partial \text{Im}(z)}\right)\)</span></p></li>
</ul>
<hr>
<p><strong>结论</strong>： 位置编码是影响变压器模型性能的关键因素，它已经从简单的正弦-余弦函数发展出了多种方式。每种方法都有其独特的优缺点和数学基础，根据问题的特性和要求选择合适的方法非常重要。近年来，受量子计算、生物学等不同领域启发的新位置编码技术正在被研究，因此可以期待未来持续的发展。</p>
</section>
</section>
</div>
</div>
</section>
</section>
<section id="变压器的完整架构" class="level2">
<h2 class="anchored" data-anchor-id="变压器的完整架构">8.4 变压器的完整架构</h2>
<p>迄今为止，我们已经了解了变压器的关键组成部分是如何发展的。现在让我们看看这些元素是如何整合成一个完整的架构的。这是变压器的完整架构。</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../../assets/images/transformer/tr_architecture.png" class="img-fluid figure-img"></p>
<figcaption>变压器架构</figcaption>
</figure>
</div>
<p><em>图片来源: The Illustrated Transformer (Jay Alammar, 2018) CC BY 4.0 License</em></p>
<p>为了教育目的而实现的变压器源代码位于 chapter_08/transformer。此实现参考并修改了哈佛NLP小组的The Annotated Transformer。主要修改如下。</p>
<ol type="1">
<li><strong>模块化:</strong> 将原本在一个文件中的实现拆分为多个模块，以提高可读性和可重用性。</li>
<li><strong>采用Pre-LN结构:</strong> 与原论文不同，我们在注意力/前馈运算<em>之前</em>应用了层归一化的Pre-LN结构。（最近的研究报告称Pre-LN对学习稳定性和性能更为有利。）</li>
<li><strong>添加<code>TransformerConfig</code>类:</strong> 引入了一个单独的类来管理模型设置，使超参数管理更加方便。</li>
<li><strong>PyTorch风格实现:</strong> 利用<code>nn.ModuleList</code>等PyTorch功能使代码更简洁直观。</li>
<li>实现了Noam优化器但未使用。</li>
</ol>
<section id="基本组件的整合" class="level3">
<h3 class="anchored" data-anchor-id="基本组件的整合">8.4.1 基本组件的整合</h3>
<p>变压器主要由<strong>编码器(Encoder)</strong>和<strong>解码器(Decoder)</strong>组成，各组成部分如下：</p>
<table class="caption-top table">
<colgroup>
<col style="width: 17%">
<col style="width: 39%">
<col style="width: 42%">
</colgroup>
<thead>
<tr class="header">
<th>组件</th>
<th>编码器</th>
<th>解码器</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>多头注意力</strong></td>
<td>自注意力 (Self-Attention)</td>
<td>掩蔽自注意力 (Masked Self-Attention)</td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td>编码器-解码器注意力 (Encoder-Decoder Attention)</td>
</tr>
<tr class="odd">
<td><strong>前馈网络</strong></td>
<td>独立应用于每个位置</td>
<td>独立应用于每个位置</td>
</tr>
<tr class="even">
<td><strong>残差连接</strong></td>
<td>将每个子层(注意力, 前馈)的输入和输出相加</td>
<td>将每个子层(注意力, 前馈)的输入和输出相加</td>
</tr>
<tr class="odd">
<td><strong>层归一化</strong></td>
<td>应用于每个子层的输入 (Pre-LN)</td>
<td>应用于每个子层的输入 (Pre-LN)</td>
</tr>
</tbody>
</table>
<p><strong>编码器层 - 代码</strong></p>
<div id="cell-60" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> TransformerEncoderLayer(nn.Module):</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config):</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.attention <span class="op">=</span> MultiHeadAttention(config)</span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.feed_forward <span class="op">=</span> FeedForward(config)</span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a>        <span class="co"># SublayerConnection for Pre-LN structure</span></span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.sublayer <span class="op">=</span> nn.ModuleList([</span>
<span id="cb33-8"><a href="#cb33-8" aria-hidden="true" tabindex="-1"></a>            SublayerConnection(config) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>)</span>
<span id="cb33-9"><a href="#cb33-9" aria-hidden="true" tabindex="-1"></a>        ])</span>
<span id="cb33-10"><a href="#cb33-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-11"><a href="#cb33-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x, attention_mask<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb33-12"><a href="#cb33-12" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.sublayer[<span class="dv">0</span>](x, <span class="kw">lambda</span> x: <span class="va">self</span>.attention(x, x, x, attention_mask))</span>
<span id="cb33-13"><a href="#cb33-13" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.sublayer[<span class="dv">1</span>](x, <span class="va">self</span>.feed_forward)</span>
<span id="cb33-14"><a href="#cb33-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<ul>
<li><p><strong>多头注意力（Multi-Head Attention）:</strong> 并行计算输入序列中所有位置对之间的关系。每个头从不同的角度分析序列，并综合这些结果以捕捉丰富的上下文信息。（例如，在“The cat sits on the mat”中，不同头部学习主语-谓语、介词短语、冠词-名词关系等）</p></li>
<li><p><strong>前馈网络（Feed-Forward Network）:</strong> 由两个线性变换和GELU激活函数组成的网络，独立应用于每个位置。</p></li>
</ul>
<div id="cell-62" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> FeedForward(nn.Module):</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config):</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.linear1 <span class="op">=</span> nn.Linear(config.hidden_size, config.intermediate_size)</span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.linear2 <span class="op">=</span> nn.Linear(config.intermediate_size, config.hidden_size)</span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.activation <span class="op">=</span> nn.GELU()</span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.linear1(x)</span>
<span id="cb34-10"><a href="#cb34-10" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.activation(x)</span>
<span id="cb34-11"><a href="#cb34-11" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.linear2(x)</span>
<span id="cb34-12"><a href="#cb34-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>前馈网络的必要性与注意力输出的信息密度有关。注意力运算(<span class="math inline">\(\text{Attention}(Q, K, V) = \text{softmax}(\frac{QK^T}{\sqrt{d\_k}})V\)</span>)的结果是<span class="math inline">\(V\)</span>向量的加权和，其中文境信息在<span class="math inline">\(d\_{model}\)</span>维度（论文中为512）内<em>密集</em>。<strong>直接应用ReLU激活函数可能会导致大量这些密集信息的丢失(ReLU将负值变为0)</strong>。因此，前馈网络首先将<span class="math inline">\(d\_{model}\)</span>维度扩展到更大的维度(<span class="math inline">\(4 \times d\_{model}\)</span>，论文中为2048)，以扩大表示空间，然后应用ReLU（或GELU），再将其缩小回原始维度，从而添加非线性。</p>
<div id="cell-64" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> W1(x)    <span class="co"># hidden_size -&gt; intermediate_size (512 -&gt; 2048)</span></span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> ReLU(x)  <span class="co"># or GELU</span></span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> W2(x)    <span class="co"># intermediate_size -&gt; hidden_size (2048 -&gt; 512)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<ul>
<li><p><strong>残差连接 (Residual Connection):</strong> 每个子层（多头注意力或前馈网络）的输入和输出相加。这可以缓解梯度消失/爆炸问题，并帮助深度网络的学习。（参见第7章 残差连接）。</p></li>
<li><p><strong>层归一化(Layer Normalization):</strong> 应用于每个子层的<em>输入</em>（预归一化）。</p></li>
</ul>
<div id="cell-66" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LayerNorm(nn.Module):</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config):</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.gamma <span class="op">=</span> nn.Parameter(torch.ones(config.hidden_size))</span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.beta <span class="op">=</span> nn.Parameter(torch.zeros(config.hidden_size))</span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.eps <span class="op">=</span> config.layer_norm_eps</span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-8"><a href="#cb36-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb36-9"><a href="#cb36-9" aria-hidden="true" tabindex="-1"></a>        mean <span class="op">=</span> x.mean(<span class="op">-</span><span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb36-10"><a href="#cb36-10" aria-hidden="true" tabindex="-1"></a>        std <span class="op">=</span> (x <span class="op">-</span> mean).<span class="bu">pow</span>(<span class="dv">2</span>).mean(<span class="op">-</span><span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>).sqrt()</span>
<span id="cb36-11"><a href="#cb36-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.gamma <span class="op">*</span> (x <span class="op">-</span> mean) <span class="op">/</span> (std <span class="op">+</span> <span class="va">self</span>.eps) <span class="op">+</span> <span class="va">self</span>.beta</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>层归一化是在2016年Ba, Kiros, Hinton的论文”<a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://arxiv.org/abs/1607.06450">Layer Normalization</a>“中提出的技术。批归一化(Batch Normalization)在批维度上进行归一化，而层归一化则是在每个样本的特征维度(feature dimension)计算均值和方差来进行归一化。</p>
<p><strong>层归一化的优点</strong></p>
<ol type="1">
<li><p><strong>独立于批量大小:</strong> 不受批量大小的影响，在小批量大小或在线学习(online learning)环境中也能稳定运行。</p></li>
<li><p><strong>与序列长度无关:</strong> 适用于处理可变长度序列的模型，如RNN、Transformer等。</p></li>
<li><p><strong>稳定和加速学习:</strong> 稳定每个层的输入分布，缓解梯度消失/爆炸问题，并提高学习速度。</p>
<p>在Transformer中使用Pre-LN方法，在每个子层（多头注意力机制、前馈网络）通过<em>之前</em>应用层归一化。</p></li>
</ol>
<p><strong>层归一化的可视化</strong></p>
<div id="cell-68" class="cell" data-execution_count="19">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_08.visualize_layer_norm <span class="im">import</span> visualize_layer_normalization</span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>visualize_layer_normalization()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="08_变压器的诞生_files/figure-html/cell-30-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>========================================
Input Data Shape: (2, 5, 6)
Mean Shape: (2, 5, 1)
Standard Deviation Shape: (2, 5, 1)
Normalized Data Shape: (2, 5, 6)
Gamma (Scale) Values:
 [0.95208258 0.9814341  0.8893665  0.88037934 1.08125258 1.135624  ]
Beta (Shift) Values:
 [-0.00720101  0.10035329  0.0361636  -0.06451198  0.03613956  0.15380366]
Scaled &amp; Shifted Data Shape: (2, 5, 6)
========================================</code></pre>
</div>
</div>
<p>上图逐步展示了层归一化（Layer Normalization）的工作原理。</p>
<ul>
<li><strong>原始数据 (左上角):</strong> 归一化前的数据分布广泛，均值和标准差不一致。</li>
<li><strong>归一化后 (右上角):</strong> 数据聚集在均值0、标准差1附近，完成归一化。</li>
<li><strong>缩放和平移 (中间):</strong> 应用可学习的参数 γ(伽玛, 缩放) 和 β(贝塔, 平移)，对数据分布进行微调。这可以调节模型的表达能力。</li>
<li><strong>热图 (底部):</strong> 基于第一个批次的数据，展示归一化前后的变化以及应用缩放/平移后的各个值的变化。</li>
<li><strong>γ/β 值 (右下角):</strong> 以条形图形式显示每个隐藏维度的 γ 和 β 值。</li>
</ul>
<p>层归一化通过归一化每一层的输入来提高学习的稳定性和速度。</p>
<p><strong>核心:</strong></p>
<ul>
<li>每层输入归一化（均值0，标准差1）</li>
<li>通过可学习的缩放(γ)和平移(β)调节表达能力</li>
<li>与批归一化不同，保持样本间的独立性</li>
</ul>
<p>这些组件（多头注意力、前馈网络、残差连接、层归一化）的组合最大化了每个元素的优点。多头注意力捕捉输入序列的不同方面，前馈网络增加了非线性，而残差连接和层归一化则在深层网络中确保稳定的学习。</p>
</section>
<section id="编码器的构成" class="level3">
<h3 class="anchored" data-anchor-id="编码器的构成">8.4.2 编码器的构成</h3>
<p>Transformer 拥有用于机器翻译的编码器-解码器结构。编码器负责理解源语言（例如：英语），而解码器则负责生成目标语言（例如：法语）。编码器和解码器共享多头注意力机制和前馈网络作为基本组件，但根据各自的目标进行了不同的配置。</p>
<p><strong>编码器与解码器构成对比</strong></p>
<table class="caption-top table">
<colgroup>
<col style="width: 17%">
<col style="width: 31%">
<col style="width: 50%">
</colgroup>
<thead>
<tr class="header">
<th>构成部分</th>
<th>编码器</th>
<th>解码器</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>注意力层数量</td>
<td>1个（自注意力）</td>
<td>2个（带掩码的自注意力，编码器-解码器注意力）</td>
</tr>
<tr class="even">
<td>掩码策略</td>
<td>只使用填充掩码</td>
<td>填充掩码 + 因果关系掩码</td>
</tr>
<tr class="odd">
<td>上下文处理</td>
<td>双向上下文处理</td>
<td>单向上下文处理（自回归的）</td>
</tr>
<tr class="even">
<td>输入引用</td>
<td>仅参考自己的输入</td>
<td>自己的输入 + 编码器的输出</td>
</tr>
</tbody>
</table>
<p>整理了多个注意力术语如下。</p>
<p><strong>注意力概念整理</strong></p>
<table class="caption-top table">
<colgroup>
<col style="width: 14%">
<col style="width: 25%">
<col style="width: 4%">
<col style="width: 55%">
</colgroup>
<thead>
<tr class="header">
<th>注意力种类</th>
<th>特征</th>
<th>说明位置</th>
<th>核心概念</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>注意力（基础）</td>
<td>- 使用相同的词向量计算相似度<br>- 通过简单的加权和生成上下文信息<br>- 是seq2seq模型应用的简化版本</td>
<td>8.2.2</td>
<td>- 通过词向量间的内积计算相似度<br>- 使用softmax转换权重<br>- 对所有注意力默认应用填充掩码</td>
</tr>
<tr class="even">
<td>自注意力 (Self-Attention)</td>
<td>- Q, K, V空间分离<br>- 每个空间独立优化<br>- 输入序列引用自身<br>- 在编码器中使用</td>
<td>8.2.3</td>
<td>- 分离相似度计算和信息传递的作用<br>- 学习Q, K, V变换<br>- 可能实现双向上下文处理</td>
</tr>
<tr class="odd">
<td>带掩码的自注意力</td>
<td>- 阻止未来信息<br>- 使用因果关系掩码<br>- 在解码器中使用</td>
<td>8.2.5</td>
<td>- 使用上三角矩阵对未来的数据进行掩码<br>- 可能实现自回归生成<br>- 实现单向上下文处理</td>
</tr>
<tr class="even">
<td>交叉（编码器-解码器）注意力</td>
<td>- Query: 解码器状态<br>- Key, Value: 编码器输出<br>- 也称为交叉注意力<br>- 在解码器中使用</td>
<td>8.4.3</td>
<td>- 解码器引用编码器的信息<br>- 计算两个序列之间的关系<br>- 翻译/生成时反映上下文</td>
</tr>
</tbody>
</table>
<p>在Transformer中，使用了自、带掩码的和交叉注意力名称。注意力机制是相同的，区别在于Q, K, V的来源。</p>
<p><strong>编码器构成组件</strong> | 构成部分 | 描述 | | ————————– | ————————————————————————————- | | Embeddings | 将输入令牌转换为向量，并添加位置信息以编码输入序列的含义和顺序。 | | TransformerEncoderLayer (x N) | 堆叠相同的层，从输入序列中分层提取更抽象和复杂的特征。 | | LayerNorm | 规范化最终输出的特征分布，使其稳定并形成解码器可以参考的形式。 |</p>
<div id="cell-71" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> TransformerEncoder(nn.Module):</span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config):</span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.embeddings <span class="op">=</span> Embeddings(config)</span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.layers <span class="op">=</span> nn.ModuleList([</span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a>            TransformerEncoderLayer(config) </span>
<span id="cb39-7"><a href="#cb39-7" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(config.num_hidden_layers)</span>
<span id="cb39-8"><a href="#cb39-8" aria-hidden="true" tabindex="-1"></a>        ])</span>
<span id="cb39-9"><a href="#cb39-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.norm <span class="op">=</span> LayerNorm(config)</span>
<span id="cb39-10"><a href="#cb39-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb39-11"><a href="#cb39-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, input_ids, attention_mask<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb39-12"><a href="#cb39-12" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.embeddings(input_ids)</span>
<span id="cb39-13"><a href="#cb39-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-14"><a href="#cb39-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i, layer <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="va">self</span>.layers):</span>
<span id="cb39-15"><a href="#cb39-15" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> layer(x, attention_mask)</span>
<span id="cb39-16"><a href="#cb39-16" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb39-17"><a href="#cb39-17" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> <span class="va">self</span>.norm(x)</span>
<span id="cb39-18"><a href="#cb39-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> output</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>编码器由嵌入层、多个编码器层和最终的归一化层组成。</p>
<p><strong>1. 自注意力机制 (示例)</strong></p>
<p>编码器的自注意力计算输入序列中所有单词对之间的关系，以丰富每个单词的上下文信息。</p>
<ul>
<li><strong>示例:</strong> “The patient bear can bear the pain no longer.”</li>
<li><strong>作用:</strong> 在理解第二个’bear’的意义时，自注意力会考虑句子中<em>所有</em>单词的关系，包括’patient’ (病人), ‘bear’ (熊), 和 ‘pain’ (痛苦)。通过这种方式，它准确地识别出’bear’在这里是指“忍受”，“忍耐”（双向上下文处理）。</li>
</ul>
<p><strong>2. Dropout位置的重要性</strong></p>
<p>Dropout在防止过拟合和提高学习稳定性方面起着重要作用。在Transformer编码器中，Dropout应用于以下位置：</p>
<ul>
<li><strong>嵌入输出后:</strong> 在令牌嵌入和位置信息结合之后。</li>
<li><strong>每个子层(注意力, FFN)输出后:</strong> 遵循Pre-LN结构 (归一化 → 子层 → Dropout → 残差连接)。</li>
<li><strong>FFN内部:</strong> 在第一次线性转换及ReLU激活函数应用后。</li>
</ul>
<p>这种Dropout布置调节信息流动，防止模型过度依赖特定特征，并提高泛化性能。</p>
<p><strong>3. 编码器堆叠结构</strong></p>
<p>Transformer编码器具有多个相同结构的编码器层堆叠(stacked)的结构。</p>
<ul>
<li><strong>原论文:</strong> 使用6个编码器层。</li>
<li><strong>任务分配:</strong>
<ul>
<li><strong>底层:</strong> 学习相邻单词、标点符号等表面语言模式。</li>
<li><strong>中间层:</strong> 学习语法结构。</li>
<li><strong>顶层:</strong> 学习如共指(coreference)等高层次的语义关系。</li>
</ul></li>
</ul>
<p>堆叠层数越深，可以学习到更抽象和复杂的特征。后续研究中，得益于硬件及学习技术的发展（Pre-LayerNorm, 梯度裁剪, 学习率预热, 混合精度训练, 梯度累积等），出现了具有更多层的模型（BERT-base: 12层, GPT-3: 96层, PaLM: 118层）。</p>
<p><strong>4. 编码器的最终输出和解码器利用</strong></p>
<p>编码器的最终输出是包含每个输入令牌丰富上下文信息的向量表示。这些输出在解码器的<strong>编码器-解码器注意力 (Cross-Attention)</strong>中作为<strong>Key</strong>和<strong>Value</strong>使用。解码器在生成输出序列的每个令牌时都会参考编码器的输出，以考虑原文句子的上下文进行准确的翻译/生成。</p>
</section>
<section id="解码器的构成" class="level3">
<h3 class="anchored" data-anchor-id="解码器的构成">8.4.3 解码器的构成</h3>
<p>解码器与编码器类似，但不同之处在于它以自回归(autoregressive)的方式生成输出。</p>
<p><strong>解码器层完整代码</strong></p>
<div id="cell-74" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> TransformerDecoderLayer(nn.Module):</span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config):</span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.self_attn <span class="op">=</span> MultiHeadAttention(config)</span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.cross_attn <span class="op">=</span> MultiHeadAttention(config)</span>
<span id="cb40-6"><a href="#cb40-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.feed_forward <span class="op">=</span> FeedForward(config)</span>
<span id="cb40-7"><a href="#cb40-7" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb40-8"><a href="#cb40-8" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Pre-LN을 위한 레이어 정규화</span></span>
<span id="cb40-9"><a href="#cb40-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.norm1 <span class="op">=</span> LayerNorm(config)</span>
<span id="cb40-10"><a href="#cb40-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.norm2 <span class="op">=</span> LayerNorm(config)</span>
<span id="cb40-11"><a href="#cb40-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.norm3 <span class="op">=</span> LayerNorm(config)</span>
<span id="cb40-12"><a href="#cb40-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(config.dropout_prob)</span>
<span id="cb40-13"><a href="#cb40-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-14"><a href="#cb40-14" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x, memory, src_mask<span class="op">=</span><span class="va">None</span>, tgt_mask<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb40-15"><a href="#cb40-15" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Pre-LN 구조</span></span>
<span id="cb40-16"><a href="#cb40-16" aria-hidden="true" tabindex="-1"></a>        m <span class="op">=</span> <span class="va">self</span>.norm1(x)</span>
<span id="cb40-17"><a href="#cb40-17" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.dropout(<span class="va">self</span>.self_attn(m, m, m, tgt_mask))</span>
<span id="cb40-18"><a href="#cb40-18" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb40-19"><a href="#cb40-19" aria-hidden="true" tabindex="-1"></a>        m <span class="op">=</span> <span class="va">self</span>.norm2(x)</span>
<span id="cb40-20"><a href="#cb40-20" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.dropout(<span class="va">self</span>.cross_attn(m, memory, memory, src_mask))</span>
<span id="cb40-21"><a href="#cb40-21" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb40-22"><a href="#cb40-22" aria-hidden="true" tabindex="-1"></a>        m <span class="op">=</span> <span class="va">self</span>.norm3(x)</span>
<span id="cb40-23"><a href="#cb40-23" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.dropout(<span class="va">self</span>.feed_forward(m))</span>
<span id="cb40-24"><a href="#cb40-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><strong>解码器的主要组成部分及作用</strong></p>
<table class="caption-top table">
<colgroup>
<col style="width: 9%">
<col style="width: 43%">
<col style="width: 47%">
</colgroup>
<thead>
<tr class="header">
<th>子层</th>
<th>作用</th>
<th>实现特点</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>掩码自注意力</td>
<td>确定已生成的输出序列中的词之间的关系，防止引用未来信息（自回归生成）</td>
<td>使用 <code>tgt_mask</code>（因果掩码 + 填充掩码），<code>self.self_attn</code></td>
</tr>
<tr class="even">
<td>编码器-解码器注意力 (交叉注意力)</td>
<td>解码器参考编码器的输出（输入句子的上下文信息）以获取与当前要生成的词相关的信息</td>
<td><code>Q</code>: 解码器，<code>K</code>, <code>V</code>: 编码器，使用 <code>src_mask</code>（填充掩码），<code>self.cross_attn</code></td>
</tr>
<tr class="odd">
<td>前馈网络</td>
<td>独立转换每个位置的表示以生成更丰富的表示</td>
<td>与编码器相同的结构，<code>self.feed_forward</code></td>
</tr>
<tr class="even">
<td>层归一化 (LayerNorm)</td>
<td>每个子层的输入归一化（预LN），提高学习稳定性及性能</td>
<td><code>self.norm1</code>, <code>self.norm2</code>, <code>self.norm3</code></td>
</tr>
<tr class="odd">
<td>Dropout</td>
<td>防止过拟合，提高泛化性能</td>
<td>应用于每个子层的输出，<code>self.dropout</code></td>
</tr>
<tr class="even">
<td>残差连接 (Residual Connection)</td>
<td>缓解深度网络中的梯度消失/爆炸问题，改善信息流动</td>
<td>将每个子层的输入和输出相加</td>
</tr>
</tbody>
</table>
<p><strong>1. 掩码自注意力 (Masked Self-Attention)</strong> * <strong>角色:</strong> 使解码器以自回归(autoregressive)方式生成输出。即，不允许参考当前生成的词之前的<em>未来</em>词。例如，在翻译 “I love you” 时，生成 “我是” 后，在生成 “你” 的时候还不能参考尚未生成的 “爱” token。 * <strong>实现:</strong> 使用结合了因果关系掩码(causal mask)和填充掩码(padding mask)的 <code>tgt_mask</code>。因果关系掩码将上三角矩阵填满 <code>-inf</code> 以使未来令牌的注意力权重为 0。(参见第8.2.5节)。在 <code>TransformerDecoderLayer</code> 的 <code>forward</code> 方法中的 <code>self.self_attn(m, m, m, tgt_mask)</code> 部分应用了此掩码。</p>
<p><strong>2. 编解码器注意力 (交叉注意力)</strong></p>
<ul>
<li><strong>角色:</strong> 使解码器能够参考编码器的输出（输入句子的上下文信息）来获取与当前生成词相关的更多信息。这是在翻译任务中，解码器准确理解源句的意义并选择合适的翻译词的关键。</li>
<li><strong>实现:</strong>
<ul>
<li><strong>Query (Q):</strong> 解码器的当前状态 (掩码自注意力的输出)</li>
<li><strong>Key (K):</strong> 编码器的输出 (<code>memory</code>)</li>
<li><strong>Value (V):</strong> 编码器的输出 (<code>memory</code>)</li>
<li>使用 <code>src_mask</code>（填充掩码）忽略编码器输出中的填充令牌。</li>
<li>在 <code>TransformerDecoderLayer</code> 的 <code>forward</code> 方法中，<code>self.cross_attn(m, memory, memory, src_mask)</code> 部分执行此注意力。<code>memory</code> 表示编码器的输出。</li>
</ul></li>
</ul>
<p><strong>3. 解码器堆栈结构</strong></p>
<div id="cell-76" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> TransformerDecoder(nn.Module):</span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config):</span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.embeddings <span class="op">=</span> Embeddings(config)</span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.layers <span class="op">=</span> nn.ModuleList([</span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a>            TransformerDecoderLayer(config)</span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(config.num_hidden_layers)</span>
<span id="cb41-8"><a href="#cb41-8" aria-hidden="true" tabindex="-1"></a>        ])</span>
<span id="cb41-9"><a href="#cb41-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.norm <span class="op">=</span> LayerNorm(config)</span>
<span id="cb41-10"><a href="#cb41-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-11"><a href="#cb41-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x, memory, src_mask<span class="op">=</span><span class="va">None</span>, tgt_mask<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb41-12"><a href="#cb41-12" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.embeddings(x)</span>
<span id="cb41-13"><a href="#cb41-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> layer <span class="kw">in</span> <span class="va">self</span>.layers:</span>
<span id="cb41-14"><a href="#cb41-14" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> layer(x, memory, src_mask, tgt_mask)</span>
<span id="cb41-15"><a href="#cb41-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.norm(x)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<ul>
<li>解码器由多个（原论文中为6个）<code>TransformerDecoderLayer</code>层堆叠组成。</li>
<li>每一层依次执行掩码自注意力、编码器-解码器注意力和前馈网络。</li>
<li>在每个子层应用了Pre-LN结构和残差连接。这使得即使在深度网络中也能实现稳定的训练。</li>
<li><code>TransformerDecoder</code>类的<code>forward</code>方法接收输入<code>x</code>（解码器输入）、<code>memory</code>（编码器输出）、<code>src_mask</code>（编码器填充掩码）、<code>tgt_mask</code>（解码器掩码），将它们依次通过解码器层，最后返回最终输出。</li>
</ul>
<p><strong>不同模型的编码器/解码器层数</strong></p>
<table class="caption-top table">
<colgroup>
<col style="width: 25%">
<col style="width: 8%">
<col style="width: 20%">
<col style="width: 13%">
<col style="width: 13%">
<col style="width: 17%">
</colgroup>
<thead>
<tr class="header">
<th>模型</th>
<th>年份</th>
<th>结构</th>
<th>编码器层数</th>
<th>解码器层数</th>
<th>总参数数</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>原论文变压器</td>
<td>2017</td>
<td>编码器-解码器</td>
<td>6</td>
<td>6</td>
<td>65M</td>
</tr>
<tr class="even">
<td>BERT-base</td>
<td>2018</td>
<td>仅编码器</td>
<td>12</td>
<td>-</td>
<td>110M</td>
</tr>
<tr class="odd">
<td>GPT-2</td>
<td>2019</td>
<td>仅解码器</td>
<td>-</td>
<td>48</td>
<td>1.5B</td>
</tr>
<tr class="even">
<td>T5-base</td>
<td>2020</td>
<td>编码器-解码器</td>
<td>12</td>
<td>12</td>
<td>220M</td>
</tr>
<tr class="odd">
<td>GPT-3</td>
<td>2020</td>
<td>仅解码器</td>
<td>-</td>
<td>96</td>
<td>175B</td>
</tr>
<tr class="even">
<td>PaLM</td>
<td>2022</td>
<td>仅解码器</td>
<td>-</td>
<td>118</td>
<td>540B</td>
</tr>
<tr class="odd">
<td>Gemma-2</td>
<td>2024</td>
<td>仅解码器</td>
<td>-</td>
<td>18-36</td>
<td>2B-27B</td>
</tr>
</tbody>
</table>
<p>最近的模型由于采用了如Pre-LN等先进的训练技术，能够更有效地学习更多的层。更深的解码器可以学习到更加抽象和复杂的语言模式，在翻译、文本生成等各种自然语言处理任务中带来了性能的提升。</p>
<p><strong>4. 解码器输出生成及终止条件</strong></p>
<ul>
<li><strong>输出生成:</strong> <code>Transformer</code>类中的<code>generator</code>（线性层）将解码器的最终输出转换为词汇大小(vocab_size)的logit向量，并应用<code>log_softmax</code>以获得每个令牌的概率分布。根据此概率分布预测下一个令牌。</li>
</ul>
<div id="cell-78" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 최종 출력 생성 (설명용)</span></span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> <span class="va">self</span>.generator(decoder_output)</span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a><span class="cf">return</span> F.log_softmax(output, dim<span class="op">=-</span><span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<ul>
<li><strong>终止条件</strong>
<ol type="1">
<li><strong>达到最大长度:</strong> 达到预先设定的最大输出长度。</li>
<li><strong>用户定义的终止条件:</strong> 满足特定条件（如：标点符号）时。</li>
<li><strong>生成特殊标记:</strong> 生成表示句子结束的特殊标记（<code>&lt;eos&gt;</code>，<code>&lt;/s&gt;</code> 等）。解码器在训练过程中学习了如何在句子结束处添加这些特殊标记。</li>
</ol></li>
<li><strong>令牌生成策略</strong></li>
</ul>
<p>通常不包含在解码器中但会影响输出生成结果的是令牌生成策略。</p>
<table class="caption-top table">
<colgroup>
<col style="width: 16%">
<col style="width: 13%">
<col style="width: 32%">
<col style="width: 17%">
<col style="width: 20%">
</colgroup>
<thead>
<tr class="header">
<th>生成策略</th>
<th>工作原理</th>
<th>优点</th>
<th>缺点</th>
<th>示例</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>贪心搜索</strong></td>
<td>每一步选择最高概率的令牌</td>
<td>快速，实现简单</td>
<td>可能陷入局部最优解，缺乏多样性</td>
<td>“我”之后 → “去学校” (最高概率)</td>
</tr>
<tr class="even">
<td><strong>束搜索</strong></td>
<td>同时跟踪 <code>k</code> 条路径</td>
<td>较宽的搜索范围，可能获得更好的结果</td>
<td>计算成本高，多样性有限</td>
<td><code>k=2</code>: 保持 “我去学校”, “我回家” 然后进行下一步</td>
</tr>
<tr class="odd">
<td><strong>Top-k 抽样</strong></td>
<td>从概率最高的 <code>k</code> 个中按概率比例选择</td>
<td>适当的多样性，防止异常令牌</td>
<td>设置 <code>k</code> 值困难，性能依赖于上下文</td>
<td><code>k=3</code>: “我”之后 → {“去学校”, “回家”, “去公园”} 中根据概率选择</td>
</tr>
<tr class="even">
<td><strong>核抽样</strong></td>
<td>选择累积概率达到 <code>p</code> 的令牌</td>
<td>动态候选集，对上下文灵活</td>
<td>需要调整 <code>p</code> 值，计算复杂度增加</td>
<td><code>p=0.9</code>: “我”之后 → {“去学校”, “回家”, “去公园”, “吃饭”} 中选择累积概率不超过 0.9 的令牌</td>
</tr>
<tr class="odd">
<td><strong>温度抽样</strong></td>
<td>调整概率分布的温度（低则确定，高则多样）</td>
<td>调节输出创造性，实现简单</td>
<td>过高可能不合适，过低可能导致重复文本生成</td>
<td><code>T=0.5</code>: 强调高概率，<code>T=1.5</code>: 也有选择低概率的可能性</td>
</tr>
</tbody>
</table>
<p>这些令牌生成策略通常作为单独的类或函数与解码器分开实现。</p>
</section>
<section id="整体结构的说明" class="level3">
<h3 class="anchored" data-anchor-id="整体结构的说明">8.4.4 整体结构的说明</h3>
<p>迄今为止，我们已经理解了变压器的设计意图和工作原理。在前文8.4.3节的基础上，我们将审视变压器的整体结构。实现时参考了Havard NLP的内容，并进行了模块化等结构性修改，尽可能简洁地编写以服务于学习目的。实际生产环境中还需要添加代码稳定性类型提示、多维张量的高效处理、输入验证和错误处理、内存优化以及支持各种设置的可扩展性等。</p>
<p>代码位于 <code>chapter_08/transformer</code> 目录中。</p>
<p><strong>嵌入层的作用与实现</strong></p>
<p>变压器的第一步是将输入令牌转换为向量空间的嵌入层。输入是一个整数令牌ID序列（例如：[101, 2045, 3012, …]），每个令牌ID是词汇表中的唯一索引。嵌入层将这些ID映射到高维向量（嵌入向量）。</p>
<p>嵌入维度对模型性能有很大影响。大维度可以表达丰富的语义信息，但计算成本会增加；小维度则相反。</p>
<p>通过嵌入层后，张量维度如下变化：</p>
<ul>
<li>输入: (batch_size, seq_length) → 输出: (batch_size, seq_length, hidden_size)</li>
<li>例如：(32, 50) → (32, 50, 768)</li>
</ul>
<p>以下是变压器中执行嵌入的代码示例。</p>
<div id="cell-81" class="cell" data-execution_count="20">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_08.transformer.config <span class="im">import</span> TransformerConfig</span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_08.transformer.embeddings <span class="im">import</span> Embeddings</span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a configuration object</span></span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a>config <span class="op">=</span> TransformerConfig()</span>
<span id="cb43-7"><a href="#cb43-7" aria-hidden="true" tabindex="-1"></a>config.vocab_size <span class="op">=</span> <span class="dv">1000</span>  <span class="co"># Vocabulary size</span></span>
<span id="cb43-8"><a href="#cb43-8" aria-hidden="true" tabindex="-1"></a>config.hidden_size <span class="op">=</span> <span class="dv">768</span>  <span class="co"># Embedding dimension</span></span>
<span id="cb43-9"><a href="#cb43-9" aria-hidden="true" tabindex="-1"></a>config.max_position_embeddings <span class="op">=</span> <span class="dv">512</span>  <span class="co"># Maximum sequence length</span></span>
<span id="cb43-10"><a href="#cb43-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-11"><a href="#cb43-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Create an embedding layer</span></span>
<span id="cb43-12"><a href="#cb43-12" aria-hidden="true" tabindex="-1"></a>embedding_layer <span class="op">=</span> Embeddings(config)</span>
<span id="cb43-13"><a href="#cb43-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-14"><a href="#cb43-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate random input tokens</span></span>
<span id="cb43-15"><a href="#cb43-15" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb43-16"><a href="#cb43-16" aria-hidden="true" tabindex="-1"></a>seq_length <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb43-17"><a href="#cb43-17" aria-hidden="true" tabindex="-1"></a>input_ids <span class="op">=</span> torch.tensor([</span>
<span id="cb43-18"><a href="#cb43-18" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">1</span>, <span class="dv">5</span>, <span class="dv">9</span>, <span class="dv">2</span>],  <span class="co"># First sequence</span></span>
<span id="cb43-19"><a href="#cb43-19" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">6</span>, <span class="dv">3</span>, <span class="dv">7</span>, <span class="dv">4</span>]   <span class="co"># Second sequence</span></span>
<span id="cb43-20"><a href="#cb43-20" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb43-21"><a href="#cb43-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-22"><a href="#cb43-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform embedding</span></span>
<span id="cb43-23"><a href="#cb43-23" aria-hidden="true" tabindex="-1"></a>embedded <span class="op">=</span> embedding_layer(input_ids)</span>
<span id="cb43-24"><a href="#cb43-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-25"><a href="#cb43-25" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Input shape: </span><span class="sc">{</span>input_ids<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb43-26"><a href="#cb43-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Output: Input shape: torch.Size([2, 4])</span></span>
<span id="cb43-27"><a href="#cb43-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-28"><a href="#cb43-28" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Shape after embedding: </span><span class="sc">{</span>embedded<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb43-29"><a href="#cb43-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Output: Shape after embedding: torch.Size([2, 4, 768])</span></span>
<span id="cb43-30"><a href="#cb43-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-31"><a href="#cb43-31" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Part of the embedding vector for the first token of the first sequence:"</span>)</span>
<span id="cb43-32"><a href="#cb43-32" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(embedded[<span class="dv">0</span>, <span class="dv">0</span>, :<span class="dv">10</span>])  <span class="co"># Print only the first 10 dimensions</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Input shape: torch.Size([2, 4])
Shape after embedding: torch.Size([2, 4, 768])

Part of the embedding vector for the first token of the first sequence:
tensor([-0.7838, -0.9194,  0.4240, -0.8408, -0.0876,  2.0239,  1.3892, -0.4484,
        -0.6902,  1.1443], grad_fn=&lt;SliceBackward0&gt;)</code></pre>
</div>
</div>
<p><strong>设置类</strong></p>
<p><code>TransformerConfig</code> 类定义了模型的所有超参数。</p>
<div id="cell-83" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> TransformerConfig:</span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.vocab_size <span class="op">=</span> <span class="dv">30000</span>          <span class="co"># Vocabulary size</span></span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hidden_size <span class="op">=</span> <span class="dv">768</span>           <span class="co"># Hidden layer dimension</span></span>
<span id="cb45-5"><a href="#cb45-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.num_hidden_layers <span class="op">=</span> <span class="dv">12</span>      <span class="co"># Number of encoder/decoder layers</span></span>
<span id="cb45-6"><a href="#cb45-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.num_attention_heads <span class="op">=</span> <span class="dv">12</span>    <span class="co"># Number of attention heads</span></span>
<span id="cb45-7"><a href="#cb45-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.intermediate_size <span class="op">=</span> <span class="dv">3072</span>    <span class="co"># FFN intermediate layer dimension</span></span>
<span id="cb45-8"><a href="#cb45-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hidden_dropout_prob <span class="op">=</span> <span class="fl">0.1</span>   <span class="co"># Hidden layer dropout probability</span></span>
<span id="cb45-9"><a href="#cb45-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.attention_probs_dropout_prob <span class="op">=</span> <span class="fl">0.1</span>  <span class="co"># Attention dropout probability</span></span>
<span id="cb45-10"><a href="#cb45-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.max_position_embeddings <span class="op">=</span> <span class="dv">512</span>  <span class="co"># Maximum sequence length</span></span>
<span id="cb45-11"><a href="#cb45-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.layer_norm_eps <span class="op">=</span> <span class="fl">1e-12</span>      <span class="co"># Layer normalization epsilon</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><code>vocab_size</code> 是模型可以处理的唯一令牌总数。这里为了简单实现，假设以单词为单位进行分词，并将其设置为30,000个。在实际的语言模型中，会使用BPE（Byte Pair Encoding）、Unigram、WordPiece等不同的子词分词器，在这种情况下，<code>vocab_size</code> 可能会更小。例如，可以将 ‘playing’ 这个单词拆分为 ‘play’ 和 ‘ing’ 两个子词进行表示。</p>
<p><strong>注意力的张量维度变化</strong></p>
<p>在多头注意力中，每个头为了独立计算注意力而重新排列输入张量的维度。</p>
<div id="cell-85" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MultiHeadAttention(nn.Module):</span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, query, key, value, mask<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a>        batch_size <span class="op">=</span> query.size(<span class="dv">0</span>)</span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb46-5"><a href="#cb46-5" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Linear transformations and head splitting</span></span>
<span id="cb46-6"><a href="#cb46-6" aria-hidden="true" tabindex="-1"></a>        query <span class="op">=</span> <span class="va">self</span>.linears[<span class="dv">0</span>](query).view(batch_size, <span class="op">-</span><span class="dv">1</span>, <span class="va">self</span>.h, <span class="va">self</span>.d_k).transpose(<span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb46-7"><a href="#cb46-7" aria-hidden="true" tabindex="-1"></a>        key <span class="op">=</span> <span class="va">self</span>.linears[<span class="dv">1</span>](key).view(batch_size, <span class="op">-</span><span class="dv">1</span>, <span class="va">self</span>.h, <span class="va">self</span>.d_k).transpose(<span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb46-8"><a href="#cb46-8" aria-hidden="true" tabindex="-1"></a>        value <span class="op">=</span> <span class="va">self</span>.linears[<span class="dv">2</span>](value).view(batch_size, <span class="op">-</span><span class="dv">1</span>, <span class="va">self</span>.h, <span class="va">self</span>.d_k).transpose(<span class="dv">1</span>, <span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>维度转换过程如下。</p>
<ol type="1">
<li>输入: (batch_size, seq_len, d_model)</li>
<li>线性变换: (batch_size, seq_len, d_model)</li>
<li><code>view</code>: (batch_size, seq_len, h, d_k)</li>
<li><code>transpose</code>: (batch_size, h, seq_len, d_k)</li>
</ol>
<p>这里 h 是头数，d_k 是每个头的维度(d_model / h)。通过这种维度重排，每个头可以独立地计算注意力。</p>
<p><strong>变压器的集成结构</strong></p>
<p>最后，我们将查看将所有组件整合在一起的 <code>Transformer</code> 类。</p>
<div id="cell-87" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Transformer(nn.Module):</span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config: TransformerConfig):</span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb47-4"><a href="#cb47-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.encoder <span class="op">=</span> TransformerEncoder(config)</span>
<span id="cb47-5"><a href="#cb47-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.decoder <span class="op">=</span> TransformerDecoder(config)</span>
<span id="cb47-6"><a href="#cb47-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.generator <span class="op">=</span> nn.Linear(config.hidden_size, config.vocab_size)</span>
<span id="cb47-7"><a href="#cb47-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._init_weights()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>变压器由三个主要组件组成。</p>
<ol type="1">
<li>编码器：处理输入序列。</li>
<li>解码器：生成输出序列。</li>
<li>生成器：将解码器输出转换为词汇概率。</li>
</ol>
<p><code>forward</code> 方法按以下顺序处理数据。</p>
<div id="cell-89" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> forward(<span class="va">self</span>, src, tgt, src_mask<span class="op">=</span><span class="va">None</span>, tgt_mask<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Encoder-decoder processing</span></span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a>    encoder_output <span class="op">=</span> <span class="va">self</span>.encode(src, src_mask)</span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a>    decoder_output <span class="op">=</span> <span class="va">self</span>.decode(encoder_output, src_mask, tgt, tgt_mask)</span>
<span id="cb48-5"><a href="#cb48-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb48-6"><a href="#cb48-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Generate final output</span></span>
<span id="cb48-7"><a href="#cb48-7" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> <span class="va">self</span>.generator(decoder_output)</span>
<span id="cb48-8"><a href="#cb48-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> F.log_softmax(output, dim<span class="op">=-</span><span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>张量的维度变化如下。</p>
<ol type="1">
<li>输入 (<code>src</code>, <code>tgt</code>): (batch_size, seq_len)</li>
<li>编码器输出: (batch_size, src_len, hidden_size)</li>
<li>解码器输出: (batch_size, tgt_len, hidden_size)</li>
<li>最终输出: (batch_size, tgt_len, vocab_size)</li>
</ol>
<p>下一节中，我们将把这个结构应用到实际例子中。</p>
</section>
</section>
<section id="变压器示例" class="level2">
<h2 class="anchored" data-anchor-id="变压器示例">8.5 变压器示例</h2>
<p>我们已经探讨了变压器的结构和工作原理。现在，让我们通过实际示例来了解变压器的操作。这些示例按照难度顺序排列，并且每个示例都旨在帮助理解变压器的特定功能。它们展示了如何逐步解决在实际项目中遇到的各种数据处理和模型设计问题。特别是涉及数据预处理、损失函数设计、评估指标设置等内容。示例的位置在 transformer/examples 中。</p>
<pre class="text"><code>examples
├── addition_task.py  # 8.5.2 加法任务
├── copy_task.py      # 8.5.1 简单复制任务
└── parser_task.py    # 8.5.3 解析器任务</code></pre>
<p>每个示例的学习内容如下。</p>
<p><strong>简单复制任务</strong>可以帮助理解变压器的基本功能。通过注意力模式可视化，可以更清楚地理解模型的工作原理。此外，还可以学习序列数据的基本处理方法、为批处理设计的张量维度、基本填充和掩码策略以及针对特定任务的损失函数设计。</p>
<p><strong>数字加法问题</strong>展示了如何实现自回归生成。可以清晰观察到解码器的顺序生成过程及其交叉注意力的作用。同时，还提供了关于数字数据标记化、有效数据集生成方法、部分/整体准确度评估以及随着位数扩展的一般性能测试的实际经验。</p>
<p><strong>解析器任务</strong>展示了变压器如何学习和表示结构关系。可以通过注意力机制理解输入序列中的层次结构是如何被捕获的。此外，还可以掌握结构化数据的序列转换、词典设计、树结构的线性化策略以及结构准确性的评估方法等实际解析问题中所需的多种技术。</p>
<p>以下是每个示例的学习内容总结表。</p>
<table class="caption-top table">
<colgroup>
<col style="width: 71%">
<col style="width: 28%">
</colgroup>
<thead>
<tr class="header">
<th>示例</th>
<th>学习内容</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>8.5.1 简单复制任务 (copy_task.py)</td>
<td>- 变压器基本功能及工作原理理解<br> - 通过注意力模式可视化进行直观理解<br> - 序列数据处理及批处理的张量维度设计<br> - 填充和掩码策略<br> - 针对特定任务的损失函数设计</td>
</tr>
<tr class="even">
<td>8.5.2 加法问题任务 (addition_task.py)</td>
<td>- 学习变压器的自回归(autoregressive)生成过程<br> - 观察解码器的顺序生成及交叉注意力(cross-attention)的作用<br> - 数字数据标记化、有效数据集生成方法<br> - 部分/整体准确度评估，随位数扩展的一般性能测试</td>
</tr>
<tr class="odd">
<td>8.5.3 解析器任务 (parser_task.py)</td>
<td>- 理解变压器学习和表示结构关系的方法<br> - 理解注意力机制如何捕获输入序列的层次结构<br> - 结构化数据的序列转换、词典设计<br> - 树结构线性化策略，结构准确性评估方法</td>
</tr>
</tbody>
</table>
<section id="简单复制任务" class="level3">
<h3 class="anchored" data-anchor-id="简单复制任务">8.5.1 简单复制任务</h3>
<p>第一个示例是将输入序列直接输出的复制任务。此任务适合验证变压器的基本操作和注意力模式可视化，虽然看似简单，但对于理解变压器的核心机制非常有用。</p>
<p><strong>数据准备</strong></p>
<p>复制任务的数据由输入和输出相同的序列组成。以下是数据生成示例。</p>
<div id="cell-92" class="cell" data-execution_count="1">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_08.transformer.examples.copy_task <span class="im">import</span> explain_copy_data</span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a>explain_copy_data(seq_length<span class="op">=</span><span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
=== Copy Task Data Explanation ===
Sequence Length: 5

1. Input Sequence:
Original Tensor Shape: torch.Size([1, 5])
Input Sequence: [7, 15, 2, 3, 12]

2. Target Sequence:
Original Tensor Shape: torch.Size([1, 5])
Target Sequence: [7, 15, 2, 3, 12]

3. Task Description:
- Basic task of copying the input sequence as is
- Tokens at each position are integer values between 1-19
- Input and output have the same sequence length
- Current Example: [7, 15, 2, 3, 12] → [7, 15, 2, 3, 12]</code></pre>
</div>
</div>
<p>create_copy_data 生成用于学习的张量，其输入和输出相同。它生成一个二维张量 (batch_size, seq_length)，用于批处理，每个元素是 1 到 19 之间的整数值。</p>
<div id="cell-94" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_copy_data(batch_size: <span class="bu">int</span> <span class="op">=</span> <span class="dv">32</span>, seq_length: <span class="bu">int</span> <span class="op">=</span> <span class="dv">5</span>) <span class="op">-&gt;</span> torch.Tensor:</span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""복사 태스크용 데이터 생성"""</span></span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a>    sequences <span class="op">=</span> torch.randint(<span class="dv">1</span>, <span class="dv">20</span>, (batch_size, seq_length))</span>
<span id="cb52-4"><a href="#cb52-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> sequences, sequences</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>这个例子的数据与自然语言处理或序列建模中使用的标记化输入数据相同。在语言处理中，每个标记在输入模型之前都会转换为唯一的整数值。</p>
<p><strong>模型训练</strong></p>
<p>使用以下代码对模型进行训练。</p>
<div id="cell-96" class="cell" data-execution_count="1">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_08.transformer.config <span class="im">import</span> TransformerConfig</span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_08.transformer.examples.copy_task <span class="im">import</span> train_copy_task</span>
<span id="cb53-3"><a href="#cb53-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-4"><a href="#cb53-4" aria-hidden="true" tabindex="-1"></a>seq_length <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb53-5"><a href="#cb53-5" aria-hidden="true" tabindex="-1"></a>config <span class="op">=</span> TransformerConfig()</span>
<span id="cb53-6"><a href="#cb53-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Modify default values</span></span>
<span id="cb53-7"><a href="#cb53-7" aria-hidden="true" tabindex="-1"></a>config.vocab_size <span class="op">=</span> <span class="dv">20</span>           <span class="co"># Small vocabulary size (minimum size to represent integers 1-19)</span></span>
<span id="cb53-8"><a href="#cb53-8" aria-hidden="true" tabindex="-1"></a>config.hidden_size <span class="op">=</span> <span class="dv">64</span>          <span class="co"># Small hidden dimension (enough representation for a simple task)</span></span>
<span id="cb53-9"><a href="#cb53-9" aria-hidden="true" tabindex="-1"></a>config.num_hidden_layers <span class="op">=</span> <span class="dv">2</span>     <span class="co"># Minimum number of layers (considering the low complexity of the copy task)</span></span>
<span id="cb53-10"><a href="#cb53-10" aria-hidden="true" tabindex="-1"></a>config.num_attention_heads <span class="op">=</span> <span class="dv">2</span>   <span class="co"># Minimum number of heads (minimum configuration for attention from various perspectives)</span></span>
<span id="cb53-11"><a href="#cb53-11" aria-hidden="true" tabindex="-1"></a>config.intermediate_size <span class="op">=</span> <span class="dv">128</span>   <span class="co"># Small FFN dimension (set to twice the hidden dimension to ensure adequate transformation capacity)</span></span>
<span id="cb53-12"><a href="#cb53-12" aria-hidden="true" tabindex="-1"></a>config.max_position_embeddings <span class="op">=</span> seq_length  <span class="co"># Short sequence length (set to the same length as the input sequence)</span></span>
<span id="cb53-13"><a href="#cb53-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-14"><a href="#cb53-14" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> train_copy_task(config, num_epochs<span class="op">=</span><span class="dv">50</span>, batch_size<span class="op">=</span><span class="dv">40</span>, steps_per_epoch<span class="op">=</span><span class="dv">100</span>, seq_length<span class="op">=</span>seq_length)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
=== Start Training ==== 
Device: cuda:0
Model saved to saved_models/transformer_copy_task.pth</code></pre>
</div>
</div>
<p><strong>模型测试</strong></p>
<p>读取保存的训练模型并进行测试。</p>
<div id="cell-98" class="cell" data-execution_count="1">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_08.transformer.examples.copy_task <span class="im">import</span> test_copy</span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-3"><a href="#cb55-3" aria-hidden="true" tabindex="-1"></a>test_copy(seq_length<span class="op">=</span><span class="dv">20</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
=== Copy Test ===
Input: [10, 10, 2, 12, 1, 5, 3, 1, 8, 18, 2, 19, 2, 2, 8, 14, 7, 19, 5, 4]
Output: [10, 10, 2, 12, 1, 5, 3, 1, 8, 18, 2, 19, 2, 2, 8, 14, 7, 19, 5, 4]
Accuracy: True</code></pre>
</div>
</div>
<p><strong>模型设置</strong></p>
<ul>
<li><code>hidden_size</code>: 64 (模型的设计维度, d_model).
<ul>
<li>变换器中设计维度(d_model)相同的值：
<ol type="1">
<li>单词嵌入维度</li>
<li>位置嵌入维度</li>
<li>注意力的Q, K, V向量维度</li>
<li>编码器/解码器各子层输出维度</li>
</ol></li>
</ul></li>
<li><code>intermediate_size</code>: FFN的大小，必须足够大以超过d_model.</li>
</ul>
<p><strong>掩码实现</strong></p>
<p>变换器使用两种类型的掩码。</p>
<ol type="1">
<li><strong>填充掩码 (Padding Mask)</strong>: 忽略为批处理添加的填充标记。
<ul>
<li>本例中序列长度相同，因此不需要填充，但为了说明变换器的一般用途而包含此部分。</li>
<li>自行实现<code>create_pad_mask</code>函数（PyTorch的<code>nn.Transformer</code>或Hugging Face的<code>transformers</code>库内部已经实现了这一点）。</li>
</ul></li>
</ol>
<div id="cell-100" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a>src_mask <span class="op">=</span> create_pad_mask(src).to(device)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<ol start="2" type="1">
<li><strong>后续掩码 (Subsequent Mask)</strong>: 用于解码器的自回归生成。</li>
</ol>
<ul>
<li><code>create_subsequent_mask</code> 函数生成一个上三角矩阵形式的掩码，以遮挡当前位置之后的令牌。</li>
<li>解码器仅参考之前生成的令牌来预测下一个令牌。</li>
</ul>
<div id="cell-102" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb58"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a>tgt_mask <span class="op">=</span> create_subsequent_mask(decoder_input.size(<span class="dv">1</span>)).to(device)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>这种掩码确保了批处理的效率和序列的因果性。</p>
<p><strong>损失函数的设计</strong></p>
<p><code>CopyLoss</code> 类实现了用于复制任务的损失函数。</p>
<ul>
<li>考虑每个标记位置的准确性以及整个序列的完全匹配情况。</li>
<li>详细监控准确性、损失值、预测/实际值，以精细掌握学习进展。</li>
</ul>
<div id="cell-104" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb59"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> CopyLoss(nn.Module):</span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, outputs: torch.Tensor, target: torch.Tensor, </span>
<span id="cb59-3"><a href="#cb59-3" aria-hidden="true" tabindex="-1"></a>                print_details: <span class="bu">bool</span> <span class="op">=</span> <span class="va">False</span>) <span class="op">-&gt;</span> Tuple[torch.Tensor, <span class="bu">float</span>]:</span>
<span id="cb59-4"><a href="#cb59-4" aria-hidden="true" tabindex="-1"></a>        batch_size <span class="op">=</span> outputs.size(<span class="dv">0</span>)</span>
<span id="cb59-5"><a href="#cb59-5" aria-hidden="true" tabindex="-1"></a>        predictions <span class="op">=</span> F.softmax(outputs, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb59-6"><a href="#cb59-6" aria-hidden="true" tabindex="-1"></a>        target_one_hot <span class="op">=</span> F.one_hot(target, num_classes<span class="op">=</span>outputs.size(<span class="op">-</span><span class="dv">1</span>)).<span class="bu">float</span>()</span>
<span id="cb59-7"><a href="#cb59-7" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb59-8"><a href="#cb59-8" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> <span class="op">-</span>torch.<span class="bu">sum</span>(target_one_hot <span class="op">*</span> torch.log(predictions <span class="op">+</span> <span class="fl">1e-10</span>)) <span class="op">/</span> batch_size</span>
<span id="cb59-9"><a href="#cb59-9" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb59-10"><a href="#cb59-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb59-11"><a href="#cb59-11" aria-hidden="true" tabindex="-1"></a>            pred_tokens <span class="op">=</span> predictions.argmax(dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb59-12"><a href="#cb59-12" aria-hidden="true" tabindex="-1"></a>            exact_match <span class="op">=</span> (pred_tokens <span class="op">==</span> target).<span class="bu">all</span>(dim<span class="op">=</span><span class="dv">1</span>).<span class="bu">float</span>()</span>
<span id="cb59-13"><a href="#cb59-13" aria-hidden="true" tabindex="-1"></a>            match_rate <span class="op">=</span> exact_match.mean().item()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<ul>
<li>仅靠交叉熵是不够的：需要评估单个令牌的准确性 + 整个序列是否匹配。</li>
<li>引导模型准确地学习顺序。</li>
</ul>
<p><strong>操作示例</strong> (<code>batch_size=2</code>, <code>sequence_length=3</code>, <code>vocab_size=5</code>):</p>
<ol type="1">
<li><strong>模型输出 (logits)</strong></li>
</ol>
<div id="cell-106" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb60"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Example: batch_size=2, sequence_length=3, vocab_size=5 (example is vocab_size=20)</span></span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-3"><a href="#cb60-3" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Model Output (logits)</span></span>
<span id="cb60-4"><a href="#cb60-4" aria-hidden="true" tabindex="-1"></a>outputs <span class="op">=</span> [</span>
<span id="cb60-5"><a href="#cb60-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># First batch</span></span>
<span id="cb60-6"><a href="#cb60-6" aria-hidden="true" tabindex="-1"></a>    [[<span class="fl">0.9</span>, <span class="fl">0.1</span>, <span class="fl">0.0</span>, <span class="fl">0.0</span>, <span class="fl">0.0</span>],  <span class="co"># First position: token 0 has the highest probability</span></span>
<span id="cb60-7"><a href="#cb60-7" aria-hidden="true" tabindex="-1"></a>     [<span class="fl">0.1</span>, <span class="fl">0.8</span>, <span class="fl">0.1</span>, <span class="fl">0.0</span>, <span class="fl">0.0</span>],  <span class="co"># Second position: token 1 has the highest probability</span></span>
<span id="cb60-8"><a href="#cb60-8" aria-hidden="true" tabindex="-1"></a>     [<span class="fl">0.0</span>, <span class="fl">0.1</span>, <span class="fl">0.9</span>, <span class="fl">0.0</span>, <span class="fl">0.0</span>]], <span class="co"># Third position: token 2 has the highest probability</span></span>
<span id="cb60-9"><a href="#cb60-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Second batch</span></span>
<span id="cb60-10"><a href="#cb60-10" aria-hidden="true" tabindex="-1"></a>    [[<span class="fl">0.8</span>, <span class="fl">0.2</span>, <span class="fl">0.0</span>, <span class="fl">0.0</span>, <span class="fl">0.0</span>],</span>
<span id="cb60-11"><a href="#cb60-11" aria-hidden="true" tabindex="-1"></a>     [<span class="fl">0.1</span>, <span class="fl">0.7</span>, <span class="fl">0.2</span>, <span class="fl">0.0</span>, <span class="fl">0.0</span>],</span>
<span id="cb60-12"><a href="#cb60-12" aria-hidden="true" tabindex="-1"></a>     [<span class="fl">0.1</span>, <span class="fl">0.1</span>, <span class="fl">0.8</span>, <span class="fl">0.0</span>, <span class="fl">0.0</span>]]</span>
<span id="cb60-13"><a href="#cb60-13" aria-hidden="true" tabindex="-1"></a>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<ol start="2" type="1">
<li><strong>实际目标</strong></li>
</ol>
<div id="cell-108" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb61"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Actual Target</span></span>
<span id="cb61-2"><a href="#cb61-2" aria-hidden="true" tabindex="-1"></a>target <span class="op">=</span> [</span>
<span id="cb61-3"><a href="#cb61-3" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">2</span>],  <span class="co"># Correct sequence for the first batch</span></span>
<span id="cb61-4"><a href="#cb61-4" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">2</span>]   <span class="co"># Correct sequence for the second batch</span></span>
<span id="cb61-5"><a href="#cb61-5" aria-hidden="true" tabindex="-1"></a>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<ol start="3" type="1">
<li><strong>损失计算过程</strong>
<ul>
<li><code>predictions = softmax(outputs)</code>（已经在上面转换为概率）</li>
<li>将<code>target</code>转换为独热向量</li>
</ul></li>
</ol>
<div id="cell-110" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb62"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. Loss Calculation Process</span></span>
<span id="cb62-2"><a href="#cb62-2" aria-hidden="true" tabindex="-1"></a><span class="co"># predictions = softmax(outputs) (already converted to probabilities above)</span></span>
<span id="cb62-3"><a href="#cb62-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert target to one-hot vectors:</span></span>
<span id="cb62-4"><a href="#cb62-4" aria-hidden="true" tabindex="-1"></a>target_one_hot <span class="op">=</span> [</span>
<span id="cb62-5"><a href="#cb62-5" aria-hidden="true" tabindex="-1"></a>    [[<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>], [<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>], [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>]],  <span class="co"># First batch</span></span>
<span id="cb62-6"><a href="#cb62-6" aria-hidden="true" tabindex="-1"></a>    [[<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>], [<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>], [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>]]   <span class="co"># Second batch</span></span>
<span id="cb62-7"><a href="#cb62-7" aria-hidden="true" tabindex="-1"></a>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<ol start="4" type="1">
<li><strong>准确度计算</strong></li>
</ol>
<div id="cell-112" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb63"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 4. Accuracy Calculation</span></span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a>pred_tokens <span class="op">=</span> [</span>
<span id="cb63-3"><a href="#cb63-3" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">2</span>],  <span class="co"># First batch prediction</span></span>
<span id="cb63-4"><a href="#cb63-4" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">2</span>]   <span class="co"># Second batch prediction</span></span>
<span id="cb63-5"><a href="#cb63-5" aria-hidden="true" tabindex="-1"></a>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<ul>
<li>序列完全匹配：<code>exact_match = [True, True]</code>（两批都准确）</li>
<li>平均准确度：<code>match_rate = 1.0</code>（100%）</li>
</ul>
<ol start="5" type="1">
<li><strong>最终损失值</strong>：交叉熵的平均值</li>
</ol>
<div id="cell-114" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb64"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Exact sequence match</span></span>
<span id="cb64-2"><a href="#cb64-2" aria-hidden="true" tabindex="-1"></a>exact_match <span class="op">=</span> [<span class="va">True</span>, <span class="va">True</span>]  <span class="co"># Both batches match exactly</span></span>
<span id="cb64-3"><a href="#cb64-3" aria-hidden="true" tabindex="-1"></a>match_rate <span class="op">=</span> <span class="fl">1.0</span>  <span class="co"># Average accuracy 100%</span></span>
<span id="cb64-4"><a href="#cb64-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-5"><a href="#cb64-5" aria-hidden="true" tabindex="-1"></a><span class="co"># The final loss value is the average of the cross-entropy</span></span>
<span id="cb64-6"><a href="#cb64-6" aria-hidden="true" tabindex="-1"></a><span class="co"># loss = -1/2 * (log(0.9) + log(0.8) + log(0.9) + log(0.8) + log(0.7) + log(0.8))</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>注意力可视化</p>
<p>通过注意力可视化，可以直观地理解变压器的工作原理。</p>
<div id="cell-116" class="cell" data-execution_count="1">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb65"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_08.transformer.examples.copy_task <span class="im">import</span> visualize_attention</span>
<span id="cb65-2"><a href="#cb65-2" aria-hidden="true" tabindex="-1"></a>visualize_attention(seq_length<span class="op">=</span><span class="dv">20</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="08_变压器的诞生_files/figure-html/cell-52-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>每个输入令牌检查其如何与其他位置的令牌交互。</p>
<p>通过这个复制任务示例，我们验证了变压器的核心机制。在下一个示例（加法问题）中，我们将探讨变压器如何学习数字之间的关系、进位等算术规则。</p>
</section>
<section id="位数加法问题" class="level3">
<h3 class="anchored" data-anchor-id="位数加法问题">8.5.2 位数加法问题</h3>
<p>第二个示例是将两个数字相加的加法任务。此任务适合理解变压器的自回归(autoregressive)生成能力和解码器的顺序计算过程。通过带有进位的计算，可以观察到变压器如何学习数字之间的关系。</p>
<p><strong>数据准备</strong></p>
<p>加法任务的数据由 <code>create_addition_data()</code> 生成。</p>
<div id="cell-119" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb66"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_addition_data(batch_size: <span class="bu">int</span> <span class="op">=</span> <span class="dv">32</span>, max_digits: <span class="bu">int</span> <span class="op">=</span> <span class="dv">3</span>) <span class="op">-&gt;</span> Tuple[torch.Tensor, torch.Tensor]:</span>
<span id="cb66-2"><a href="#cb66-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Create addition dataset"""</span></span>
<span id="cb66-3"><a href="#cb66-3" aria-hidden="true" tabindex="-1"></a>    max_value <span class="op">=</span> <span class="dv">10</span> <span class="op">**</span> max_digits <span class="op">-</span> <span class="dv">1</span></span>
<span id="cb66-4"><a href="#cb66-4" aria-hidden="true" tabindex="-1"></a>    num1 <span class="op">=</span> torch.randint(<span class="dv">0</span>, max_value <span class="op">//</span> <span class="dv">2</span> <span class="op">+</span> <span class="dv">1</span>, (batch_size,))</span>
<span id="cb66-5"><a href="#cb66-5" aria-hidden="true" tabindex="-1"></a>    num2 <span class="op">=</span> torch.randint(<span class="dv">0</span>, max_value <span class="op">//</span> <span class="dv">2</span> <span class="op">+</span> <span class="dv">1</span>, (batch_size,))</span>
<span id="cb66-6"><a href="#cb66-6" aria-hidden="true" tabindex="-1"></a>    result <span class="op">=</span> num1 <span class="op">+</span> num2</span>
<span id="cb66-7"><a href="#cb66-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-8"><a href="#cb66-8" aria-hidden="true" tabindex="-1"></a>    [See source below]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<ul>
<li>生成两个数字，使它们的和不超过指定的位数。</li>
<li>输入：两个数字 + ‘+’ 符号。</li>
<li>包含位数限制的有效性检查。</li>
</ul>
<p><strong>学习数据说明</strong></p>
<div id="cell-121" class="cell" data-execution_count="4">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb67"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_08.transformer.config <span class="im">import</span> TransformerConfig</span>
<span id="cb67-2"><a href="#cb67-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_08.transformer.examples.addition_task <span class="im">import</span> explain_addition_data</span>
<span id="cb67-3"><a href="#cb67-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-4"><a href="#cb67-4" aria-hidden="true" tabindex="-1"></a>explain_addition_data()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
=== Addition Data Explanation ====
Maximum Digits: 3

1. Input Sequence:
Original Tensor Shape: torch.Size([1, 7])
First Number: 153 (Indices [np.int64(1), np.int64(5), np.int64(3)])
Plus Sign: '+' (Index 10)
Second Number: 391 (Indices [np.int64(3), np.int64(9), np.int64(1)])
Full Input: [1, 5, 3, 10, 3, 9, 1]

2. Target Sequence:
Original Tensor Shape: torch.Size([1, 3])
Actual Sum: 544
Target Sequence: [5, 4, 4]</code></pre>
</div>
</div>
<p>模型训练及测试</p>
<div id="cell-123" class="cell" data-execution_count="1">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb69"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_08.transformer.config <span class="im">import</span> TransformerConfig</span>
<span id="cb69-2"><a href="#cb69-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_08.transformer.examples.addition_task <span class="im">import</span> train_addition_task</span>
<span id="cb69-3"><a href="#cb69-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-4"><a href="#cb69-4" aria-hidden="true" tabindex="-1"></a>config <span class="op">=</span> TransformerConfig()</span>
<span id="cb69-5"><a href="#cb69-5" aria-hidden="true" tabindex="-1"></a>config.vocab_size <span class="op">=</span> <span class="dv">11</span>       </span>
<span id="cb69-6"><a href="#cb69-6" aria-hidden="true" tabindex="-1"></a>config.hidden_size <span class="op">=</span> <span class="dv">256</span></span>
<span id="cb69-7"><a href="#cb69-7" aria-hidden="true" tabindex="-1"></a>config.num_hidden_layers <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb69-8"><a href="#cb69-8" aria-hidden="true" tabindex="-1"></a>config.num_attention_heads <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb69-9"><a href="#cb69-9" aria-hidden="true" tabindex="-1"></a>config.intermediate_size <span class="op">=</span> <span class="dv">512</span></span>
<span id="cb69-10"><a href="#cb69-10" aria-hidden="true" tabindex="-1"></a>config.max_position_embeddings <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb69-11"><a href="#cb69-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-12"><a href="#cb69-12" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> train_addition_task(config, num_epochs<span class="op">=</span><span class="dv">10</span>, batch_size<span class="op">=</span><span class="dv">128</span>, steps_per_epoch<span class="op">=</span><span class="dv">300</span>, max_digits<span class="op">=</span><span class="dv">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 0, Average Loss: 6.1352, Final Accuracy: 0.0073, Learning Rate: 0.000100
Epoch 5, Average Loss: 0.0552, Final Accuracy: 0.9852, Learning Rate: 0.000100

=== Loss Calculation Details (Step: 3000) ===
Predicted Sequences (First 10): tensor([[6, 5, 4],
        [5, 3, 3],
        [1, 7, 5],
        [6, 0, 6],
        [7, 5, 9],
        [5, 2, 8],
        [2, 8, 1],
        [3, 5, 8],
        [0, 7, 1],
        [6, 2, 1]], device='cuda:0')

Actual Target Sequences (First 10): tensor([[6, 5, 4],
        [5, 3, 3],
        [1, 7, 5],
        [6, 0, 6],
        [7, 5, 9],
        [5, 2, 8],
        [2, 8, 1],
        [3, 5, 8],
        [0, 7, 1],
        [6, 2, 1]], device='cuda:0')

Exact Match per Sequence (First 10): tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')

Calculated Loss: 0.0106
Calculated Accuracy: 1.0000
========================================
Model saved to saved_models/transformer_addition_task.pth</code></pre>
</div>
</div>
<p>学习结束后，加载存储的模型进行测试。</p>
<div id="cell-125" class="cell" data-execution_count="3">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb71"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_08.transformer.examples.addition_task <span class="im">import</span> test_addition</span>
<span id="cb71-2"><a href="#cb71-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-3"><a href="#cb71-3" aria-hidden="true" tabindex="-1"></a>test_addition(max_digits<span class="op">=</span><span class="dv">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
Addition Test (Digits: 3):
310 + 98 = 408 (Actual Answer: 408)
Correct: True</code></pre>
</div>
</div>
<p>模型设置</p>
<p>加法任务的变压器设置如下。</p>
<div id="cell-127" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb73"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a>config <span class="op">=</span> TransformerConfig()</span>
<span id="cb73-2"><a href="#cb73-2" aria-hidden="true" tabindex="-1"></a>config.vocab_size <span class="op">=</span> <span class="dv">11</span>          <span class="co"># 0-9 digits + '+' symbol</span></span>
<span id="cb73-3"><a href="#cb73-3" aria-hidden="true" tabindex="-1"></a>config.hidden_size <span class="op">=</span> <span class="dv">256</span>        <span class="co"># Larger hidden dimension than copy task (sufficient capacity for learning arithmetic operations)</span></span>
<span id="cb73-4"><a href="#cb73-4" aria-hidden="true" tabindex="-1"></a>config.num_hidden_layers <span class="op">=</span> <span class="dv">3</span>    <span class="co"># Deeper layers (hierarchical feature extraction for handling carry operations)</span></span>
<span id="cb73-5"><a href="#cb73-5" aria-hidden="true" tabindex="-1"></a>config.num_attention_heads <span class="op">=</span> <span class="dv">4</span>  <span class="co"># Increased number of heads (learning relationships between different digit positions)</span></span>
<span id="cb73-6"><a href="#cb73-6" aria-hidden="true" tabindex="-1"></a>config.intermediate_size <span class="op">=</span> <span class="dv">512</span>  <span class="co">#  FFN dimension: should be larger than hidden_size.</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><strong>遮罩实现</strong></p>
<p>在加法任务中，填充掩码是<em>必需的</em>。由于输入数字的位数可能不同，因此必须忽略填充位置以进行准确计算。</p>
<div id="cell-129" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb74"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb74-1"><a href="#cb74-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> _number_to_digits(number: torch.Tensor, max_digits: <span class="bu">int</span>) <span class="op">-&gt;</span> torch.Tensor:</span>
<span id="cb74-2"><a href="#cb74-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""숫자를 자릿수 시퀀스로 변환하며 패딩 적용"""</span></span>
<span id="cb74-3"><a href="#cb74-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> torch.tensor([[<span class="bu">int</span>(d) <span class="cf">for</span> d <span class="kw">in</span> <span class="bu">str</span>(n.item()).zfill(max_digits)] </span>
<span id="cb74-4"><a href="#cb74-4" aria-hidden="true" tabindex="-1"></a>                        <span class="cf">for</span> n <span class="kw">in</span> number])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>该方法的具体操作如下。</p>
<div id="cell-131" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb75"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb75-1"><a href="#cb75-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb75-2"><a href="#cb75-2" aria-hidden="true" tabindex="-1"></a>number <span class="op">=</span> torch.tensor([<span class="dv">7</span>, <span class="dv">25</span>, <span class="dv">348</span>])</span>
<span id="cb75-3"><a href="#cb75-3" aria-hidden="true" tabindex="-1"></a>max_digits <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb75-4"><a href="#cb75-4" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> _number_to_digits(number, max_digits)</span>
<span id="cb75-5"><a href="#cb75-5" aria-hidden="true" tabindex="-1"></a><span class="co"># 입력: [7, 25, 348]</span></span>
<span id="cb75-6"><a href="#cb75-6" aria-hidden="true" tabindex="-1"></a><span class="co"># 과정: </span></span>
<span id="cb75-7"><a href="#cb75-7" aria-hidden="true" tabindex="-1"></a><span class="co">#   7   -&gt; "7"   -&gt; "007" -&gt; [0,0,7]</span></span>
<span id="cb75-8"><a href="#cb75-8" aria-hidden="true" tabindex="-1"></a><span class="co">#   25  -&gt; "25"  -&gt; "025" -&gt; [0,2,5]</span></span>
<span id="cb75-9"><a href="#cb75-9" aria-hidden="true" tabindex="-1"></a><span class="co">#   348 -&gt; "348" -&gt; "348" -&gt; [3,4,8]</span></span>
<span id="cb75-10"><a href="#cb75-10" aria-hidden="true" tabindex="-1"></a><span class="co"># 결과: tensor([[0, 0, 7],</span></span>
<span id="cb75-11"><a href="#cb75-11" aria-hidden="true" tabindex="-1"></a><span class="co">#               [0, 2, 5],</span></span>
<span id="cb75-12"><a href="#cb75-12" aria-hidden="true" tabindex="-1"></a><span class="co">#               [3, 4, 8]])</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><strong>损失函数的设计</strong></p>
<p><code>AdditionLoss</code> 类实现了加法任务的损失函数。</p>
<ul>
<li>与复制任务不同，区分评估<em>按位精度</em>和<em>整个答案的精度</em>。</li>
</ul>
<div id="cell-133" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb76"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb76-1"><a href="#cb76-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> AdditionLoss(nn.Module):</span>
<span id="cb76-2"><a href="#cb76-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, outputs: torch.Tensor, target: torch.Tensor, </span>
<span id="cb76-3"><a href="#cb76-3" aria-hidden="true" tabindex="-1"></a>                print_details: <span class="bu">bool</span> <span class="op">=</span> <span class="va">False</span>) <span class="op">-&gt;</span> Tuple[torch.Tensor, <span class="bu">float</span>]:</span>
<span id="cb76-4"><a href="#cb76-4" aria-hidden="true" tabindex="-1"></a>        batch_size <span class="op">=</span> outputs.size(<span class="dv">0</span>)</span>
<span id="cb76-5"><a href="#cb76-5" aria-hidden="true" tabindex="-1"></a>        predictions <span class="op">=</span> F.softmax(outputs, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb76-6"><a href="#cb76-6" aria-hidden="true" tabindex="-1"></a>        target_one_hot <span class="op">=</span> F.one_hot(target, num_classes<span class="op">=</span>outputs.size(<span class="op">-</span><span class="dv">1</span>)).<span class="bu">float</span>()</span>
<span id="cb76-7"><a href="#cb76-7" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb76-8"><a href="#cb76-8" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> <span class="op">-</span>torch.<span class="bu">sum</span>(target_one_hot <span class="op">*</span> torch.log(predictions <span class="op">+</span> <span class="fl">1e-10</span>)) <span class="op">/</span> batch_size</span>
<span id="cb76-9"><a href="#cb76-9" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb76-10"><a href="#cb76-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb76-11"><a href="#cb76-11" aria-hidden="true" tabindex="-1"></a>            pred_digits <span class="op">=</span> predictions.argmax(dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb76-12"><a href="#cb76-12" aria-hidden="true" tabindex="-1"></a>            exact_match <span class="op">=</span> (pred_digits <span class="op">==</span> target).<span class="bu">all</span>(dim<span class="op">=</span><span class="dv">1</span>).<span class="bu">float</span>()</span>
<span id="cb76-13"><a href="#cb76-13" aria-hidden="true" tabindex="-1"></a>            match_rate <span class="op">=</span> exact_match.mean().item()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<ul>
<li>损失计算: 各位数预测准确性 + <em>进位</em> 准确性检查。</li>
<li>不是简单的位数映射，而是引导学习加法规则。</li>
</ul>
<p><code>AdditionLoss</code> 工作示例 (<code>batch_size=2</code>, <code>sequence_length=3</code>, <code>vocab_size=10</code>)</p>
<div id="cell-135" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb77"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb77-1"><a href="#cb77-1" aria-hidden="true" tabindex="-1"></a>outputs <span class="op">=</span> [</span>
<span id="cb77-2"><a href="#cb77-2" aria-hidden="true" tabindex="-1"></a>    [[<span class="fl">0.1</span>, <span class="fl">0.8</span>, <span class="fl">0.1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>],  <span class="co"># 첫 번째 자리</span></span>
<span id="cb77-3"><a href="#cb77-3" aria-hidden="true" tabindex="-1"></a>     [<span class="fl">0.1</span>, <span class="fl">0.1</span>, <span class="fl">0.7</span>, <span class="fl">0.1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>], <span class="co"># 두 번째 자리</span></span>
<span id="cb77-4"><a href="#cb77-4" aria-hidden="true" tabindex="-1"></a>     [<span class="fl">0.8</span>, <span class="fl">0.1</span>, <span class="fl">0.1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>]]   <span class="co"># 세 번째 자리</span></span>
<span id="cb77-5"><a href="#cb77-5" aria-hidden="true" tabindex="-1"></a>]  <span class="co"># 첫 번째 배치</span></span>
<span id="cb77-6"><a href="#cb77-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-7"><a href="#cb77-7" aria-hidden="true" tabindex="-1"></a>target <span class="op">=</span> [</span>
<span id="cb77-8"><a href="#cb77-8" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">0</span>]  <span class="co"># 실제 정답: "120"</span></span>
<span id="cb77-9"><a href="#cb77-9" aria-hidden="true" tabindex="-1"></a>]  <span class="co"># 첫 번째 배치</span></span>
<span id="cb77-10"><a href="#cb77-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-11"><a href="#cb77-11" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. softmax는 이미 적용되어 있다고 가정 (outputs)</span></span>
<span id="cb77-12"><a href="#cb77-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-13"><a href="#cb77-13" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. target을 원-핫 인코딩으로 변환</span></span>
<span id="cb77-14"><a href="#cb77-14" aria-hidden="true" tabindex="-1"></a>target_one_hot <span class="op">=</span> [</span>
<span id="cb77-15"><a href="#cb77-15" aria-hidden="true" tabindex="-1"></a>    [[<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>],  <span class="co"># 1</span></span>
<span id="cb77-16"><a href="#cb77-16" aria-hidden="true" tabindex="-1"></a>     [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>],  <span class="co"># 2</span></span>
<span id="cb77-17"><a href="#cb77-17" aria-hidden="true" tabindex="-1"></a>     [<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>]]  <span class="co"># 0</span></span>
<span id="cb77-18"><a href="#cb77-18" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb77-19"><a href="#cb77-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-20"><a href="#cb77-20" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. 손실 계산</span></span>
<span id="cb77-21"><a href="#cb77-21" aria-hidden="true" tabindex="-1"></a><span class="co"># -log(0.8) - log(0.7) - log(0.8) = 0.223 + 0.357 + 0.223 = 0.803</span></span>
<span id="cb77-22"><a href="#cb77-22" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> <span class="fl">0.803</span> <span class="op">/</span> batch_size</span>
<span id="cb77-23"><a href="#cb77-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-24"><a href="#cb77-24" aria-hidden="true" tabindex="-1"></a><span class="co"># 4. 정확도 계산</span></span>
<span id="cb77-25"><a href="#cb77-25" aria-hidden="true" tabindex="-1"></a>pred_digits <span class="op">=</span> [<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">0</span>]  <span class="co"># argmax 적용</span></span>
<span id="cb77-26"><a href="#cb77-26" aria-hidden="true" tabindex="-1"></a>exact_match <span class="op">=</span> <span class="va">True</span>  <span class="co"># 모든 자릿수가 일치</span></span>
<span id="cb77-27"><a href="#cb77-27" aria-hidden="true" tabindex="-1"></a>match_rate <span class="op">=</span> <span class="fl">1.0</span>  <span class="co"># 배치의 평균 정확도</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>变压器解码器的输出在最后一层通过 <code>vocab_size</code> 进行线性变换，因此，logit 具有 <code>vocab_size</code> 维度。</p>
<p>下一节我们将通过解析任务来看变压器是如何学习更复杂的结构关系的。</p>
</section>
<section id="解析器任务" class="level3">
<h3 class="anchored" data-anchor-id="解析器任务">8.5.3 解析器任务</h3>
<p>最后一个示例是解析器(Parser)任务的实现。此任务接收表达式并将其转换为解析树(parse tree)，是一个可以验证变压器如何处理结构化信息的例子。</p>
<p><strong>数据准备过程说明</strong></p>
<p>解析器任务的训练数据通过以下步骤生成：</p>
<ol type="1">
<li><strong>生成表达式</strong>：
<ul>
<li>使用 <code>generate_random_expression()</code> 函数组合变量(x, y, z)、运算符(+, -, *, /)和数字(0-9)，创建如 “x=1+2” 这样的简单表达式。</li>
</ul></li>
<li><strong>转换为解析树</strong>：
<ul>
<li>利用 <code>parse_to_tree()</code> 函数将生成的表达式转换为形如 <code>['ASSIGN', 'x', ['ADD', '1', '2']]</code> 的嵌套列表形式的解析树。该树表示了表达式的层次结构。</li>
</ul></li>
<li><strong>分词处理</strong>：
<ul>
<li>表达式和解析树分别被转换成整数序列。</li>
<li>根据预先定义的 <code>TOKEN_DICT</code>，每个分词都被映射为一个唯一的整数ID。</li>
</ul></li>
</ol>
<div id="cell-138" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb78"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb78-1"><a href="#cb78-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_addition_data(batch_size: <span class="bu">int</span> <span class="op">=</span> <span class="dv">32</span>, max_digits: <span class="bu">int</span> <span class="op">=</span> <span class="dv">3</span>) <span class="op">-&gt;</span> Tuple[torch.Tensor, torch.Tensor]:</span>
<span id="cb78-2"><a href="#cb78-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Create addition dataset"""</span></span>
<span id="cb78-3"><a href="#cb78-3" aria-hidden="true" tabindex="-1"></a>    max_value <span class="op">=</span> <span class="dv">10</span> <span class="op">**</span> max_digits <span class="op">-</span> <span class="dv">1</span></span>
<span id="cb78-4"><a href="#cb78-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb78-5"><a href="#cb78-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Generate input numbers</span></span>
<span id="cb78-6"><a href="#cb78-6" aria-hidden="true" tabindex="-1"></a>    num1 <span class="op">=</span> torch.randint(<span class="dv">0</span>, max_value <span class="op">//</span> <span class="dv">2</span> <span class="op">+</span> <span class="dv">1</span>, (batch_size,))</span>
<span id="cb78-7"><a href="#cb78-7" aria-hidden="true" tabindex="-1"></a>    num2 <span class="op">=</span> torch.randint(<span class="dv">0</span>, max_value <span class="op">//</span> <span class="dv">2</span> <span class="op">+</span> <span class="dv">1</span>, (batch_size,))</span>
<span id="cb78-8"><a href="#cb78-8" aria-hidden="true" tabindex="-1"></a>    result <span class="op">=</span> num1 <span class="op">+</span> num2</span>
<span id="cb78-9"><a href="#cb78-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-10"><a href="#cb78-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># [이하 생략]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<ul>
<li>生成两个数字，使其和不超过指定的位数。</li>
<li>输入：两个数字 + ‘+’ 符号。</li>
<li>包含位数限制的有效性检查。</li>
</ul>
<p><strong>学习数据说明</strong> 以下解释了学习数据的结构。展示了表达式在分词后会变成什么值。</p>
<div id="cell-140" class="cell" data-execution_count="2">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb79"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb79-1"><a href="#cb79-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_08.transformer.examples.parser_task <span class="im">import</span> explain_parser_data</span>
<span id="cb79-2"><a href="#cb79-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-3"><a href="#cb79-3" aria-hidden="true" tabindex="-1"></a>explain_parser_data()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
=== Parsing Data Explanation ===
Max Tokens: 5

1. Input Sequence:
Original Tensor Shape: torch.Size([1, 5])
Expression: x = 4 + 9
Tokenized Input: [11, 1, 17, 2, 22]

2. Target Sequence:
Original Tensor Shape: torch.Size([1, 5])
Parse Tree: ['ASSIGN', 'x', 'ADD', '4', '9']
Tokenized Output: [6, 11, 7, 17, 22]</code></pre>
</div>
</div>
<p><strong>解析示例说明</strong></p>
<p>执行以下代码后，将依次显示解释，以便更容易理解解析示例数据的结构。</p>
<div id="cell-142" class="cell" data-execution_count="4">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb81"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb81-1"><a href="#cb81-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_08.transformer.examples.parser_task <span class="im">import</span> show_parser_examples</span>
<span id="cb81-2"><a href="#cb81-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-3"><a href="#cb81-3" aria-hidden="true" tabindex="-1"></a>show_parser_examples(num_examples<span class="op">=</span><span class="dv">3</span> )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
=== Generating 3 Parsing Examples ===

Example 1:
Generated Expression: y=7/7
Parse Tree: ['ASSIGN', 'y', ['DIV', '7', '7']]
Expression Tokens: [12, 1, 21, 5, 21]
Tree Tokens: [6, 12, 10, 21, 21]
Padded Expression Tokens: [12, 1, 21, 5, 21]
Padded Tree Tokens: [6, 12, 10, 21, 21]

Example 2:
Generated Expression: x=4/3
Parse Tree: ['ASSIGN', 'x', ['DIV', '4', '3']]
Expression Tokens: [11, 1, 18, 5, 17]
Tree Tokens: [6, 11, 10, 18, 17]
Padded Expression Tokens: [11, 1, 18, 5, 17]
Padded Tree Tokens: [6, 11, 10, 18, 17]

Example 3:
Generated Expression: x=1*4
Parse Tree: ['ASSIGN', 'x', ['MUL', '1', '4']]
Expression Tokens: [11, 1, 15, 4, 18]
Tree Tokens: [6, 11, 9, 15, 18]
Padded Expression Tokens: [11, 1, 15, 4, 18]
Padded Tree Tokens: [6, 11, 9, 15, 18]
</code></pre>
</div>
</div>
<p>模型训练及测试</p>
<div id="cell-144" class="cell" data-execution_count="2">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb83"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb83-1"><a href="#cb83-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_08.transformer.config <span class="im">import</span> TransformerConfig</span>
<span id="cb83-2"><a href="#cb83-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_08.transformer.examples.parser_task <span class="im">import</span> train_parser_task</span>
<span id="cb83-3"><a href="#cb83-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-4"><a href="#cb83-4" aria-hidden="true" tabindex="-1"></a>config <span class="op">=</span> TransformerConfig()</span>
<span id="cb83-5"><a href="#cb83-5" aria-hidden="true" tabindex="-1"></a>config.vocab_size <span class="op">=</span> <span class="dv">25</span>  <span class="co"># Adjusted to match the token dictionary size</span></span>
<span id="cb83-6"><a href="#cb83-6" aria-hidden="true" tabindex="-1"></a>config.hidden_size <span class="op">=</span> <span class="dv">128</span></span>
<span id="cb83-7"><a href="#cb83-7" aria-hidden="true" tabindex="-1"></a>config.num_hidden_layers <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb83-8"><a href="#cb83-8" aria-hidden="true" tabindex="-1"></a>config.num_attention_heads <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb83-9"><a href="#cb83-9" aria-hidden="true" tabindex="-1"></a>config.intermediate_size <span class="op">=</span> <span class="dv">512</span></span>
<span id="cb83-10"><a href="#cb83-10" aria-hidden="true" tabindex="-1"></a>config.max_position_embeddings <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb83-11"><a href="#cb83-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-12"><a href="#cb83-12" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> train_parser_task(config, num_epochs<span class="op">=</span><span class="dv">6</span>, batch_size<span class="op">=</span><span class="dv">64</span>, steps_per_epoch<span class="op">=</span><span class="dv">100</span>, max_tokens<span class="op">=</span><span class="dv">5</span>, print_progress<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
=== Start Training ===
Device: cuda:0
Batch Size: 64
Steps per Epoch: 100
Max Tokens: 5

Epoch 0, Average Loss: 6.3280, Final Accuracy: 0.2309, Learning Rate: 0.000100

=== Prediction Result Samples ===
Input: y = 8 * 8
Prediction: ['ASSIGN', 'y', 'MUL', '8', '8']
Truth: ['ASSIGN', 'y', 'MUL', '8', '8']
Result: Correct

Input: z = 6 / 5
Prediction: ['ASSIGN', 'z', 'DIV', '8', 'a']
Truth: ['ASSIGN', 'z', 'DIV', '6', '5']
Result: Incorrect

Epoch 5, Average Loss: 0.0030, Final Accuracy: 1.0000, Learning Rate: 0.000100

=== Prediction Result Samples ===
Input: z = 5 - 6
Prediction: ['ASSIGN', 'z', 'SUB', '5', '6']
Truth: ['ASSIGN', 'z', 'SUB', '5', '6']
Result: Correct

Input: y = 9 + 9
Prediction: ['ASSIGN', 'y', 'ADD', '9', '9']
Truth: ['ASSIGN', 'y', 'ADD', '9', '9']
Result: Correct

Model saved to saved_models/transformer_parser_task.pth</code></pre>
</div>
</div>
<p>执行测试。</p>
<div id="cell-146" class="cell" data-execution_count="3">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb85"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb85-1"><a href="#cb85-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_08.transformer.config <span class="im">import</span> TransformerConfig</span>
<span id="cb85-2"><a href="#cb85-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_08.transformer.examples.parser_task <span class="im">import</span> test_parser</span>
<span id="cb85-3"><a href="#cb85-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-4"><a href="#cb85-4" aria-hidden="true" tabindex="-1"></a>test_parser()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
=== Parser Test ===
Input Expression: x = 8 * 3
Predicted Parse Tree: ['ASSIGN', 'x', 'MUL', '8', '3']
Actual Parse Tree: ['ASSIGN', 'x', 'MUL', '8', '3']
Correct: True

=== Additional Tests ===

Input: x=1+2
Predicted Parse Tree: ['ASSIGN', 'x', 'ADD', '2', '3']

Input: y=3*4
Predicted Parse Tree: ['ASSIGN', 'y', 'MUL', '4', '5']

Input: z=5-1
Predicted Parse Tree: ['ASSIGN', 'z', 'SUB', '6', '2']

Input: x=2/3
Predicted Parse Tree: ['ASSIGN', 'x', 'DIV', '3', '4']</code></pre>
</div>
</div>
<p><strong>模型设置</strong> - <code>vocab_size</code>: 25 (词汇表的大小) - <code>hidden_size</code>: 128 - <code>num_hidden_layers</code>: 3 - <code>num_attention_heads</code>: 4 - <code>intermediate_size</code>: 512 - <code>max_position_embeddings</code>: 10 (最大令牌数)</p>
<p><strong>损失函数设计</strong></p>
<p>解析任务的损失函数使用交叉熵损失。</p>
<ol type="1">
<li><strong>输出转换</strong>: 使用softmax函数将模型的输出转换为概率。</li>
<li><strong>目标转换</strong>: 将目标（正确答案）序列进行独热编码。</li>
<li><strong>损失计算</strong>: 通过计算负对数概率的平均值来确定损失。</li>
<li><strong>准确度</strong>: 通过检查预测序列是否与正确序列完全一致来计算准确度。这反映了此任务中解析树必须精确生成的特点。</li>
</ol>
<p><strong>损失函数操作示例</strong></p>
<div id="cell-148" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb87"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb87-1"><a href="#cb87-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Example input values (batch_size=2, sequence_length=4, vocab_size=5)</span></span>
<span id="cb87-2"><a href="#cb87-2" aria-hidden="true" tabindex="-1"></a><span class="co"># vocab = {'=':0, 'x':1, '+':2, '1':3, '2':4}</span></span>
<span id="cb87-3"><a href="#cb87-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-4"><a href="#cb87-4" aria-hidden="true" tabindex="-1"></a>outputs <span class="op">=</span> [</span>
<span id="cb87-5"><a href="#cb87-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># First batch: prediction probabilities for "x=1+2"</span></span>
<span id="cb87-6"><a href="#cb87-6" aria-hidden="true" tabindex="-1"></a>    [[<span class="fl">0.1</span>, <span class="fl">0.7</span>, <span class="fl">0.1</span>, <span class="fl">0.1</span>, <span class="fl">0.0</span>],  <span class="co"># predicting x</span></span>
<span id="cb87-7"><a href="#cb87-7" aria-hidden="true" tabindex="-1"></a>     [<span class="fl">0.8</span>, <span class="fl">0.1</span>, <span class="fl">0.0</span>, <span class="fl">0.1</span>, <span class="fl">0.0</span>],  <span class="co"># predicting =</span></span>
<span id="cb87-8"><a href="#cb87-8" aria-hidden="true" tabindex="-1"></a>     [<span class="fl">0.1</span>, <span class="fl">0.0</span>, <span class="fl">0.1</span>, <span class="fl">0.7</span>, <span class="fl">0.1</span>],  <span class="co"># predicting 1</span></span>
<span id="cb87-9"><a href="#cb87-9" aria-hidden="true" tabindex="-1"></a>     [<span class="fl">0.0</span>, <span class="fl">0.1</span>, <span class="fl">0.8</span>, <span class="fl">0.0</span>, <span class="fl">0.1</span>]], <span class="co"># predicting +</span></span>
<span id="cb87-10"><a href="#cb87-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb87-11"><a href="#cb87-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Second batch: prediction probabilities for "x=2+1"</span></span>
<span id="cb87-12"><a href="#cb87-12" aria-hidden="true" tabindex="-1"></a>    [[<span class="fl">0.1</span>, <span class="fl">0.8</span>, <span class="fl">0.0</span>, <span class="fl">0.1</span>, <span class="fl">0.0</span>],  <span class="co"># predicting x</span></span>
<span id="cb87-13"><a href="#cb87-13" aria-hidden="true" tabindex="-1"></a>     [<span class="fl">0.7</span>, <span class="fl">0.1</span>, <span class="fl">0.1</span>, <span class="fl">0.0</span>, <span class="fl">0.1</span>],  <span class="co"># predicting =</span></span>
<span id="cb87-14"><a href="#cb87-14" aria-hidden="true" tabindex="-1"></a>     [<span class="fl">0.1</span>, <span class="fl">0.0</span>, <span class="fl">0.1</span>, <span class="fl">0.1</span>, <span class="fl">0.7</span>],  <span class="co"># predicting 2</span></span>
<span id="cb87-15"><a href="#cb87-15" aria-hidden="true" tabindex="-1"></a>     [<span class="fl">0.0</span>, <span class="fl">0.0</span>, <span class="fl">0.9</span>, <span class="fl">0.1</span>, <span class="fl">0.0</span>]]  <span class="co"># predicting +</span></span>
<span id="cb87-16"><a href="#cb87-16" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb87-17"><a href="#cb87-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-18"><a href="#cb87-18" aria-hidden="true" tabindex="-1"></a>target <span class="op">=</span> [</span>
<span id="cb87-19"><a href="#cb87-19" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">3</span>, <span class="dv">2</span>],  <span class="co"># Actual answer: "x=1+"</span></span>
<span id="cb87-20"><a href="#cb87-20" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">4</span>, <span class="dv">2</span>]   <span class="co"># Actual answer: "x=2+"</span></span>
<span id="cb87-21"><a href="#cb87-21" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb87-22"><a href="#cb87-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-23"><a href="#cb87-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert target to one-hot encoding</span></span>
<span id="cb87-24"><a href="#cb87-24" aria-hidden="true" tabindex="-1"></a>target_one_hot <span class="op">=</span> [</span>
<span id="cb87-25"><a href="#cb87-25" aria-hidden="true" tabindex="-1"></a>    [[<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>],  <span class="co"># x</span></span>
<span id="cb87-26"><a href="#cb87-26" aria-hidden="true" tabindex="-1"></a>     [<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>],  <span class="co"># =</span></span>
<span id="cb87-27"><a href="#cb87-27" aria-hidden="true" tabindex="-1"></a>     [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>],  <span class="co"># 1</span></span>
<span id="cb87-28"><a href="#cb87-28" aria-hidden="true" tabindex="-1"></a>     [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>]], <span class="co"># +</span></span>
<span id="cb87-29"><a href="#cb87-29" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb87-30"><a href="#cb87-30" aria-hidden="true" tabindex="-1"></a>    [[<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>],  <span class="co"># x</span></span>
<span id="cb87-31"><a href="#cb87-31" aria-hidden="true" tabindex="-1"></a>     [<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>],  <span class="co"># =</span></span>
<span id="cb87-32"><a href="#cb87-32" aria-hidden="true" tabindex="-1"></a>     [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>],  <span class="co"># 2</span></span>
<span id="cb87-33"><a href="#cb87-33" aria-hidden="true" tabindex="-1"></a>     [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>]]  <span class="co"># +</span></span>
<span id="cb87-34"><a href="#cb87-34" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb87-35"><a href="#cb87-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-36"><a href="#cb87-36" aria-hidden="true" tabindex="-1"></a><span class="co"># Loss calculation (first batch)</span></span>
<span id="cb87-37"><a href="#cb87-37" aria-hidden="true" tabindex="-1"></a><span class="co"># -log(0.7) - log(0.8) - log(0.7) - log(0.8) = 0.357 + 0.223 + 0.357 + 0.223 = 1.16</span></span>
<span id="cb87-38"><a href="#cb87-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-39"><a href="#cb87-39" aria-hidden="true" tabindex="-1"></a><span class="co"># Loss calculation (second batch)</span></span>
<span id="cb87-40"><a href="#cb87-40" aria-hidden="true" tabindex="-1"></a><span class="co"># -log(0.8) - log(0.7) - log(0.7) - log(0.9) = 0.223 + 0.357 + 0.357 + 0.105 = 1.042</span></span>
<span id="cb87-41"><a href="#cb87-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-42"><a href="#cb87-42" aria-hidden="true" tabindex="-1"></a><span class="co"># Total loss</span></span>
<span id="cb87-43"><a href="#cb87-43" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> (<span class="fl">1.16</span> <span class="op">+</span> <span class="fl">1.042</span>) <span class="op">/</span> <span class="dv">2</span> <span class="op">=</span> <span class="fl">1.101</span></span>
<span id="cb87-44"><a href="#cb87-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-45"><a href="#cb87-45" aria-hidden="true" tabindex="-1"></a><span class="co"># Accuracy calculation</span></span>
<span id="cb87-46"><a href="#cb87-46" aria-hidden="true" tabindex="-1"></a>pred_tokens <span class="op">=</span> [</span>
<span id="cb87-47"><a href="#cb87-47" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">3</span>, <span class="dv">2</span>],  <span class="co"># First batch prediction</span></span>
<span id="cb87-48"><a href="#cb87-48" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">4</span>, <span class="dv">2</span>]   <span class="co"># Second batch prediction</span></span>
<span id="cb87-49"><a href="#cb87-49" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb87-50"><a href="#cb87-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-51"><a href="#cb87-51" aria-hidden="true" tabindex="-1"></a>exact_match <span class="op">=</span> [<span class="va">True</span>, <span class="va">True</span>]  <span class="co"># Both batches match exactly</span></span>
<span id="cb87-52"><a href="#cb87-52" aria-hidden="true" tabindex="-1"></a>match_rate <span class="op">=</span> <span class="fl">1.0</span>  <span class="co"># Overall accuracy</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>到目前为止，通过示例我们可以了解到变压器能够有效地处理结构化信息。</p>
</section>
</section>
<section id="结语" class="level2">
<h2 class="anchored" data-anchor-id="结语">结语</h2>
<p>在第8章中，我们深入探讨了变压器的诞生背景及其核心组成部分。研究者们为了克服基于RNN模型的局限性而进行的努力、注意力机制的发现和发展、以及通过Q、K、V向量空间分离和多头注意力实现并行处理和从不同角度捕捉上下文信息等构成变压器的核心理念是如何逐步具体化的。此外，我们详细分析了用于有效表示位置信息的位置编码、防止信息泄露的精妙掩码策略，以及编码器-解码器结构及其各个组成部分的作用和工作方式。</p>
<p>通过三个示例（简单复制、位数相加、解析器），我们可以直观地理解变压器实际上是如何工作的，以及各个组件起到了什么作用。这些示例展示了变压器的基本功能、自回归生成能力和处理结构化信息的能力，并提供了将其应用于实际自然语言处理问题的基础知识。</p>
<p>在第9章中，我们将跟随其发展的旅程，了解自“Attention is All You Need”论文发表以来变压器是如何发展的。我们将探讨BERT、GPT等基于变压器的各种模型是如何出现的，以及这些模型如何不仅在自然语言处理领域，在计算机视觉、语音识别等多个领域带来了哪些创新。</p>
</section>
<section id="练习题" class="level2">
<h2 class="anchored" data-anchor-id="练习题">练习题</h2>
<section id="基本问题" class="level3">
<h3 class="anchored" data-anchor-id="基本问题">基本问题</h3>
<ol type="1">
<li>变压器相比于RNN最大的两个优点是什么？</li>
<li>注意力机制的核心思想是什么，通过它可以获得什么效果？</li>
<li>多头注意力相对于自注意力提供了哪些优势？</li>
<li>位置编码为什么是必要的，它是如何表示位置信息的？</li>
<li>在变压器中，编码器和解码器各自执行什么角色？</li>
</ol>
</section>
<section id="应用问题" class="level3">
<h3 class="anchored" data-anchor-id="应用问题">应用问题</h3>
<ol type="1">
<li><strong>文本摘要任务</strong>：设计一个可以接收给定长文本输入并生成包含核心内容的简短摘要的变压器模型，并说明使用哪些评估指标来衡量模型性能。</li>
<li><strong>问答系统分析</strong>：逐步解释基于变压器的问答系统如何找到对给定问题的正确答案，并分析注意力机制在这个过程中发挥的关键作用。</li>
<li><strong>其他领域应用案例调查</strong>：调查在自然语言处理之外的其他领域（如图像、语音、图等）中成功应用变压器的两个以上实例，说明每个实例中变压器是如何被利用的，提供了哪些优势。</li>
</ol>
</section>
<section id="深化问题" class="level3">
<h3 class="anchored">深化问题</h3>
<ol type="1">
<li><strong>计算复杂度改进方法比较分析</strong>：调查至少两种旨在改善变压器计算复杂度的方法（例如：Reformer, Performer, Longformer），并比较分析每种方法的核心思想、优缺点及适用场景。</li>
<li><strong>新架构建议与评估</strong>：针对特定问题（如长文本分类、多语言翻译）提出基于变压器的新架构，并从理论上解释其相对于现有变压器模型的优势，以及如何通过实验进行验证的方法。</li>
<li><strong>伦理和社会影响分析及应对方案</strong>：分析基于变压器的大规模语言模型（如GPT-3, BERT）的发展可能对社会产生的积极和消极影响，特别是针对偏见、虚假新闻生成、就业岗位减少等负面效应，提出技术性和政策性应对措施。</li>
</ol>
<div class="callout callout-style-default callout-note callout-titled" title="点击查看内容（练习题解答）">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
点击查看内容（练习题解答）
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<section id="练习题解答" class="level2 callout-body-container callout-body">
<h2 class="anchored" data-anchor-id="练习题解答">练习题解答</h2>
<section id="基本问题-1" class="level3">
<h3 class="anchored" data-anchor-id="基本问题-1">基本问题</h3>
<ol type="1">
<li><p><strong>RNN 对比变压器的优点:</strong> 变压器相比 RNN 有两个主要优点：<strong>并行处理</strong>和<strong>解决长期依赖性问题</strong>。RNN 由于顺序处理速度较慢，而变压器通过注意力机制可以同时处理所有单词，支持 GPU 并行计算且学习速度快。此外，RNN 在长序列中会导致信息丢失，而变压器则通过自注意力直接计算词之间的关系，无论距离多远都能保留重要信息。</p></li>
<li><p><strong>注意力机制的核心与效果:</strong> 注意力计算<strong>输入序列的各个部分对输出序列生成的重要性</strong>。解码器在预测输出单词时，并不是同等看待整个输入，而是“关注（attention）”相关性较高的部分，从而更好地理解上下文并更准确地进行预测。</p></li>
<li><p><strong>多头注意力的优点:</strong> 多头注意力<strong>并行执行多个自注意力</strong>。每个头部从不同角度学习输入序列中词之间的关系，帮助模型捕捉到更丰富和多样化的上下文信息。（类似于多位侦探各司其职合作的情况）</p></li>
<li><p><strong>位置编码的必要性与方法:</strong> 变压器不是顺序处理方式，因此需要告诉单词的<strong>位置信息</strong>。位置编码通过将包含每个单词位置信息的向量加到词嵌入上来实现。这样，变压器不仅考虑了单词的意义，还同时考虑了句子内的位置信息以理解上下文。通常使用正弦-余弦函数来表示位置信息。</p></li>
<li><p><strong>编码器与解码器的作用:</strong> 变压器采用编码器-解码器结构。<strong>编码器</strong>接收输入序列并生成反映每个词的上下文表达（上下文向量）。<strong>解码器</strong>基于编码器生成的上下文向量和上一步生成的输出单词，重复预测下一个单词的过程，最终生成输出序列。</p></li>
</ol>
</section>
<section id="应用问题-1" class="level3">
<h3 class="anchored" data-anchor-id="应用问题-1">应用问题</h3>
<ol type="1">
<li><strong>文本摘要任务:</strong>
<ul>
<li><strong>模型设计:</strong> 使用编码器-解码器结构的变压器模型。编码器接收长文本并生成上下文向量，解码器基于此上下文向量生成摘要。在解码器中使用掩码自注意力机制以防止在生成过程中参考未来时点的单词。</li>
<li><strong>评估指标:</strong> 模型性能主要通过 ROUGE (Recall-Oriented Understudy for Gisting Evaluation) 分数进行评估。ROUGE 依据生成的摘要与标准摘要之间重叠的 n-gram（连续的 n 个词）数量来衡量相似度，包括 ROUGE-N, ROUGE-L, ROUGE-S 等不同类型。此外，还可以参考 BLEU (Bilingual Evaluation Understudy) 分数。</li>
</ul></li>
<li><strong>问答系统分析:</strong> 基于变压器的问答系统按照以下步骤处理给定问题以从文档中找到正确答案：
<ol type="1">
<li>将问题和文档分别输入到变压器编码器中获取嵌入向量。</li>
<li>计算问题嵌入与文档嵌入之间的注意力权重。（确定问题中的每个词与文档的哪些词相关）</li>
<li>使用注意力权重计算文档嵌入的加权平均，作为针对该问题的上下文向量使用。</li>
<li>基于上下文向量预测答案的开始位置和结束位置以提取最终答案。 在此过程中，<strong>注意力机制</strong>确定了问题与文档之间的语义相关性，起到了识别对回答问题最为关键的文档部分的关键作用。</li>
</ol></li>
<li><strong>其他领域应用案例调查:</strong>
<ul>
<li><strong>图像:</strong> Vision Transformer (ViT) 将图像分割成多个补丁(patch)，并像处理变压器的输入序列一样处理每个补丁，从而在图像分类、对象检测等任务中表现出色。这表明变压器不仅适用于顺序数据，还能够有效地应用于图像这样的二维数据。</li>
<li><strong>语音:</strong> Conformer 结合了 CNN 和变压器，在语音识别中实现了高精度。通过有效建模语音信号的局部特征(local features)和全局特征(global features)，提高了语音识别性能。</li>
</ul></li>
</ol>
</section>
<section id="深入问题" class="level3">
<h3 class="anchored" data-anchor-id="深入问题">深入问题</h3>
<ol type="1">
<li><p><strong>计算复杂度改进方案比较分析:</strong></p>
<p>变压器由于自注意力机制，具有与输入序列长度成平方的计算复杂度。为了改善这一点，提出了各种方法。</p>
<ul>
<li><strong>Reformer:</strong> 使用局部敏感哈希 (LSH) 注意力来近似计算查询和键之间的相似性。LSH 是一种将类似向量分配到同一桶中的哈希技术，通过这种方式可以避免对整个序列进行注意力计算，并专注于附近的标记以减少计算复杂度。虽然 Reformer 可以大幅减少内存使用和计算时间，但由于 LSH 的近似特性，可能会导致准确度略有下降。</li>
<li><strong>Longformer:</strong> 结合了滑动窗口(sliding window) 注意力和全局注意力(global attention)，以便高效处理长序列。每个标记只对其周围固定大小的窗口内的标记执行注意力操作，并且某些标记（例如：句子开始标记）对整个序列执行注意力操作。Longformer 在处理长序列时速度快、内存使用少，但性能可能因窗口大小而异。</li>
</ul></li>
<li><p><strong>新架构提议及评估:</strong></p>
<ul>
<li><strong>问题定义:</strong> 在分类长文本时，现有的变压器计算复杂度高且难以捕捉长期依赖性。</li>
<li><strong>架构提议:</strong> 将文本分割成多个段(segment)，对每个段应用变压器编码器以获得段嵌入。然后，将这些段嵌入再次输入到变压器编码器中，获取整个文本的表示，并基于此进行分类。</li>
<li><strong>理论优势:</strong> 通过层次结构有效地捕捉长期依赖性并减少计算复杂度。</li>
<li><strong>实验设计:</strong> 使用 IMDB 电影评论数据集等长文本分类数据集来比较所提议架构与现有变压器模型（如 BERT）的性能（准确率、F1-score）。此外，还应改变文本长度、段大小等超参数，并分析其对性能的影响，以验证所提议架构的有效性。</li>
</ul></li>
<li><p><strong>伦理和社会影响分析及应对措施:</strong> 基于变压器的大规模语言模型（如 GPT-3、BERT）的发展可能会对社会产生各种积极和消极的影响。</p></li>
</ol>
<ul>
<li><strong>积极影响：</strong> 通过自动翻译、聊天机器人、虚拟助手等降低沟通障碍，提高信息的可访问性。此外，通过内容生成、代码生成、自动摘要等方式提升生产率，并应用于科学研究（如蛋白质结构预测）、医疗诊断等领域的新应用，加速创新。</li>
<li><strong>消极影响：</strong> 学习训练数据中存在的偏见（性别、种族、宗教等）可能导致歧视性的结果。恶意用户可能大量生成假新闻以操纵舆论或损害特定个人/群体的声誉。此外，自动化写作、翻译、客户服务等可能导致相关行业的就业岗位减少，并可能出现个人信息泄露、版权侵犯等问题。</li>
<li><strong>应对措施：</strong> 为减轻这些消极影响，需要采取技术上和政策上的努力，包括消除数据偏见、开发假新闻检测技术、针对自动化导致的工作变化进行社会讨论并安排再培训计划、增强算法透明度及责任感、制定伦理指南等。</li>
</ul>
</section>
</section>
</div>
</div>
</section>
</section>
<section id="参考资料" class="level2">
<h2 class="anchored" data-anchor-id="参考资料">参考资料</h2>
<ol type="1">
<li><a href="https://arxiv.org/abs/1706.03762">Attention is All You Need</a> (Vaswani 等, 2017) - 变压器原论文</li>
<li><a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html">The Annotated Transformer</a> (哈佛 NLP) - 与 PyTorch 实现一起详细解释变压器</li>
<li><a href="http://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a> (Jay Alammar) - 视觉化解释变压器</li>
<li><a href="https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html">Transformer: A Novel Neural Network Architecture for Language Understanding</a> (Google AI 博客) - 介绍变压器</li>
<li><a href="https://lilianweng.github.io/posts/2020-04-07-the-transformer-family/">The Transformer Family</a> (Lilian Weng) - 介绍各种变压器变体模型</li>
<li><a href="https://arxiv.org/abs/1810.04805">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a> (Devlin 等, 2018) - 介绍 BERT</li>
<li><a href="https://arxiv.org/abs/2005.14165">GPT-3: Language Models are Few-Shot Learners</a> (Brown 等, 2020) - 介绍 GPT-3</li>
<li><a href="https://huggingface.co/transformers/">Hugging Face Transformers</a> - 提供各种变压器模型和工具</li>
<li><a href="https://www.tensorflow.org/text/tutorials/transformer">TensorFlow Transformer Tutorial</a> - 使用 TensorFlow 实现变压器的教程</li>
<li><a href="https://pytorch.org/docs/stable/nn.html#transformer-layers">PyTorch Transformer Documentation</a> - 解释 PyTorch 的变压器模块</li>
<li><a href="https://arxiv.org/abs/1904.02679">Visualizing Attention in Transformer-Based Language Representation Models</a> - 视觉化变压器注意力</li>
<li><a href="https://arxiv.org/abs/2107.03789">A Survey of Long-Term Context in Transformers</a> - 处理长上下文的变压器研究趋势</li>
<li><a href="https://arxiv.org/abs/2001.04451">Reformer: The Efficient Transformer</a> - 提高变压器效率的 Reformer 模型</li>
<li><a href="https://arxiv.org/abs/2009.06732">Efficient Transformers: A Survey</a> - 高效变压器模型研究趋势</li>
<li><a href="https://arxiv.org/abs/2011.04006">Long Range Arena: A Benchmark for Efficient Transformers</a> - 用于处理长上下文的变压器基准</li>
</ol>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>