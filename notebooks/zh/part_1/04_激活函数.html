<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>quarto-input7751a676e6155a25 – Deep Learning DNA: Surviving Architectures and Essential Principles</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap-f507c7d0488cb7630e20aad62ad8c2aa.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>
<script src="https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js" crossorigin="anonymous"></script>
<script>window.MathJax = {loader: {load: ['[tex]/boldsymbol']},tex: {packages: {'[+]': ['boldsymbol']}}};</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../../notebooks/zh/part_1/01_深度学习的开始.html">part_1</a></li><li class="breadcrumb-item"><a href="../../../notebooks/zh/part_1/04_激活函数.html">4. 激活函数</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../../../">中文</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Language</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index_de.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Deutsch</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index_en.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">English</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index_es.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Español</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">한국어</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index_zh.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">中文</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/zh/part_1/00_引言.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">引言</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">part_1</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/zh/part_1/01_深度学习的开始.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">1. 深度学习的开始</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/zh/part_1/02_深度学习的数学.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">2. 深度学习的数学</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/zh/part_1/03_深度学习框架.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">3. 深度学习框架</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/zh/part_1/04_激活函数.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">4. 激活函数</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/zh/part_1/05_优化和可视化.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">5. 优化和可视化</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/zh/part_1/06_过拟合和解决技术的发展.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">6. 过拟合和解决技术的发展</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/zh/part_1/07_卷积神经网络的进化.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">7. 卷积神经网络的进化</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/zh/part_1/08_变压器的诞生.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">8. 变压器的诞生</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/zh/part_1/09_变压器的进化.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">9. 变压器的进化</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/zh/part_1/10_多模态深度学习：多感官融合的开始.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">10. 多模态深度学习：多感官融合的开始</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/zh/part_1/11_多模态深度学习：超越智能的极限.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">11. 多模态深度学习：超越智能的极限</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">深度学习的最前沿</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/zh/深度学习的最前沿/01_SLM: 小但强大的语言模型.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">1. SLM: 小但强大的语言模型</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/zh/深度学习的最前沿/02_自动驾驶.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">2. 自动驾驶</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#激活函数-activation-functions" id="toc-激活函数-activation-functions" class="nav-link active" data-scroll-target="#激活函数-activation-functions">4 激活函数 (Activation Functions)</a>
  <ul class="collapse">
  <li><a href="#激活函数向神经网络引入非线性" id="toc-激活函数向神经网络引入非线性" class="nav-link" data-scroll-target="#激活函数向神经网络引入非线性">4.1 激活函数：向神经网络引入非线性</a>
  <ul class="collapse">
  <li><a href="#激活函数的作用克服线性的局限" id="toc-激活函数的作用克服线性的局限" class="nav-link" data-scroll-target="#激活函数的作用克服线性的局限">4.1.1 激活函数的作用：克服线性的局限</a></li>
  <li><a href="#激活函数的选择模型规模任务和效率" id="toc-激活函数的选择模型规模任务和效率" class="nav-link" data-scroll-target="#激活函数的选择模型规模任务和效率">4.1.3 激活函数的选择：模型规模、任务和效率</a></li>
  </ul></li>
  <li><a href="#激活函数的比较" id="toc-激活函数的比较" class="nav-link" data-scroll-target="#激活函数的比较">4.2 激活函数的比较</a>
  <ul class="collapse">
  <li><a href="#激活函数生成" id="toc-激活函数生成" class="nav-link" data-scroll-target="#激活函数生成">4.2.1 激活函数生成</a></li>
  <li><a href="#激活函数的可视化" id="toc-激活函数的可视化" class="nav-link" data-scroll-target="#激活函数的可视化">4.2.2 激活函数的可视化</a></li>
  <li><a href="#激活函数对比表" id="toc-激活函数对比表" class="nav-link" data-scroll-target="#激活函数对比表">4.2.3 激活函数对比表</a></li>
  <li><a href="#可视化激活函数对神经网络的影响" id="toc-可视化激活函数对神经网络的影响" class="nav-link" data-scroll-target="#可视化激活函数对神经网络的影响">4.3 可视化激活函数对神经网络的影响</a></li>
  <li><a href="#模型训练" id="toc-模型训练" class="nav-link" data-scroll-target="#模型训练">4.4 模型训练</a></li>
  <li><a href="#训练模型的逐层输出和非激活神经元分析" id="toc-训练模型的逐层输出和非激活神经元分析" class="nav-link" data-scroll-target="#训练模型的逐层输出和非激活神经元分析">4.5 训练模型的逐层输出和非激活神经元分析</a></li>
  <li><a href="#确定激活函数候选" id="toc-确定激活函数候选" class="nav-link" data-scroll-target="#确定激活函数候选">4.6 确定激活函数候选</a></li>
  </ul></li>
  <li><a href="#练习题" id="toc-练习题" class="nav-link" data-scroll-target="#练习题">练习题</a>
  <ul class="collapse">
  <li><a href="#基本问题" id="toc-基本问题" class="nav-link" data-scroll-target="#基本问题">4.2.1 基本问题</a></li>
  <li><a href="#应用问题" id="toc-应用问题" class="nav-link" data-scroll-target="#应用问题">4.2.2 应用问题</a></li>
  <li><a href="#深化问题" id="toc-深化问题" class="nav-link" data-scroll-target="#深化问题">4.2.3 深化问题</a></li>
  <li><a href="#参考资料" id="toc-参考资料" class="nav-link" data-scroll-target="#参考资料">参考资料</a></li>
  </ul></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../../notebooks/zh/part_1/01_深度学习的开始.html">part_1</a></li><li class="breadcrumb-item"><a href="../../../notebooks/zh/part_1/04_激活函数.html">4. 激活函数</a></li></ol></nav></header>




<p><a href="https://colab.research.google.com/github/Quantum-Intelligence-Frontier/dldna/blob/main/notebooks/zh/part_1/04_激活函数.ipynb" target="_parent"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="在 Colab 中打开"> </a></p>
<section id="激活函数-activation-functions" class="level1">
<h1>4 激活函数 (Activation Functions)</h1>
<blockquote class="blockquote">
<p>“如果你确切知道自己在做什么，那就不叫研究。” - 阿尔伯特·爱因斯坦</p>
</blockquote>
<p>在深度学习的历史中，激活函数和优化技术取得了非常重要的进展。1943年，当McCulloch-Pitts的人工神经元模型首次出现时，仅使用了简单的阈值函数（阶跃函数）。这是模仿生物神经元操作方式的模型，即神经元仅在输入超过特定阈值时激活。然而，这种简单形式的激活函数限制了神经网络表示复杂函数的能力。</p>
<p>直到1980年代，机器学习仍然侧重于特征工程(feature engineering)和复杂的算法设计。神经网络只是众多机器学习算法之一，在许多情况下，传统的SVM（支持向量机）或随机森林等算法表现更为出色。例如，在MNIST手写识别问题中，SVM在2012年之前一直保持最高的准确度。</p>
<p>2012年，AlexNet利用GPU实现了高效的训练，并在ImageNet挑战赛中取得了压倒性的性能，这标志着深度学习时代的真正开始。2017年，谷歌的Transformer架构进一步推动了这一创新的发展，成为当今GPT-4、Gemini等大规模语言模型（LLM）的基础。</p>
<p>这些发展的核心是<strong>激活函数的进化</strong>和<strong>优化技术的进步</strong>。在本章中，我们将详细探讨激活函数，旨在为你们开发新模型和解决复杂问题提供必要的理论基础。</p>
<section id="激活函数向神经网络引入非线性" class="level2">
<h2 class="anchored" data-anchor-id="激活函数向神经网络引入非线性">4.1 激活函数：向神经网络引入非线性</h2>
<blockquote class="blockquote">
<p><strong>研究者的苦恼:</strong> 初期的神经网络研究人员意识到仅通过线性变换无法解决复杂的问题。但使用哪种非线性函数可以使神经网络有效学习并解决各种问题是不清楚的。模仿生物神经元的操作是最佳方法吗？还是有具有更好数学、计算特性的其他函数？</p>
</blockquote>
<p>激活函数是在神经网络层之间引入非线性的关键元素。1.4.1节中提到的<strong>通用近似定理(Universal Approximation Theorem)</strong>（1988年）证明，一个具有<em>非线性</em>激活函数的隐藏层的神经网络可以逼近任何连续函数。也就是说，通过在层之间引入分离和非线性，激活函数使神经网络能够超越简单线性模型的限制，作为通用函数近似器(universal function approximator)运行。</p>
<section id="激活函数的作用克服线性的局限" class="level3">
<h3 class="anchored" data-anchor-id="激活函数的作用克服线性的局限">4.1.1 激活函数的作用：克服线性的局限</h3>
<p>如果没有激活函数，无论叠加多少层，神经网络最终只是一个<em>线性变换</em>。这一点可以通过以下简单的证明来说明。</p>
<p>考虑连续应用两个线性变换的情况：</p>
<ul>
<li>第一层: <span class="math inline">\(y_1 = W_1x + b_1\)</span></li>
<li>第二层: <span class="math inline">\(y_2 = W_2y_1 + b_2\)</span></li>
</ul>
<p>这里，<span class="math inline">\(x\)</span>是输入，<span class="math inline">\(W_1\)</span>、<span class="math inline">\(W_2\)</span>是权重矩阵，<span class="math inline">\(b_1\)</span>、<span class="math inline">\(b_2\)</span>是偏置向量。将第一层的表达式代入第二层的表达式中：</p>
<p><span class="math inline">\(y_2 = W_2(W_1x + b_1) + b_2 = (W_2W_1)x + (W_2b_1 + b_2)\)</span></p>
<p>定义新的权重矩阵 <span class="math inline">\(W' = W_2W_1\)</span> 和新的偏置向量 <span class="math inline">\(b' = W_2b_1 + b_2\)</span>，</p>
<p><span class="math inline">\(y_2 = W'x + b'\)</span></p>
<p>这最终等同于<em>一个线性变换</em>。无论叠加多少层都是如此。因此，仅通过线性变换无法表示复杂的非线性关系。 ### 4.1.2 激活函数的进化：从生物模仿到高效计算</p>
<ul>
<li><p><strong>1943年，McCulloch-Pitts 神经元</strong>：在最初的神经元模型中使用了简单的<em>阈值函数(threshold function)</em>，即阶跃函数(step function)。这是对生物神经元工作方式的模拟，即仅当输入超过特定阈值时神经元才激活。</p>
<p><span class="math display">\[
f(x) = \begin{cases}
1, &amp; \text{if } x \ge \theta \\
0, &amp; \text{if } x &lt; \theta
\end{cases}
\]</span></p>
<p>其中 <span class="math inline">\(\theta\)</span> 是阈值。</p></li>
<li><p><strong>20世纪60年代，Sigmoid 函数</strong>：为了更平滑地模拟生物神经元的放电率(firing rate)，引入了Sigmoid函数。Sigmoid函数呈S形曲线，将输入值压缩在0和1之间。</p>
<p><span class="math display">\[
\sigma(x) = \frac{1}{1 + e^{-x}}
\]</span></p>
<p>Sigmoid函数的优点在于可微分性，这使得可以应用基于梯度下降(gradient descent)的学习算法。然而，Sigmoid函数被认为是导致深度神经网络中<em>梯度消失问题(vanishing gradient problem)</em>的原因之一。当输入值非常大或非常小时，Sigmoid函数的斜率（导数值）接近0，从而导致学习过程变慢甚至停止。</p></li>
<li><p><strong>2010年，ReLU(Rectified Linear Unit)</strong>：Nair和Hinton提出了ReLU函数，开启了深度神经网络学习的新纪元。ReLU具有非常简单的形式。</p>
<p><span class="math display">\[
ReLU(x) = \max(0, x)
\]</span></p>
<p>当输入大于0时，ReLU将输入值直接输出；当输入小于0时，输出0。与Sigmoid函数不同，ReLU的梯度消失问题较少发生，并且计算效率更高。由于这些优点，ReLU对深度神经网络的成功做出了巨大贡献，目前是使用最广泛的激活函数之一。</p></li>
</ul>
</section>
<section id="激活函数的选择模型规模任务和效率" class="level3">
<h3 class="anchored" data-anchor-id="激活函数的选择模型规模任务和效率">4.1.3 激活函数的选择：模型规模、任务和效率</h3>
<p>激活函数的选择对模型的性能和效率有很大影响。</p>
<ul>
<li><p><strong>大规模语言模型(LLM)</strong>：由于计算效率非常重要，因此倾向于选择简单的激活函数。最新的基础模型如Llama 3、GPT-4、Gemini等采用了像GELU(Gaussian Error Linear Unit)或ReLU这样简单高效的激活函数。特别是Gemini 1.5引入了MoE(Mixture of Experts)架构，在每个专家网络(expert network)中使用优化的激活函数。</p></li>
<li><p><strong>特定目的模型</strong>：在开发针对特定任务优化的模型时，有时会尝试更精细的方法。例如，像TEAL这样的最新研究表明，通过激活稀疏性(activation sparsity)可以将推理速度提高至1.8倍。此外，还有一些研究正在探索根据输入数据动态调整行为的自适应激活函数(adaptive activation functions)。</p></li>
</ul>
<p>选择激活函数时应综合考虑模型规模、任务特性、可用计算资源以及所需的性能特征（准确性、速度、内存使用量等）。</p>
</section>
</section>
<section id="激活函数的比较" class="level2">
<h2 class="anchored" data-anchor-id="激活函数的比较">4.2 激活函数的比较</h2>
<blockquote class="blockquote">
<p><strong>挑战:</strong> 在众多激活函数中，哪种函数最适合特定问题和架构？</p>
<p><strong>研究者的困境:</strong> 截至2025年，已提出了500多种激活函数，但没有一种能够在所有情况下完美适用。研究人员需要理解每个函数的特点，并考虑问题特性、模型架构、计算资源等因素来选择最合适的激活函数，甚至开发新的激活函数。</p>
</blockquote>
<p>激活函数通常要求具备以下性质： 1. 必须为神经网络增加非线性曲率 2. 不应过度增加计算复杂度，以至于使训练变得困难 3. 为了不妨碍梯度流动，必须是可微的 4. 训练时各层数据分布应适当</p>
<p>许多符合这些要求且高效的激活函数已被提出。无法一言以蔽之哪种激活函数最好，因为这取决于所训练的模型和数据等。找到最优激活函数的方法是进行实际测试。</p>
<p>截至2025年，激活函数主要分为三大类： 1. 经典激活函数：Sigmoid、Tanh、ReLU 等固定形式的函数。 2. 自适应激活函数：PReLU、TeLU、STAF 等在学习过程中形态可调的参数包含其中。 3. 专业激活函数：ENN（表达性神经网络）、物理信息激活函数等针对特定领域优化的函数。</p>
<p>本章将比较多种激活函数。虽然大多数实现都是基于PyTorch，但对于Swish、STAF等未实现的，则继承<code>nn.Module</code>重新创建。全部实现位于<code>chapter_04/models/activations.py</code>中。</p>
<section id="激活函数生成" class="level3">
<h3 class="anchored" data-anchor-id="激活函数生成">4.2.1 激活函数生成</h3>
<div id="cell-3" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip install dldna[colab] <span class="co"># in Colab</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="co"># !pip install dldna[all] # in your local</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>load_ext autoreload</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>autoreload <span class="dv">2</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="cell-4" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Set seed</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">7</span>)</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">7</span>)</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="co"># STAF (Sinusoidal Trainable Activation Function)</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> STAF(nn.Module):</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, tau<span class="op">=</span><span class="dv">25</span>):</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.tau <span class="op">=</span> tau</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.C <span class="op">=</span> nn.Parameter(torch.randn(tau))</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.Omega <span class="op">=</span> nn.Parameter(torch.randn(tau))</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.Phi <span class="op">=</span> nn.Parameter(torch.randn(tau))</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>        result <span class="op">=</span> torch.zeros_like(x)</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.tau):</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>            result <span class="op">+=</span> <span class="va">self</span>.C[i] <span class="op">*</span> torch.sin(<span class="va">self</span>.Omega[i] <span class="op">*</span> x <span class="op">+</span> <span class="va">self</span>.Phi[i])</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> result</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a><span class="co"># TeLU (Trainable exponential Linear Unit)</span></span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> TeLU(nn.Module):</span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, alpha<span class="op">=</span><span class="fl">1.0</span>):</span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.alpha <span class="op">=</span> nn.Parameter(torch.tensor(alpha))</span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.where(x <span class="op">&gt;</span> <span class="dv">0</span>, x, <span class="va">self</span>.alpha <span class="op">*</span> (torch.exp(x) <span class="op">-</span> <span class="dv">1</span>))</span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a><span class="co"># Swish (Custom Implementation)</span></span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Swish(nn.Module):</span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x <span class="op">*</span> torch.sigmoid(x)</span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a><span class="co"># Activation function dictionary</span></span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a>act_functions <span class="op">=</span> {</span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Classic activation functions</span></span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Sigmoid"</span>: nn.Sigmoid,     <span class="co"># Binary classification output layer</span></span>
<span id="cb2-42"><a href="#cb2-42" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Tanh"</span>: nn.Tanh,          <span class="co"># RNN/LSTM</span></span>
<span id="cb2-43"><a href="#cb2-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-44"><a href="#cb2-44" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Modern basic activation functions</span></span>
<span id="cb2-45"><a href="#cb2-45" aria-hidden="true" tabindex="-1"></a>    <span class="st">"ReLU"</span>: nn.ReLU,          <span class="co"># CNN default</span></span>
<span id="cb2-46"><a href="#cb2-46" aria-hidden="true" tabindex="-1"></a>    <span class="st">"GELU"</span>: nn.GELU,          <span class="co"># Transformer standard</span></span>
<span id="cb2-47"><a href="#cb2-47" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Mish"</span>: nn.Mish,          <span class="co"># Performance/stability balance</span></span>
<span id="cb2-48"><a href="#cb2-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-49"><a href="#cb2-49" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ReLU variants</span></span>
<span id="cb2-50"><a href="#cb2-50" aria-hidden="true" tabindex="-1"></a>    <span class="st">"LeakyReLU"</span>: nn.LeakyReLU,<span class="co"># Handles negative inputs</span></span>
<span id="cb2-51"><a href="#cb2-51" aria-hidden="true" tabindex="-1"></a>    <span class="st">"SiLU"</span>: nn.SiLU,          <span class="co"># Efficient sigmoid</span></span>
<span id="cb2-52"><a href="#cb2-52" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Hardswish"</span>: nn.Hardswish,<span class="co"># Mobile optimized</span></span>
<span id="cb2-53"><a href="#cb2-53" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Swish"</span>: Swish,           <span class="co"># Custom implementation</span></span>
<span id="cb2-54"><a href="#cb2-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-55"><a href="#cb2-55" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Adaptive/trainable activation functions</span></span>
<span id="cb2-56"><a href="#cb2-56" aria-hidden="true" tabindex="-1"></a>    <span class="st">"PReLU"</span>: nn.PReLU,        <span class="co"># Trainable slope</span></span>
<span id="cb2-57"><a href="#cb2-57" aria-hidden="true" tabindex="-1"></a>    <span class="st">"RReLU"</span>: nn.RReLU,        <span class="co"># Randomized slope</span></span>
<span id="cb2-58"><a href="#cb2-58" aria-hidden="true" tabindex="-1"></a>    <span class="st">"TeLU"</span>: TeLU,             <span class="co"># Trainable exponential</span></span>
<span id="cb2-59"><a href="#cb2-59" aria-hidden="true" tabindex="-1"></a>    <span class="st">"STAF"</span>: STAF             <span class="co"># Fourier-based</span></span>
<span id="cb2-60"><a href="#cb2-60" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>STAF是在2025年ICLR上介绍的最新激活函数，使用了基于傅里叶级数的学习参数。ENN采用了DCT来提高网络的表达能力。TeLU是ELU的扩展形式，将alpha参数设置为可学习版本。</p>
</section>
<section id="激活函数的可视化" class="level3">
<h3 class="anchored" data-anchor-id="激活函数的可视化">4.2.2 激活函数的可视化</h3>
<p>通过可视化激活函数和梯度来比较其特性。利用PyTorch的自动微分功能，可以通过调用backward()轻松计算梯度。以下是一个通过可视化分析激活函数特性的示例。梯度流的计算是通过给定的激活函数在一定范围内的输入值进行的。<code>compute_gradient_flow</code>方法负责这一任务。</p>
<div id="cell-6" class="cell" data-execution_count="3">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> compute_gradient_flow(activation, x_range<span class="op">=</span>(<span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>), y_range<span class="op">=</span>(<span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>), points<span class="op">=</span><span class="dv">100</span>):</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Computes the 3D gradient flow.</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Calculates the output surface of the activation function for two-dimensional</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="co">    inputs and the magnitude of the gradient with respect to those inputs.</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="co">        activation: Activation function (nn.Module or function).</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="co">        x_range (tuple): Range for the x-axis (default: (-5, 5)).</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="co">        y_range (tuple): Range for the y-axis (default: (-5, 5)).</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a><span class="co">        points (int): Number of points to use for each axis (default: 100).</span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a><span class="co">        X, Y (ndarray): Meshgrid coordinates.</span></span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a><span class="co">        Z (ndarray): Activation function output values.</span></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a><span class="co">        grad_magnitude (ndarray): Gradient magnitude at each point.</span></span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> np.linspace(x_range[<span class="dv">0</span>], x_range[<span class="dv">1</span>], points)</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> np.linspace(y_range[<span class="dv">0</span>], y_range[<span class="dv">1</span>], points)</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>    X, Y <span class="op">=</span> np.meshgrid(x, y)</span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Stack the two dimensions to create a 2D input tensor (first row: X, second row: Y)</span></span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>    input_tensor <span class="op">=</span> torch.tensor(np.stack([X, Y], axis<span class="op">=</span><span class="dv">0</span>), dtype<span class="op">=</span>torch.float32, requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Construct the surface as the sum of the activation function outputs for the two inputs</span></span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a>    Z <span class="op">=</span> activation(input_tensor[<span class="dv">0</span>]) <span class="op">+</span> activation(input_tensor[<span class="dv">1</span>])</span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a>    Z.<span class="bu">sum</span>().backward()</span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a>    grad_x <span class="op">=</span> input_tensor.grad[<span class="dv">0</span>].numpy()</span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a>    grad_y <span class="op">=</span> input_tensor.grad[<span class="dv">1</span>].numpy()</span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a>    grad_magnitude <span class="op">=</span> np.sqrt(grad_x<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> grad_y<span class="op">**</span><span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>对所有定义的激活函数执行3D可视化。</p>
<div id="cell-8" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_04.visualization.activations <span class="im">import</span> visualize_all_activations</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>visualize_all_activations()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_激活函数_files/figure-html/cell-5-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_激活函数_files/figure-html/cell-5-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_激活函数_files/figure-html/cell-5-output-3.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_激活函数_files/figure-html/cell-5-output-4.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_激活函数_files/figure-html/cell-5-output-5.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_激活函数_files/figure-html/cell-5-output-6.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_激活函数_files/figure-html/cell-5-output-7.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_激活函数_files/figure-html/cell-5-output-8.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_激活函数_files/figure-html/cell-5-output-9.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_激活函数_files/figure-html/cell-5-output-10.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_激活函数_files/figure-html/cell-5-output-11.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_激活函数_files/figure-html/cell-5-output-12.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_激活函数_files/figure-html/cell-5-output-13.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>图形表示了两个输入（X轴，Y轴）的输出值（Z轴）和梯度大小（热图）。</p>
<ol type="1">
<li><p><strong>Sigmoid:</strong> 呈“S”形。两端趋近于0和1且较为平缓，中间部分较陡。将输入压缩在0到1之间。梯度在两端接近0，几乎消失，在中间较大。非常大或非常小的输入可能会导致“梯度消失”问题，从而减慢学习速度。</p></li>
<li><p><strong>ReLU:</strong> 呈斜坡状。如果任何一个输入为负数，则输出为0且平缓；两个输入都为正时则沿对角线上升。梯度在负数输入处为0，在正数输入处保持恒定。由于在正数输入时没有梯度消失问题，因此计算效率高，被广泛使用。</p></li>
<li><p><strong>GELU:</strong> 类似于Sigmoid但更为平滑。左侧略微向下弯曲，右侧超过1。梯度逐渐变化且没有为0的区间。即使在非常小的负数输入下，梯度也不会完全消失，有利于学习。在Transformer等最新模型中使用。</p></li>
<li><p><strong>STAF:</strong> 呈波浪状。基于正弦函数，并可通过可学习参数调整振幅、频率和相位。神经网络可以自我学习适合任务的激活函数形状。梯度复杂变化。有利于非线性关系的学习。</p></li>
</ol>
<p>3D图形（Surface）显示了两个输入经过激活函数后的输出值，结果显示在Z轴上。热图（Gradient Magnitude）表示梯度大小，即输入变化引起的输出变化率，颜色越亮表示梯度越大。这些可视化资料展示了每个激活函数如何转换输入，并显示了梯度在哪一区域较强或较弱，对于理解神经网络的学习过程非常重要。</p>
</section>
<section id="激活函数对比表" class="level3">
<h3 class="anchored">4.2.3 激活函数对比表</h3>
<p>激活函数是赋予神经网络非线性的核心元素，其特性在梯度形态中表现得尤为明显。最新的深度学习模型会根据任务和架构的特性选择合适的激活函数，或者使用可学习的自适应激活函数。</p>
<section id="激活函数比较总结" class="level4">
<h4 class="anchored">激活函数比较总结</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 14%">
<col style="width: 21%">
<col style="width: 14%">
<col style="width: 26%">
<col style="width: 21%">
</colgroup>
<thead>
<tr class="header">
<th>分类</th>
<th>激活函数</th>
<th>特性</th>
<th>主要用途</th>
<th>优缺点</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>经典</td>
<td>Sigmoid</td>
<td>输出在0到1之间归一化，并且具有平滑的梯度，可以很好地捕捉连续特征的变化</td>
<td>二元分类输出层</td>
<td>在深度神经网络中可能会引起梯度消失问题</td>
</tr>
<tr class="even">
<td></td>
<td>Tanh</td>
<td>类似于Sigmoid，但输出范围为-1到1，在接近0处表现出更陡峭的梯度，学习更为有效</td>
<td>RNN/LSTM门</td>
<td>输出中心化有利于学习，但仍可能发生梯度消失</td>
</tr>
<tr class="odd">
<td>现代基础</td>
<td>ReLU</td>
<td>当x小于0时梯度为0，大于0时为1，具有简单的结构，对边界检测有用</td>
<td>CNN基础</td>
<td>计算非常高效，但在负输入处神经元可能完全失活</td>
</tr>
<tr class="even">
<td></td>
<td>GELU</td>
<td>结合ReLU的特性和高斯累积分布函数，提供平滑的非线性</td>
<td>变压器</td>
<td>具有自然的正则化效果，但计算成本高于ReLU</td>
</tr>
<tr class="odd">
<td></td>
<td>Mish</td>
<td>拥有平滑的梯度和自正则特性，在多种任务中表现出稳定性能</td>
<td>通用目的</td>
<td>性能与稳定性平衡良好，但计算复杂度增加</td>
</tr>
<tr class="even">
<td>ReLU变体</td>
<td>LeakyReLU</td>
<td>允许负输入具有小斜率以减少信息损失</td>
<td>CNN</td>
<td>缓解死神经元问题，但需要手动设置斜率值</td>
</tr>
<tr class="odd">
<td></td>
<td>Hardswish</td>
<td>为移动网络优化设计的计算高效版本</td>
<td>移动网络</td>
<td>轻量结构效率高，但表达能力略有限</td>
</tr>
<tr class="even">
<td></td>
<td>Swish</td>
<td>x与Sigmoid的乘积，提供平滑的梯度和弱边界效应</td>
<td>深层网络</td>
<td>边界平滑使学习更加稳定，但计算成本增加</td>
</tr>
<tr class="odd">
<td>自适应型</td>
<td>PReLU</td>
<td>能够学习负区域的斜率，以数据为依据找到最佳形态</td>
<td>CNN</td>
<td>对数据具有自适应性，但由于额外参数可能导致过拟合</td>
</tr>
<tr class="even">
<td></td>
<td>RReLU</td>
<td>训练时在负区使用随机斜率防止过拟合</td>
<td>通用目的</td>
<td>具有正则化效果，但结果可重复性可能降低</td>
</tr>
<tr class="odd">
<td></td>
<td>TeLU</td>
<td>学习指数函数的缩放以增强ELU的优势，并根据数据进行调整</td>
<td>通用目的</td>
<td>增强了ELU的优点，但收敛可能不稳定</td>
</tr>
<tr class="even">
<td></td>
<td>STAF</td>
<td>基于傅立叶级数学习复杂的非线性模式并提供高表达能力</td>
<td>复杂模式</td>
<td>表达能力极高，但计算成本和内存使用量大</td>
</tr>
</tbody>
</table>
<div class="callout callout-style-default callout-note callout-titled" title="点击查看内容（深入研究：激活函数的数学特性及最新研究趋势）">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
点击查看内容（深入研究：激活函数的数学特性及最新研究趋势）
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<section id="激活函数的数学特性与最新研究趋势" class="level2 callout-body-container callout-body">
<h2 class="anchored" data-anchor-id="激活函数的数学特性与最新研究趋势">激活函数的数学特性与最新研究趋势</h2>
<section id="主要激活函数的数学定义特点及其在深度学习中的作用" class="level3">
<h3 class="anchored" data-anchor-id="主要激活函数的数学定义特点及其在深度学习中的作用">1. 主要激活函数的数学定义、特点及其在深度学习中的作用</h3>
<table class="caption-top table">
<colgroup>
<col style="width: 57%">
<col style="width: 21%">
<col style="width: 21%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">激活函数</th>
<th style="text-align: left;">公式</th>
<th style="text-align: left;">数学特征及在深度学习中的作用</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Sigmoid</strong></td>
<td style="text-align: left;"><span class="math inline">\(\sigma(x) = \frac{1}{1 + e^{-x}}\)</span></td>
<td style="text-align: left;"><strong>历史意义</strong>: - 1943年首次用于McCulloch-Pitts神经网络模型 <strong>最新研究</strong>: - NTK理论中证明了无限宽网络的线性可分性 - <span class="math inline">\(\frac{\partial^2 \mathcal{L}}{\partial w_{ij}^2} = \sigma(x)(1-\sigma(x))(1-2\sigma(x))x_i x_j\)</span> (凸度变化)</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Tanh</strong></td>
<td style="text-align: left;"><span class="math inline">\(tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}\)</span></td>
<td style="text-align: left;"><strong>动力学分析</strong>: - 李雅普诺夫指数 <span class="math inline">\(\lambda_{max} \approx 0.9\)</span> 引发混沌动态 - 在LSTM的遗忘门中使用时: <span class="math inline">\(\frac{\partial c_t}{\partial c_{t-1}} = tanh'( \cdot )W_c\)</span> (缓解梯度爆炸)</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>ReLU</strong></td>
<td style="text-align: left;"><span class="math inline">\(ReLU(x) = max(0, x)\)</span></td>
<td style="text-align: left;"><strong>损失景观</strong>: - 2023年的研究表明ReLU神经网络的损失景观是分段凸的 - Dying ReLU概率: <span class="math inline">\(\prod_{l=1}^L \Phi(-\mu_l/\sigma_l)\)</span> (层间均值/方差)</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Leaky ReLU</strong></td>
<td style="text-align: left;"><span class="math inline">\(LReLU(x) = max(αx, x)\)</span></td>
<td style="text-align: left;"><strong>优化优势</strong>: - 2024年SGD收敛率分析: <span class="math inline">\(O(1/\sqrt{T})\)</span> → <span class="math inline">\(O(1/T)\)</span> 改进 - NTK谱: <span class="math inline">\(\lambda_{min} \geq α\)</span> 保证条件数改善</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>ELU</strong></td>
<td style="text-align: left;"><span class="math inline">\(ELU(x) = \begin{cases} x &amp; x&gt;0 \\ \alpha \cdot (e^x - 1) &amp; x≤0 \end{cases}\)</span></td>
<td style="text-align: left;"><strong>动力学特征</strong>: - 结合ReLU的收敛速度和GELU的稳定性 - <span class="math inline">\(tanh(e^x)\)</span>项在负区域实现平滑过渡 - Hessian谱分析显示23%更快的收敛速度</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>SwiGLU</strong></td>
<td style="text-align: left;"><span class="math inline">\(SwiGLU(x) = Swish(xW + b) \otimes (xV + c)\)</span></td>
<td style="text-align: left;"><strong>变压器优化</strong>: - LLAMA 2及EVA-02模型中15%准确度提升 - 结合GLU门机制和Swish的自我门控效果 - <span class="math inline">\(\beta=1.7889\)</span>时达到最佳性能</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Adaptive Sigmoid</strong></td>
<td style="text-align: left;"><span class="math inline">\(\sigma_{adapt}(x) = \frac{1}{1 + e^{-k(x-\theta)}}\)</span></td>
<td style="text-align: left;"><strong>自适应学习</strong>: - 可学习的<span class="math inline">\(k\)</span>和<span class="math inline">\(\theta\)</span>参数动态调整形状 - 在SSHG模型中比传统Sigmoid快37%收敛 - 负区域信息保留率提高89%</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>SGT (Scaled Gamma-Tanh)</strong></td>
<td style="text-align: left;"><span class="math inline">\(SGT(x) = \Gamma(1.5) \cdot tanh(\gamma x)\)</span></td>
<td style="text-align: left;"><strong>医疗影像专用</strong>: - 在3D CNN中比ReLU高12%的DSC分数 - <span class="math inline">\(\gamma\)</span>参数反映局部特征 - 基于Fokker-Planck方程的稳定性证明</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>NIPUNA</strong></td>
<td style="text-align: left;"><span class="math inline">\(NIPUNA(x) = \begin{cases} x &amp; x&gt;0 \\ \alpha \cdot softplus(x) &amp; x≤0 \end{cases}\)</span></td>
<td style="text-align: left;"><strong>优化融合</strong>: - 与BFGS算法结合时实现二次收敛速度 - 负区域梯度噪声比ELU低18% - ImageNet中ResNet-50基准Top-1达81.3%</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>TeLU</strong></td>
<td style="text-align: left;"><span class="math inline">\(TeLU(x) = x \cdot tanh(e^x)\)</span></td>
<td style="text-align: left;"><strong>动力学特性</strong>: - 结合ReLU的收敛速度和GELU的稳定性 - <span class="math inline">\(tanh(e^x)\)</span>项在负区域实现平滑过渡 - Hessian谱分析显示23%更快的收敛速度</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Swish</strong></td>
<td style="text-align: left;"><span class="math inline">\(Swish(x) = x \cdot \sigma(\beta x)\)</span></td>
<td style="text-align: left;"><strong>自适应激活</strong>: - <span class="math inline">\(\beta\)</span>参数可学习，调整非线性程度 - 在ImageNet上比ReLU性能提高1.1% - 负区域保持信息能力更强</td>
</tr>
</tbody>
</table>
</section>
<section id="损失景观高级分析" class="level3">
<h3 class="anchored" data-anchor-id="损失景观高级分析">2. 损失景观高级分析</h3>
<ol type="1">
<li><p><strong>不同激活函数的损失Hessian谱</strong></p>
<p><span class="math display">\[\rho(\lambda) = \frac{1}{d}\sum_{i=1}^d \delta(\lambda-\lambda_i)\]</span></p>
<ul>
<li>ReLU: Marchenko-Pastur分布偏差42%</li>
<li>GELU: 接近半圆定律 (KLD 0.12)<br>
</li>
<li>Mish: 重尾分布 (α=2.3)</li>
</ul></li>
<li><p><strong>动态不稳定性指数</strong><br>
<span class="math display">\[\xi = \frac{\mathbb{E}[\| \nabla^2 \mathcal{L} \|_F]}{\mathbb{E}[ \| \nabla \mathcal{L} \|^2 ]}\]</span></p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>激活函数</th>
<th>ξ 值</th>
<th>学习稳定性</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>ReLU</td>
<td>1.78</td>
<td>低</td>
</tr>
<tr class="even">
<td>GELU</td>
<td>0.92</td>
<td>中等</td>
</tr>
<tr class="odd">
<td>Mish</td>
<td>0.61</td>
<td>高</td>
</tr>
</tbody>
</table></li>
<li><p><strong>最新优化理论的相互作用</strong></p>
<ul>
<li><strong>LION优化器</strong>: <span class="math inline">\(m_t = β_1 m_{t-1} + (1-β_1)sign(g_t)\)</span><br>
→ 在ReLU系列中学习率可能增加37%<br>
</li>
<li><strong>Sophia</strong>: 基于Hessian估计的预调节<br>
<span class="math display">\[\eta_{eff} = \eta / \sqrt{\mathbb{E}[H_{diag}] + \epsilon}\]</span><br>
→ 在Swish上比Adam快2倍</li>
</ul></li>
</ol>
</section>
<section id="局部最小值鞍点损失景观数学分析与最新研究" class="level3">
<h3 class="anchored" data-anchor-id="局部最小值鞍点损失景观数学分析与最新研究">3. 局部最小值、鞍点、损失景观：数学分析与最新研究</h3>
<section id="损失函数地形的几何特性" class="level4">
<h4 class="anchored" data-anchor-id="损失函数地形的几何特性">损失函数地形的几何特性</h4>
<p>深度神经网络的损失函数 <span class="math inline">\(\mathcal{L}(\theta)\)</span> 是在高维参数空间 <span class="math inline">\(\theta \in \mathbb{R}^d\)</span> (通常 <span class="math inline">\(d &gt; 10^6\)</span>) 上定义的非凸（non-convex）函数。以下公式通过二次泰勒展开来分析临界点附近的地形。</p>
<p><span class="math display">\[
\mathcal{L}(\theta + \Delta\theta) \approx \mathcal{L}(\theta) + \nabla\mathcal{L}(\theta)^T\Delta\theta + \frac{1}{2}\Delta\theta^T\mathbf{H}\Delta\theta
\]</span></p>
<p>其中 <span class="math inline">\(\mathbf{H} = \nabla^2\mathcal{L}(\theta)\)</span> 是Hessian矩阵。临界点(<span class="math inline">\(\nabla\mathcal{L}=0\)</span>)处的地形由Hessian的特征值分解决定。</p>
<p><span class="math display">\[
\mathbf{H} = \mathbf{Q}\Lambda\mathbf{Q}^T, \quad \Lambda = \text{diag}(\lambda_1, ..., \lambda_d)
\]</span></p>
<p><strong>关键观察</strong></p>
<ol type="1">
<li><strong>高维空间中的鞍点优势</strong>: Dauphin等 (2014) [^1] 证明了在 <span class="math inline">\(d\)</span> 维空间中，临界点为鞍点的概率收敛于 <span class="math inline">\(1 - (1/2)^{d-1}\)</span></li>
<li><strong>平缓最小值的泛化</strong>: Chaudhari等 (2017) [^2] 实验性地证明，平滑的最小值(<span class="math inline">\(\lambda_{\min}(\mathbf{H}) \geq -\epsilon\)</span>) 比尖锐的最小值具有更低的测试误差</li>
</ol>
</section>
<section id="最新分析技术" class="level4">
<h4 class="anchored" data-anchor-id="最新分析技术">最新分析技术</h4>
<p><strong>神经切线核 (NTK) 理论</strong> [Jacot等, 2018] 解释无限宽神经网络中参数更新动态的关键工具</p>
<p><span class="math display">\[
\mathbf{K}_{NTK}(x_i, x_j) = \mathbb{E}_{\theta\sim p}[\langle \nabla_\theta f(x_i), \nabla_\theta f(x_j) \rangle]
\]</span> - 当NTK随时间保持恒定时，损失函数表现出凸性（convex）行为 - 在实际的有限神经网络中，NTK进化决定了学习动力学</p>
<p><strong>损失景观可视化技术</strong> [Li et al., 2018]]: 通过滤波器归一化（Filter Normalization）进行高维地形投影</p>
<p><span class="math display">\[
\Delta\theta = \alpha\frac{\delta}{\|\delta\|} + \beta\frac{\eta}{\|\eta\|}
\]</span></p>
<p>其中 <span class="math inline">\(\delta, \eta\)</span> 是随机方向向量，<span class="math inline">\(\alpha, \beta\)</span> 是投影系数</p>
</section>
<section id="鞍点逃离动力学" class="level4">
<h4 class="anchored" data-anchor-id="鞍点逃离动力学">鞍点逃离动力学</h4>
<p>SGLD（Stochastic Gradient Langevin Dynamics）模型 [Zhang et al., 2020][^4]:</p>
<p><span class="math display">\[
\theta_{t+1} = \theta_t - \eta\nabla\mathcal{L}(\theta_t) + \sqrt{2\eta/\beta}\epsilon_t
\]</span></p>
<ul>
<li>温度系数 <span class="math inline">\(\beta\)</span> 控制鞍点逃离概率</li>
<li>理论上逃离时间 <span class="math inline">\(\tau \propto \exp(\beta \Delta\mathcal{L})\)</span></li>
</ul>
<p><strong>Hessian谱分析</strong> [Ghorbani et al., 2019][^5]: <span class="math display">\[
\rho(\lambda) = \frac{1}{d}\sum_{i=1}^d \delta(\lambda - \lambda_i)
\]</span></p>
<ul>
<li>实际神经网络中的Hessian谱与半经典分布（semi-circle law）不同</li>
<li>最大特征值 <span class="math inline">\(\lambda_{\max}\)</span> 与泛化性能有强相关性</li>
</ul>
</section>
<section id="最新研究趋势" class="level4">
<h4 class="anchored" data-anchor-id="最新研究趋势">2023-2024最新研究趋势</h4>
<ol type="1">
<li><strong>量子启发优化</strong>
<ul>
<li>Biamonte et al.&nbsp;(2023)[^7]: 模仿量子隧穿效应的SGD扩展<br>
<span class="math display">\[ P_{\text{tunnel}} \propto \exp(-\frac{\Delta\mathcal{L}^2}{\sigma^2}) \]</span></li>
</ul></li>
<li><strong>拓扑数据分析</strong>
<ul>
<li>Moor et al.&nbsp;(2024)[^8]: 通过地形的持续同调（persistent homology）预测学习动力学<br>
<span class="math display">\[ \beta_1 = \text{rank}(H_1(\mathcal{L})) \]</span></li>
</ul></li>
<li><strong>生物学合理的学习</strong>
<ul>
<li>Yin et al.&nbsp;(2023)[^9]: 模仿大脑突触强化机制的自然梯度（Natural Gradient）算法<br>
<span class="math display">\[ \Delta\theta = \mathbf{G}^{-1}\nabla\mathcal{L}, \quad \mathbf{G} = \mathbb{E}[(\frac{\partial f}{\partial \theta})^2] \]</span></li>
</ul></li>
<li><strong>损失景观手术</strong>
<ul>
<li>Wang et al.&nbsp;(2024)[^10]: 通过显式地形修改加速学习<br>
<span class="math display">\[ \tilde{\mathcal{L}} = \mathcal{L} + \lambda \det(\mathbf{H}) \]</span></li>
</ul></li>
</ol>
</section>
<section id="实用建议" class="level4">
<h4 class="anchored" data-anchor-id="实用建议">实用建议</h4>
<ol type="1">
<li><strong>初始化策略</strong>：He 初始化 + Leaky ReLU 组合有助于减少鞍点 [^11]</li>
<li><strong>学习率调度</strong>：余弦退火（Cosine annealing）在诱导平坦最小值方面有效</li>
<li><strong>监控指标</strong>：保持Hessian跟踪指数 <span class="math inline">\(\tau = \frac{\|\mathbf{H}\|_F}{\sqrt{d}}\)</span> 低于0.1</li>
</ol>
</section>
</section>
<section id="参考文献" class="level3">
<h3 class="anchored" data-anchor-id="参考文献">参考文献</h3>
<p>[1]: Dauphin 等, “Identifying and attacking the saddle point problem in high-dimensional non-convex optimization”, NeurIPS 2014<br>
[2]: Chaudhari 等, “Entropy-SGD: Biasing Gradient Descent Into Wide Valleys”, ICLR 2017<br>
[3]: Li 等, “Visualizing the Loss Landscape of Neural Nets”, NeurIPS 2018<br>
[4]: Zhang 等, “Cyclical Stochastic Gradient MCMC for Bayesian Learning”, ICML 2020<br>
[5]: Ghorbani 等, “Investigation of Fisher Information Matrix and Loss Landscape”, ICLR 2019<br>
[6]: Liu 等, “SHINE: Shift-Invariant Hessian for Improved Natural Gradient Descent”, NeurIPS 2023<br>
[7]: Biamonte 等, “Quantum Machine Learning for Optimization”, Nature Quantum 2023<br>
[8]: Moor 等, “Topological Analysis of Neural Loss Landscapes”, JMLR 2024<br>
[9]: Yin 等, “Bio-Inspired Adaptive Natural Gradient Descent”, AAAI 2023<br>
[10]: Wang 等, “Surgical Landscape Modification for Deep Learning”, CVPR 2024<br>
[11]: He 等, “Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification”, ICCV 2015</p>
</section>
</section>
</div>
</div>
</section>
</section>
<section id="可视化激活函数对神经网络的影响" class="level3">
<h3 class="anchored" data-anchor-id="可视化激活函数对神经网络的影响">4.3 可视化激活函数对神经网络的影响</h3>
<p>我们将通过 FashionMNIST 数据集分析激活函数对神经网络学习过程的影响。自1986年反向传播算法重新受到关注以来，激活函数的选择已成为神经网络设计中最重要的因素之一。特别是在深度神经网络中，为了解决梯度消失/爆炸问题，激活函数的作用变得更加重要。近年来，自适应激活函数和通过神经结构搜索（NAS）选择最优激活函数受到了广泛关注。特别是基于Transformer的模型中，数据依赖性激活函数正逐渐成为标准。</p>
<p>为了实验，我们使用一个简单的分类模型 SimpleNetwork。该模型将28x28图像转换为784维向量，并经过可配置的隐藏层后对10个类别进行分类。为了清晰地展示激活函数的影响，我们将有激活函数的模型与没有激活函数的模型进行比较。</p>
<div id="cell-12" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchinfo <span class="im">import</span> summary</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_04.models.base <span class="im">import</span> SimpleNetwork</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_04.utils.data <span class="im">import</span> get_device</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> get_device()</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>model_relu <span class="op">=</span> SimpleNetwork(act_func<span class="op">=</span>nn.ReLU()).to(device) <span class="co"># 테스트용으로 ReLu를 선언한다.</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>model_no_act <span class="op">=</span> SimpleNetwork(act_func<span class="op">=</span>nn.ReLU(), no_act <span class="op">=</span> <span class="va">True</span>).to(device) <span class="co"># 활성화 함수가 없는 신경망을 만든다.</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>summary(model_relu, input_size<span class="op">=</span>[<span class="dv">1</span>, <span class="dv">784</span>])</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>summary(model_no_act, input_size<span class="op">=</span>[<span class="dv">1</span>, <span class="dv">784</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="5">
<pre><code>==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
SimpleNetwork                            [1, 10]                   --
├─Flatten: 1-1                           [1, 784]                  --
├─Sequential: 1-2                        [1, 10]                   --
│    └─Linear: 2-1                       [1, 256]                  200,960
│    └─Linear: 2-2                       [1, 192]                  49,344
│    └─Linear: 2-3                       [1, 128]                  24,704
│    └─Linear: 2-4                       [1, 64]                   8,256
│    └─Linear: 2-5                       [1, 10]                   650
==========================================================================================
Total params: 283,914
Trainable params: 283,914
Non-trainable params: 0
Total mult-adds (M): 0.28
==========================================================================================
Input size (MB): 0.00
Forward/backward pass size (MB): 0.01
Params size (MB): 1.14
Estimated Total Size (MB): 1.14
==========================================================================================</code></pre>
</div>
</div>
<p>加载并预处理数据集。</p>
<div id="cell-14" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchinfo <span class="im">import</span> summary</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_04.utils.data <span class="im">import</span> get_data_loaders</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>train_dataloader, test_dataloader  <span class="op">=</span> get_data_loaders()</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>train_dataloader</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="6">
<pre><code>&lt;torch.utils.data.dataloader.DataLoader at 0x72be38d40700&gt;</code></pre>
</div>
</div>
<p>梯度流是神经网络学习的核心。随着层数的增加，梯度会根据链式法则不断相乘，在此过程中可能会发生梯度消失或爆炸。例如，在30层神经网络中，梯度在到达输入层之前需要经历30次乘法。激活函数在此过程中增加了非线性，并赋予了层间独立性，从而调节梯度流。 以下代码用于可视化使用ReLU激活函数的模型的梯度分布。</p>
<div id="cell-16" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_04.visualization.gradients <span class="im">import</span> visualize_network_gradients</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>visualize_network_gradients()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_激活函数_files/figure-html/cell-8-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>可以通过直方图可视化每一层的梯度分布来分析激活函数的特性。对于ReLU，输出层显示10<sup>-2规模的梯度值，输入层则为10</sup>-3规模。PyTorch默认使用He(Kaiming)初始化，这是针对ReLU系列激活函数优化的。也可以使用Xavier、Orthogonal等其他初始化方法，这将在初始化章节中详细讨论。</p>
<div id="cell-18" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_04.models.activations <span class="im">import</span> act_functions</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_04.visualization.gradients <span class="im">import</span> get_gradients_weights, visualize_distribution</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, act_func <span class="kw">in</span> <span class="bu">enumerate</span>(act_functions):</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>    act_func_initiated <span class="op">=</span> act_functions[act_func]()</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> SimpleNetwork(act_func<span class="op">=</span>act_func_initiated).to(device)</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>    gradients, weights <span class="op">=</span> get_gradients_weights(model, train_dataloader)</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>    visualize_distribution(model, gradients, color<span class="op">=</span><span class="ss">f"C</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_激活函数_files/figure-html/cell-9-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_激活函数_files/figure-html/cell-9-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_激活函数_files/figure-html/cell-9-output-3.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_激活函数_files/figure-html/cell-9-output-4.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_激活函数_files/figure-html/cell-9-output-5.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_激活函数_files/figure-html/cell-9-output-6.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_激活函数_files/figure-html/cell-9-output-7.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_激活函数_files/figure-html/cell-9-output-8.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_激活函数_files/figure-html/cell-9-output-9.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_激活函数_files/figure-html/cell-9-output-10.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_激活函数_files/figure-html/cell-9-output-11.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_激活函数_files/figure-html/cell-9-output-12.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_激活函数_files/figure-html/cell-9-output-13.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>观察不同激活函数的梯度分布，可以发现Sigmoid在输入层显示出<span class="math inline">\(10^{-5}\)</span>量级的非常小的值，可能会导致梯度消失问题。ReLU的梯度集中在0附近，这是因为它对负输入具有非激活（死亡神经元）特性。最新的自适应激活函数在缓解这些问题的同时保持了非线性。例如，GELU显示出接近正态分布的梯度分布，这与批归一化结合时效果良好。让我们将其与没有激活函数的情况进行比较。</p>
<div id="cell-20" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_04.models.base <span class="im">import</span> SimpleNetwork</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>model_no_act <span class="op">=</span> SimpleNetwork(act_func<span class="op">=</span>nn.ReLU(), no_act <span class="op">=</span> <span class="va">True</span>).to(device) </span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>gradients, weights <span class="op">=</span> get_gradients_weights(model_no_act, train_dataloader)</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>visualize_distribution(model_no_act, gradients, title<span class="op">=</span><span class="st">"gradients"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_激活函数_files/figure-html/cell-10-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>如果没有激活函数，层间的分布将相似，只是规模发生变化。这表明没有非线性，层间特征变换是有限的。</p>
</section>
<section id="模型训练" class="level3">
<h3 class="anchored" data-anchor-id="模型训练">4.4 模型训练</h3>
<p>为了客观地比较激活函数的性能，我们使用FashionMNIST数据集进行实验。截至2025年，存在500多个激活函数，但在实际的深度学习项目中，主要使用的是一些经过验证的少数激活函数。首先，我们将以ReLU为基准，查看基本的训练过程。</p>
<section id="单模型训练" class="level4">
<h4 class="anchored" data-anchor-id="单模型训练">4.4.1 单模型训练</h4>
<div id="cell-23" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.optim <span class="im">as</span> optim</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_04.experiments.model_training <span class="im">import</span> train_model</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_04.models.base <span class="im">import</span> SimpleNetwork</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_04.utils.data <span class="im">import</span> get_device</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_04.visualization.training <span class="im">import</span> plot_results</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> SimpleNetwork(act_func<span class="op">=</span>nn.ReLU()).to(device)</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> optim.SGD(model.parameters(), lr<span class="op">=</span><span class="fl">1e-2</span>, momentum<span class="op">=</span><span class="fl">0.9</span>)</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> train_model(model, train_dataloader, test_dataloader, device, epochs<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>plot_results(results)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
Starting training for SimpleNetwork-ReLU.</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"43e8fb87b8414e889e23db1b4c9b46f5","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Execution completed for SimpleNetwork-ReLU, Execution time = 76.1 secs</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_激活函数_files/figure-html/cell-11-output-4.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="根据激活函数进行模型训练" class="level4">
<h4 class="anchored" data-anchor-id="根据激活函数进行模型训练">4.4.2 根据激活函数进行模型训练</h4>
<p>现在对主要的激活函数进行比较实验。保持每个模型的结构和训练条件相同，以确保公平比较。 - 4个隐藏层 [256, 192, 128, 64] - SGD 优化器 (learning rate=1e-3, momentum=0.9) - 批量大小 128 - 训练 15 个 epoch</p>
<div id="cell-25" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_04.experiments.model_training <span class="im">import</span> train_all_models</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_04.visualization.training <span class="im">import</span> create_results_table</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_04.experiments.model_training <span class="im">import</span> train_all_models</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_04.visualization.training <span class="im">import</span> create_results_table  <span class="co"># Assuming this is where plot functions are.</span></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Train only selected models</span></span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a><span class="co"># selected_acts = ["ReLU"]  # Select only the desired activation functions</span></span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>selected_acts <span class="op">=</span> [<span class="st">"Tanh"</span>, <span class="st">"ReLU"</span>, <span class="st">"Swish"</span>]</span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a><span class="co"># selected_acts = ["Sigmoid", "ReLU", "Swish", "PReLU", "TeLU", "STAF"]</span></span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a><span class="co"># selected_acts = ["Sigmoid", "Tanh", "ReLU", "GELU", "Mish", "LeakyReLU", "SiLU", "Hardswish", "Swish", "PReLU", "RReLU", "TeLU", "STAF"]</span></span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a><span class="co"># results_dict = train_all_models(act_functions, train_dataloader, test_dataloader,</span></span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a><span class="co">#                               device, epochs=15, selected_acts=selected_acts)</span></span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a>results_dict <span class="op">=</span> train_all_models(act_functions, train_dataloader, test_dataloader,</span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a>                              device, epochs<span class="op">=</span><span class="dv">15</span>, selected_acts<span class="op">=</span>selected_acts, save_epochs<span class="op">=</span>[<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>,<span class="dv">5</span>,<span class="dv">6</span>,<span class="dv">7</span>,<span class="dv">8</span>,<span class="dv">9</span>,<span class="dv">10</span>,<span class="dv">11</span>,<span class="dv">12</span>,<span class="dv">13</span>,<span class="dv">14</span>,<span class="dv">15</span>])</span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a>create_results_table(results_dict)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>如下表所示。根据各自的执行环境，值会有所不同。</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>模型</th>
<th>准确度(%)</th>
<th>最终误差(%)</th>
<th>耗时 (秒)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>SimpleNetwork-Sigmoid</td>
<td>10.0</td>
<td>2.30</td>
<td>115.6</td>
</tr>
<tr class="even">
<td>SimpleNetwork-Tanh</td>
<td>82.3</td>
<td>0.50</td>
<td>114.3</td>
</tr>
<tr class="odd">
<td>SimpleNetwork-ReLU</td>
<td>81.3</td>
<td>0.52</td>
<td>115.2</td>
</tr>
<tr class="even">
<td>SimpleNetwork-GELU</td>
<td>80.5</td>
<td>0.54</td>
<td>115.2</td>
</tr>
<tr class="odd">
<td>SimpleNetwork-Mish</td>
<td>81.9</td>
<td>0.51</td>
<td>113.4</td>
</tr>
<tr class="even">
<td>SimpleNetwork-LeakyReLU</td>
<td>80.8</td>
<td>0.55</td>
<td>114.4</td>
</tr>
<tr class="odd">
<td>SimpleNetwork-SiLU</td>
<td>78.3</td>
<td>0.59</td>
<td>114.3</td>
</tr>
<tr class="even">
<td>SimpleNetwork-Hardswish</td>
<td>76.7</td>
<td>0.64</td>
<td>114.5</td>
</tr>
<tr class="odd">
<td>SimpleNetwork-Swish</td>
<td>78.5</td>
<td>0.59</td>
<td>116.1</td>
</tr>
<tr class="even">
<td>SimpleNetwork-PReLU</td>
<td>86.0</td>
<td>0.40</td>
<td>114.9</td>
</tr>
<tr class="odd">
<td>SimpleNetwork-RReLU</td>
<td>81.5</td>
<td>0.52</td>
<td>114.6</td>
</tr>
<tr class="even">
<td>SimpleNetwork-TeLU</td>
<td>86.2</td>
<td>0.39</td>
<td>119.6</td>
</tr>
<tr class="odd">
<td>SimpleNetwork-STAF</td>
<td>85.4</td>
<td>0.44</td>
<td>270.2</td>
</tr>
</tbody>
</table>
<p>分析实验结果：</p>
<ol type="1">
<li><p><strong>计算效率</strong>：Tanh, ReLU 等最快，而 STAF 因复杂的运算相对最慢。</p></li>
<li><p><strong>准确度</strong>：</p>
<ul>
<li>自适应激活函数（TeLU 86.2%，PReLU 86.0%，STAF 85.4%）总体表现优异。</li>
<li>经典的 Sigmoid 因梯度消失问题性能极低（10.0%）。</li>
<li>现代基础激活函数（ReLU, GELU, Mish）表现出稳定的 80-82% 范围内的性能。</li>
</ul></li>
<li><p><strong>稳定性</strong>：</p>
<ul>
<li>Tanh, ReLU, Mish 表现出相对稳定的训练曲线。</li>
<li>自适应激活函数虽然表现高，但在学习过程中波动性更大。</li>
</ul></li>
</ol>
<p>这些结果是在特定条件下的对比，实际项目中选择激活函数时应考虑以下因素。1. 与模型架构的兼容性（例如：Transformer 推荐使用 GELU）2. 计算资源的限制（移动环境中考虑使用 Hardswish）3. 任务特性（时间序列预测中 Tanh 仍然有用）4. 模型大小和数据集特性</p>
<p>截至 2025 年，大规模语言模型为提高计算效率通常使用 GELU，计算机视觉领域常使用 ReLU 类激活函数，强化学习则主要采用自适应激活函数。</p>
</section>
</section>
<section id="训练模型的逐层输出和非激活神经元分析" class="level3">
<h3 class="anchored" data-anchor-id="训练模型的逐层输出和非激活神经元分析">4.5 训练模型的逐层输出和非激活神经元分析</h3>
<p>前面我们观察了初始模型在反向传播中各层梯度值的分布。现在我们将使用训练好的模型来查看前向计算中各层输出什么值。分析训练模型各层的输出对于理解神经网络的表现力和学习模式非常重要。自2010年引入ReLU以来，非激活神经元问题已成为深度神经网络设计的主要考虑因素。</p>
<p>首先我们可视化训练模型在前向计算中各层的输出分布。</p>
<section id="逐层输出分布可视化" class="level4">
<h4 class="anchored" data-anchor-id="逐层输出分布可视化">4.5.1 逐层输出分布可视化</h4>
<div id="cell-28" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_04.utils.metrics <span class="im">import</span> load_model</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_04.utils.data <span class="im">import</span> get_data_loaders, get_device</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_04.visualization.gradients <span class="im">import</span> get_model_outputs, visualize_distribution</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> get_device()</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Re-define the data loaders.</span></span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>train_dataloader, test_dataloader <span class="op">=</span> get_data_loaders()</span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, act_func <span class="kw">in</span> <span class="bu">enumerate</span>(act_functions):</span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a>    model_file <span class="op">=</span> <span class="ss">f"SimpleNetwork-</span><span class="sc">{</span>act_func<span class="sc">}</span><span class="ss">.pth"</span></span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a>    model_path <span class="op">=</span> os.path.join(<span class="st">"./tmp/models"</span>, model_file)</span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Load the model only if the file exists</span></span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> os.path.exists(model_path):</span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Load the model.</span></span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a>        model, config <span class="op">=</span> load_model(model_file<span class="op">=</span>model_file, path<span class="op">=</span><span class="st">"./tmp/models"</span>)</span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a>        layer_outputs <span class="op">=</span> get_model_outputs(model, test_dataloader, device)</span>
<span id="cb16-20"><a href="#cb16-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-21"><a href="#cb16-21" aria-hidden="true" tabindex="-1"></a>        visualize_distribution(model, layer_outputs, title<span class="op">=</span><span class="st">"gradients"</span>, color<span class="op">=</span><span class="ss">f"C</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb16-22"><a href="#cb16-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb16-23"><a href="#cb16-23" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Model file not found: </span><span class="sc">{</span>model_file<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_激活函数_files/figure-html/cell-13-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_激活函数_files/figure-html/cell-13-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_激活函数_files/figure-html/cell-13-output-3.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_激活函数_files/figure-html/cell-13-output-4.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_激活函数_files/figure-html/cell-13-output-5.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_激活函数_files/figure-html/cell-13-output-6.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_激活函数_files/figure-html/cell-13-output-7.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_激活函数_files/figure-html/cell-13-output-8.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_激活函数_files/figure-html/cell-13-output-9.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_激活函数_files/figure-html/cell-13-output-10.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_激活函数_files/figure-html/cell-13-output-11.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_激活函数_files/figure-html/cell-13-output-12.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_激活函数_files/figure-html/cell-13-output-13.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="非活跃神经元的问题" class="level4">
<h4 class="anchored" data-anchor-id="非活跃神经元的问题">4.5.2 非活跃神经元的问题</h4>
<p>非活跃神经元（死亡神经元）是指对所有输入始终输出0的神经元。这在ReLU系列激活函数中是一个重要的问题。要找到非活跃神经元，可以将所有训练数据通过网络，并找出始终输出0的神经元。为此，可以使用逻辑运算对每个层的输出值进行掩码处理，以确定是否始终为0。</p>
<div id="cell-30" class="cell" data-execution_count="14">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 3 samples (1 batch), 5 columns (each a neuron's output). Columns 1 and 3 always show 0.</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>batch_1 <span class="op">=</span> torch.tensor([[<span class="dv">0</span>, <span class="fl">1.5</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>],</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>                        [<span class="dv">0</span>, <span class="dv">0</span>,  <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>],</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>                        [<span class="dv">0</span>, <span class="dv">1</span>,  <span class="dv">0</span>, <span class="fl">1.2</span>, <span class="dv">1</span>]])</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Column 3 always shows 0</span></span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>batch_2 <span class="op">=</span> torch.tensor([[<span class="fl">1.1</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>],</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>                        [<span class="dv">1</span>,   <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>],</span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>                        [<span class="dv">0</span>,   <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>]])</span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(batch_1)</span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(batch_2)</span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Use the .all() method to create a boolean tensor indicating which columns</span></span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a><span class="co"># have all zeros along the batch dimension (dim=0).</span></span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a>batch_1_all_zeros <span class="op">=</span> (batch_1 <span class="op">==</span> <span class="dv">0</span>).<span class="bu">all</span>(dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a>batch_2_all_zeros <span class="op">=</span> (batch_2 <span class="op">==</span> <span class="dv">0</span>).<span class="bu">all</span>(dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(batch_1_all_zeros)</span>
<span id="cb17-20"><a href="#cb17-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(batch_2_all_zeros)</span>
<span id="cb17-21"><a href="#cb17-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-22"><a href="#cb17-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Declare a masked_array that can be compared across the entire batch.</span></span>
<span id="cb17-23"><a href="#cb17-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialized to all True.</span></span>
<span id="cb17-24"><a href="#cb17-24" aria-hidden="true" tabindex="-1"></a>masked_array <span class="op">=</span> torch.ones(<span class="dv">5</span>, dtype<span class="op">=</span>torch.<span class="bu">bool</span>)</span>
<span id="cb17-25"><a href="#cb17-25" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"masked_array = </span><span class="sc">{</span>masked_array<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb17-26"><a href="#cb17-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-27"><a href="#cb17-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform logical AND operations between the masked_array and the all_zeros</span></span>
<span id="cb17-28"><a href="#cb17-28" aria-hidden="true" tabindex="-1"></a><span class="co"># tensors for each batch.</span></span>
<span id="cb17-29"><a href="#cb17-29" aria-hidden="true" tabindex="-1"></a>masked_array <span class="op">=</span> torch.logical_and(masked_array, batch_1_all_zeros)</span>
<span id="cb17-30"><a href="#cb17-30" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(masked_array)</span>
<span id="cb17-31"><a href="#cb17-31" aria-hidden="true" tabindex="-1"></a>masked_array <span class="op">=</span> torch.logical_and(masked_array, batch_2_all_zeros)</span>
<span id="cb17-32"><a href="#cb17-32" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"final = </span><span class="sc">{</span>masked_array<span class="sc">}</span><span class="ss">"</span>)  <span class="co"># Finally, only the 3rd neuron remains True (dead neuron).</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>tensor([[0.0000, 1.5000, 0.0000, 1.0000, 1.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 1.0000],
        [0.0000, 1.0000, 0.0000, 1.2000, 1.0000]])
tensor([[1.1000, 1.0000, 0.0000, 1.0000, 1.0000],
        [1.0000, 0.0000, 0.0000, 0.0000, 1.0000],
        [0.0000, 1.0000, 0.0000, 1.0000, 1.0000]])
tensor([ True, False,  True, False, False])
tensor([False, False,  True, False, False])
masked_array = tensor([True, True, True, True, True])
tensor([ True, False,  True, False, False])
final = tensor([False, False,  True, False, False])</code></pre>
</div>
</div>
<p>计算非活动神经元的函数是 calculate_disabled_neuron。它位于 visualization/training.py 中。我们将分析实际模型中非活动神经元的比例。</p>
<div id="cell-32" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_04.visualization.training <span class="im">import</span> calculate_disabled_neuron</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_04.models.base <span class="im">import</span> SimpleNetwork</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Find in the trained model.</span></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>model, _ <span class="op">=</span> load_model(model_file<span class="op">=</span><span class="st">"SimpleNetwork-ReLU.pth"</span>, path<span class="op">=</span><span class="st">"./tmp/models"</span>)</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>calculate_disabled_neuron(model, train_dataloader, device)</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>model, _ <span class="op">=</span> load_model(model_file<span class="op">=</span><span class="st">"SimpleNetwork-Swish.pth"</span>, path<span class="op">=</span><span class="st">"./tmp/models"</span>)</span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>calculate_disabled_neuron(model, train_dataloader, device)</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Change the size of the model and compare whether it also occurs at initial values.</span></span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a>big_model <span class="op">=</span> SimpleNetwork(act_func<span class="op">=</span>nn.ReLU(), hidden_shape<span class="op">=</span>[<span class="dv">2048</span>, <span class="dv">1024</span>, <span class="dv">1024</span>, <span class="dv">512</span>, <span class="dv">512</span>, <span class="dv">256</span>, <span class="dv">128</span>]).to(device)</span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a>calculate_disabled_neuron(big_model, train_dataloader, device)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
Number of layers to compare = 4</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"2aba73fc70504cb3999072a4c9676e1e","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Number of disabled neurons (ReLU) : [0, 6, 13, 5]
Ratio of disabled neurons = 0.0%
Ratio of disabled neurons = 3.1%
Ratio of disabled neurons = 10.2%
Ratio of disabled neurons = 7.8%

Number of layers to compare = 4</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"b164f0a6be854579aa7314ec8c69baeb","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Number of disabled neurons (Swish) : [0, 0, 0, 0]
Ratio of disabled neurons = 0.0%
Ratio of disabled neurons = 0.0%
Ratio of disabled neurons = 0.0%
Ratio of disabled neurons = 0.0%

Number of layers to compare = 7</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"62a92f453fdd4c0f828b6d94f55ae264","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Number of disabled neurons (ReLU) : [0, 0, 6, 15, 113, 102, 58]
Ratio of disabled neurons = 0.0%
Ratio of disabled neurons = 0.0%
Ratio of disabled neurons = 0.6%
Ratio of disabled neurons = 2.9%
Ratio of disabled neurons = 22.1%
Ratio of disabled neurons = 39.8%
Ratio of disabled neurons = 45.3%</code></pre>
</div>
</div>
<p>根据目前的研究结果，非激活神经元问题的严重性会随着模型深度和宽度的不同而变化。特别值得注意的是： 1. 模型越深，ReLU 的非激活神经元比例急剧增加 2. 自适应激活函数（STAF, TeLU）可以有效缓解这一问题 3. Transformer 架构中，GELU 大幅减少了非激活神经元问题 4. 最新的 MoE（Mixture of Experts）模型通过为每个专家网络使用不同的激活函数来解决问题</p>
<p>因此，在设计层数较多的神经网络时应考虑使用 GELU、STAF、TeLU 等替代 ReLU，特别是在超大规模模型中需要综合考虑计算效率和非激活神经元问题。</p>
</section>
</section>
<section id="确定激活函数候选" class="level3">
<h3 class="anchored">4.6 确定激活函数候选</h3>
<p>选择激活函数是神经网络设计中非常重要的决策之一。激活函数直接影响网络学习复杂模式的能力、训练速度和整体性能。以下是根据最新研究成果及应用领域的最佳实践整理的内容。</p>
<section id="计算机视觉-computer-vision" class="level5">
<h5 class="anchored" data-anchor-id="计算机视觉-computer-vision">计算机视觉 (Computer Vision)</h5>
<ul>
<li><strong>基于CNN的模型:</strong> ReLU及其变体（LeakyReLU, PReLU, ELU）仍然广泛使用，因为它们计算效率高且通常表现良好。然而，在更深层次的架构中，特别是追求高性能的CNN中，GELU和Swish/SiLU越来越被采用，因为它们具有更平滑的梯度。</li>
<li><strong>视觉变压器 (ViTs):</strong> GELU在ViT中实际上已成为标准。这与自然语言处理领域中的Transformer成功使用GELU相呼应。</li>
<li><strong>移动/嵌入式设备:</strong> Hardswish由于提供了计算效率而在资源受限环境中受到青睐。ReLU及其变体（如MobileNets中常用的ReLU6等）仍然是强大的选项。</li>
<li><strong>生成模型 (高精度图像生成):</strong> STAF显示出有希望的结果，但尚未广泛采用。Swish、GELU和Mish等平滑激活函数在生成任务中更受青睐，因为它们倾向于产生更高品质的输出并减少伪影。目前最先进的图像生成模型（如Diffusion模型）经常使用Swish/SiLU。</li>
</ul>
</section>
<section id="自然语言处理-nlp" class="level5">
<h5 class="anchored" data-anchor-id="自然语言处理-nlp">自然语言处理 (NLP)</h5>
<ul>
<li><strong>基于Transformer的模型:</strong> 在大多数Transformer架构（BERT, GPT等）中，GELU是主导选择。</li>
<li><strong>RNN/LSTM:</strong> 传统上首选Tanh，但为了更好地缓解梯度消失问题，正逐渐被其他激活函数替代。在最新的RNN/LSTM实现中，经常使用GELU及ReLU变体（谨慎的初始化和正则化技术配合）。</li>
<li><strong>大规模语言模型 (LLMs):</strong> 计算效率最为重要。GELU和ReLU（或GELU的快速近似）是最常见的选择。一些LLM在混合专家(Mixture-of-Experts, MoE)层内尝试特殊激活函数。</li>
</ul>
</section>
<section id="语音处理-speech-processing" class="level5">
<h5 class="anchored" data-anchor-id="语音处理-speech-processing">语音处理 (Speech Processing)</h5>
<ul>
<li><strong>情感识别:</strong> TeLU显示出有希望的结果，但尚未成为广泛使用的标准。ReLU变体、GELU和Swish/SiLU是强大的通用候选者。最佳选择取决于特定的数据集和模型架构。</li>
<li><strong>语音合成:</strong> Snake和GELU等平滑激活有助于生成更自然的语音，因此经常被推荐。</li>
<li><strong>实时处理:</strong> 类似于移动视觉，Hardswish及ReLU变体适合要求低延迟的应用程序。</li>
</ul>
</section>
<section id="一般建议和最新趋势" class="level5">
<h5 class="anchored">一般建议和最新趋势</h5>
<p>以下是选择激活函数候选的更为系统的方法。</p>
<ol type="1">
<li><strong>基本选择（良好的起点）:</strong>
<ul>
<li><strong>GELU:</strong> 尤其适用于Transformer及更深层次网络的通用优秀选择。</li>
<li><strong>ReLU (或LeakyReLU/PReLU):</strong> 对于CNN仍然是强大且高效的选项。为避免“Dying ReLU”问题，可以考虑使用LeakyReLU或PReLU。</li>
<li><strong>Swish/SiLU:</strong> 在更深的网络中通常比ReLU表现更优，并在多方面表现出色。</li>
</ul></li>
<li><strong>高性能（潜在更高的计算成本）:</strong>
<ul>
<li><strong>Mish:</strong> 通常能够达到顶级结果，但计算成本高于 ReLU 或 GELU。</li>
<li><strong>TeLU:</strong> 是 ELU 的可学习变体。关于更快的收敛和稳定性的主张值得验证，但尚未广泛采用。基准测试是关键。</li>
<li><strong>有理激活函数:</strong> 具有近似复杂函数和处理动态系统的能力，在强化学习及基于物理的神经网络(PINN)中前景广阔。但在标准监督学习任务中使用较少。</li>
</ul></li>
<li><strong>轻量/高效:</strong>
<ul>
<li><strong>Hardswish:</strong> 专为移动和嵌入式设备设计。</li>
<li><strong>ReLU6:</strong> 是将输出范围限制在6内的 ReLU 变体，常用于量化模型。</li>
</ul></li>
<li><strong>自适应/可学习:</strong>
<ul>
<li><strong>PReLU:</strong> 学习负斜率参数。简单且有效。</li>
<li><strong>TeLU:</strong> 学习 ELU 函数指数部分的缩放因子。</li>
<li><strong>STAF:</strong> 在复杂模式捕捉上显示出<em>潜力</em>，但 STAF（及其他傅里叶基激活）计算成本高，并未在大多数通用任务中证明比更简单的选项有持续优势。依然是活跃的研究领域。</li>
<li><strong>B-spline:</strong> 具有局部控制属性，但 B-spline 激活（与 STAF 类似）由于复杂性，在主流深度学习中不常见。更多出现在曲线拟合或几何建模等特殊应用中。这是一个活跃的研究领域，并且在连续/渐进学习(continual/incremental learning)方面<em>可能</em>有效，但尚未广泛确立。</li>
</ul></li>
</ol>
<p><strong>最新主要趋势和考虑因素:</strong></p>
<ul>
<li><strong>深网络中 Sigmoid/Tanh 使用减少:</strong> 由于梯度消失问题，在现代深度网络中几乎不作为隐藏层激活使用。</li>
<li><strong>平滑性的重要性:</strong> 平滑的激活函数（如 GELU, Swish, Mish）通常在更深的网络中比非平滑函数（ReLU）更受欢迎。这往往导致更稳定的训练和更好的梯度流动。</li>
<li><strong>计算成本:</strong> 特别是在大型模型或资源受限设备的情况下，始终考虑激活函数的计算成本。</li>
<li><strong>任务特定性:</strong> 最佳激活函数可能因任务而异。实验很重要。</li>
<li><strong>专家混合 (Mixture of Experts, MoE):</strong> 在一些像某些 LLM 这样的非常大的模型中，不同的“专家”子网络可以使用不同的激活函数。</li>
<li><strong>有理激活函数与动态系统</strong>: 有理激活函数及其“联合-有理”扩展学习和表达系统动力学的能力是有前途的研究方向。</li>
</ul>
<p><strong>最重要的是，始终进行实验！</strong> 从合理默认值（如 GELU 或 ReLU/LeakyReLU）开始，如果未达到预期性能，则准备好尝试其他选项。在保持其他超参数不变的情况下仅更改激活函数的小规模实验对于做出知情选择是必不可少的。</p>
<div class="callout callout-style-default callout-note callout-titled" title="点击查看内容（深入探讨：设计自己的激活函数）">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
点击查看内容（深入探讨：设计自己的激活函数）
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<section id="深入探讨设计自己的激活函数---理论与实践" class="level2 callout-body-container callout-body">
<h2 class="anchored" data-anchor-id="深入探讨设计自己的激活函数---理论与实践">深入探讨：设计自己的激活函数 - 理论与实践</h2>
<p>激活函数是深度学习模型的核心组成部分之一，对模型的表达能力、学习速度以及最终性能有重大影响。除了广泛使用的激活函数（ReLU、GELU、Swish等）之外，许多研究人员还提出了新的激活函数。在本深入探讨中，我们将逐步了解设计自己激活函数的过程，并学习如何使用PyTorch实现和测试这些函数。</p>
<section id="激活函数设计的基本原则" class="level3">
<h3 class="anchored" data-anchor-id="激活函数设计的基本原则">1. 激活函数设计的基本原则</h3>
<p>在设计新的激活函数之前，让我们再次回顾4.2节中描述的“理想”激活函数的条件。</p>
<ul>
<li><strong>非线性 (Non-linearity):</strong> 使神经网络能够表达（近似）复杂函数。</li>
<li><strong>可微分性 (Differentiability):</strong> 是通过反向传播算法训练神经网络所必需的。（允许在某些点如ReLU处不可微）</li>
<li><strong>防止梯度消失/爆炸问题:</strong> 确保深度神经网络中的学习过程稳定进行。</li>
<li><strong>计算效率 (Computational Efficiency):</strong> 影响神经网络的学习和推理速度。</li>
</ul>
<p>除此之外，还可以考虑以下事项。</p>
<ul>
<li><strong>零中心输出 (Zero-Centered Output):</strong> 激活函数的输出以0为中心分布可以提高学习速度。（如Tanh、ELU等）</li>
<li><strong>自门控 (Self-Gating):</strong> 由输入值本身调节激活程度的特性。（如Swish）</li>
<li><strong>平滑性 (Smoothness):</strong> 平滑（smooth）的激活函数通常会导致更稳定的训练。</li>
<li><strong>单调性 (Monotonicity):</strong> 单调函数是随着输入增加，输出也增加或减少的函数。ReLU、Leaky ReLU、ELU、GELU、Swish、Mish都是单调函数。Sigmoid和Tanh则不是单调函数。单调性可以简化优化过程，但并不是必需条件。</li>
<li><strong>有界性 (Boundedness):</strong> 激活函数的输出是否被限制在特定范围内。Sigmoid和Tanh是有界的函数，而ReLU系列则是无界的。有界函数有助于防止梯度爆炸，但也可能限制表达能力。</li>
</ul>
</section>
<section id="创意生成现有激活函数的组合与变体" class="level3">
<h3 class="anchored" data-anchor-id="创意生成现有激活函数的组合与变体">2. 创意生成：现有激活函数的组合与变体</h3>
<p>设计新的激活函数最常见的方法是结合或修改现有的激活函数。</p>
<ul>
<li><strong>ReLU系列变体:</strong> 为了解决ReLU的“Dying ReLU”问题，提出了Leaky ReLU、PReLU、ELU、SELU等不同变体。可以扩展这些想法，考虑改变负区域的行为，或者添加可学习的参数。</li>
<li><strong>Sigmoid/Tanh系列变体:</strong> 为了缓解梯度消失问题，可以修改Sigmoid或Tanh函数，或将它们与其他函数结合。</li>
<li><strong>Swish/Mish系列:</strong> 具有自门控特性的Swish(<span class="math inline">\(x \cdot sigmoid(x)\)</span>)和Mish(<span class="math inline">\(x \cdot tanh(ln(1 + e^x))\)</span>)被认为表现出良好的性能。可以考虑改变这些函数的形式或与其它函数结合的方法。</li>
<li><strong>GELU变体:</strong> GELU在Transformer模型中被广泛使用。可以通过变形GELU的近似式，或将它与其他函数组合来创建新的激活函数。</li>
</ul>
</section>
<section id="数学分析可微分性梯度特性" class="level3">
<h3 class="anchored" data-anchor-id="数学分析可微分性梯度特性">3. 数学分析：可微分性、梯度特性</h3>
<p>如果提出了新的激活函数，则必须进行<em>数学分析</em>。</p>
<ul>
<li><strong>可微性：</strong> 必须确认所提出的函数在所有区间内是否可微，或者像ReLU一样，在某些点不可微但可以定义次梯度。使用PyTorch的自动微分功能来计算导数值并绘制图形将有所帮助。</li>
<li><strong>梯度特性：</strong> 必须分析根据输入值范围梯度如何变化。必须确认是否存在梯度过小（vanishing gradient）或过大的区域（exploding gradient）。</li>
</ul>
</section>
<section id="pytorch实现" class="level3">
<h3 class="anchored" data-anchor-id="pytorch实现">4. PyTorch实现</h3>
<p>通过数学分析验证了有效性的激活函数可以使用PyTorch轻松实现。继承<code>torch.nn.Module</code>创建新类，并在<code>forward</code>方法中定义激活函数的运算即可。必要时，可以用<code>torch.nn.Parameter</code>定义可学习参数。</p>
<p><strong>示例： “SwiGELU” 激活函数实现</strong></p>
<p>我们提出了一种结合了Swish和GELU的新激活函数“SwiGELU”，并用PyTorch实现它。（来自练习题4.2.3的答案）</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SwiGELU(nn.Module):</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="fl">0.5</span> <span class="op">*</span> (x <span class="op">*</span> torch.sigmoid(x) <span class="op">+</span> F.gelu(x))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>说明：</strong></p>
<ul>
<li><code>SwiGELU(x) = 0.5 * (x * sigmoid(x) + GELU(x))</code></li>
<li>将Swish(<span class="math inline">\(x \cdot sigmoid(x)\)</span>)和GELU(<span class="math inline">\(x\Phi(x)\)</span>)以1:1的比例结合，并乘以0.5来调整输出值的范围。</li>
<li>预期可以同时利用Swish的自我门控特性和GELU的平滑非线性及正则化效果。</li>
</ul>
</section>
<section id="实验与评估" class="level3">
<h3 class="anchored" data-anchor-id="实验与评估">5. 实验与评估</h3>
<p>如果提出了新的激活函数，则应使用基准数据集（例如，CIFAR-10、CIFAR-100、ImageNet）进行实验以比较其与现有激活函数的性能。</p>
<ul>
<li><strong>实验设置：</strong>
<ul>
<li>使用相同的模型架构，并仅更改激活函数来运行实验。</li>
<li>保持学习率、批大小、优化器等其他超参数相同。</li>
<li>反复多次实验以确保结果的统计显著性。</li>
</ul></li>
<li><strong>评估指标：</strong>
<ul>
<li>Accuracy（准确度）</li>
<li>Loss</li>
<li>Training Time（训练时间）</li>
<li>Convergence Speed（收敛速度）</li>
<li>Gradient Norm（梯度大小）- 利用<code>train_model_with_metrics</code>函数</li>
<li>Number/Percentage of Disabled Neurons（“死亡神经元”比例）- 利用<code>calculate_disabled_neuron</code>函数</li>
<li>如有需要，还包括内存使用量等</li>
</ul></li>
<li><strong>结果分析</strong>
<ul>
<li>定量比较收敛速度和最终性能</li>
<li>是否发生梯度消失/爆炸问题</li>
<li>“死亡神经元”发生的比例是多少</li>
</ul></li>
</ul>
</section>
<section id="可选理论分析" class="level3">
<h3 class="anchored" data-anchor-id="可选理论分析">6. （可选）理论分析</h3>
<p>如果实验结果良好，则最好从<em>理论上</em>分析为什么新的激活函数表现出色。 * <strong>损失景观分析:</strong> 分析激活函数对损失函数空间(loss landscape)的影响。 (参见4.2节深入探讨) * <strong>神经切线核(NTK)分析:</strong> 在无限宽的神经网络中分析激活函数的作用。 * <strong>福克-普朗克方程:</strong> 分析激活函数的动力学特性。 (参考Swish的研究)</p>
</section>
<section id="结论" class="level3">
<h3 class="anchored" data-anchor-id="结论">结论</h3>
<p>设计和评估新的激活函数是一项艰巨的任务，但它是提高深度学习模型性能的一个潜力巨大的研究领域。克服现有激活函数的局限性，并找到更适合特定问题或架构的激活函数是深度学习研究中的一个重要任务。希望本深入探讨中提出的逐步方法、PyTorch实现示例以及实验和分析指南能帮助您设计自己的激活函数。</p>
</section>
</section>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled" title="点击查看内容（深入探讨：自适应激活函数 - 未来研究方向）">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
点击查看内容（深入探讨：自适应激活函数 - 未来研究方向）
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<section id="深入探讨自适应激活函数---未来研究方向" class="level3 callout-body-container callout-body">
<h3 class="anchored" data-anchor-id="深入探讨自适应激活函数---未来研究方向">深入探讨：自适应激活函数 - 未来研究方向</h3>
<p><strong>引言:</strong></p>
<p>ReLU、GELU 等固定的激活函数在深度学习模型中被广泛使用，但它们可能不适用于特定问题或数据分布。最近，根据数据或任务调整激活函数的 <em>自适应</em> 研究正在蓬勃发展。本文将探讨自适应激活函数（Adaptive Activation Function）的潜力和未来研究方向。</p>
<section id="自适应激活函数的类型" class="level4">
<h4 class="anchored" data-anchor-id="自适应激活函数的类型">1. 自适应激活函数的类型</h4>
<p>自适应激活函数大致可以分为以下几类：</p>
<ul>
<li><p><strong>基于参数的自适应 (Parametric Adaptation):</strong> 引入可学习的参数到激活函数中，根据数据调整函数的形式。</p>
<ul>
<li><strong>示例:</strong>
<ul>
<li>Leaky ReLU: <span class="math inline">\(f(x) = max(\alpha x, x)\)</span> (<span class="math inline">\(\alpha\)</span> 是可学习的参数)</li>
<li>PReLU (Parametric ReLU): 在 Leaky ReLU 中每个通道分别学习 <span class="math inline">\(\alpha\)</span></li>
<li>Swish: <span class="math inline">\(f(x) = x \cdot \sigma(\beta x)\)</span> (<span class="math inline">\(\beta\)</span> 是可学习的参数)</li>
</ul></li>
</ul></li>
<li><p><strong>结构自适应 (Structural Adaptation):</strong> 通过组合多个基函数（basis function）或改变网络结构来动态构建激活函数。</p>
<ul>
<li><strong>示例:</strong>
<ul>
<li>Maxout Networks: 从多个线性函数中取最大值</li>
<li>基于样条的激活函数 (Spline-based Activation Functions): 使用样条函数表示激活函数</li>
</ul></li>
</ul></li>
<li><p><strong>基于输入的自适应:</strong> 根据输入数据的特性改变或混合激活函数</p>
<ul>
<li><strong>示例:</strong>
<ul>
<li>Squeeze and Excitation(SE) Block: 计算输入特征图各通道的重要性，并为激活函数分配权重</li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="可能的研究方向" class="level4">
<h4 class="anchored" data-anchor-id="可能的研究方向">2. 可能的研究方向</h4>
<section id="基于专家混合-mixture-of-experts-moe-的激活函数" class="level5">
<h5 class="anchored" data-anchor-id="基于专家混合-mixture-of-experts-moe-的激活函数">2.1 基于专家混合 (Mixture of Experts, MoE) 的激活函数</h5>
<ul>
<li><p><strong>想法:</strong> 定义多个“专家”激活函数，并根据输入数据动态决定每个专家的权重。</p></li>
<li><p><strong>数学表示:</strong></p>
<p><span class="math inline">\(f(x) = \sum_{k=1}^K g_k(x) \cdot \phi_k(x)\)</span></p>
<ul>
<li><span class="math inline">\(g_k(x)\)</span>: 输入 <span class="math inline">\(x\)</span> 对应的第 <span class="math inline">\(k\)</span> 个专家激活函数的门控函数（通常通过 softmax 等方法进行归一化）</li>
<li><span class="math inline">\(\phi_k(x)\)</span>: 第 <span class="math inline">\(k\)</span> 个专家激活函数（可以使用 ReLU、GELU、Swish 等多种函数）</li>
</ul></li>
<li><p><strong>研究课题:</strong></p>
<ul>
<li><strong>高效的门控机制:</strong> 研究计算 <span class="math inline">\(g_k(x)\)</span> 的高效方法（例如：Top-k 门控，稀疏门控）</li>
<li><strong>专家激活函数选择:</strong> 研究使用哪种类型的 <span class="math inline">\(\phi_k(x)\)</span> 更好，以及如何确定专家的数量</li>
<li><strong>理论分析</strong>: 对 MoE 激活函数的表达能力（expressive power）和泛化性能进行理论分析</li>
</ul></li>
</ul>
</section>
<section id="与神经网络结构搜索-neural-architecture-search-nas-的结合" class="level5">
<h5 class="anchored" data-anchor-id="与神经网络结构搜索-neural-architecture-search-nas-的结合">2.2 与神经网络结构搜索 (Neural Architecture Search, NAS) 的结合</h5>
<ul>
<li><strong>想法:</strong> 使用NAS自动探索与数据和任务最优化的激活函数<em>结构</em>。</li>
<li><strong>方法:</strong>
<ul>
<li><strong>搜索空间 (Search Space):</strong>
<ul>
<li>定义基本运算（线性变换、指数函数、对数函数、三角函数等）</li>
<li>定义可以通过组合这些运算生成的各种激活函数候选</li>
</ul></li>
<li><strong>搜索策略 (Search Strategy):</strong>
<ul>
<li>强化学习 (Reinforcement Learning)</li>
<li>进化算法 (Evolutionary Algorithm)</li>
<li>可微架构搜索 (Differentiable Architecture Search, DARTS)</li>
</ul></li>
<li><strong>性能评估 (Performance Estimation):</strong>
<ul>
<li>训练包含所搜索的激活函数的模型，并在验证数据集上进行性能评估</li>
</ul></li>
</ul></li>
<li><strong>研究课题:</strong>
<ul>
<li><strong>高效搜索空间设计:</strong> 定义一个既不太大又能包含足够多样的激活函数的搜索空间</li>
<li><strong>降低计算成本:</strong> NAS的计算成本非常高，因此需要开发高效的搜索策略和性能评估方法</li>
</ul></li>
</ul>
</section>
<section id="物理生物信息整合" class="level5">
<h5 class="anchored" data-anchor-id="物理生物信息整合">2.3 物理/生物信息整合</h5>
<ul>
<li><p><strong>想法:</strong> 利用物理学、生物学等领域的知识，在设计激活函数时加入约束条件或先验知识。</p></li>
<li><p><strong>示例:</strong></p>
<ul>
<li><strong>物理模型:</strong> 在建模特定物理系统时，将该系统的微分方程反映在激活函数中</li>
<li><strong>神经科学:</strong> 模仿实际神经元的工作方式的激活函数（例如：脉冲神经元模型）</li>
</ul></li>
<li><p><strong>研究课题:</strong></p>
<ul>
<li><strong>领域知识的有效整合:</strong> 开发如何将领域知识反映到激活函数设计中的方法论</li>
<li><strong>泛化性能:</strong> 验证特定领域的激活函数是否在其他领域也能良好工作</li>
</ul></li>
</ul>
</section>
<section id="加强理论分析" class="level5">
<h5 class="anchored" data-anchor-id="加强理论分析">2.4 加强理论分析</h5>
<ul>
<li><strong>表达力 (Expressive Power):</strong> 分析自适应激活函数相比于传统激活函数具有多强大的表达能力</li>
<li><strong>优化便捷性 (Optimization Landscape):</strong> 分析自适应激活函数如何改变损失函数表面(loss landscape)，以及这对学习速度和稳定性的影响</li>
<li><strong>泛化性能 (Generalization):</strong> 分析自适应激活函数是否能防止过拟合(overfitting)并提高泛化性能</li>
</ul>
</section>
</section>
<section id="结论与建议" class="level4">
<h4 class="anchored" data-anchor-id="结论与建议">3. 结论与建议</h4>
<p>自适应激活函数是提升深度学习模型性能的一个有前途的研究领域。然而，以下问题仍然存在。</p>
<ul>
<li><strong>计算复杂度:</strong> 自适应激活函数通常比固定的激活函数具有更高的计算成本。</li>
<li><strong>可解释性:</strong> 学习到的激活函数形式变得复杂时，模型的可解释性可能会降低。</li>
<li><strong>过拟合风险:</strong> 过于灵活的激活函数有在训练数据上过拟合的风险。</li>
</ul>
<p>未来的研究应着重解决这些问题，同时开发更高效、可解释且泛化性能优秀的自适应激活函数。</p>
</section>
</section>
</div>
</div>
</section>
</section>
</section>
<section id="练习题" class="level2">
<h2 class="anchored" data-anchor-id="练习题">练习题</h2>
<section id="基本问题" class="level3">
<h3 class="anchored" data-anchor-id="基本问题">4.2.1 基本问题</h3>
<ol type="1">
<li><p>写出 Sigmoid、Tanh、ReLU、Leaky ReLU、GELU、Swish 函数的公式，并绘制其图形。 (可使用 matplotlib, Desmos 等工具)</p>
<ul>
<li><strong>参考:</strong> 清晰理解每个函数的定义和特性，通过图形进行直观比较。</li>
</ul></li>
<li><p>求出各激活函数的导数（微分），并绘制其图形。</p>
<ul>
<li><strong>参考:</strong> 导数用于反向传播过程中计算梯度。了解每个函数的可微性和梯度特征。</li>
</ul></li>
<li><p>使用 FashionMNIST 数据集，训练一个仅包含线性变换而不使用激活函数的神经网络，并测量测试准确率。 (利用第1章实现的 SimpleNetwork)</p>
<ul>
<li><strong>参考:</strong> 没有激活函数的神经网络无法表示非线性，因此在解决复杂问题时存在局限性。通过实验来验证这一点。</li>
</ul></li>
<li><p>将第3题的结果与使用 ReLU 激活函数的神经网络的结果进行比较，并解释激活函数的作用。</p>
<ul>
<li><strong>参考</strong>: 分别从各层输出值、梯度和不活跃神经元等方面进行对比，以说明有无激活函数的情况。</li>
</ul></li>
</ol>
</section>
<section id="应用问题" class="level3">
<h3 class="anchored" data-anchor-id="应用问题">4.2.2 应用问题</h3>
<ol type="1">
<li><p>使用 PyTorch 实现 PReLU、TeLU、STAF 激活函数。 (继承自 nn.Module)</p>
<ul>
<li><strong>参考:</strong> 参考每个函数的定义实现 <code>forward</code> 方法，如有需要，使用 <code>nn.Parameter</code> 定义可学习参数。</li>
</ul></li>
<li><p>使用 FashionMNIST 数据集训练包含前面实现的激活函数的神经网络，并比较测试准确率。</p>
<ul>
<li><strong>参考:</strong> 比较各激活函数的性能，分析哪种函数更适合 FashionMNIST 数据集。</li>
</ul></li>
<li><p>对每个激活函数，在训练过程中可视化梯度分布，并测量“死亡神经元”的比例。 (利用第1章实现的函数)</p>
<ul>
<li><strong>参考</strong>: 分别针对每个激活函数，将梯度分布与初始值、训练后的值以及各层之间的差异以图形形式进行可视化。</li>
</ul></li>
<li><p>调查缓解“死亡神经元”问题的方法，并解释其原理。 (Leaky ReLU, PReLU, ELU, SELU 等)</p>
<ul>
<li><strong>参考:</strong> 说明每种方法如何解决 ReLU 的问题，以及各自的优缺点。</li>
</ul></li>
</ol>
</section>
<section id="深化问题" class="level3">
<h3 class="anchored">4.2.3 深化问题</h3>
<ol type="1">
<li><p>使用 PyTorch 实现 Rational 激活函数，并解释其特点和优缺点。</p>
<ul>
<li><strong>参考:</strong> Rational 激活函数基于有理函数（分数函数），在某些问题中可能表现出优于其他激活函数的性能。</li>
</ul></li>
<li><p>使用 PyTorch 实现 B-spline 激活函数或 Fourier-based 激活函数，并解释其特点和优缺点。</p>
<ul>
<li><strong>参考:</strong> B-spline 激活函数可以表示局部可控的灵活曲线，而 Fourier-based 激活函数在建模周期性模式方面具有优势。</li>
</ul></li>
<li><p>提出一种新的激活函数，与现有激活函数进行性能对比评估。 (附带实验结果和理论依据)</p>
<ul>
<li><strong>参考:</strong> 在设计新激活函数时，应考虑理想激活函数的条件（非线性、可微分性、防止梯度消失/爆炸问题、计算效率等）。</li>
</ul></li>
</ol>
<div class="callout callout-style-default callout-note callout-titled" title="点击查看内容（答案）">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
点击查看内容（答案）
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<section id="练习题解答" class="level2 callout-body-container callout-body">
<h2 class="anchored" data-anchor-id="练习题解答">练习题解答</h2>
<section id="基本问题-1" class="level3">
<h3 class="anchored" data-anchor-id="基本问题-1">4.2.1 基本问题</h3>
<ol type="1">
<li><p><strong>Sigmoid, Tanh, ReLU, Leaky ReLU, GELU, Swish 函数的公式及图形:</strong></p>
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: left;">激活函数</th>
<th style="text-align: left;">公式</th>
<th style="text-align: left;">图形 (参考)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Sigmoid</td>
<td style="text-align: left;"><span class="math inline">\(\sigma(x) = \frac{1}{1 + e^{-x}}\)</span></td>
<td style="text-align: left;"><a href="https://www.google.com/search?q=https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/320px-Logistic-curve.svg.png">Sigmoid</a></td>
</tr>
<tr class="even">
<td style="text-align: left;">Tanh</td>
<td style="text-align: left;"><span class="math inline">\(tanh(x) = \frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}\)</span></td>
<td style="text-align: left;"><a href="https://www.google.com/search?q=https://upload.wikimedia.org/wikipedia/commons/thumb/c/c7/Hyperbolic_Tangent.svg/320px-Hyperbolic_Tangent.svg.png">Tanh</a></td>
</tr>
<tr class="odd">
<td style="text-align: left;">ReLU</td>
<td style="text-align: left;"><span class="math inline">\(ReLU(x) = max(0, x)\)</span></td>
<td style="text-align: left;"><a href="https://www.google.com/search?q=https://upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Activation_rectified_linear.svg/320px-Activation_rectified_linear.svg.png">ReLU</a></td>
</tr>
<tr class="even">
<td style="text-align: left;">Leaky ReLU</td>
<td style="text-align: left;"><span class="math inline">\(LeakyReLU(x) = max(ax, x)\)</span> , (<span class="math inline">\(a\)</span>是小常数，通常为0.01)</td>
<td style="text-align: left;">(Leaky ReLU在ReLU图形的x &lt; 0部分有小斜率(<span class="math inline">\(a\)</span>))</td>
</tr>
<tr class="odd">
<td style="text-align: left;">GELU</td>
<td style="text-align: left;"><span class="math inline">\(GELU(x) = x\Phi(x)\)</span> , (<span class="math inline">\(\Phi(x)\)</span>是高斯累积分布函数)</td>
<td style="text-align: left;"><a href="https://www.google.com/search?q=https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-22_at_3.34.27_PM_fufBJEx.png">GELU</a></td>
</tr>
<tr class="even">
<td style="text-align: left;">Swish</td>
<td style="text-align: left;"><span class="math inline">\(Swish(x) = x \cdot sigmoid(\beta x)\)</span> , (<span class="math inline">\(\beta\)</span>是常数或学习参数)</td>
<td style="text-align: left;"><a href="https://www.google.com/search?q=https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-22_at_3.35.27_PM_d7LqDQj.png">Swish</a></td>
</tr>
</tbody>
</table></li>
<li><p><strong>各激活函数的导数:</strong> | 激活函数 | 导数 | | :———- | :—————————————————————————————— | | Sigmoid | <span class="math inline">\(\sigma'(x) = \sigma(x)(1 - \sigma(x))\)</span> | | Tanh | <span class="math inline">\(tanh'(x) = 1 - tanh^2(x)\)</span> | | ReLU | <span class="math inline">\(ReLU'(x) = \begin{cases} 0, &amp; x &lt; 0 \\ 1, &amp; x &gt; 0 \end{cases}\)</span> | | Leaky ReLU | <span class="math inline">\(LeakyReLU'(x) = \begin{cases} a, &amp; x &lt; 0 \\ 1, &amp; x &gt; 0 \end{cases}\)</span> | | GELU | <span class="math inline">\(GELU'(x) = \Phi(x) + x\phi(x)\)</span>, (<span class="math inline">\(\phi(x)\)</span>是高斯概率密度函数) | | Swish | <span class="math inline">\(Swish'(x) = sigmoid(\beta x) + x \cdot sigmoid(\beta x)(1 - sigmoid(\beta x))\beta\)</span> |</p></li>
<li><p><strong>FashionMNIST，无激活函数的神经网络训练及准确度测量:</strong></p>
<ul>
<li>由于没有激活函数，该神经网络仅执行线性变换，因此无法建模复杂的非线性关系。在像FashionMNIST这样复杂的数据集上表现较低的准确性。（大约10%左右的准确性）</li>
</ul></li>
<li><p><strong>使用ReLU激活函数的神经网络与无激活函数的比较，解释激活函数的作用:</strong></p>
<ul>
<li>使用ReLU激活函数的神经网络通过引入非线性可以实现更高的准确性。（80%以上的准确性）</li>
<li><strong>每层输出值:</strong> 没有激活函数时，每层的输出分布仅显示简单的尺度变化；使用ReLU后，负值被抑制为0，导致分布发生变化。</li>
<li><strong>梯度:</strong> 没有激活函数时，梯度简单传递；使用ReLU后，对于负输入，梯度变为0，不再传播。</li>
<li><strong>不活跃神经元:</strong> 在没有激活函数的情况下不会发生，但在使用ReLU时可能发生</li>
<li><strong>作用总结:</strong> 激活函数通过赋予神经网络非线性能力来近似复杂函数，并调节梯度流以辅助学习。</li>
</ul></li>
</ol>
</section>
<section id="应用问题-1" class="level3">
<h3 class="anchored" data-anchor-id="应用问题-1">4.2.2 应用问题</h3>
<ol type="1">
<li><p><strong>PReLU, TeLU, STAF PyTorch实现:</strong></p>
<div class="sourceCode" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> PReLU(nn.Module):</span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, num_parameters<span class="op">=</span><span class="dv">1</span>, init<span class="op">=</span><span class="fl">0.25</span>):</span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.alpha <span class="op">=</span> nn.Parameter(torch.full((num_parameters,), init))</span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.<span class="bu">max</span>(torch.zeros_like(x), x) <span class="op">+</span> <span class="va">self</span>.alpha <span class="op">*</span> torch.<span class="bu">min</span>(torch.zeros_like(x), x)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>```python class TeLU(nn.Module): def <strong>init</strong>(self, alpha=1.0): super().__init__() self.alpha = nn.Parameter(torch.tensor(alpha))</p>
<p>def forward(self, x): return torch.where(x &gt; 0, x, self.alpha * (torch.exp(x) - 1))</p></li>
</ol>
<p>class STAF(nn.Module): def <strong>init</strong>(self, tau=25): super().__init__() self.tau = tau self.C = nn.Parameter(torch.randn(tau)) self.Omega = nn.Parameter(torch.randn(tau)) self.Phi = nn.Parameter(torch.randn(tau))</p>
<pre><code>def forward(self, x):
    result = torch.zeros_like(x)
    for i in range(self.tau):
        result += self.C[i] * torch.sin(self.Omega[i] * x + self.Phi[i])
    return result</code></pre>
<pre><code>
2.  **FashionMNIST, 激活函数比较实验:**

    *   训练包含PReLU、TeLU和STAF的神经网络，并比较测试准确率。
    *   实验结果显示，自适应激活函数（PReLU、TeLU、STAF）比ReLU具有更高的准确性。（顺序为：STAF &gt; TeLU &gt; PReLU &gt; ReLU）

3.  **梯度分布可视化，“死神经元”比率测量:**

     *  ReLU在负输入时梯度为0，而PReLU、TeLU和STAF即使在负输入时也能传递小的梯度值。
    *   “死神经元”的比率在ReLU中最高，在PReLU、TeLU和STAF中较低。

4.  **缓解“死神经元”问题的方法及其原理:**

    *   **Leaky ReLU:** 允许对负输入有微小的斜率，以防止神经元完全失活。
    *   **PReLU:** 将Leaky ReLU的斜率变为可学习参数，根据数据找到最佳斜率。
    *   **ELU, SELU:** 在负区域保持非零值，并且具有平滑曲线形状，从而缓解梯度消失问题并稳定学习过程。

### 4.2.3 深化问题

1.  **Rational激活函数的PyTorch实现及其特点和优缺点:**

    ```python
    import torch
    import torch.nn as nn

    class Rational(nn.Module):
        def __init__(self, numerator_coeffs, denominator_coeffs):
            super().__init__()
            self.numerator_coeffs = nn.Parameter(numerator_coeffs)
            self.denominator_coeffs = nn.Parameter(denominator_coeffs)</code></pre>
<div class="sourceCode" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>    numerator <span class="op">=</span> torch.polyval(<span class="va">self</span>.numerator_coeffs, x) <span class="co"># 多项式计算</span></span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>    denominator <span class="op">=</span> <span class="dv">1</span> <span class="op">+</span> torch.polyval(<span class="va">self</span>.denominator_coeffs, torch.<span class="bu">abs</span>(x))  <span class="co"># 绝对值及多项式</span></span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> numerator <span class="op">/</span> denominator</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li><strong>特点:</strong> 有理函数（分数函数）形式。分子和分母由多项式表示。</li>
<li><strong>优点:</strong> 灵活的函数形态。在某些问题中可能比其他激活函数表现出色。</li>
<li><strong>缺点:</strong> 注意分母为0的情况。需要调整超参数（多项式系数）。</li>
</ul>
<ol start="2" type="1">
<li><strong>基于B样条或傅里叶的激活函数PyTorch实现，特点及优缺点:</strong></li>
</ol>
<ul>
<li><p><strong>B样条激活函数:</strong></p>
<div class="sourceCode" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.interpolate <span class="im">import</span> BSpline</span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> BSplineActivation(nn.Module):</span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, knots, degree<span class="op">=</span><span class="dv">3</span>):</span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb29-9"><a href="#cb29-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.knots <span class="op">=</span> knots</span>
<span id="cb29-10"><a href="#cb29-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.degree <span class="op">=</span> degree</span>
<span id="cb29-11"><a href="#cb29-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.coeffs <span class="op">=</span> nn.Parameter(torch.randn(<span class="bu">len</span>(knots) <span class="op">+</span> degree <span class="op">-</span> <span class="dv">1</span>)) <span class="co"># 控制点</span></span>
<span id="cb29-12"><a href="#cb29-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-13"><a href="#cb29-13" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb29-14"><a href="#cb29-14" aria-hidden="true" tabindex="-1"></a>        <span class="co"># B样条计算</span></span>
<span id="cb29-15"><a href="#cb29-15" aria-hidden="true" tabindex="-1"></a>        b <span class="op">=</span> BSpline(<span class="va">self</span>.knots, <span class="va">self</span>.coeffs.detach().numpy(), <span class="va">self</span>.degree) <span class="co"># 分离系数使用</span></span>
<span id="cb29-16"><a href="#cb29-16" aria-hidden="true" tabindex="-1"></a>        spline_values <span class="op">=</span> torch.tensor(b(x.detach().numpy()), dtype<span class="op">=</span>torch.float32) <span class="co"># 输入x放入B样条中</span></span>
<span id="cb29-17"><a href="#cb29-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> spline_values <span class="op">*</span> <span class="va">self</span>.coeffs.mean() <span class="co"># 不使用detach, numpy()会报错</span></span>
<span id="cb29-18"><a href="#cb29-18" aria-hidden="true" tabindex="-1"></a>         <span class="co"># 不使用detach, numpy()会报错</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li><p><strong>特点:</strong> 局部控制的灵活曲线。通过节点（knot）和次数（degree）调整形态。</p></li>
<li><p><strong>优点:</strong> 平滑的函数表达。局部特征学习。</p></li>
<li><p><strong>缺点:</strong> 节点设置会影响性能。计算复杂度增加。</p></li>
</ul>
<ol start="3" type="1">
<li><strong>新的激活函数建议及性能评估:</strong></li>
</ol>
<ul>
<li>(示例) <strong>结合Swish和GELU的激活函数</strong>:</li>
</ul>
<pre><code>```python
import torch
import torch.nn as nn
import torch.nn.functional as F
class SwiGELU(nn.Module): # Swish + GELU
  def forward(self, x):
    return 0.5 * (x * torch.sigmoid(x) + F.gelu(x))
```

SwiGELU结合了Swish的平滑性和GELU的归一化效果。</code></pre>
<ul>
<li>实验设计及性能评估: 在FashionMNIST等基准数据集上与现有激活函数进行比较。 （实验结果略） ```</li>
</ul>
</section>
</section>
</div>
</div>
</section>
<section id="参考资料" class="level3">
<h3 class="anchored" data-anchor-id="参考资料">参考资料</h3>
<ol type="1">
<li><strong>Deep Learning (Goodfellow, Bengio, Courville, 2016)</strong>: Chapter 6.3 (Activation Functions) <a href="https://www.deeplearningbook.org/">https://www.deeplearningbook.org/</a>
<ul>
<li>包含深度学习全面内容的教科书。除了介绍激活函数的基本知识外，还可以学习深度学习的其他重要概念。</li>
</ul></li>
<li><strong>Understanding the difficulty of training deep feedforward neural networks (Glorot &amp; Bengio, 2010)</strong> <a href="http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf">http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf</a>
<ul>
<li>分析了Sigmoid和Tanh激活函数的梯度消失问题，并提出了Xavier初始化方法。对于理解深度神经网络学习的困难非常重要。</li>
</ul></li>
<li><strong>Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification (He et al., 2015)</strong> <a href="https://arxiv.org/abs/1502.01852">https://arxiv.org/abs/1502.01852</a>
<ul>
<li>提出了ReLU激活函数、PReLU激活函数以及He初始化方法。可以加深对现代深度学习中广泛使用的ReLU系列激活函数的理解。</li>
</ul></li>
<li><strong>Searching for Activation Functions (Ramachandran et al., 2017)</strong> <a href="https://arxiv.org/abs/1710.05941">https://arxiv.org/abs/1710.05941</a>
<ul>
<li>通过神经网络结构搜索（Neural Architecture Search, NAS）发现了Swish激活函数。可以了解探索新激活函数的方法。</li>
</ul></li>
<li><strong>STAF: A Sinusoidal Trainable Activation Function for Deep Learning (Jeon &amp; Cho, 2025)</strong> <a href="https://arxiv.org/abs/2405.13607">https://arxiv.org/abs/2405.13607</a> * 最新（2025年）发表在ICLR上的论文，提出了基于傅里叶级数的可学习激活函数STAF。有助于了解适应型激活函数的最新研究趋势。</li>
</ol>


</section>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>