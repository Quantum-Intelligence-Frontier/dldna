<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>quarto-input58d35ca2ac763b8 – Deep Learning DNA: Surviving Architectures and Essential Principles</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap-f507c7d0488cb7630e20aad62ad8c2aa.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script>window.MathJax = {loader: {load: ['[tex]/boldsymbol']},tex: {packages: {'[+]': ['boldsymbol']}}};</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../../notebooks/zh/part_1/01_深度学习的开始.html">part_1</a></li><li class="breadcrumb-item"><a href="../../../notebooks/zh/part_1/10_多模态深度学习：多感官融合的开始.html">10. 多模态深度学习：多感官融合的开始</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../../../">中文</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Language</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index_de.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Deutsch</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index_en.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">English</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index_es.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Español</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">한국어</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index_zh.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">中文</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/zh/part_1/00_引言.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">引言</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">part_1</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/zh/part_1/01_深度学习的开始.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">1. 深度学习的开始</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/zh/part_1/02_深度学习的数学.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">2. 深度学习的数学</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/zh/part_1/03_深度学习框架.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">3. 深度学习框架</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/zh/part_1/04_激活函数.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">4. 激活函数</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/zh/part_1/05_优化和可视化.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">5. 优化和可视化</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/zh/part_1/06_过拟合和解决技术的发展.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">6. 过拟合和解决技术的发展</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/zh/part_1/07_卷积神经网络的进化.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">7. 卷积神经网络的进化</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/zh/part_1/08_变压器的诞生.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">8. 变压器的诞生</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/zh/part_1/09_变压器的进化.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">9. 变压器的进化</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/zh/part_1/10_多模态深度学习：多感官融合的开始.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">10. 多模态深度学习：多感官融合的开始</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/zh/part_1/11_多模态深度学习：超越智能的极限.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">11. 多模态深度学习：超越智能的极限</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">深度学习的最前沿</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/zh/深度学习的最前沿/01_SLM: 小但强大的语言模型.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">1. SLM: 小但强大的语言模型</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/zh/深度学习的最前沿/02_自动驾驶.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">2. 自动驾驶</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#章-多模态深度学习多感官融合的开始" id="toc-章-多模态深度学习多感官融合的开始" class="nav-link active" data-scroll-target="#章-多模态深度学习多感官融合的开始">10章 多模态深度学习：多感官融合的开始</a>
  <ul class="collapse">
  <li><a href="#多模态与深度学习" id="toc-多模态与深度学习" class="nav-link" data-scroll-target="#多模态与深度学习">10.1 多模态与深度学习</a>
  <ul class="collapse">
  <li><a href="#多模态数据与深度学习的相遇" id="toc-多模态数据与深度学习的相遇" class="nav-link" data-scroll-target="#多模态数据与深度学习的相遇">10.1.1 多模态数据与深度学习的相遇</a></li>
  <li><a href="#多模態深度學習的重要性和應用領域" id="toc-多模態深度學習的重要性和應用領域" class="nav-link" data-scroll-target="#多模態深度學習的重要性和應用領域">10.1.2 多模態深度學習的重要性和應用領域</a></li>
  <li><a href="#多模態深度學習的歷史和發展過程" id="toc-多模態深度學習的歷史和發展過程" class="nav-link" data-scroll-target="#多模態深度學習的歷史和發展過程">10.1.3 多模態深度學習的歷史和發展過程</a></li>
  </ul></li>
  <li><a href="#早期多模态方法" id="toc-早期多模态方法" class="nav-link" data-scroll-target="#早期多模态方法">10.2 早期多模态方法</a>
  <ul class="collapse">
  <li><a href="#图像字幕多模态融合的第一步" id="toc-图像字幕多模态融合的第一步" class="nav-link" data-scroll-target="#图像字幕多模态融合的第一步">10.2.1 图像字幕：多模态融合的第一步</a></li>
  <li><a href="#视觉问答vqa图像理解和推理" id="toc-视觉问答vqa图像理解和推理" class="nav-link" data-scroll-target="#视觉问答vqa图像理解和推理">10.2.2 视觉问答（VQA）：图像理解和推理</a></li>
  </ul></li>
  <li><a href="#多模态融合fusion理论cmu课程基于的分类" id="toc-多模态融合fusion理论cmu课程基于的分类" class="nav-link" data-scroll-target="#多模态融合fusion理论cmu课程基于的分类">10.3 多模态融合（Fusion）理论：CMU课程基于的分类</a>
  <ul class="collapse">
  <li><a href="#joint-representations" id="toc-joint-representations" class="nav-link" data-scroll-target="#joint-representations">10.3.1 Joint Representations</a></li>
  <li><a href="#协调表示" id="toc-协调表示" class="nav-link" data-scroll-target="#协调表示">10.3.2 协调表示</a></li>
  <li><a href="#编码器-解码器" id="toc-编码器-解码器" class="nav-link" data-scroll-target="#编码器-解码器">10.3.3 编码器-解码器</a></li>
  <li><a href="#模态性整合策略" id="toc-模态性整合策略" class="nav-link" data-scroll-target="#模态性整合策略">10.3.4 模态性整合策略</a></li>
  </ul></li>
  <li><a href="#多模态表达学习技术" id="toc-多模态表达学习技术" class="nav-link" data-scroll-target="#多模态表达学习技术">10.4 多模态表达学习技术</a>
  <ul class="collapse">
  <li><a href="#模态间表达学习" id="toc-模态间表达学习" class="nav-link" data-scroll-target="#模态间表达学习">10.4.1 模态间表达学习</a></li>
  <li><a href="#跨模态注意力结构" id="toc-跨模态注意力结构" class="nav-link" data-scroll-target="#跨模态注意力结构">10.4.2 跨模态注意力结构</a></li>
  <li><a href="#perceiver-架构" id="toc-perceiver-架构" class="nav-link" data-scroll-target="#perceiver-架构">10.4.3 Perceiver 架构</a></li>
  <li><a href="#跨模态注意力机制的实现和训练稳定性" id="toc-跨模态注意力机制的实现和训练稳定性" class="nav-link" data-scroll-target="#跨模态注意力机制的实现和训练稳定性">10.4.4 跨模态注意力机制的实现和训练稳定性</a></li>
  </ul></li>
  <li><a href="#视觉变换器vit" id="toc-视觉变换器vit" class="nav-link" data-scroll-target="#视觉变换器vit">10.5 视觉变换器（ViT）</a>
  <ul class="collapse">
  <li><a href="#从cnn到vit的范式转变" id="toc-从cnn到vit的范式转变" class="nav-link" data-scroll-target="#从cnn到vit的范式转变">10.5.1 从CNN到ViT的范式转变</a></li>
  <li><a href="#图像patch嵌入的原理" id="toc-图像patch嵌入的原理" class="nav-link" data-scroll-target="#图像patch嵌入的原理">10.5.2 图像patch嵌入的原理</a></li>
  <li><a href="#位姿编码机制" id="toc-位姿编码机制" class="nav-link" data-scroll-target="#位姿编码机制">10.5.3 位姿编码机制</a></li>
  <li><a href="#vit-的结构和主要组成部分" id="toc-vit-的结构和主要组成部分" class="nav-link" data-scroll-target="#vit-的结构和主要组成部分">10.5.4 ViT 的结构和主要组成部分</a></li>
  <li><a href="#vit训练示例" id="toc-vit训练示例" class="nav-link" data-scroll-target="#vit训练示例">10.5.5 ViT训练示例</a></li>
  <li><a href="#vit-22b极致的规模" id="toc-vit-22b极致的规模" class="nav-link" data-scroll-target="#vit-22b极致的规模">10.5.6 ViT-22B：极致的规模</a></li>
  </ul></li>
  <li><a href="#clip多模态学习的里程碑" id="toc-clip多模态学习的里程碑" class="nav-link" data-scroll-target="#clip多模态学习的里程碑">10.6 CLIP：多模态学习的里程碑</a>
  <ul class="collapse">
  <li><a href="#clip的基本结构双编码器dual-encoder" id="toc-clip的基本结构双编码器dual-encoder" class="nav-link" data-scroll-target="#clip的基本结构双编码器dual-encoder">10.6.1 CLIP的基本结构：双编码器（Dual Encoder）</a></li>
  <li><a href="#图像编码器" id="toc-图像编码器" class="nav-link" data-scroll-target="#图像编码器">10.6.2 图像编码器</a></li>
  <li><a href="#文本编码器" id="toc-文本编码器" class="nav-link" data-scroll-target="#文本编码器">10.6.3 文本编码器</a></li>
  <li><a href="#零样本传递的机制" id="toc-零样本传递的机制" class="nav-link" data-scroll-target="#零样本传递的机制">10.6.4 零样本传递的机制</a></li>
  </ul></li>
  <li><a href="#练习题" id="toc-练习题" class="nav-link" data-scroll-target="#练习题">练习题</a></li>
  <li><a href="#参考资料" id="toc-参考资料" class="nav-link" data-scroll-target="#参考资料">参考资料</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../../notebooks/zh/part_1/01_深度学习的开始.html">part_1</a></li><li class="breadcrumb-item"><a href="../../../notebooks/zh/part_1/10_多模态深度学习：多感官融合的开始.html">10. 多模态深度学习：多感官融合的开始</a></li></ol></nav></header>




<p><a href="https://colab.research.google.com/github/Quantum-Intelligence-Frontier/dldna/blob/main/notebooks/zh/part_1/10_多模态深度学习：多感官融合的开始.ipynb" target="_parent"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="在Colab中打开"> </a></p>
<section id="章-多模态深度学习多感官融合的开始" class="level1">
<h1>10章 多模态深度学习：多感官融合的开始</h1>
<blockquote class="blockquote">
<p>“感知不是单一感官的碎片，而是所有感官和谐地融合在一起的交响曲。” —— 詹姆斯·吉布森，生态心理学创始人。</p>
</blockquote>
<p>在人工智能历史上，长期以来存在的一个未解之谜就是“多模态性”（Multimodality）。人类在感知世界时，同时使用视觉、听觉、触觉等多种感官，并将它们有机地整合起来。例如，我们在咖啡店喝咖啡时，会同时接收到杯子的温度（触觉）、咖啡的香气（嗅觉）、周围人聊天的声音（听觉）和咖啡店内部的景象（视觉）等多种信息，从而形成“在咖啡店”的整体经验。</p>
<p>但是，早期的人工智能模型在处理这些多模态信息时遇到了困难。从20世纪50年代开始的人工智能研究主要集中在单一模态（文本、图像、语音）处理上。各领域都取得了令人瞩目的成就，如翻译和语音识别，但将它们整合起来以类似人类的方式理解世界却是一个完全不同的挑战。</p>
<p>这一章我们将深入探讨多模态深度学习的核心理论和成功的架构。每种架构如何扩展和演化深度学习的DNA，以及它们如何为解决现实世界复杂问题做出贡献，将是我们的重点。</p>
<section id="多模态与深度学习" class="level2">
<h2 class="anchored" data-anchor-id="多模态与深度学习">10.1 多模态与深度学习</h2>
<blockquote class="blockquote">
<p><strong>挑战：</strong> 如何在一个模型中整合文本、图像、音频等不同类型的数据？这些数据具有不同的表现方式、维度和统计特性。如何融合这些异质信息以获得有意义的表达？</p>
<p><strong>研究者的困扰：</strong> 研究者们必须找到一种方法来保持每种模态的独特特征，同时有效地建模它们之间的交互作用，需要一种新的深度学习方法，即深度学习的新DNA。超越简单的拼接（concatenation），真正意义上的融合是必要的，这种融合使得每种模态都能理解其他模态的背景并提供互补信息。</p>
</blockquote>
<section id="多模态数据与深度学习的相遇" class="level3">
<h3 class="anchored" data-anchor-id="多模态数据与深度学习的相遇">10.1.1 多模态数据与深度学习的相遇</h3>
<p>多模态数据指的是两个或以上不同类型的数据组合，例如文本和图像、音频和视频。人类天然地整合这些多模态信息来理解世界。同时阅读文字和观看图片，或是听声音并感知情况，对于人类来说都是非常自然的事情。</p>
<p><strong>为什么多模态深度学习是一个难题？</strong></p>
<ol type="1">
<li><strong>异质的数据表现：</strong> 文本、图像、音频具有不同的表现方式、维度和统计特性。如何在一个模型中有效地表示和处理这些异质数据是困难的。</li>
<li><strong>信息融合的复杂性：</strong> 简单地将每种模态的信息拼接起来并不构成真正意义上的融合。需要建模每种模态之间的复杂交互作用，使得它们能够理解彼此的背景，提供互补信息，有时还需要调和冲突的信息。</li>
<li><strong>数据不足和不平衡：</strong> 多模态数据相对于单一模态数据来说数量较少，并且不同模态之间存在数据不平衡的问题。例如，图像和文本成对出现的数据很多，但同时包含图像、文本和音频的数据则相对较少。</li>
</ol>
<p>尽管面临这些挑战，深度学习为处理多模态数据提供了新的可能性。 2010年代以來，深度學習技術的發展，特別是Transformer架構的出現，在多模態深度學習的發展中發揮了決定性的作用。这是一個對深度學習DNA具有重要意義的轉折點。 Transformer的自我注意力機制（self-attention）使得不僅可以有效地建模每個模態內的元素之間的關係，而且還可以對不同模態之間的複雜交互作用進行有效的建模。在以前，CNN主要用於圖像，RNN主要用於序列數據，而Transformer則提供了一種通用的架構，可以應用于多個不同的模態，具有很強的靈活性。</p>
</section>
<section id="多模態深度學習的重要性和應用領域" class="level3">
<h3 class="anchored" data-anchor-id="多模態深度學習的重要性和應用領域">10.1.2 多模態深度學習的重要性和應用領域</h3>
<p>多模態深度學習是一種人工智慧中非常重要的技術，它使得機器可以像人類一樣理解和交互於世界。它不僅僅是處理不同形式的數據，還需要將每個數據中的含義有機地聯繫起來，以實現更豐富和準確的推理。就像大腦的各個區域合作以完成複雜的認知功能一樣，多模態深度學習是提升人工智慧智能水平的核心動力。</p>
<p><strong>主要應用領域</strong></p>
<ul>
<li><p><strong>視覺問答 (Visual Question Answering, VQA):</strong> 輸入圖像和問題（文字），生成答案。它需要超越單純識別圖像中的物體，對圖像和問題的含義進行綜合理解。例如，回答“圖片中男人戴著什麼顏色的帽子？”這種問題，需要找到男人，識別帽子，并判斷顏色，這是一個複雜的過程。</p></li>
<li><p><strong>圖像標題生成 (Image Captioning):</strong> 自動生成描述圖像的文字。需要準確抓住圖像的內容，并將其以自然的句子表達出來。</p></li>
<li><p><strong>多模態情感分析 (Multimodal Sentiment Analysis):</strong> 綜合文字、語音、面部表情等多種信息，判斷用戶的情感。有些通過文字難以捕捉到的微妙情感變化，可以通過語音的語調或面部表情的變化來感知。</p></li>
<li><p><strong>自主駕駛:</strong> 集成來自攝像頭（圖像）、LiDAR（3D傳感器）、GPS（位置信息）和雷達等多種傳感器數據，實現環境的感知和駕駛決策。每個傳感器提供不同的信息，它們之間的融合是安全、準確駕駛的關鍵。</p></li>
<li><p><strong>機器人學:</strong> 機器人通過視覺、觸覺、聽覺等多種傳感器信息的融合，完成複雜任務。例如，機器人抓取物品需要視覺上判斷物體位置和形狀，并通過觸覺信息調整適當的抓握力。</p></li>
<li><p><strong>醫學診斷:</strong> 整合X光、MRI（圖像）、病人記錄（文字）、生理信號（時序數據）和基因組信息等，進行疾病診斷和預測。每種數據提供了疾病的不同線索，它們的整合分析是準確診斷的關鍵。</p></li>
</ul>
</section>
<section id="多模態深度學習的歷史和發展過程" class="level3">
<h3 class="anchored" data-anchor-id="多模態深度學習的歷史和發展過程">10.1.3 多模態深度學習的歷史和發展過程</h3>
<p>多模態深度學習研究是一個展示深度學習DNA演化的迷人旅程。這段旅程可以大致分為以下幾個主要階段：</p>
<section id="初期階段2010年代初" class="level4">
<h4 class="anchored" data-anchor-id="初期階段2010年代初">初期階段（2010年代初）</h4>
<p>2010年代初，多模態深度學習的早期研究主要集中在圖像標題生成和視覺問答上。當時，CNN-RNN基礎模型占據主流地位，CNN用於從圖像中提取特徵，而RNN則用於處理文字。CNN在捕捉圖像的空間特徵方面表現突出，而RNN在處理文字的序列信息方面具有強大的能力。 但是，初期模型主要采用的是<em>late fusion</em>的方法，即在最后阶段将每个模态的结果进行结合。这种方法可以保留每个模态的独特特性，但不能充分反映模态之间在早期阶段的交互作用。</p>
<p>这一时期的典型模型包括将图像和词嵌入投影到同一空间中以计算图像-文本相似度的<strong>DeViSE (Frome et al., 2013)</strong>，以及通过结合CNN和RNN并添加多模态信息融合层（multimodal layer）来进行图像标题生成的<strong>m-RNN (Mao et al., 2014)</strong>等。</p>
</section>
<section id="引入注意力机制2010年代中期" class="level4">
<h4 class="anchored" data-anchor-id="引入注意力机制2010年代中期">引入注意力机制（2010年代中期）</h4>
<p>2010年代中期，注意力机制（attention mechanism）的出现为多模态深度学习研究带来了巨大的转变。注意力机制使得图像和文本之间的相关性可以被更细致地建模。例如，在图像标题生成中，注意力可以学习到在生成特定单词时应该关注图像的哪个区域，而在VQA中，可以通过决定图像的哪个部分来回答问题。</p>
<p>注意力机制的引入大大提高了图像标题生成和VQA模型的性能。代表性的模型包括将注意力引入图像标题生成以使得生成的单词与相关的图像区域相对应的<strong>Show, Attend and Tell (Xu et al., 2015)</strong>，以及通过多次应用注意力来回答问题的<strong>Stacked Attention Networks (Yang et al., 2016)</strong>等。</p>
</section>
<section id="transformer的出现和多模态创新2017年之后" class="level4">
<h4 class="anchored" data-anchor-id="transformer的出现和多模态创新2017年之后">Transformer的出现和多模态创新（2017年之后）</h4>
<p>2017年，在”Attention is All You Need”论文中，Transformer架构被提出，这使得多模态深度学习进入了一个新的阶段。Transformer基于自注意力机制，可以直接地建模输入序列内所有元素之间的关系。</p>
<ul>
<li><strong>ViT (Vision Transformer, 2020)</strong>：将图像分割为patch并输入到Transformer中的ViT成为图像处理领域中CNN的一个强大的替代者。ViT可以有效地建模图像内部的长距离依赖性，从而在图像分类等多种任务中表现出色。</li>
<li><strong>CLIP (Contrastive Language-Image Pre-training, 2021)</strong>：CLIP通过使用大量图像-文本对数据，学习将图像和文本嵌入到同一空间中的方法。这样，不需要额外的fine-tuning，就可以在各种下游任务（如图像分类、对象检测等）中实现零样本学习（zero-shot），取得了突破性的成果。</li>
<li><strong>DALL-E (2021)</strong>、<strong>Imagen (2022)</strong>、<strong>Stable Diffusion (2022)</strong>：基于文本描述生成高质量图像的模型展示了Transformer基于生成模型的惊人能力。它们通过学习文本和图像之间的复杂关系，实现了之前难以想象的图像生成水平。</li>
<li><strong>GPT-4V (2023), Gemini (2023):</strong> 能够同时理解和处理文本和图像的巨型多模态模型（LMM，Large Multimodal Model）的出现开启了多模态深度学习的新可能性。这些拥有数十亿参数的巨型模型在各种多模态任务中达到人类级别的性能，站在人工智能研究的最前沿。</li>
</ul>
</section>
<section id="最近趋势智慧的扩展和融合" class="level4">
<h4 class="anchored" data-anchor-id="最近趋势智慧的扩展和融合">最近趋势：智慧的扩展和融合</h4>
<p>最近的多模态深度学习研究已经超越了简单的信息融合，正在朝着利用每个模态的信息来生成新知识和改善推理能力的方向发展。</p>
<ul>
<li><p><strong>LMM（Large Multimodal Model）的进步：</strong> 更多的模态（如音频、视频、3D传感器数据等）被整合，具有更复杂推理能力的LMM不断涌现。</p></li>
<li><p><strong>高效融合技术研究：</strong> 另一方面，对于如何在有限的计算资源下有效利用多模态模型的研究也在积极进行，以减少计算成本同时最大化信息融合效果。</p></li>
<li><p><strong>可解释性（XAI）和伦理问题：</strong> 随着多模态模型复杂性的增加，理解模型决策过程并解决偏见等伦理问题的研究变得愈发重要。</p></li>
</ul>
<p>下一节中，我们将更详细地探讨多模态深度学习的早期方法以及在此过程中“幸存”的主要架构。</p>
</section>
</section>
</section>
<section id="早期多模态方法" class="level2">
<h2 class="anchored" data-anchor-id="早期多模态方法">10.2 早期多模态方法</h2>
<p>10.1.3节中，我们看到Transformer和CLIP为多模态深度学习带来了创新。但是，这些进展并非突然实现的。在此之前，也有许多尝试将图像和文本，甚至更广泛的模态结合起来的研究。这些早期研究为现代多模态深度学习奠定了坚实的基础。本节中，我们将探讨2010年初至中期，引领深度学习基于多模态研究初期的主要方法及其意义。</p>
<section id="图像字幕多模态融合的第一步" class="level3">
<h3 class="anchored" data-anchor-id="图像字幕多模态融合的第一步">10.2.1 图像字幕：多模态融合的第一步</h3>
<p>图像字幕（Image Captioning）是指自动生成描述给定图像的自然语言句子（字幕）的任务。这是一个将视觉信息（图像）转换为语言信息（文本）的典型多模态问题，是深度学习基于多模态研究早期的主要研究对象。图像字幕类似于儿童观看图画书时，指着图片说“这里有一只狗，那里有一个球！”</p>
<section id="早期cnn-rnn结构2014年之前" class="level4">
<h4 class="anchored" data-anchor-id="早期cnn-rnn结构2014年之前">早期CNN-RNN结构（2014年之前）</h4>
<p>在图像字幕研究初期，结合了卷积神经网络（CNN）和循环神经网络（RNN）的模型占据主流。这种结构类似于连接处理视觉信息的CNN“脑半球”和处理语言信息的RNN“脑半球”。CNN作为图像编码器，使用VGGNet、AlexNet等提取图像特征向量，而RNN作为文本解码器，利用长短时记忆（LSTM）等模型根据图像特征向量生成字幕句子。</p>
<p>代表性的模型包括Show and Tell（Vinyals et al., 2015），该模型使用CNN提取的图像特征作为LSTM的初始隐藏状态来生成字幕，提出了一种端到端的方法。然而，这样的CNN-RNN结构虽然能较好地理解图像整体内容，但在模拟图像的细节区域和文本特定词语之间的对应关系方面存在不足。</p>
</section>
<section id="引入注意力机制2015年之后" class="level4">
<h4 class="anchored" data-anchor-id="引入注意力机制2015年之后">引入注意力机制（2015年之后）</h4>
<p>注意力机制的引入极大地提高了图像字幕模型的性能。这种机制使得模型能够“关注”图像的特定区域，类似于我们观赏图片时，视线自然集中在重要部分。</p>
<p>注意力机制主要分为软注意力（Soft Attention）和硬注意力（Hard Attention）。软注意力计算图像所有区域的权重并使用加权平均特征向量，而硬注意力则选择图像中的一个特定区域来生成字幕。</p>
<p>Show, Attend and Tell（Xu et al., 2015）是首个将软注意力机制应用于图像字幕的模型，它通过在生成每个词时学习哪些图像区域需要关注，从而能够产生更准确、更详细的字幕。</p>
</section>
<section id="底向上和顶向下注意力2017年之后" class="level4">
<h4 class="anchored" data-anchor-id="底向上和顶向下注意力2017年之后">底向上和顶向下注意力（2017年之后）</h4>
<p>2017年后，出现了同时利用图像整体背景（顶向下）和个别对象（底向上）信息的底向上和顶向下注意力方法。底向上方法使用Faster R-CNN等对象检测模型识别图像中的主要对象，而顶向下方法则在字幕生成过程中计算这些对象特征的注意力权重。</p>
<p>Bottom-Up and Top-Down Attention（Anderson et al., 2018）模型将这两种方法结合起来，显著提高了图像字幕的性能。这种方法类似于同时考虑故事整体线索和详细描述每个场景中出现的对象。</p>
</section>
<section id="从深度学习dna角度看图像字幕的演化" class="level4">
<h4 class="anchored" data-anchor-id="从深度学习dna角度看图像字幕的演化">从深度学习DNA角度看图像字幕的演化</h4>
<p>图像字幕研究为深度学习DNA添加了重要的元素。CNN-RNN结合提出了有效地融合不同模态的基本框架，注意力机制成为多模态深度学习中的核心技术。此外，Bottom-Up和Top-Down Attention提高了深度学习模型的图像理解能力。</p>
<p>这种进步不仅推动了图像字幕的发展，也为后来的VQA、多模态机器翻译等各种多模态任务提供了基础。最近，基于Transformer的模型（如BLIP）出现了，不仅在图像字幕中表现出色，也在其他多模态任务中展示出了良好的性能。</p>
</section>
<section id="图像字幕模型blip示例" class="level4">
<h4 class="anchored" data-anchor-id="图像字幕模型blip示例">图像字幕模型（BLIP）示例</h4>
<p>BLIP（Bootstrapping Language-Image Pre-training）是一种用于图像字幕的Transformer基于模型。BLIP通过同时预训练图像和文本，不仅在图像字幕中表现出色，也在VQA、图像-文本搜索等多模态任务中展示出了良好的性能。</p>
<p>以下是使用Hugging Face Transformers库利用BLIP模型生成图像字幕的示例代码。</p>
<div id="cell-3" class="cell" data-execution_count="4">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># !pip install dldna[colab] # in Colab</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="co"># !pip install dldna[all] # in your local</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>load_ext autoreload</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>autoreload <span class="dv">2</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="cell-4" class="cell" data-execution_count="5">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> BlipProcessor, BlipForConditionalGeneration</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> Image</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> requests</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the model and processor</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>processor <span class="op">=</span> BlipProcessor.from_pretrained(<span class="st">"Salesforce/blip-image-captioning-base"</span>)</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> BlipForConditionalGeneration.from_pretrained(<span class="st">"Salesforce/blip-image-captioning-base"</span>)</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Download the image</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>url <span class="op">=</span> <span class="st">"http://images.cocodataset.org/val2017/000000000632.jpg"</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>image <span class="op">=</span> Image.<span class="bu">open</span>(requests.get(url, stream<span class="op">=</span><span class="va">True</span>).raw)</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the image</span></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>plt.imshow(image)</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>plt.axis(<span class="st">'off'</span>)</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Preprocess the input</span></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>inputs <span class="op">=</span> processor(image, return_tensors<span class="op">=</span><span class="st">"pt"</span>)</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate the caption</span></span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>outputs <span class="op">=</span> model.generate(<span class="op">**</span>inputs)</span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Decode and print the caption</span></span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>caption <span class="op">=</span> processor.decode(outputs[<span class="dv">0</span>], skip_special_tokens<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Generated caption:"</span>, caption)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="10_多模态深度学习：多感官融合的开始_files/figure-html/cell-3-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Generated caption: a bedroom with a bed and a window</code></pre>
</div>
</div>
</section>
</section>
<section id="视觉问答vqa图像理解和推理" class="level3">
<h3 class="anchored" data-anchor-id="视觉问答vqa图像理解和推理">10.2.2 视觉问答（VQA）：图像理解和推理</h3>
<p>视觉问答（Visual Question Answering，VQA）是指给定一张图像和一个自然语言问题，根据图像内容生成对问题的回答的任务。图像字幕生成关注于“描述”图像的内容，而VQA则是对图像进行“问答”。例如，对于问题“猫在吃什么？”给出答案。VQA比图像字幕生成更复杂和高维，特别是需要理解图像与问题（文本）之间的关系，并进行推理。</p>
<section id="早期vqa模型cnn-rnn2015年之前" class="level4">
<h4 class="anchored" data-anchor-id="早期vqa模型cnn-rnn2015年之前"><strong>早期VQA模型（CNN + RNN）（2015年之前）</strong></h4>
<p>早期VQA模型与图像字幕生成类似，也采用了CNN和RNN结合的结构。使用CNN提取图像特征，使用RNN编码问题，然后将这两个特征结合起来生成答案。但是，只是简单地结合图像特征和问题特征，却难以回答复杂的问题。</p>
</section>
<section id="多模态注意力机制2016年之后" class="level4">
<h4 class="anchored" data-anchor-id="多模态注意力机制2016年之后"><strong>多模态注意力机制（2016年之后）</strong></h4>
<p>在图像字幕生成中，注意力机制取得了成功，因此也被引入到VQA中。Co-Attention同时对图像和问题应用注意力，以计算问题的每个词与图像的每个区域之间的相关性，从而更准确地找到与问题相关的图像区域。</p>
<p>Stacked Attention通过多次重复注意力的应用，逐步深入理解图像和问题之间的复杂关系。这就像侦探多次审视照片，以逐渐加深对问题相关性的理解。</p>
<p>代表性的模型包括Stacked Attention Networks（SAN）（Yang et al.，2016）和Dual Attention Networks（DAN）（Nam et al.，2017）。SAN是通过多次对图像应用注意力来生成问题答案的模型，而DAN则分别计算图像和问题的注意力，并将它们结合起来生成答案。</p>
</section>
<section id="外部知识整合2018年之后" class="level4">
<h4 class="anchored" data-anchor-id="外部知识整合2018年之后">外部知识整合（2018年之后）</h4>
<p>图像字幕生成与VQA的一个关键区别在于外部知识的整合。为了进一步提高VQA模型的推理能力，研究人员开始利用外部知识（常识、百科全书知识等）。知识库（Knowledge Base）利用结构化的知识库（如Wikipedia、ConceptNet），提供回答问题所需的信息。</p>
<p>Memory Networks以存储外部知识的形式，将其保存在内存中，并根据问题从内存中检索相关信息，以便生成答案。然而，有效地利用外部知识仍然是一个具有挑战性的任务，包括知识库的不完善、与问题的相关性判断以及推理过程的复杂性等待解决的问题。</p>
</section>
<section id="从深度学习dna角度看vqa的演进" class="level4">
<h4 class="anchored" data-anchor-id="从深度学习dna角度看vqa的演进">从深度学习DNA角度看VQA的演进</h4>
<p>VQA研究为深度学习DNA添加了重要的基因。CNN-RNN结合提供了一个基本框架，用于图像和文本的整合，这与图像字幕生成共享。多模态注意力赋予了深度学习模型理解图像和问题之间复杂关系的能力。这意味着深度学习模型不仅可以简单地整合信息，还能理解信息间的相互作用并进行推理。</p>
<p>外部知识的整合为深度学习模型打开了利用外部知识进行更高层次推理的可能性。这表明，深度学习模型不仅依赖于数据，还能够利用人类的知识和经验。 10.2.1节和10.2.2节中我们所看到的图像字幕和VQA是早期多模态深度学习研究的两个重要支柱。这些研究在将CNN、RNN、注意力机制等深度学习的核心技术应用于和发展多模态问题方面做出了巨大的贡献，并为后来基于Transformer的更强大的多模态模型（如CLIP、DALL-E、GPT-4V、Gemini等）的出现奠定了重要基础。</p>
<p>最近，像ViLT（Vision-and-Language Transformer）这样的基于Transformer的VQA模型已经涌现并表现出良好的性能。ViLT通过将图像块和文本标记输入到同一个Transformer模型中，从而有效地建模图像和文本之间的复杂交互。</p>
</section>
<section id="vqa-模型vilt示例" class="level4">
<h4 class="anchored" data-anchor-id="vqa-模型vilt示例">VQA 模型（ViLT）示例</h4>
<p>ViLT（Vision-and-Language Transformer）是代表性的基于Transformer的VQA模型之一。它通过将图像块和文本标记输入到同一个Transformer模型中，从而有效地建模图像和文本之间的复杂交互。</p>
<p>以下是使用Hugging Face Transformers库来执行ViLT模型的VQA示例代码。</p>
<div id="cell-6" class="cell" data-execution_count="6">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> ViltProcessor, ViltForQuestionAnswering</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> Image</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> requests</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="co"># 모델과 프로세서 로드</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>processor <span class="op">=</span> ViltProcessor.from_pretrained(<span class="st">"dandelin/vilt-b32-finetuned-vqa"</span>)</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> ViltForQuestionAnswering.from_pretrained(<span class="st">"dandelin/vilt-b32-finetuned-vqa"</span>)</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a><span class="co"># 이미지 다운로드</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>url <span class="op">=</span> <span class="st">"http://images.cocodataset.org/val2017/000000039769.jpg"</span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>image <span class="op">=</span> Image.<span class="bu">open</span>(requests.get(url, stream<span class="op">=</span><span class="va">True</span>).raw)</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a><span class="co"># 이미지 출력</span></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>plt.imshow(image)</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>plt.axis(<span class="st">'off'</span>)  <span class="co"># 축 제거</span></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a><span class="co"># 질문 설정</span></span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>question <span class="op">=</span> <span class="st">"How many cats are in the image?"</span></span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Question:"</span>, question)</span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a><span class="co"># 입력 전처리</span></span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>encoding <span class="op">=</span> processor(image, question, return_tensors<span class="op">=</span><span class="st">"pt"</span>)</span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a><span class="co"># 추론</span></span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>outputs <span class="op">=</span> model(<span class="op">**</span>encoding)</span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a>logits <span class="op">=</span> outputs.logits</span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a>idx <span class="op">=</span> logits.argmax(<span class="op">-</span><span class="dv">1</span>).item()</span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Predicted answer:"</span>, model.config.id2label[idx])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="10_多模态深度学习：多感官融合的开始_files/figure-html/cell-4-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Question: How many cats are in the image?
Predicted answer: 2</code></pre>
</div>
</div>
</section>
</section>
</section>
<section id="多模态融合fusion理论cmu课程基于的分类" class="level2">
<h2 class="anchored" data-anchor-id="多模态融合fusion理论cmu课程基于的分类">10.3 多模态融合（Fusion）理论：CMU课程基于的分类</h2>
<p>假设我们有两种信息：图像和文本。如何将这两种信息结合起来呢？最简单的方法是将文本向量连接到图像向量后面，形成一个新的向量。这被称为融合（Fusion），它将来自不同数据源的信息联系起来。多模态的核心是从两个不同的数据特征中有效地融合信息。</p>
<p>由于多模态深度学习是一个快速发展的领域，缺乏系统性的总结，因此很难入门。本节基于卡内基梅隆大学（CMU）多模态机器学习课程的内容，将多模态融合分为三个主要类别。这种分类不是当前多模态研究的标准分类，但它对于理解各种融合技术非常有用。</p>
<section id="joint-representations" class="level3">
<h3 class="anchored" data-anchor-id="joint-representations">10.3.1 Joint Representations</h3>
<p>Joint Representations是一种将多个模态的数据表示在一个共同的向量空间中的方法。就像同时在一个画布上绘制文本和图像一样。</p>
<p>与其分别处理每个模态的数据，不如将它们融合成一个集成的特征向量（feature vector）。这个向量包含了各个模态的信息，使得模型能够学习到不同模态之间的深层次关联。这种方法可以用一个模型来处理多个模态，并且通过压缩表示，可以使模型结构相对简单和高效。然而，在融合过程中，每个模态的独特特征可能会被稀释或丢失。如果某个模态比其他模态拥有更多信息，可能会出现信息不平衡的问题。而将不同模态的数据融合成一个有意义的向量是非常困难的。</p>
<p>最简单的方法是将每个模态的特征向量直接连接起来（concatenate）。此外，还有多模态分解模型（Multi-modal Factorization Model, MFM），它通过矩阵分解来结合多种数据，生成一个共同的表示空间。还有多模态判别二进制嵌入（Multi-modal Discriminative Binary Embedding, MDBE），它将图像和文本等多模态数据表示为二进制代码。</p>
<p>最近的研究中，提出了一些新的方法，如COSA（Concatenated Sample），它通过顺序连接多个图像-文本对，并应用基于Transformer的模型来共同学习视觉内容和时间线索。另外，还有注意力连接（Attentional Concatenation）用于从文本生成高分辨率图像，使用了多级梯度结构，并利用前一层的结果和词向量作为下一层的输入。</p>
<p><strong>结构示例</strong></p>
<p>以下是三个方法（Concatenation、MFM、MDBF）的融合示意图：</p>
<p><img src="../../../assets/images/10_01.png" width="800"></p>
<p><strong>示例</strong></p>
<div id="cell-8" class="cell" data-execution_count="7">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoModel, AutoProcessor, AutoTokenizer</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> Image</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> requests</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Load pre-trained models and processor/tokenizer for image and text</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>image_model_name <span class="op">=</span> <span class="st">"google/vit-base-patch16-224-in21k"</span>  <span class="co"># ViT (Vision Transformer)</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>text_model_name <span class="op">=</span> <span class="st">"bert-base-uncased"</span>  <span class="co"># BERT</span></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>image_processor <span class="op">=</span> AutoProcessor.from_pretrained(image_model_name)</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>image_model <span class="op">=</span> AutoModel.from_pretrained(image_model_name)</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(text_model_name)</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>text_model <span class="op">=</span> AutoModel.from_pretrained(text_model_name)</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Example image and text</span></span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>url <span class="op">=</span> <span class="st">"http://images.cocodataset.org/val2017/000000039769.jpg"</span></span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>image <span class="op">=</span> Image.<span class="bu">open</span>(requests.get(url, stream<span class="op">=</span><span class="va">True</span>).raw)</span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>text <span class="op">=</span> <span class="st">"Two cats sleeping on a couch."</span></span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the image</span></span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a>plt.imshow(image)</span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a>plt.axis(<span class="st">'off'</span>)  <span class="co"># Remove axes</span></span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Preprocess image and text</span></span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a>image_inputs <span class="op">=</span> image_processor(images<span class="op">=</span>image, return_tensors<span class="op">=</span><span class="st">"pt"</span>)</span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a>text_inputs <span class="op">=</span> tokenizer(text, return_tensors<span class="op">=</span><span class="st">"pt"</span>)</span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-31"><a href="#cb6-31" aria-hidden="true" tabindex="-1"></a><span class="co"># Feature extraction (embeddings) for each modality</span></span>
<span id="cb6-32"><a href="#cb6-32" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():  <span class="co"># Disable gradient calculation (inference mode)</span></span>
<span id="cb6-33"><a href="#cb6-33" aria-hidden="true" tabindex="-1"></a>    image_features <span class="op">=</span> image_model(<span class="op">**</span>image_inputs).last_hidden_state[:, <span class="dv">0</span>, :]  <span class="co"># [CLS] token embedding</span></span>
<span id="cb6-34"><a href="#cb6-34" aria-hidden="true" tabindex="-1"></a>    text_features <span class="op">=</span> text_model(<span class="op">**</span>text_inputs).last_hidden_state[:, <span class="dv">0</span>, :]   <span class="co"># [CLS] token embedding</span></span>
<span id="cb6-35"><a href="#cb6-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-36"><a href="#cb6-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-37"><a href="#cb6-37" aria-hidden="true" tabindex="-1"></a><span class="co"># Create Joint Representation (Concatenation)</span></span>
<span id="cb6-38"><a href="#cb6-38" aria-hidden="true" tabindex="-1"></a>joint_representation <span class="op">=</span> torch.cat((image_features, text_features), dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb6-39"><a href="#cb6-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-40"><a href="#cb6-40" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Image Features Shape:"</span>, image_features.shape)  <span class="co"># Image feature vector size</span></span>
<span id="cb6-41"><a href="#cb6-41" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Text Features Shape:"</span>, text_features.shape)     <span class="co"># Text feature vector size</span></span>
<span id="cb6-42"><a href="#cb6-42" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Joint Representation Shape:"</span>, joint_representation.shape) <span class="co"># Combined feature vector size (image + text)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stderr">
<pre><code>Fast image processor class &lt;class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'&gt; is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="10_多模态深度学习：多感官融合的开始_files/figure-html/cell-5-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Image Features Shape: torch.Size([1, 768])
Text Features Shape: torch.Size([1, 768])
Joint Representation Shape: torch.Size([1, 1536])</code></pre>
</div>
</div>
</section>
<section id="协调表示" class="level3">
<h3 class="anchored" data-anchor-id="协调表示">10.3.2 协调表示</h3>
<p>协调表示是一种方法，各个模态都被表示在单独的空间中，但它们之间的关系是显式地学习的。就像多个画布上的图画需要相互协调一样。</p>
<p>每个模态都被表示为一个单独的特征向量，但这些向量需要”协调”。也就是说，每个模态的特征空间都是独立的，但是通过学习它们之间的相似性、顺序关系等，可以建立起有意义的联系。这一方法的优点是可以最大限度地保留每个模态的独特特征，同时考虑到不同模态之间的关联。另外，这种方法还可以应用于各种多模态问题。</p>
<p>然而，由于需要分别处理各个模态，因此模型结构可能会比联合表示更复杂。这使得模型设计和训练更加困难。同时，显式地学习每个模态之间的关系并不是一个简单的问题。</p>
<p>典型的例子是CLIP（对比语言-图像预训练）。CLIP使用单独的编码器分别处理图像和文本，以获取特征向量，并学习它们之间的相似度。CLIP通过使图像和文本”配对”来学习图像和文本之间有意义的关系。</p>
<p>CLIP的成功在零样本学习能力方面尤为突出。预训练好的CLIP模型可以在不需要针对特定任务进行额外训练的情况下，分类或搜索新图像。这是因为它有效地学习了文本和图像之间的语义联系。</p>
<p><strong>结构示例</strong></p>
<p>以下是CLIP融合的图示：</p>
<p><img src="../../../assets/images/10_02.png" width="800"></p>
<ul>
<li>图像编码器：ViT（视觉Transformer）或ResNet</li>
<li>文本编码器：Transformer</li>
</ul>
<p><strong>示例</strong></p>
<div id="cell-10" class="cell" data-execution_count="8">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> CLIPProcessor, CLIPModel</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> Image</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> requests</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Load CLIP model and processor</span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> CLIPModel.from_pretrained(<span class="st">"openai/clip-vit-base-patch32"</span>)</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>processor <span class="op">=</span> CLIPProcessor.from_pretrained(<span class="st">"openai/clip-vit-base-patch32"</span>)</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Example image and text</span></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>url <span class="op">=</span> <span class="st">"http://images.cocodataset.org/val2017/000000039769.jpg"</span></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>image <span class="op">=</span> Image.<span class="bu">open</span>(requests.get(url, stream<span class="op">=</span><span class="va">True</span>).raw)</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>text <span class="op">=</span> <span class="st">"Two cats sleeping on a couch."</span></span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Display image</span></span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>plt.imshow(image)</span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>plt.axis(<span class="st">'off'</span>)  <span class="co"># Remove axes</span></span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Preprocess image and text</span></span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>inputs <span class="op">=</span> processor(text<span class="op">=</span>[text], images<span class="op">=</span>image, return_tensors<span class="op">=</span><span class="st">"pt"</span>, padding<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract image and text features (embeddings)</span></span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a>    outputs <span class="op">=</span> model(<span class="op">**</span>inputs)</span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a>    image_features <span class="op">=</span> outputs.image_embeds</span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a>    text_features <span class="op">=</span> outputs.text_embeds</span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-30"><a href="#cb9-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Coordinated Representation: Keep features of each modality separate</span></span>
<span id="cb9-31"><a href="#cb9-31" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Image Features Shape:"</span>, image_features.shape)</span>
<span id="cb9-32"><a href="#cb9-32" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Text Features Shape:"</span>, text_features.shape)</span>
<span id="cb9-33"><a href="#cb9-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-34"><a href="#cb9-34" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate similarity between image and text (dot product)</span></span>
<span id="cb9-35"><a href="#cb9-35" aria-hidden="true" tabindex="-1"></a>similarity <span class="op">=</span> torch.matmul(image_features, text_features.T)  <span class="co"># Or text_features @ image_features.T</span></span>
<span id="cb9-36"><a href="#cb9-36" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Image-Text Similarity:"</span>, similarity.item())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="10_多模态深度学习：多感官融合的开始_files/figure-html/cell-6-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Image Features Shape: torch.Size([1, 512])
Text Features Shape: torch.Size([1, 512])
Image-Text Similarity: 0.29803216457366943</code></pre>
</div>
</div>
<p>應用上述方法，則可以進行如下簡單的零拍測試。</p>
<div id="cell-12" class="cell" data-execution_count="9">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Zero-shot 이미지 분류</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="co">#   - 여러 텍스트 후보군을 만들고, 각 텍스트와 이미지 간의 유사도를 계산하여 가장 높은 유사도를 갖는 텍스트를 선택</span></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>candidate_texts <span class="op">=</span> [<span class="st">"a photo of a cat"</span>, <span class="st">"a photo of a dog"</span>, <span class="st">"a photo of a bird"</span>]</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>inputs <span class="op">=</span> processor(text<span class="op">=</span>candidate_texts, images<span class="op">=</span>image, return_tensors<span class="op">=</span><span class="st">"pt"</span>, padding<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>    outputs <span class="op">=</span> model(<span class="op">**</span>inputs)</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>    image_features <span class="op">=</span> outputs.image_embeds</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>    text_features <span class="op">=</span> outputs.text_embeds</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>    logits_per_image <span class="op">=</span> outputs.logits_per_image <span class="co"># 유사도 점수</span></span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>probs <span class="op">=</span> logits_per_image.softmax(dim<span class="op">=</span><span class="dv">1</span>)  <span class="co">#  확률</span></span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>predicted_class_idx <span class="op">=</span> probs.argmax().item()</span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a>predicted_class <span class="op">=</span> candidate_texts[predicted_class_idx]</span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Predicted Class:"</span>, predicted_class)</span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Probabilities:"</span>, probs)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Predicted Class: a photo of a cat
Probabilities: tensor([[9.9403e-01, 5.1377e-03, 8.3070e-04]])</code></pre>
</div>
</div>
</section>
<section id="编码器-解码器" class="level3">
<h3 class="anchored">10.3.3 编码器-解码器</h3>
<p>编码器-解码器是一种将一种模态的数据转换为另一种模态的数据的方法。这种技术通常用于语言翻译。</p>
<p>在这种结构中，编码器（Encoder）将输入模态（例如图像）的数据转换为特征向量。这个特征向量以压缩形式表示输入数据的核心信息。解码器（Decoder）利用编码器生成的特征向量来生成另一种模态的数据（例如文本）。解码器通过“解释”编码器的输出来创建新的数据形式。此外，通过注意力机制，解码器在生成输出数据时学习如何“关注”编码器特征向量的哪个部分。</p>
<p>这种方法的优点是可以应用于连接不同类型数据的各种任务，例如图像字幕、VQA、机器翻译等。另外，即使输入和输出模态不同，也可以应用此方法，例如文本-图像、图像-文本、音频-文本等多种组合。</p>
<p>典型的例子包括图像字幕和视觉问答（VQA）。图像字幕使用编码器处理图像以获取特征向量，然后使用解码器生成字幕（文本）。VQA分别使用编码器处理图像和问题（文本），然后使用注意力机制来理解图像和问题之间的关系，最后使用解码器生成答案（文本）。</p>
<p>然而，当输入或输出数据变得更长时，可能会发生信息损失或计算量增加。尤其是基于RNN的模型，由于梯度消失问题（gradient vanishing problem），学习长距离依赖性可能很困难。此外，由于需要同时训练编码器和解码器，因此训练可能不稳定或具有挑战性。</p>
<p><strong>结构示例</strong></p>
<p>以下是编码器-解码器融合的图示：</p>
<p><img src="../../../assets/images/10_03.png" width="800"></p>
<ul>
<li>图像输入，文本输入：分别表示图像和文本输入（问题或其他文本信息）。</li>
<li>图像编码器，文本编码器：各自对应模态的编码器。图像编码器通常使用CNN或ViT（视觉Transformer），而文本编码器则使用RNN或Transformer。</li>
<li>注意力机制：决定解码器在生成文本时应该“关注”图像特征（图像编码器的特征）的哪个部分。文本编码器的特征也可以用于注意力机制（跨模态注意力）。</li>
</ul>
<p><strong>示例</strong></p>
<div id="cell-14" class="cell" data-execution_count="10">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> BlipProcessor, BlipForConditionalGeneration</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> Image</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> requests</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Load model and processor</span></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>processor <span class="op">=</span> BlipProcessor.from_pretrained(<span class="st">"Salesforce/blip-image-captioning-base"</span>)</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> BlipForConditionalGeneration.from_pretrained(<span class="st">"Salesforce/blip-image-captioning-base"</span>)</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Download image</span></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>url <span class="op">=</span> <span class="st">"http://images.cocodataset.org/val2017/000000000139.jpg"</span></span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>image <span class="op">=</span> Image.<span class="bu">open</span>(requests.get(url, stream<span class="op">=</span><span class="va">True</span>).raw)</span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Display image</span></span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>plt.imshow(image)</span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>plt.axis(<span class="st">'off'</span>)</span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Input text (optional - Conditional Generation)</span></span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a><span class="co"># text = "describe this image:"  # Prompt (guide image description)</span></span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a>text <span class="op">=</span> <span class="st">"a photo of"</span></span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Preprocess image and text (optional)</span></span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a><span class="co"># If text is provided, it uses the text as a prompt to generate the caption.</span></span>
<span id="cb13-25"><a href="#cb13-25" aria-hidden="true" tabindex="-1"></a>inputs <span class="op">=</span> processor(image, text<span class="op">=</span>text, return_tensors<span class="op">=</span><span class="st">"pt"</span>)</span>
<span id="cb13-26"><a href="#cb13-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-27"><a href="#cb13-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate caption</span></span>
<span id="cb13-28"><a href="#cb13-28" aria-hidden="true" tabindex="-1"></a>outputs <span class="op">=</span> model.generate(<span class="op">**</span>inputs)</span>
<span id="cb13-29"><a href="#cb13-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-30"><a href="#cb13-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Decode and print caption</span></span>
<span id="cb13-31"><a href="#cb13-31" aria-hidden="true" tabindex="-1"></a>caption <span class="op">=</span> processor.decode(outputs[<span class="dv">0</span>], skip_special_tokens<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb13-32"><a href="#cb13-32" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Generated caption:"</span>, caption)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="10_多模态深度学习：多感官融合的开始_files/figure-html/cell-8-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Generated caption: a photo of a living room with a television and a fireplace</code></pre>
</div>
</div>
<p>此示例展示了编码器-解码器结构的代表性例子，即图像字幕生成。编码器接收图像（BLIP的视觉编码器）并提取特征向量。解码器生成文本（BLIP的文本解码器）。通过注意力机制决定图像特征向量的哪个部分需要关注，同时生成字幕。可以指定影响由文本生成的字幕的提示。BLIP既可以使用图像也可以使用文本作为输入，但这里只使用图像作为输入，并在解码器中生成文本。</p>
<p>10.3.1、10.3.2、10.3.3节中，我们查看了多模态融合的三个核心理论：联合表示、协调表示和编码器-解码器。每种方法都有其自身的特点和优缺点，因此根据应用领域选择合适的方法至关重要。</p>
<div class="callout callout-style-default callout-note callout-titled" title="点击查看内容（深度探索：多模态融合和最新研究趋势）">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
点击查看内容（深度探索：多模态融合和最新研究趋势）
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<section id="多模态融合fusion和最新研究动态" class="level2 callout-body-container callout-body">
<h2 class="anchored" data-anchor-id="多模态融合fusion和最新研究动态">多模态融合（Fusion）和最新研究动态</h2>
<p>在多模态深度学习中，“融合（Fusion）”是将不同模态的信息结合起来，创造更丰富和强大的表示的核心过程。10.3节中，我们简要地介绍了基于CMU讲座的融合理论，但实际的多模态融合研究更加多样化和动态。在这个深度探讨中，我们将深入分析融合的各种分类体系和最新研究动态，并看看2025年当前哪些技术受到关注。</p>
<section id="多模态融合的多种分类" class="level3">
<h3 class="anchored" data-anchor-id="多模态融合的多种分类">1. 多模态融合的多种分类</h3>
<p>多模态融合不能仅凭一个标准来分类。研究人员从不同的角度出发，分类融合方式，每一种分类都不是相互排斥，而是相互补充的。</p>
<section id="按照融合时点的分类earlylatehybrid-fusion" class="level4">
<h4 class="anchored" data-anchor-id="按照融合时点的分类earlylatehybrid-fusion">1.1 按照融合时点的分类（Early、Late、Hybrid Fusion）</h4>
<p>这种分类关注多模态深度学习模型中的“哪个阶段”进行融合。（参考10.3.4节）</p>
<ul>
<li><p><strong>Early Fusion（早期融合）</strong>：在模型的输入阶段，结合各个模态的“原始”数据（或非常早期处理的特征）。</p>
<ul>
<li><strong>最新研究</strong>：LUMA数据集（2024年6月发布），包含音频、图像、文本数据的50类多模态基准测试，可以验证early fusion在不确定和多样化模态数据中的有效性。该数据集扩展自CIFAR 10/100，并包含从三个音频语料库中提取的音频样本和使用Gemma-7B LLM生成的文本数据，能够以受控的方式注入各种类型和程度的不确定性。</li>
</ul></li>
<li><p><strong>Late Fusion（晚期融合）</strong>：分别处理每个模态，然后在最后阶段结合各个模型的输出（例如预测结果）。</p>
<ul>
<li><strong>最新研究</strong>：2024年1月发表的研究表明，在医疗诊断领域，late fusion（也称为模型融合）是一种将来自不同模态的决策结合起来的方法，特别是在皮肤疾病诊断中取得了显著成效。这种方法允许使用针对每个模态优化的模型，从而充分利用各个模态的优势。</li>
</ul></li>
<li><p><strong>Hybrid Fusion（混合融合）</strong>：Early Fusion和Late Fusion的结合。在模型的多个阶段进行融合，以便利用不同层次的信息。</p>
<ul>
<li><strong>最新研究</strong>：CAST（基于结构和文本的跨注意力多模态融合），2025年2月6日发表，提出了一种混合方法来有效地融合材料科学领域中的结构和文本数据。该模型通过结合节点级别和令牌级别的特征，使其能够进行密切的相互作用，并利用基于链接查询键值的交叉注意力机制来捕捉节点与文本之间的关系。</li>
</ul></li>
</ul>
</section>
<section id="按照模型结构的分类" class="level4">
<h4 class="anchored" data-anchor-id="按照模型结构的分类">1.2 按照模型结构的分类</h4>
<ul>
<li><p><strong>Model-Agnostic Fusion</strong>：不依赖于特定模型的一般融合技术（Early、Late、Hybrid Fusion等）。</p></li>
<li><p><strong>Model-Specific Fusion</strong>：针对特定模型结构优化的融合技术。</p>
<ul>
<li><strong>Transformer的Cross-Modal Attention</strong>：（详见10.4.2节）</li>
</ul></li>
<li><p><strong>最新研究：</strong> 2025年6月11日-12日に開催予定のCVPRワークショップ（MULA 2025）では、自律走行分野でさまざまなセンサデータ（カメラ、LiDAR、レーダー等）を効果的に融合するためのモデル構造に関する研究が議論される予定です。このワークショップは、コンピュータビジョン、<em>MULTIMEDIA</em>、リモートセンシング、ロボティクスコミュニティ間の学際的な相互作用と協力を促進することを目的としており、特に自律走行分野でのマルチモーダルアプローチに大きな関心が集まっています。</p></li>
</ul>
</section>
<section id="其他分类" class="level4">
<h4 class="anchored" data-anchor-id="其他分类">1.3 其他分类</h4>
<ul>
<li><p><strong>对称的（Symmetric）与不对称的（Asymmetric）融合：</strong></p>
<ul>
<li><p><strong>对称的：</strong> 所有模态都被等同地对待。</p></li>
<li><p><strong>不对称的：</strong> 特定的模态被赋予更大的权重或不同的角色。</p></li>
<li><p><strong>最新研究：</strong> “Learning Deep Multimodal Feature Representation with Asymmetric Multi-layer Fusion”提出了一种在单一网络内从多个层次融合多模态特征的有效框架。该研究引入了两个不对称融合操作：通道混洗和像素偏移，以便沿着不同的融合方向学习不同特征。此外，2025年1月发表的”Multimodal sentiment analysis based on multi-layer feature fusion”提出了一个新的方法，以实现准确的情感分析，特别是在模态不平衡和隐式表示条件下。</p></li>
</ul></li>
<li><p><strong>显式（Explicit）与隐式（Implicit）融合：</strong></p>
<ul>
<li><p><strong>显式：</strong> 模态之间的关系被明确定义或建模。（例如：注意力机制）</p></li>
<li><p><strong>隐式：</strong> 模态之间的关系没有直接定义，模型通过学习来发现这种关系。（例如：简单组合）</p></li>
<li><p><strong>最新研究：</strong> HCI International 2025会议（25年6月）将发表一项比较显式融合和隐式融合优缺点的研究。</p></li>
</ul></li>
</ul>
</section>
</section>
<section id="最新趋势基于注意力的融合与自监督学习" class="level3">
<h3 class="anchored" data-anchor-id="最新趋势基于注意力的融合与自监督学习">2. 最新趋势：基于注意力的融合与自监督学习</h3>
<p>2024-2025年的研究中，最引人注目的融合方式是<strong>基于注意力的机制</strong>。</p>
<section id="跨模态注意力" class="level4">
<h4 class="anchored" data-anchor-id="跨模态注意力">2.1 跨模态注意力</h4>
<ul>
<li><p><strong>概念：</strong> 使用一个模态的特征作为查询（query），应用于另一个模态的特征（key-value）上。 （参见10.4.2节）这样，模型可以精细地捕捉一个模态的特定元素与另一个模态的哪些元素相关。</p></li>
<li><p><strong>优点：</strong> 能够捕捉模态之间的细致和灵活的关系。例如，在图像字幕生成中，当生成“正在奔跑”的词语时，模型可以专注于图像中狗“奔跑”的动作对应的区域。</p></li>
<li><p><strong>最新研究</strong></p>
<ul>
<li><p>2025年1月发表的”Bi-Att3DDet”研究引入了双向注意力融合方式，用于自律走行中的3D物体检测。该研究提出了一种双向交互方式，以最大限度地利用LiDAR和摄像头数据之间的互补信息。</p></li>
<li><p>2024年3月发表、2025年2月修订的”LANMSFF”研究结合了轻量注意力网络和多尺度特征融合，用于多视点面部表情识别。这种方法同时生成通道和空间注意力图，以强调重要特征并抑制不相关特征。</p></li>
<li><p>最近的神经科学研究（2025年）调查了跨模态一致性对感官信息处理和积累的影响。该研究表明，听觉和视觉刺激之间的一致性在感官处理的早期阶段起着重要作用。 #### 2.2 多头注意力</p></li>
</ul></li>
<li><p><strong>概念：</strong> 使用多个注意力头来从不同角度捕捉模态间的关系。每个头使用不同的权重矩阵（W_Q, W_K, W_V）来转换输入数据并计算注意力，因此每个头可以关注输入数据的不同方面（例如，语义、语法结构、风格）。</p></li>
<li><p><strong>优点：</strong> 可以同时建模多种类型的关系，从而学习更丰富和复杂的表示。例如，在融合图像和文本时，一些头可以关注图像中的对象与文本中的单词之间的关系，而其他头可以关注图像的整体氛围与文本的语气之间的关系。</p></li>
<li><p><strong>最新研究：</strong>最近的大规模多模态模型（LMM）进一步扩展和完善了这种技术，有效地建模了图像、文本、音频、视频等不同模态之间的复杂交互。</p></li>
</ul>
</section>
<section id="自监督学习与多模态融合" class="level4">
<h4 class="anchored" data-anchor-id="自监督学习与多模态融合">2.3 自监督学习与多模态融合</h4>
<ul>
<li><p><strong>对比学习（Contrastive Learning）：</strong></p>
<ul>
<li><strong>概念：</strong> 学习使相关的模态对（例如图像及其标题）在嵌入空间中靠近，而不相关的对远离。</li>
<li><strong>优点：</strong> 即使在没有标签的大规模数据集上也能有效地学习，帮助解决数据不足的问题。</li>
<li><strong>最新研究：</strong>”Dual-Level Cross-Modal Contrastive Clustering”(2024) 提出了一种新的对比学习方法，以弥合视觉表示和文本意义之间的差距。</li>
</ul></li>
<li><p><strong>基于掩蔽的学习（Masking-based Learning）：</strong></p>
<ul>
<li><strong>概念：</strong> 掩盖输入的一部分，并使用其他模态的信息来恢复它。</li>
<li><strong>优点：</strong> 可以学习模态间的互补关系。例如，通过掩盖图像的一部分并使用文本描述来预测被掩盖的部分，或者掩盖文本中的某些单词并使用图像来预测被掩盖的单词。</li>
<li><strong>最新研究：</strong> CAST(2025) 通过 Masked Node Prediction（MNP）预训练策略提高了图结构节点和文本令牌之间的对齐。</li>
</ul></li>
</ul>
</section>
</section>
<section id="令牌级别与实例级别融合-2025-年研究" class="level3">
<h3 class="anchored" data-anchor-id="令牌级别与实例级别融合-2025-年研究">3. 令牌级别与实例级别融合 (2025 年研究)</h3>
<ul>
<li><p><strong>令牌级别融合（Token-level Fusion）：</strong> 对每个模态的个体令牌（例如图像补丁、文本令牌）进行细粒度的交互建模。</p>
<ul>
<li><strong>优点：</strong> 可以捕获更精细的模态间关系。例如，可以学习图像中特定对象与文本中特定单词之间的直接对应关系。</li>
<li><strong>最新研究：</strong> CAST(2025) 证明，在材料科学领域，图节点和文本令牌之间的令牌级别融合比实例级别融合具有更好的性能。</li>
</ul></li>
<li><p><strong>实例级别融合（Instance-level Fusion）：</strong> 将每个模态的整个实例（例如整个图像、整个文本）视为一个单元来进行融合。</p>
<ul>
<li><strong>优点：</strong> 计算效率高，实现简单。</li>
<li><strong>缺点：</strong> 可能无法捕获模态内部的细致关系。</li>
</ul></li>
</ul>
</section>
<section id="结论" class="level3">
<h3 class="anchored" data-anchor-id="结论">4. 结论</h3>
<p>多模态融合可以通过不同的方式分类，每种分类方法提供了不同的视角。实际研究中经常将这些分类方法结合使用。 2025年現在，多模態融合研究關注於令牌級別的細膩交互作用、跨注意力機制、自監督學習方法等高效融合技術的開發。特別是在CVPR 2025研討會（25年6月，納什維爾）等主要學術活動中，將會充分探討多模態融合技術在自駕車、醫療診斷、材料科學等各個應用領域的發展。</p>
<p>通過這次深入介紹，我們可以理解多模態融合的不同類別及其特點，从而更深入地分析後續介紹的多種多模態模型。</p>
</section>
</section>
</div>
</div>
<p>严格指令：</p>
<ul>
<li>不要翻译用 <code>$...$</code> 或 <code>$$...$$</code> 包围的 LaTeX 数学表达式。将它们保持原样。</li>
<li>不要翻译表格 markdown 语法。保持表格格式（管道 <code>|</code>、连字符 <code>-</code>、冒号 <code>:</code>）完全相同。仅翻译表格单元格内的文本内容。</li>
<li>不要添加任何多余的文本。这包括但不限于： *問候或结束语。
<ul>
<li>请求反馈。</li>
<li>翻译解释。</li>
<li>任何对话元素。</li>
<li>除直接翻译外的任何文本。</li>
</ul></li>
<li>只执行翻译。不要进行推理、推断或其他任务。</li>
</ul>
</section>
<section id="模态性整合策略" class="level3">
<h3 class="anchored" data-anchor-id="模态性整合策略">10.3.4 模态性整合策略</h3>
<p>从10.3.1节到10.3.3节，我们探讨了多模态数据融合的方法。这是一个理论上的分类。在设计实际的多模态模型时，需要根据给定的问题和数据特征来决定<em>采用什么样的融合方式</em>、<em>在什么时候应用</em>以及<em>如何应用</em>。本节将介绍最新的多模态模型采用的复杂模态性整合策略。</p>
<section id="早期融合early-fusion" class="level4">
<h4 class="anchored" data-anchor-id="早期融合early-fusion">10.3.4.1 早期融合（Early Fusion）</h4>
<p>早期融合是在模型的早期阶段结合多个模态的输入。最简单的形式是连接各个模态的特征向量。早期融合的优点是可以轻松地捕捉模态之间的低级相互作用。例如，如果图像的颜色和文本的特定单词有很强的关联，则可以通过早期融合来容易地学习这种关系。但是，它可能不能充分利用每个模态的特性，特别是在需要针对每个模态进行专门处理的情况下（例如：图像使用CNN，文本使用RNN），早期融合可能效率低下。</p>
<p>最近的研究中，不仅提出简单连接，还提出了验证早期融合在具有不确定性的多模态数据环境中的有效性基准测试。</p>
<p>让我们看一个早期融合的简单例子。Joint Representation中使用concatenation进行早期融合。相同的代码被使用。在最后，使用一个简单的线性分类器来判断是否有猫。</p>
<div id="cell-19" class="cell" data-execution_count="11">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoModel, AutoProcessor, AutoTokenizer</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> Image</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> requests</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a><span class="co"># 이미지와 텍스트를 위한 사전 학습된 모델 및 프로세서/토크나이저 로드</span></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>image_model_name <span class="op">=</span> <span class="st">"google/vit-base-patch16-224-in21k"</span>  <span class="co">#  ViT (Vision Transformer)</span></span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>text_model_name <span class="op">=</span> <span class="st">"bert-base-uncased"</span>  <span class="co"># BERT</span></span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>image_processor <span class="op">=</span> AutoProcessor.from_pretrained(image_model_name)</span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a>image_model <span class="op">=</span> AutoModel.from_pretrained(image_model_name)</span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(text_model_name)</span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a>text_model <span class="op">=</span> AutoModel.from_pretrained(text_model_name)</span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a><span class="co"># 예제 이미지 및 텍스트</span></span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a>url <span class="op">=</span> <span class="st">"http://images.cocodataset.org/val2017/000000039769.jpg"</span></span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a>image <span class="op">=</span> Image.<span class="bu">open</span>(requests.get(url, stream<span class="op">=</span><span class="va">True</span>).raw)</span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a>text <span class="op">=</span> <span class="st">"Two cats sleeping on a couch."</span></span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-22"><a href="#cb15-22" aria-hidden="true" tabindex="-1"></a><span class="co"># 이미지 출력</span></span>
<span id="cb15-23"><a href="#cb15-23" aria-hidden="true" tabindex="-1"></a>plt.imshow(image)</span>
<span id="cb15-24"><a href="#cb15-24" aria-hidden="true" tabindex="-1"></a>plt.axis(<span class="st">'off'</span>)  <span class="co"># 축 제거</span></span>
<span id="cb15-25"><a href="#cb15-25" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb15-26"><a href="#cb15-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-27"><a href="#cb15-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-28"><a href="#cb15-28" aria-hidden="true" tabindex="-1"></a><span class="co"># 이미지와 텍스트 전처리</span></span>
<span id="cb15-29"><a href="#cb15-29" aria-hidden="true" tabindex="-1"></a>image_inputs <span class="op">=</span> image_processor(images<span class="op">=</span>image, return_tensors<span class="op">=</span><span class="st">"pt"</span>)</span>
<span id="cb15-30"><a href="#cb15-30" aria-hidden="true" tabindex="-1"></a>text_inputs <span class="op">=</span> tokenizer(text, return_tensors<span class="op">=</span><span class="st">"pt"</span>)</span>
<span id="cb15-31"><a href="#cb15-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-32"><a href="#cb15-32" aria-hidden="true" tabindex="-1"></a><span class="co"># 각 모달리티에 대한 특징 추출 (임베딩)</span></span>
<span id="cb15-33"><a href="#cb15-33" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():  <span class="co"># 기울기 계산 비활성화 (추론 모드)</span></span>
<span id="cb15-34"><a href="#cb15-34" aria-hidden="true" tabindex="-1"></a>    image_features <span class="op">=</span> image_model(<span class="op">**</span>image_inputs).last_hidden_state[:, <span class="dv">0</span>, :]  <span class="co"># [CLS] 토큰 임베딩</span></span>
<span id="cb15-35"><a href="#cb15-35" aria-hidden="true" tabindex="-1"></a>    text_features <span class="op">=</span> text_model(<span class="op">**</span>text_inputs).last_hidden_state[:, <span class="dv">0</span>, :]   <span class="co"># [CLS] 토큰 임베딩</span></span>
<span id="cb15-36"><a href="#cb15-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-37"><a href="#cb15-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-38"><a href="#cb15-38" aria-hidden="true" tabindex="-1"></a><span class="co"># Joint Representation 생성 (Concatenation)</span></span>
<span id="cb15-39"><a href="#cb15-39" aria-hidden="true" tabindex="-1"></a>joint_representation <span class="op">=</span> torch.cat((image_features, text_features), dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb15-40"><a href="#cb15-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-41"><a href="#cb15-41" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Image Features Shape:"</span>, image_features.shape)  <span class="co"># 이미지 특징 벡터 크기</span></span>
<span id="cb15-42"><a href="#cb15-42" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Text Features Shape:"</span>, text_features.shape)     <span class="co"># 텍스트 특징 벡터 크기</span></span>
<span id="cb15-43"><a href="#cb15-43" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Joint Representation Shape:"</span>, joint_representation.shape) <span class="co"># 결합된 특징 벡터 크기 (image + text)</span></span>
<span id="cb15-44"><a href="#cb15-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-45"><a href="#cb15-45" aria-hidden="true" tabindex="-1"></a><span class="co">#  Joint Representation을 활용한 추가 작업 (예: 분류)</span></span>
<span id="cb15-46"><a href="#cb15-46" aria-hidden="true" tabindex="-1"></a>num_labels <span class="op">=</span> <span class="dv">2</span>  <span class="co">#  예: "고양이 없음(0)" "고양이 있음(1)", 두 가지 클래스로 분류</span></span>
<span id="cb15-47"><a href="#cb15-47" aria-hidden="true" tabindex="-1"></a>classifier <span class="op">=</span> torch.nn.Linear(joint_representation.size(<span class="dv">1</span>), num_labels) <span class="co"># 간단한 선형 분류기</span></span>
<span id="cb15-48"><a href="#cb15-48" aria-hidden="true" tabindex="-1"></a>outputs <span class="op">=</span> classifier(joint_representation)</span>
<span id="cb15-49"><a href="#cb15-49" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Classification Outputs:"</span>, outputs)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stderr">
<pre><code>Fast image processor class &lt;class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'&gt; is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="10_多模态深度学习：多感官融合的开始_files/figure-html/cell-9-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Image Features Shape: torch.Size([1, 768])
Text Features Shape: torch.Size([1, 768])
Joint Representation Shape: torch.Size([1, 1536])
Classification Outputs: tensor([[0.1817, 0.0355]], grad_fn=&lt;AddmmBackward0&gt;)</code></pre>
</div>
</div>
<p>在上面的例子中，图像和文本分别是 ViT 和 BERT 这两个单独模型的输出直接结合，没有对这两个向量进行额外的处理（如注意力机制、复杂变换）来结合图像特征和文本特征。因此，这属于早期融合。</p>
</section>
<section id="晚期融合late-fusion" class="level4">
<h4 class="anchored" data-anchor-id="晚期融合late-fusion">10.3.4.2 晚期融合（Late Fusion）</h4>
<p>晚期融合是指分别使用单独的模型处理每个模态，并在最后阶段将每个模型的输出（例如预测结果）结合起来。这种方法的优点是可以利用针对每个模态特定的模型。例如，可以使用预训练的 CNN 处理图像，使用预训练的 Transformer 处理文本，从而有效地提取每个模态的复杂特征。然而，其缺点是只考虑了模态之间的高层交互，而中间阶段的信息交换较难。</p>
<p>晚期融合与集成（ensemble）方法类似，研究表明通过结合不同模态模型的输出可以提高性能。</p>
</section>
<section id="混合融合hybrid-fusion" class="level4">
<h4 class="anchored" data-anchor-id="混合融合hybrid-fusion">10.3.4.3 混合融合（Hybrid Fusion）</h4>
<p>混合融合是指同时使用早期融合和晚期融合的方法。在模型的多个阶段进行融合，以利用多种层次的信息。这种方法的优点是可以同时利用早期融合和晚期融合的优势，即，可以考虑模态之间的低层交互和高层交互。然而，其缺点是模型结构变得更加复杂，需要调节的超参数也更多。</p>
<p>混合融合的一个著名例子是跨模态注意力（Cross-Modal Attention）。这种方法使用一个模态的特征作为查询（query），应用于另一个模态的特征（key-value）上。它是在中间阶段进行融合的典型方法。</p>
<p>最近的研究还尝试了除注意力机制以外的其他中间阶段融合方法，例如门控机制（gated mechanism）和双线性池化（bilinear pooling）。</p>
</section>
<section id="最新模型的精细集成策略2023年以后" class="level4">
<h4 class="anchored" data-anchor-id="最新模型的精细集成策略2023年以后">10.3.4.4 最新模型的精细集成策略（2023年以后）</h4>
<p>2023年以后，大规模多模态模型（LMM），如Gemini和GPT-4V，引入了更加精细的模态集成策略，从而大幅提高了性能。</p>
<p><strong>选择性融合机制（Selective Fusion Mechanism）</strong> 动态判断每个模态的重要性，并有选择地整合信息。例如，当图像中包含文本时，更强烈地关联文本区域的视觉特征和文本内容。这与人类根据情况调整视觉信息和文本信息重要性的方式类似。</p>
<p><strong>动态加权（Dynamic Weighting）</strong> 根据任务和输入的特性自动调节每个模态的贡献度。例如，在视觉问答（VQA）任务中，根据问题的性质不同地为图像和文本信息分配不同的权重。“图片的颜色是什么？”这个问题更多地依赖于视觉信息，而“这张图片代表什么意思？”这个问题更多地依赖于文本信息。</p>
<p><strong>任务特定融合方式（Task-Specific Fusion）</strong> 根据特定任务的要求优化模态集成方法。例如，在图像标题生成中，重点是从视觉信息到文本的单向转换，而在视觉问答中，则强调双向信息交互。</p>
<p>这些精细的集成策略大幅提高了多模态模型的性能。尤其是，通过超越简单的信息结合，动态调整每个模态的角色和重要性，并根据任务特性优化融合方式，在需要复杂推理的任务中表现出色。 这些整合策略需要大量的数据集和计算资源，因此通过学习示例直接实现和实验是很困难的。相反，通过每个模型的论文和技术文档来获得概念性的理解是更为理想的。</p>
</section>
</section>
</section>
<section id="多模态表达学习技术" class="level2">
<h2 class="anchored" data-anchor-id="多模态表达学习技术">10.4 多模态表达学习技术</h2>
<p>10.3节中，我们探讨了各种理论方法和策略来融合多模态数据。基于此，本节将具体介绍实际的多模态模型如何有效地表示每个模态的信息，并学习不同模态之间的关系。完整的实现可以在<code>chapter_10/multimodal_embeding.py</code>中找到。</p>
<section id="模态间表达学习" class="level3">
<h3 class="anchored" data-anchor-id="模态间表达学习">10.4.1 模态间表达学习</h3>
<p>多模态学习的一个核心任务是将具有不同特性的模态表示到一个有意义的<em>共同空间</em>中。图像是一维数组，文本是一维序列，音频是时间上的幅度值等，每个模态都有其独特的表示方式。为了有效地处理这些异构数据，我们需要一种可以捕捉每个模态的基本特征并使它们之间具有语义关系的表达学习技术。</p>
<p><strong>早期方法：个别编码器 + 投影</strong></p>
<p>早期的多模态模型使用了针对每个模态的专用编码器（例如，图像使用CNN，文本使用RNN）来提取特征向量，然后通过线性变换或浅层MLP将它们投影到共同维度的向量空间中。（参考10.3.1节中的Joint Representation和Concatenation方法）</p>
<p><strong>最近方法：语义对齐</strong></p>
<p>最近，研究重点转向了使每个模态的特征向量在语义上“对齐”的学习方法。也就是说，相关的图像和文本应该在嵌入空间中彼此接近，而不相关的图像和文本应该相距较远。</p>
<ul>
<li><p><strong>对比学习</strong>：（参考10.3.2节中的Coordinated Representation和CLIP示例）将图像-文本对视为“正样本”，随机混合的图像-文本对视为“负样本”，通过提高正样本之间的相似度和降低负样本之间的相似度来进行学习。</p></li>
<li><p><strong>三元组损失</strong>：使用图像锚点、正文本（与图像相关的标题）和负文本（其他图像的标题）的三个元素，通过使锚点图像和正文本之间的距离更近，锚点图像和负文本之间的距离更远来进行学习。</p></li>
</ul>
<p><strong>实现示例（对比学习）</strong></p>
<div id="cell-22" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MultimodalEmbedding(nn.Module):</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, embedding_dim<span class="op">=</span><span class="dv">512</span>):</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.image_encoder <span class="op">=</span> models.resnet18(pretrained<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.image_encoder.fc <span class="op">=</span> nn.Sequential(</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">512</span>, embedding_dim),</span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>            nn.LayerNorm(embedding_dim)</span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.text_encoder <span class="op">=</span> BertModel.from_pretrained(<span class="st">'bert-base-uncased'</span>)</span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.text_projection <span class="op">=</span> nn.Sequential(</span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">768</span>, embedding_dim),  <span class="co"># BERT output dimension is 768</span></span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a>            nn.LayerNorm(embedding_dim)</span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-17"><a href="#cb18-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.logit_scale <span class="op">=</span> nn.Parameter(torch.ones([]) <span class="op">*</span> np.log(<span class="dv">1</span> <span class="op">/</span> <span class="fl">0.07</span>))</span>
<span id="cb18-18"><a href="#cb18-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-19"><a href="#cb18-19" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> encode_image(<span class="va">self</span>, image):</span>
<span id="cb18-20"><a href="#cb18-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.image_encoder(image)</span>
<span id="cb18-21"><a href="#cb18-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-22"><a href="#cb18-22" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> encode_text(<span class="va">self</span>, input_ids, attention_mask):</span>
<span id="cb18-23"><a href="#cb18-23" aria-hidden="true" tabindex="-1"></a>        text_features <span class="op">=</span> <span class="va">self</span>.text_encoder(input_ids, attention_mask)[<span class="dv">0</span>][:, <span class="dv">0</span>, :]  <span class="co"># [CLS] token, keep batch dim</span></span>
<span id="cb18-24"><a href="#cb18-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.text_projection(text_features)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<ul>
<li><strong><code>MultimodalEmbedding</code> 类：</strong>
<ul>
<li><code>image_encoder</code>：使用 ResNet18 将图像转换为大小为 <code>embedding_dim</code> 的特征向量。</li>
<li><code>text_encoder</code>：使用 BERT 模型将文本转换为特征向量，并通过 <code>text_projection</code> 层调整到大小为 <code>embedding_dim</code>。</li>
<li><code>logit_scale</code>：CLIP 中使用的可学习温度参数。</li>
</ul></li>
</ul>
<p><strong>语义对齐机制</strong></p>
<p>语义对齐主要在两个部分实现：MultimodalEmbedding 类的 forward 方法和 constrasive_loss()。</p>
<div id="cell-24" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> forward(<span class="va">self</span>, image, input_ids, attention_mask):</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>    image_features <span class="op">=</span> <span class="va">self</span>.encode_image(image)</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>    text_features <span class="op">=</span> <span class="va">self</span>.encode_text(input_ids, attention_mask)</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>    image_features <span class="op">=</span> image_features <span class="op">/</span> image_features.norm(dim<span class="op">=-</span><span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>    text_features <span class="op">=</span> text_features <span class="op">/</span> text_features.norm(dim<span class="op">=-</span><span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>    logit_scale <span class="op">=</span> <span class="va">self</span>.logit_scale.exp()</span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>    logits <span class="op">=</span> logit_scale <span class="op">*</span> image_features <span class="op">@</span> text_features.transpose(<span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">2</span>)</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># print("logits:", logits.shape)</span></span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> logits   <span class="co"># Return a single value</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<ul>
<li><strong><code>forward</code> 方法：</strong>
<ol type="1">
<li><p>使用 <code>encode_image</code> 和 <code>encode_text</code> 分别对图像和文本进行编码。</p></li>
<li><p><strong>特征归一化（Feature Normalization）</strong>：使用 L2 归一化（L2 normalization）将 <code>image_features</code> 和 <code>text_features</code> 向量的大小设置为 1。这是为了仅考虑向量方向计算相似度。</p></li>
<li><p><strong>温度缩放（Temperature Scaling）</strong>：使用 <code>logit_scale</code> 调整相似度得分分布。对 logit_scale 应用指数函数以获得缩放值，然后将其乘以图像特征矩阵和转置的文本特征矩阵的矩阵乘积。矩阵乘积通过计算每个图像特征向量与所有文本特征向量之间的点积来生成相似度得分。</p></li>
<li><p><code>logits</code>：计算图像特征向量和文本特征向量之间的相似度（点积）。使用 <code>text_features.transpose(-1, -2)</code> 代替 <code>text_features.t()</code> 进行转置。交换文本特征矩阵的最后两个维度（批次、文本特征维度）以形成（批次、特征维度、文本）形式，使其可以与（批次、图像特征维度）形式的图像特征矩阵相乘。</p></li>
</ol></li>
</ul>
<div id="cell-26" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> contrastive_loss(logits): <span class="co"># removed enhanced_similarity</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>    labels <span class="op">=</span> torch.arange(logits.size(<span class="dv">0</span>), device<span class="op">=</span>logits.device) <span class="co"># Use logits.size(0)</span></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Image-to-text and text-to-image contrastive loss</span></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>    img_txt_loss <span class="op">=</span> nn.CrossEntropyLoss()(logits, labels)</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>    txt_img_loss <span class="op">=</span> nn.CrossEntropyLoss()(logits.T, labels)</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Average loss</span></span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (img_txt_loss <span class="op">+</span> txt_img_loss) <span class="op">/</span> <span class="dv">2</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><code>contrastive_loss</code> 函数会根据 <code>logits</code> 矩阵的大小生成从 0 到（批量大小 - 1）的整数标签。<code>logits</code> 矩阵中的对角线元素（i，i）表示第 i 个图像和第 i 个文本之间的相似度，即图像和文本形成正向配对的相似度，因此会将这些对角线元素设置为正确答案。此外，<code>img_txt_loss</code> 计算从图像到文本的相似度损失（image-to-text loss），而 <code>txt_img_loss</code> 计算从文本到图像的相似度损失（text-to-image loss）。通过平均这些损失，可以同时考虑双向（image-to-text 和 text-to-image）语义对齐。</p>
<p>语义对齐机制将不同模态的特征映射到语义一致的空间中。首先，通过 L2 正则化将所有特征向量投影到单位球面上，以消除模态之间的尺度差异。引入温度缩放参数来调整相似度值的分布。高温会产生更平滑的分布，而低温会产生更尖锐的分布，从而提高训练的稳定性。此外，通过对比学习，使相关图像-文本配对在嵌入空间中靠近，而无关配对则远离。特别地，图像到文本和文本到图像的映射同时被优化，以实现双向语义对齐。</p>
<p>CLIP 的对比学习类似，相关内容会被学习为相近，而不相关内容会被学习为相远。这种基于对比学习的语义对齐策略从 2021 年 OpenAI 的 CLIP 开始发展，包括 Google 的 PaLM-E、Anthropic 的 Claude 和 DeepMind 的 Gemini 等模型。早期的 CLIP 重点关注图像-文本配对的简单对比学习，而最新的模型则更细致地捕捉多个模态之间的相互关系。特别是 Gemini，会同时学习图像、文本、音频和视频等多种模态之间的语义对齐，同时保留每个模态的独特特征并构建一个集成的语义空间。</p>
<p><strong>示例运行</strong></p>
<p>训练使用的数据集为 Flicker8k。可以使用 <code>train_multimodal_embedding</code> 函数来训练 <code>EnhancedMultimodalEmbedding</code>（或 <code>EnhancedMultimodalEmbedding_no_p</code>）模型在 Flicker8k 数据集上。在 <code>main</code> 函数中设置模型、数据加载器、优化器等，然后调用 <code>train_multimodal_embedding</code> 函数即可开始训练。</p>
<div id="cell-28" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="co"># download flickr8k.</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>mkdir data<span class="op">;</span>cd data<span class="op">;</span>wget <span class="st">"https://github.com/awsaf49/flickr-dataset/releases/download/v1.0/flickr8k.zip"</span><span class="op">;</span>unzip <span class="op">-</span>q flickr8k.<span class="bu">zip</span> <span class="op">-</span>d .<span class="op">/</span>flickr8k</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>mkdir: cannot create directory ‘data’: File exists
--2025-03-09 16:33:12--  https://github.com/awsaf49/flickr-dataset/releases/download/v1.0/flickr8k.zip
Resolving github.com (github.com)... 20.200.245.247
Connecting to github.com (github.com)|20.200.245.247|:443... connected.
HTTP request sent, awaiting response... 302 Found
Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/753516996/d7c62b13-1e50-40ea-8fae-f34a44b1695f?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=releaseassetproduction%2F20250309%2Fus-east-1%2Fs3%2Faws4_request&amp;X-Amz-Date=20250309T073156Z&amp;X-Amz-Expires=300&amp;X-Amz-Signature=ff62cf7df8ac3deba8bd6f4f775e164abf03c6d2d6d86d740e5407e52702c6a3&amp;X-Amz-SignedHeaders=host&amp;response-content-disposition=attachment%3B%20filename%3Dflickr8k.zip&amp;response-content-type=application%2Foctet-stream [following]
--2025-03-09 16:33:12--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/753516996/d7c62b13-1e50-40ea-8fae-f34a44b1695f?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=releaseassetproduction%2F20250309%2Fus-east-1%2Fs3%2Faws4_request&amp;X-Amz-Date=20250309T073156Z&amp;X-Amz-Expires=300&amp;X-Amz-Signature=ff62cf7df8ac3deba8bd6f4f775e164abf03c6d2d6d86d740e5407e52702c6a3&amp;X-Amz-SignedHeaders=host&amp;response-content-disposition=attachment%3B%20filename%3Dflickr8k.zip&amp;response-content-type=application%2Foctet-stream
Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.110.133, ...
Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.109.133|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 1112971163 (1.0G) [application/octet-stream]
Saving to: ‘flickr8k.zip’

flickr8k.zip        100%[===================&gt;]   1.04G  56.8MB/s    in 19s     

2025-03-09 16:33:32 (56.9 MB/s) - ‘flickr8k.zip’ saved [1112971163/1112971163]
</code></pre>
</div>
</div>
<div id="cell-29" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision <span class="im">import</span> models, transforms</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> Dataset, DataLoader</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Assuming dldna.chapter_10.multimodal_embedding is in the same directory or Python path.</span></span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Adjust if necessary (e.g., from multimodal_embedding import ...).</span></span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_10.multimodal_embedding <span class="im">import</span> Flickr8kDataset, MultimodalEmbedding, train_multimodal_embedding, generate_example</span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Data transformation setup</span></span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a>transform <span class="op">=</span> transforms.Compose([</span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a>    transforms.Resize((<span class="dv">224</span>, <span class="dv">224</span>)),</span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a>    transforms.ToTensor(),</span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a>    transforms.Normalize(mean<span class="op">=</span>[<span class="fl">0.485</span>, <span class="fl">0.456</span>, <span class="fl">0.406</span>], std<span class="op">=</span>[<span class="fl">0.229</span>, <span class="fl">0.224</span>, <span class="fl">0.225</span>])</span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-15"><a href="#cb23-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Dataset and DataLoader setup</span></span>
<span id="cb23-16"><a href="#cb23-16" aria-hidden="true" tabindex="-1"></a>image_dir <span class="op">=</span> <span class="st">'./data/flickr8k/Images'</span>  <span class="co"># Replace with the actual path to your image directory</span></span>
<span id="cb23-17"><a href="#cb23-17" aria-hidden="true" tabindex="-1"></a>caption_file <span class="op">=</span> <span class="st">'./data/flickr8k/captions.txt'</span>  <span class="co"># Replace with the actual path to your caption file</span></span>
<span id="cb23-18"><a href="#cb23-18" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> Flickr8kDataset(image_dir, caption_file, transform<span class="op">=</span>transform)</span>
<span id="cb23-19"><a href="#cb23-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-20"><a href="#cb23-20" aria-hidden="true" tabindex="-1"></a>train_size <span class="op">=</span> <span class="bu">int</span>(<span class="fl">0.8</span> <span class="op">*</span> <span class="bu">len</span>(dataset))</span>
<span id="cb23-21"><a href="#cb23-21" aria-hidden="true" tabindex="-1"></a>val_size <span class="op">=</span> <span class="bu">len</span>(dataset) <span class="op">-</span> train_size</span>
<span id="cb23-22"><a href="#cb23-22" aria-hidden="true" tabindex="-1"></a>train_dataset, val_dataset <span class="op">=</span> torch.utils.data.random_split(dataset, [train_size, val_size])</span>
<span id="cb23-23"><a href="#cb23-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-24"><a href="#cb23-24" aria-hidden="true" tabindex="-1"></a>train_loader <span class="op">=</span> DataLoader(train_dataset, batch_size<span class="op">=</span><span class="dv">32</span>, shuffle<span class="op">=</span><span class="va">True</span>, num_workers<span class="op">=</span><span class="dv">4</span>)</span>
<span id="cb23-25"><a href="#cb23-25" aria-hidden="true" tabindex="-1"></a>val_loader <span class="op">=</span> DataLoader(val_dataset, batch_size<span class="op">=</span><span class="dv">32</span>, shuffle<span class="op">=</span><span class="va">False</span>, num_workers<span class="op">=</span><span class="dv">4</span>)</span>
<span id="cb23-26"><a href="#cb23-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-27"><a href="#cb23-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Model initialization</span></span>
<span id="cb23-28"><a href="#cb23-28" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> MultimodalEmbedding()</span>
<span id="cb23-29"><a href="#cb23-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-30"><a href="#cb23-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Model training</span></span>
<span id="cb23-31"><a href="#cb23-31" aria-hidden="true" tabindex="-1"></a>train_multimodal_embedding(model, train_loader, val_loader, num_epochs<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb23-32"><a href="#cb23-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-33"><a href="#cb23-33" aria-hidden="true" tabindex="-1"></a><span class="co"># Model saving</span></span>
<span id="cb23-34"><a href="#cb23-34" aria-hidden="true" tabindex="-1"></a>torch.save(model.state_dict(), <span class="st">'multimodal_embedding_model.pth'</span>)</span>
<span id="cb23-35"><a href="#cb23-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-36"><a href="#cb23-36" aria-hidden="true" tabindex="-1"></a><span class="co"># Example generation</span></span>
<span id="cb23-37"><a href="#cb23-37" aria-hidden="true" tabindex="-1"></a>model_path <span class="op">=</span> <span class="st">'multimodal_embedding_model.pth'</span></span>
<span id="cb23-38"><a href="#cb23-38" aria-hidden="true" tabindex="-1"></a>generate_example(model_path, image_dir, caption_file)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 1/3:  15%|█▍        | 147/1012 [00:16&lt;01:36,  8.96it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Image file not found: ./data/flickr8k/Images/image</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 1/3: 100%|██████████| 1012/1012 [01:53&lt;00:00,  8.90it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/3 - Train Loss: 0.9618</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code></code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/3 - Validation Loss: 0.5212
Epoch 1: Saved best model with Validation Loss = 0.5212</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 2/3:  52%|█████▏    | 525/1012 [00:59&lt;00:55,  8.84it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Image file not found: ./data/flickr8k/Images/image</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 2/3: 100%|██████████| 1012/1012 [01:54&lt;00:00,  8.83it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 2/3 - Train Loss: 0.3393</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code></code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 2/3 - Validation Loss: 0.4240
Epoch 2: Saved best model with Validation Loss = 0.4240</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 3/3:  34%|███▍      | 347/1012 [00:39&lt;01:15,  8.85it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Image file not found: ./data/flickr8k/Images/image</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 3/3: 100%|██████████| 1012/1012 [01:54&lt;00:00,  8.83it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 3/3 - Train Loss: 0.2313</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code></code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 3/3 - Validation Loss: 0.3891
Epoch 3: Saved best model with Validation Loss = 0.3891
Image 0:</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="10_多模态深度学习：多感官融合的开始_files/figure-html/cell-14-output-19.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Top 3 Captions (Image -&gt; Text):
  - football players in red congratulate each other as crowds in red cheer behind. (prob: 0.9970)
  - a man in black holds up an obama 08 sign. (prob: 0.0023)
  - a large group of bicycles racing on the street (prob: 0.0004)

Caption: football players in red congratulate each other as crowds in red cheer behind.

Top 3 Images (Text -&gt; Image):
 - Image 0 (prob: 0.9983)</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="10_多模态深度学习：多感官融合的开始_files/figure-html/cell-14-output-21.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> - Image 17 (prob: 0.0013)</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="10_多模态深度学习：多感官融合的开始_files/figure-html/cell-14-output-23.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> - Image 2 (prob: 0.0001)</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="10_多模态深度学习：多感官融合的开始_files/figure-html/cell-14-output-25.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="跨模态注意力结构" class="level3">
<h3 class="anchored" data-anchor-id="跨模态注意力结构">10.4.2 跨模态注意力结构</h3>
<p>跨模态注意力用于有效地建模不同模态之间的关系。它扩展了ViT的自注意力机制，使得图像和文本等异质数据之间可以相互作用。</p>
<p><strong>模态间注意力设计</strong></p>
<p>跨模态注意力具有考虑到各个模态特性的非对称结构。</p>
<div id="cell-31" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> CrossModalAttention(nn.Module):</span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config):</span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.image_proj <span class="op">=</span> nn.Linear(config.image_dim, config.hidden_dim)</span>
<span id="cb45-5"><a href="#cb45-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.text_proj <span class="op">=</span> nn.Linear(config.text_dim, config.hidden_dim)</span>
<span id="cb45-6"><a href="#cb45-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.attention <span class="op">=</span> nn.MultiheadAttention(config.hidden_dim, config.num_heads)</span>
<span id="cb45-7"><a href="#cb45-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb45-8"><a href="#cb45-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, image_features, text_features):</span>
<span id="cb45-9"><a href="#cb45-9" aria-hidden="true" tabindex="-1"></a>        image_proj <span class="op">=</span> <span class="va">self</span>.image_proj(image_features)</span>
<span id="cb45-10"><a href="#cb45-10" aria-hidden="true" tabindex="-1"></a>        text_proj <span class="op">=</span> <span class="va">self</span>.text_proj(text_features)</span>
<span id="cb45-11"><a href="#cb45-11" aria-hidden="true" tabindex="-1"></a>        attn_output, _ <span class="op">=</span> <span class="va">self</span>.attention(text_proj, image_proj, image_proj)</span>
<span id="cb45-12"><a href="#cb45-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> attn_output</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>图像和文本特征被投影到共同的潜在空间中，然后通过多头注意力机制学习两个模态之间的关系。文本特征作为查询，图像特征作为键和值，以使文本关注图像的相关部分。</p>
<p><strong>非对称注意力模式</strong></p>
<p>使用非对称注意力模式来保持每个模态的独特性质，同时实现有效的信息交换。</p>
<div id="cell-33" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> HierarchicalCrossModalAttention(nn.Module):</span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config):</span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.local_image_attention <span class="op">=</span> nn.MultiheadAttention(config.hidden_dim, config.num_heads)</span>
<span id="cb46-5"><a href="#cb46-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.local_text_attention <span class="op">=</span> nn.MultiheadAttention(config.hidden_dim, config.num_heads)</span>
<span id="cb46-6"><a href="#cb46-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.image_to_text_attention <span class="op">=</span> CrossModalAttention(config)</span>
<span id="cb46-7"><a href="#cb46-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.text_to_image_attention <span class="op">=</span> CrossModalAttention(config)</span>
<span id="cb46-8"><a href="#cb46-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.output_layer <span class="op">=</span> nn.Linear(config.hidden_dim <span class="op">*</span> <span class="dv">2</span>, config.hidden_dim)</span>
<span id="cb46-9"><a href="#cb46-9" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb46-10"><a href="#cb46-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, image_features, text_features):</span>
<span id="cb46-11"><a href="#cb46-11" aria-hidden="true" tabindex="-1"></a>        local_image <span class="op">=</span> <span class="va">self</span>.local_image_attention(image_features, image_features, image_features)[<span class="dv">0</span>]</span>
<span id="cb46-12"><a href="#cb46-12" aria-hidden="true" tabindex="-1"></a>        local_text <span class="op">=</span> <span class="va">self</span>.local_text_attention(text_features, text_features, text_features)[<span class="dv">0</span>]</span>
<span id="cb46-13"><a href="#cb46-13" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb46-14"><a href="#cb46-14" aria-hidden="true" tabindex="-1"></a>        image_attended_text <span class="op">=</span> <span class="va">self</span>.image_to_text_attention(image_features, local_text)</span>
<span id="cb46-15"><a href="#cb46-15" aria-hidden="true" tabindex="-1"></a>        text_attended_image <span class="op">=</span> <span class="va">self</span>.text_to_image_attention(text_features, local_image)</span>
<span id="cb46-16"><a href="#cb46-16" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb46-17"><a href="#cb46-17" aria-hidden="true" tabindex="-1"></a>        combined_features <span class="op">=</span> torch.cat([image_attended_text, text_attended_image], dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb46-18"><a href="#cb46-18" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> <span class="va">self</span>.output_layer(combined_features)</span>
<span id="cb46-19"><a href="#cb46-19" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb46-20"><a href="#cb46-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> output</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>这里分别进行图像到文本、文本到图像的双向注意力机制。通过这样，可以使得每个模态都能选择性地关注相应模态的相关信息。</p>
<p><strong>层次注意力结构</strong></p>
<p>为了捕捉复杂的多模态关系，采用了层次化的注意力机制结构。底层处理每个模态内部的局部特征，而高层则建模模态之间的全局关系。这种层次结构在GPT-4V和Gemini等模型中发挥着核心作用。</p>
<div id="cell-35" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> EnhancedMultimodalEmbedding_no_p(MultimodalEmbedding):</span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, image, input_ids, attention_mask):</span>
<span id="cb47-4"><a href="#cb47-4" aria-hidden="true" tabindex="-1"></a>        image_features <span class="op">=</span> <span class="va">self</span>.encode_image(image)</span>
<span id="cb47-5"><a href="#cb47-5" aria-hidden="true" tabindex="-1"></a>        text_features <span class="op">=</span> <span class="va">self</span>.encode_text(input_ids, attention_mask)</span>
<span id="cb47-6"><a href="#cb47-6" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb47-7"><a href="#cb47-7" aria-hidden="true" tabindex="-1"></a>        image_features <span class="op">=</span> <span class="va">self</span>.image_preserve(image_features)</span>
<span id="cb47-8"><a href="#cb47-8" aria-hidden="true" tabindex="-1"></a>        text_features <span class="op">=</span> <span class="va">self</span>.text_preserve(text_features)</span>
<span id="cb47-9"><a href="#cb47-9" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb47-10"><a href="#cb47-10" aria-hidden="true" tabindex="-1"></a>        combined_features <span class="op">=</span> <span class="va">self</span>.cross_modal_attention(image_features, text_features)</span>
<span id="cb47-11"><a href="#cb47-11" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb47-12"><a href="#cb47-12" aria-hidden="true" tabindex="-1"></a>        combined_features <span class="op">=</span> combined_features <span class="op">/</span> combined_features.norm(dim<span class="op">=-</span><span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb47-13"><a href="#cb47-13" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb47-14"><a href="#cb47-14" aria-hidden="true" tabindex="-1"></a>        logit_scale <span class="op">=</span> <span class="va">self</span>.logit_scale.exp()</span>
<span id="cb47-15"><a href="#cb47-15" aria-hidden="true" tabindex="-1"></a>        logits <span class="op">=</span> logit_scale <span class="op">*</span> combined_features <span class="op">@</span> combined_features.t()</span>
<span id="cb47-16"><a href="#cb47-16" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb47-17"><a href="#cb47-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> logits</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="cell-36" class="cell" data-execution_count="2">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision <span class="im">import</span> models, transforms</span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> Dataset, DataLoader</span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-5"><a href="#cb48-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> namedtuple</span>
<span id="cb48-6"><a href="#cb48-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-7"><a href="#cb48-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_10.crossmodal_attention <span class="im">import</span> Flickr8kDataset, CrossModalEmbedding, train_crossmodal_embedding, generate_example</span>
<span id="cb48-8"><a href="#cb48-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-9"><a href="#cb48-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Configuration</span></span>
<span id="cb48-10"><a href="#cb48-10" aria-hidden="true" tabindex="-1"></a>config <span class="op">=</span> namedtuple(<span class="st">'Config'</span>, [<span class="st">'embedding_dim'</span>, <span class="st">'image_dim'</span>, <span class="st">'text_dim'</span>, <span class="st">'hidden_dim'</span>, <span class="st">'num_heads'</span>])(</span>
<span id="cb48-11"><a href="#cb48-11" aria-hidden="true" tabindex="-1"></a>                    embedding_dim<span class="op">=</span><span class="dv">512</span>, <span class="co"># Output embedding dimension</span></span>
<span id="cb48-12"><a href="#cb48-12" aria-hidden="true" tabindex="-1"></a>                    image_dim<span class="op">=</span><span class="dv">512</span>, <span class="co"># ResNet18 image encoder output dimension</span></span>
<span id="cb48-13"><a href="#cb48-13" aria-hidden="true" tabindex="-1"></a>                    text_dim<span class="op">=</span><span class="dv">512</span>, <span class="co"># Text feature (768 from BERT -&gt; 512 after projection)</span></span>
<span id="cb48-14"><a href="#cb48-14" aria-hidden="true" tabindex="-1"></a>                    hidden_dim<span class="op">=</span><span class="dv">512</span>, <span class="co"># Cross-modal attention internal hidden dimension</span></span>
<span id="cb48-15"><a href="#cb48-15" aria-hidden="true" tabindex="-1"></a>                    num_heads<span class="op">=</span><span class="dv">8</span> <span class="co"># Number of multi-head attention heads</span></span>
<span id="cb48-16"><a href="#cb48-16" aria-hidden="true" tabindex="-1"></a>                    )</span>
<span id="cb48-17"><a href="#cb48-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Data transformation setup</span></span>
<span id="cb48-18"><a href="#cb48-18" aria-hidden="true" tabindex="-1"></a>transform <span class="op">=</span> transforms.Compose([</span>
<span id="cb48-19"><a href="#cb48-19" aria-hidden="true" tabindex="-1"></a>    transforms.Resize((<span class="dv">224</span>, <span class="dv">224</span>)),</span>
<span id="cb48-20"><a href="#cb48-20" aria-hidden="true" tabindex="-1"></a>    transforms.ToTensor(),</span>
<span id="cb48-21"><a href="#cb48-21" aria-hidden="true" tabindex="-1"></a>    transforms.Normalize(mean<span class="op">=</span>[<span class="fl">0.485</span>, <span class="fl">0.456</span>, <span class="fl">0.406</span>], std<span class="op">=</span>[<span class="fl">0.229</span>, <span class="fl">0.224</span>, <span class="fl">0.225</span>])</span>
<span id="cb48-22"><a href="#cb48-22" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb48-23"><a href="#cb48-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-24"><a href="#cb48-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Dataset and DataLoader setup</span></span>
<span id="cb48-25"><a href="#cb48-25" aria-hidden="true" tabindex="-1"></a>image_dir <span class="op">=</span> <span class="st">'./data/flickr8k/Images'</span>  <span class="co"># Change to the actual path</span></span>
<span id="cb48-26"><a href="#cb48-26" aria-hidden="true" tabindex="-1"></a>caption_file <span class="op">=</span> <span class="st">'./data/flickr8k/captions.txt'</span>  <span class="co"># Change to the actual path</span></span>
<span id="cb48-27"><a href="#cb48-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-28"><a href="#cb48-28" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> Flickr8kDataset(image_dir, caption_file, transform<span class="op">=</span>transform)</span>
<span id="cb48-29"><a href="#cb48-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-30"><a href="#cb48-30" aria-hidden="true" tabindex="-1"></a>train_size <span class="op">=</span> <span class="bu">int</span>(<span class="fl">0.8</span> <span class="op">*</span> <span class="bu">len</span>(dataset))</span>
<span id="cb48-31"><a href="#cb48-31" aria-hidden="true" tabindex="-1"></a>val_size <span class="op">=</span> <span class="bu">len</span>(dataset) <span class="op">-</span> train_size</span>
<span id="cb48-32"><a href="#cb48-32" aria-hidden="true" tabindex="-1"></a>train_dataset, val_dataset <span class="op">=</span> torch.utils.data.random_split(dataset, [train_size, val_size])</span>
<span id="cb48-33"><a href="#cb48-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-34"><a href="#cb48-34" aria-hidden="true" tabindex="-1"></a>train_loader <span class="op">=</span> DataLoader(train_dataset, batch_size<span class="op">=</span><span class="dv">32</span>, shuffle<span class="op">=</span><span class="va">True</span>, num_workers<span class="op">=</span><span class="dv">4</span>, pin_memory<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb48-35"><a href="#cb48-35" aria-hidden="true" tabindex="-1"></a>val_loader <span class="op">=</span> DataLoader(val_dataset, batch_size<span class="op">=</span><span class="dv">32</span>, shuffle<span class="op">=</span><span class="va">False</span>, num_workers<span class="op">=</span><span class="dv">4</span>, pin_memory<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb48-36"><a href="#cb48-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-37"><a href="#cb48-37" aria-hidden="true" tabindex="-1"></a><span class="co"># Model initialization</span></span>
<span id="cb48-38"><a href="#cb48-38" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> CrossModalEmbedding(config)</span>
<span id="cb48-39"><a href="#cb48-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-40"><a href="#cb48-40" aria-hidden="true" tabindex="-1"></a><span class="co"># Model training</span></span>
<span id="cb48-41"><a href="#cb48-41" aria-hidden="true" tabindex="-1"></a>train_crossmodal_embedding(model, train_loader, val_loader, num_epochs<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb48-42"><a href="#cb48-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-43"><a href="#cb48-43" aria-hidden="true" tabindex="-1"></a><span class="co"># Model saving</span></span>
<span id="cb48-44"><a href="#cb48-44" aria-hidden="true" tabindex="-1"></a>torch.save(model.state_dict(), <span class="st">'crossmodal_embedding_model.pth'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 1/3:   4%|▍         | 40/1012 [00:04&lt;01:41,  9.53it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Image file not found: ./data/flickr8k/Images/image</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 1/3: 100%|██████████| 1012/1012 [01:47&lt;00:00,  9.41it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/3 - Train Loss: 0.9663</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code></code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/3 - Validation Loss: 0.5378</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 2/3:  58%|█████▊    | 582/1012 [01:02&lt;00:45,  9.36it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Image file not found: ./data/flickr8k/Images/image</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 2/3: 100%|██████████| 1012/1012 [01:48&lt;00:00,  9.31it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 2/3 - Train Loss: 0.3381</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code></code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 2/3 - Validation Loss: 0.4452</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 3/3:   0%|          | 4/1012 [00:00&lt;02:27,  6.82it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Image file not found: ./data/flickr8k/Images/image</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 3/3: 100%|██████████| 1012/1012 [01:48&lt;00:00,  9.35it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 3/3 - Train Loss: 0.2288</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code></code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 3/3 - Validation Loss: 0.3743</code></pre>
</div>
</div>
<div id="cell-37" class="cell" data-execution_count="3">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb67"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Example generation</span></span>
<span id="cb67-2"><a href="#cb67-2" aria-hidden="true" tabindex="-1"></a>model_path <span class="op">=</span> <span class="st">'crossmodal_embedding_model.pth'</span></span>
<span id="cb67-3"><a href="#cb67-3" aria-hidden="true" tabindex="-1"></a>generate_example(model_path, image_dir, caption_file)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Image 0:</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="10_多模态深度学习：多感官融合的开始_files/figure-html/cell-19-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Top 3 Captions (Image -&gt; Text):
  - two people walk out onto the desert sand. (prob: 0.9862)
  - a man takes a picture of him and his friend with his phone. (prob: 0.0092)
  - the little boy wearing the blue shirt is putting dirt in his mouth. (prob: 0.0013)

Caption: two people walk out onto the desert sand.

Top 3 Images (Text -&gt; Image):
 - Image 0 (prob: 0.9898)</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="10_多模态深度学习：多感官融合的开始_files/figure-html/cell-19-output-4.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> - Image 2 (prob: 0.0089)</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="10_多模态深度学习：多感官融合的开始_files/figure-html/cell-19-output-6.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> - Image 4 (prob: 0.0005)</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="10_多模态深度学习：多感官融合的开始_files/figure-html/cell-19-output-8.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="perceiver-架构" class="level3">
<h3 class="anchored" data-anchor-id="perceiver-架构">10.4.3 Perceiver 架构</h3>
<p>Perceiver 是 2021 年 DeepMind 提出的多模态架构。它解决了现有 Transformer 的二次复杂度问题（输入序列长度增加，计算量呈平方增长），同时提供了一种可以有效处理各种模态（图像、文本、音频、点云等）的结构。Perceiver 特别适用于输入数据大小很大的情况（例如高分辨率图像、长文本）。这里我们将描述整个架构，并省略示例。代码仅为说明目的的示例代码。</p>
<p><strong>Perceiver 的核心思想</strong></p>
<p>Perceiver 基于以下思想：</p>
<ol type="1">
<li><strong>瓶颈结构（Bottleneck Architecture）：</strong></li>
</ol>
<p>Perceiver 无论输入序列长度如何，都使用固定大小的潜在向量（latent array）。该潜在向量压缩并表示输入数据信息，类似于瓶颈，将大量输入信息总结为少量潜在向量。因此，即使输入数据大小很大（例如 10,000 个 token），由于潜在向量的数量是固定的（例如 256），可以大大减少计算复杂度和内存使用。</p>
<div id="cell-39" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb72"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Perceiver(nn.Module):</span>
<span id="cb72-2"><a href="#cb72-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, ..., num_latents<span class="op">=</span><span class="dv">256</span>, latent_dim<span class="op">=</span><span class="dv">512</span>, ...):</span>
<span id="cb72-3"><a href="#cb72-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb72-4"><a href="#cb72-4" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Latent vector initialization (key!)</span></span>
<span id="cb72-5"><a href="#cb72-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.latents <span class="op">=</span> nn.Parameter(torch.randn(num_latents, latent_dim))</span>
<span id="cb72-6"><a href="#cb72-6" aria-hidden="true" tabindex="-1"></a>        <span class="co"># ...</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>在上面的代码中，<code>self.latents</code> 就是表示那个潜在向量的。它被定义为 <code>nn.Parameter</code>，因此是一个可学习的参数。</p>
<ol start="2" type="1">
<li><strong>模态无关处理（Modality-Agnostic Processing）</strong>：</li>
</ol>
<p>Perceiver 不使用针对特定输入模态（如图像、文本、音频等）的专用处理方式（例如 CNN、RNN）。相反，每个模态经过简单的预处理（例如图像补丁、文本分词）后，都会被转换为一个共同的形式（向量序列）。然后，Perceiver 使用相同的基于 Transformer 的架构（交叉注意力、自注意力），无论输入模态的类型如何，都可以进行统一的处理。这使得 Perceiver 能够灵活地处理各种模态，并且容易添加新的模态。</p>
<ol start="3" type="1">
<li><strong>适应性潜在表示（Adaptive Latent Representation）</strong>：</li>
</ol>
<p>Perceiver 使用多层自注意力机制逐步更新潜在向量。在每一层中，潜在向量之间交换信息，以学习输入数据的复杂模式。最初表示简单特征的潜在向量，经过多层处理后，逐渐表达出抽象和高级的意义。</p>
<p><strong>Perceiver 的工作方式（简化代码示例）</strong></p>
<div id="cell-41" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb73"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb73-2"><a href="#cb73-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb73-3"><a href="#cb73-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-4"><a href="#cb73-4" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Perceiver(nn.Module):</span>
<span id="cb73-5"><a href="#cb73-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>,</span>
<span id="cb73-6"><a href="#cb73-6" aria-hidden="true" tabindex="-1"></a>                 input_channels<span class="op">=</span><span class="dv">3</span>,  <span class="co"># Input channels (e.g., RGB image)</span></span>
<span id="cb73-7"><a href="#cb73-7" aria-hidden="true" tabindex="-1"></a>                 input_axis<span class="op">=</span><span class="dv">2</span>,      <span class="co"># Input dimension (image=2, video=3)</span></span>
<span id="cb73-8"><a href="#cb73-8" aria-hidden="true" tabindex="-1"></a>                 num_latents<span class="op">=</span><span class="dv">256</span>,  <span class="co"># Number of latent vectors</span></span>
<span id="cb73-9"><a href="#cb73-9" aria-hidden="true" tabindex="-1"></a>                 latent_dim<span class="op">=</span><span class="dv">512</span>,    <span class="co"># Latent vector dimension</span></span>
<span id="cb73-10"><a href="#cb73-10" aria-hidden="true" tabindex="-1"></a>                 num_heads<span class="op">=</span><span class="dv">8</span>,       <span class="co"># Number of attention heads</span></span>
<span id="cb73-11"><a href="#cb73-11" aria-hidden="true" tabindex="-1"></a>                 depth<span class="op">=</span><span class="dv">6</span>):          <span class="co"># Model depth (number of self-attention layers)</span></span>
<span id="cb73-12"><a href="#cb73-12" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb73-13"><a href="#cb73-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-14"><a href="#cb73-14" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 1. Latent vector initialization (key!)</span></span>
<span id="cb73-15"><a href="#cb73-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.latents <span class="op">=</span> nn.Parameter(torch.randn(num_latents, latent_dim))</span>
<span id="cb73-16"><a href="#cb73-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-17"><a href="#cb73-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 2. Input projection (matches input dimension to latent dimension)</span></span>
<span id="cb73-18"><a href="#cb73-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.input_proj <span class="op">=</span> nn.Linear(input_dim, latent_dim)</span>
<span id="cb73-19"><a href="#cb73-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-20"><a href="#cb73-20" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 3. Cross-Attention (learns relationships between input and latent vectors)</span></span>
<span id="cb73-21"><a href="#cb73-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># self.cross_attention = nn.MultiheadAttention(latent_dim, num_heads, batch_first=True)</span></span>
<span id="cb73-22"><a href="#cb73-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-23"><a href="#cb73-23" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 4. Self-Attention (learns relationships between latent vectors) - repeated multiple times</span></span>
<span id="cb73-24"><a href="#cb73-24" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.self_attention_layers <span class="op">=</span> nn.ModuleList([</span>
<span id="cb73-25"><a href="#cb73-25" aria-hidden="true" tabindex="-1"></a>            nn.MultiheadAttention(latent_dim, num_heads, batch_first<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb73-26"><a href="#cb73-26" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(depth)</span>
<span id="cb73-27"><a href="#cb73-27" aria-hidden="true" tabindex="-1"></a>        ])</span>
<span id="cb73-28"><a href="#cb73-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-29"><a href="#cb73-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-30"><a href="#cb73-30" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):  <span class="co"># x: Input data (image, text, ...)</span></span>
<span id="cb73-31"><a href="#cb73-31" aria-hidden="true" tabindex="-1"></a>        batch_size <span class="op">=</span> x.shape[<span class="dv">0</span>]</span>
<span id="cb73-32"><a href="#cb73-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-33"><a href="#cb73-33" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 1. Input projection</span></span>
<span id="cb73-34"><a href="#cb73-34" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.input_proj(x)</span>
<span id="cb73-35"><a href="#cb73-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-36"><a href="#cb73-36" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 2. Latent vector replication (for each item in the batch)</span></span>
<span id="cb73-37"><a href="#cb73-37" aria-hidden="true" tabindex="-1"></a>        latents <span class="op">=</span> <span class="va">self</span>.latents.unsqueeze(<span class="dv">0</span>).expand(batch_size, <span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>)  <span class="co"># (B, num_latents, latent_dim)</span></span>
<span id="cb73-38"><a href="#cb73-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-39"><a href="#cb73-39" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 3. (Optional) Cross-attention (between input and latent vectors)</span></span>
<span id="cb73-40"><a href="#cb73-40" aria-hidden="true" tabindex="-1"></a>        <span class="co"># latents, _ = self.cross_attention(latents, x, x)  # query, key, value</span></span>
<span id="cb73-41"><a href="#cb73-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-42"><a href="#cb73-42" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 4. Self-attention (between latent vectors) - repeated multiple times</span></span>
<span id="cb73-43"><a href="#cb73-43" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> layer <span class="kw">in</span> <span class="va">self</span>.self_attention_layers:</span>
<span id="cb73-44"><a href="#cb73-44" aria-hidden="true" tabindex="-1"></a>            latents, _ <span class="op">=</span> layer(latents, latents, latents) <span class="co"># query, key, value</span></span>
<span id="cb73-45"><a href="#cb73-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-46"><a href="#cb73-46" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> latents  <span class="co"># Return the processed latent vectors</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><strong>Perceiver的優缺點</strong></p>
<p>Perceiver具有計算複雜度幾乎不變的效率，並可以以相同方式處理不同的模態，提供了靈活性。此外，新增模態容易的可擴展性也是Perceiver的優點。但是，Perceiver仍然基於Transformer，因此結構複雜，潛在向量的維度和層數增加時，模型可能會變得非常大。另外，在圖像分類等特定任務中，與CNN等專門為該任務設計的模型相比，性能可能會降低。</p>
<p><strong>Perceiver IO</strong></p>
<p>作為Perceiver的後續研究，Perceiver IO提出了一種不僅處理輸入還對輸出進行潛在向量處理的方法。通過這種方式，可以靈活地處理多種輸出形式（分類、回歸、序列生成等）。Perceiver IO被評為比Perceiver更為通用和強大的模型。</p>
</section>
<section id="跨模态注意力机制的实现和训练稳定性" class="level3">
<h3 class="anchored" data-anchor-id="跨模态注意力机制的实现和训练稳定性">10.4.4 跨模态注意力机制的实现和训练稳定性</h3>
<p>这里，我们从跨模态注意力的基本结构开始，逐渐添加机制，并比较其训练可能性和性能。通过这种方式，我们可以了解多模态学习中出现的问题，并探讨解决这些问题的实用方法。</p>
<p>设计跨模态注意力机制时，以这种逐步增加复杂度的实验方式是非常常见和推荐的方法。这种被称为<strong>消除研究（Ablation study）</strong>的方法，对于了解每个组成机制的重要性以及确定最终模型性能的核心要素非常有效。许多提议新架构的论文都使用这种方法。此外，不仅讨论最终的性能，还一起讨论训练过程中的稳定性问题，从实践角度来说是非常重要的。</p>
<section id="训练结构" class="level4">
<h4 class="anchored" data-anchor-id="训练结构">10.4.4.1 训练结构</h4>
<p><strong>比较训练方式</strong></p>
<p>实验使用之前查看过的flickr8k数据集，拥有文本和图像两个输入，并训练它们之间的相似度。训练中，跨模态注意力的版本是固定的，每个版本的复杂度都会增加。每次添加一个跨模态注意力的机制，然后进行训练以便比较。所有训练使用相同的超参数。训练epochs被固定为5。</p>
<p><strong>示例结构</strong></p>
<p>示例采用如下结构：</p>
<pre class="text"><code>
chapter_10/mm
├── cat_resized.png
├── cross_attention
│&nbsp;&nbsp; ├── v0.py
│&nbsp;&nbsp; ├── v1.py
│&nbsp;&nbsp; ├── v2.py
│&nbsp;&nbsp; ├── v3.py 
│&nbsp;&nbsp; .... (continue to exist)
├── train_multimodal.py
└── evaluate_models.py
</code></pre>
<p>cross_attention 文件夹下，从 v1 到 v11 顺序地增加了跨注意力的复杂度。 <code>train_mulimodal.py</code> 会在一个训练完成后动态生成下一个版本的模型并继续训练。在训练中，记录准确率、对比损失、执行时间等指标，以生成最终比较表。仅凭损失值和准确率来判断是否可以进行训练是不够好的，因为对比学习的特性，使得用现有数据无法确认训练是否正确进行。确认训练是否成功的最简单方法是使用以前没有的数据进行评估。零样本（zero-shot）评估模型的文件是 <code>evalute_models.py</code>。</p>
<p>被评估的图像是：</p>
<p><img src="../../../assets/images/cat_resized.png" class="img-fluid"></p>
<p>评估的方式是测量 5 段文本与上述图像的相似度。</p>
<pre><code>test_captions = [
    "A dog playing in the park",
    "A cat sleeping on a couch",
    "Children playing soccer",
    "A sunset over the ocean",
    "A person cooking in the kitchen"
]</code></pre>
<p>如果模型训练正确，则 5 个字幕中，第二个 “A cat sleeping on a couch” 应该具有最高的相似度。上述图像是训练数据中不存在的，属于典型的零样本测试。</p>
<p>跨注意力动态分配</p>
<p>cross_attion的版本通过动态分配来改变。</p>
<div id="cell-47" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb76"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb76-1"><a href="#cb76-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_10.mm.cross_attention.v0 <span class="im">import</span> CrossAttention <span class="im">as</span> v0</span>
<span id="cb76-2"><a href="#cb76-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_10.mm.cross_attention.v1 <span class="im">import</span> CrossAttention <span class="im">as</span> v1</span>
<span id="cb76-3"><a href="#cb76-3" aria-hidden="true" tabindex="-1"></a><span class="co"># ... (import other versions) ...</span></span>
<span id="cb76-4"><a href="#cb76-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_10.mm.cross_attention.v11 <span class="im">import</span> CrossAttention <span class="im">as</span> v11</span>
<span id="cb76-5"><a href="#cb76-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-6"><a href="#cb76-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_cross_attention(version, config<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb76-7"><a href="#cb76-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> config <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb76-8"><a href="#cb76-8" aria-hidden="true" tabindex="-1"></a>        config <span class="op">=</span> {}</span>
<span id="cb76-9"><a href="#cb76-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-10"><a href="#cb76-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> version <span class="op">==</span> <span class="st">'v0'</span>:</span>
<span id="cb76-11"><a href="#cb76-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> v0(<span class="op">**</span>config)</span>
<span id="cb76-12"><a href="#cb76-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> version <span class="op">==</span> <span class="st">'v1'</span>:</span>
<span id="cb76-13"><a href="#cb76-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> v1(<span class="op">**</span>config)</span>
<span id="cb76-14"><a href="#cb76-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ... (other version conditions) ...</span></span>
<span id="cb76-15"><a href="#cb76-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> version <span class="op">==</span> <span class="st">'v11'</span>:</span>
<span id="cb76-16"><a href="#cb76-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> v11(<span class="op">**</span>config)</span>
<span id="cb76-17"><a href="#cb76-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb76-18"><a href="#cb76-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="ss">f"Invalid cross-attention version: </span><span class="sc">{</span>version<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb76-19"><a href="#cb76-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-20"><a href="#cb76-20" aria-hidden="true" tabindex="-1"></a><span class="co"># ...</span></span>
<span id="cb76-21"><a href="#cb76-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-22"><a href="#cb76-22" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ImageTextMatchingModel(nn.Module):</span>
<span id="cb76-23"><a href="#cb76-23" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, image_encoder_dim<span class="op">=</span><span class="dv">2048</span>, text_encoder_dim<span class="op">=</span><span class="dv">768</span>, projection_dim<span class="op">=</span><span class="dv">256</span>):</span>
<span id="cb76-24"><a href="#cb76-24" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb76-25"><a href="#cb76-25" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.image_encoder <span class="op">=</span> ImageEncoder(image_encoder_dim, projection_dim)</span>
<span id="cb76-26"><a href="#cb76-26" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.text_encoder <span class="op">=</span> TextEncoder(text_encoder_dim, projection_dim)</span>
<span id="cb76-27"><a href="#cb76-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-28"><a href="#cb76-28" aria-hidden="true" tabindex="-1"></a>        <span class="co"># The CrossAttention module is dynamically assigned in main().</span></span>
<span id="cb76-29"><a href="#cb76-29" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.cross_attention <span class="op">=</span> <span class="va">None</span>  <span class="co"># CrossAttention(projection_dim)</span></span>
<span id="cb76-30"><a href="#cb76-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-31"><a href="#cb76-31" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, image, input_ids, attention_mask):</span>
<span id="cb76-32"><a href="#cb76-32" aria-hidden="true" tabindex="-1"></a>        <span class="co"># ...</span></span>
<span id="cb76-33"><a href="#cb76-33" aria-hidden="true" tabindex="-1"></a>        image_attended, text_attended <span class="op">=</span> <span class="va">self</span>.cross_attention(</span>
<span id="cb76-34"><a href="#cb76-34" aria-hidden="true" tabindex="-1"></a>            image_features.unsqueeze(<span class="dv">1</span>),</span>
<span id="cb76-35"><a href="#cb76-35" aria-hidden="true" tabindex="-1"></a>            text_features.unsqueeze(<span class="dv">1</span>)</span>
<span id="cb76-36"><a href="#cb76-36" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb76-37"><a href="#cb76-37" aria-hidden="true" tabindex="-1"></a>        <span class="co"># ...</span></span>
<span id="cb76-38"><a href="#cb76-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-39"><a href="#cb76-39" aria-hidden="true" tabindex="-1"></a><span class="co"># ...</span></span>
<span id="cb76-40"><a href="#cb76-40" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> run_training(model_versions, ...):</span>
<span id="cb76-41"><a href="#cb76-41" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ...</span></span>
<span id="cb76-42"><a href="#cb76-42" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> model_version <span class="kw">in</span> model_versions:</span>
<span id="cb76-43"><a href="#cb76-43" aria-hidden="true" tabindex="-1"></a>        <span class="co"># ...</span></span>
<span id="cb76-44"><a href="#cb76-44" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Model initialization</span></span>
<span id="cb76-45"><a href="#cb76-45" aria-hidden="true" tabindex="-1"></a>        model <span class="op">=</span> ImageTextMatchingModel()</span>
<span id="cb76-46"><a href="#cb76-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-47"><a href="#cb76-47" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Dynamically load the CrossAttention module</span></span>
<span id="cb76-48"><a href="#cb76-48" aria-hidden="true" tabindex="-1"></a>        model.cross_attention <span class="op">=</span> get_cross_attention(model_version, config<span class="op">=</span>config)</span>
<span id="cb76-49"><a href="#cb76-49" aria-hidden="true" tabindex="-1"></a>        <span class="co"># ...</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>该部分实现了实验核心中的各种版本的 <code>Cross-Attention</code> 模块的动态加载和应用逻辑。<code>get_cross_attention</code> 函数接受字符串形式的版本（v0、v1、…、v11）作为输入，并返回对应版本的 <code>CrossAttention</code> 类的实例。在 <code>run_training</code> 函数内部，对于 <code>model_versions</code> 列表中指定的每个版本，初始化 ImageTextMatchingModel，并通过调用 <code>get_cross_attention</code> 函数将该版本的 <code>Cross-Attention</code> 模块分配给 <code>model.cross_attention</code>。</p>
<p>这种动态分配方式提高了代码的复用性，并使实验管理变得更加容易。当添加新的 <code>Cross-Attention</code> 版本时，只需要在 <code>get_cross_attention</code> 函数中添加该版本，因此无需大幅修改训练代码。另外，通过 <code>run_training</code> 函数中的 <code>model_versions</code> 列表，可以轻松控制要训练的版本。</p>
<p><strong>Contrastive Loss 计算与训练循环</strong></p>
<div id="cell-49" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb77"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb77-1"><a href="#cb77-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> contrastive_loss(logits):</span>
<span id="cb77-2"><a href="#cb77-2" aria-hidden="true" tabindex="-1"></a>    labels <span class="op">=</span> torch.arange(<span class="bu">len</span>(logits), device<span class="op">=</span>logits.device)</span>
<span id="cb77-3"><a href="#cb77-3" aria-hidden="true" tabindex="-1"></a>    loss_i <span class="op">=</span> nn.CrossEntropyLoss()(logits, labels)</span>
<span id="cb77-4"><a href="#cb77-4" aria-hidden="true" tabindex="-1"></a>    loss_t <span class="op">=</span> nn.CrossEntropyLoss()(logits.t(), labels)</span>
<span id="cb77-5"><a href="#cb77-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (loss_i <span class="op">+</span> loss_t) <span class="op">/</span> <span class="dv">2</span></span>
<span id="cb77-6"><a href="#cb77-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-7"><a href="#cb77-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train(model, train_loader, val_loader, epochs<span class="op">=</span><span class="dv">10</span>, lr<span class="op">=</span><span class="fl">1e-4</span>, model_version<span class="op">=</span><span class="st">'v0'</span>):</span>
<span id="cb77-8"><a href="#cb77-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ...</span></span>
<span id="cb77-9"><a href="#cb77-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb77-10"><a href="#cb77-10" aria-hidden="true" tabindex="-1"></a>        model.train()</span>
<span id="cb77-11"><a href="#cb77-11" aria-hidden="true" tabindex="-1"></a>        total_loss <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb77-12"><a href="#cb77-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># ...</span></span>
<span id="cb77-13"><a href="#cb77-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> batch <span class="kw">in</span> tqdm(train_loader, ...):</span>
<span id="cb77-14"><a href="#cb77-14" aria-hidden="true" tabindex="-1"></a>            images, input_ids, attention_mask <span class="op">=</span> [x.to(device) <span class="cf">for</span> x <span class="kw">in</span> batch]</span>
<span id="cb77-15"><a href="#cb77-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-16"><a href="#cb77-16" aria-hidden="true" tabindex="-1"></a>            optimizer.zero_grad()</span>
<span id="cb77-17"><a href="#cb77-17" aria-hidden="true" tabindex="-1"></a>            logits <span class="op">=</span> model(images, input_ids, attention_mask)</span>
<span id="cb77-18"><a href="#cb77-18" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">=</span> contrastive_loss(logits)</span>
<span id="cb77-19"><a href="#cb77-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-20"><a href="#cb77-20" aria-hidden="true" tabindex="-1"></a>            loss.backward()</span>
<span id="cb77-21"><a href="#cb77-21" aria-hidden="true" tabindex="-1"></a>            optimizer.step()</span>
<span id="cb77-22"><a href="#cb77-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-23"><a href="#cb77-23" aria-hidden="true" tabindex="-1"></a>            total_loss <span class="op">+=</span> loss.item()</span>
<span id="cb77-24"><a href="#cb77-24" aria-hidden="true" tabindex="-1"></a>        <span class="co"># ... (validation 및 지표 계산) ...</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>本部分定义了模型的训练中使用的对比损失计算和训练循环。<code>contrastive_loss</code> 函数接受图像-文本对的相似度得分（logits）作为输入，计算对比损失。此时，正确标签是 logits 的对角线（即相同索引的图像-文本对）中元素为 1（相似），其余为 0（不相似）（使用 torch.arange）。计算以图像为基础的交叉熵损失（loss_i）和以文本为基础的交叉熵损失（loss_t），并将这两个损失的平均值用作最终损失。</p>
<p><strong>训练方式：机制的添加</strong></p>
<p>我们将从最简单的注意力结构开始，一步一步地添加功能进行测试。添加的功能称为“机制”。当每个机制被添加时，我们将研究它如何影响多模态注意力设计。首先，我们来看看部分训练代码，然后直接看训练结果。在此之后，我们还将研究哪些机制在跨模态注意力的训练中起到了决定性的作用。</p>
<p>以下是训练代码。当进行训练时，每个模型都将以 <code>model_final_{版本}.pth</code> 的形式保存。使用这个保存的模型来执行评估。</p>
<div id="cell-51" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb78"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb78-1"><a href="#cb78-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_10.mm.train_multimodal <span class="im">import</span> run_training</span>
<span id="cb78-2"><a href="#cb78-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-3"><a href="#cb78-3" aria-hidden="true" tabindex="-1"></a><span class="co"># model_versions = ['v0', 'v1']  # List of model versions to train</span></span>
<span id="cb78-4"><a href="#cb78-4" aria-hidden="true" tabindex="-1"></a>model_versions <span class="op">=</span> [<span class="st">'v0'</span>, <span class="st">'v1'</span>, <span class="st">'v2'</span>, <span class="st">'v3'</span>, <span class="st">'v4'</span>, <span class="st">'v5'</span>, <span class="st">'v6'</span>, <span class="st">'v7'</span>, <span class="st">'v8'</span>, <span class="st">'v9'</span>, <span class="st">'v10_1'</span>, <span class="st">'v10_2'</span>, <span class="st">'v10_3'</span>, <span class="st">'v10_4'</span>, <span class="st">'v10_5'</span>, <span class="st">'v10_6'</span>, <span class="st">'v11'</span>]</span>
<span id="cb78-5"><a href="#cb78-5" aria-hidden="true" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb78-6"><a href="#cb78-6" aria-hidden="true" tabindex="-1"></a>lr <span class="op">=</span> <span class="fl">1e-4</span></span>
<span id="cb78-7"><a href="#cb78-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-8"><a href="#cb78-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Dataset </span></span>
<span id="cb78-9"><a href="#cb78-9" aria-hidden="true" tabindex="-1"></a>image_dir <span class="op">=</span> <span class="st">'./data/flickr8k/Images'</span>  <span class="co"># Change to the actual path</span></span>
<span id="cb78-10"><a href="#cb78-10" aria-hidden="true" tabindex="-1"></a>caption_file <span class="op">=</span> <span class="st">'./data/flickr8k/captions.txt'</span>  <span class="co"># Change to the actual path</span></span>
<span id="cb78-11"><a href="#cb78-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-12"><a href="#cb78-12" aria-hidden="true" tabindex="-1"></a>results_df <span class="op">=</span> run_training(model_versions, epochs<span class="op">=</span>epochs, lr<span class="op">=</span>lr, image_dir<span class="op">=</span>image_dir, caption_file<span class="op">=</span>caption_file) <span class="co"># Train multiple versions</span></span>
<span id="cb78-13"><a href="#cb78-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-14"><a href="#cb78-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Print results</span></span>
<span id="cb78-15"><a href="#cb78-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Training Results:"</span>)</span>
<span id="cb78-16"><a href="#cb78-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Print results in Markdown table format</span></span>
<span id="cb78-17"><a href="#cb78-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(results_df.to_markdown(index<span class="op">=</span><span class="va">False</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>模型进行评估。</p>
<div id="cell-53" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb79"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb79-1"><a href="#cb79-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_10.mm.evaluate_models <span class="im">import</span> evaluate_all_models</span>
<span id="cb79-2"><a href="#cb79-2" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb79-3"><a href="#cb79-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Test captions (fixed)</span></span>
<span id="cb79-4"><a href="#cb79-4" aria-hidden="true" tabindex="-1"></a>test_captions <span class="op">=</span> [</span>
<span id="cb79-5"><a href="#cb79-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">"A dog playing in the park"</span>,</span>
<span id="cb79-6"><a href="#cb79-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">"A cat sleeping on a couch"</span>,</span>
<span id="cb79-7"><a href="#cb79-7" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Children playing soccer"</span>,</span>
<span id="cb79-8"><a href="#cb79-8" aria-hidden="true" tabindex="-1"></a>    <span class="st">"A sunset over the ocean"</span>,</span>
<span id="cb79-9"><a href="#cb79-9" aria-hidden="true" tabindex="-1"></a>    <span class="st">"A person cooking in the kitchen"</span></span>
<span id="cb79-10"><a href="#cb79-10" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb79-11"><a href="#cb79-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-12"><a href="#cb79-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Run model evaluation</span></span>
<span id="cb79-13"><a href="#cb79-13" aria-hidden="true" tabindex="-1"></a>image_path <span class="op">=</span> <span class="st">'./cat_resized.png'</span></span>
<span id="cb79-14"><a href="#cb79-14" aria-hidden="true" tabindex="-1"></a>model_dir <span class="op">=</span> <span class="st">'.'</span></span>
<span id="cb79-15"><a href="#cb79-15" aria-hidden="true" tabindex="-1"></a>model_versions <span class="op">=</span> [<span class="st">'v0'</span>, <span class="st">'v1'</span>, <span class="st">'v2'</span>, <span class="st">'v3'</span>, <span class="st">'v4'</span>, <span class="st">'v5'</span>, <span class="st">'v6'</span>, <span class="st">'v7'</span>, <span class="st">'v8'</span>, <span class="st">'v9'</span>, <span class="st">'v10_1'</span>, <span class="st">'v10_2'</span>, <span class="st">'v10_3'</span>, <span class="st">'v10_4'</span>, <span class="st">'v10_5'</span>, <span class="st">'v10_6'</span>, <span class="st">'v11'</span>]</span>
<span id="cb79-16"><a href="#cb79-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-17"><a href="#cb79-17" aria-hidden="true" tabindex="-1"></a>results_df <span class="op">=</span> evaluate_all_models(model_dir, image_path, test_captions, model_versions)</span>
<span id="cb79-18"><a href="#cb79-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-19"><a href="#cb79-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Print results (Markdown table)</span></span>
<span id="cb79-20"><a href="#cb79-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(results_df.to_markdown(index<span class="op">=</span><span class="va">False</span>))</span>
<span id="cb79-21"><a href="#cb79-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-22"><a href="#cb79-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Print results (detailed)</span></span>
<span id="cb79-23"><a href="#cb79-23" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> _, row <span class="kw">in</span> results_df.iterrows():</span>
<span id="cb79-24"><a href="#cb79-24" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Model: </span><span class="sc">{</span>row[<span class="st">'model_version'</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb79-25"><a href="#cb79-25" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  Best Caption: </span><span class="sc">{</span>row[<span class="st">'best_caption'</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb79-26"><a href="#cb79-26" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  Trained Well: </span><span class="sc">{</span>row[<span class="st">'trained_well'</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb79-27"><a href="#cb79-27" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  Similarity Ratio: </span><span class="sc">{</span>row[<span class="st">'similarity_ratio'</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb79-28"><a href="#cb79-28" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  Similarity Gap: </span><span class="sc">{</span>row[<span class="st">'similarity_gap'</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb79-29"><a href="#cb79-29" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"  All Similarities:"</span>)</span>
<span id="cb79-30"><a href="#cb79-30" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> caption, sim <span class="kw">in</span> <span class="bu">zip</span>(test_captions, row[<span class="st">'all_similarities'</span>]):</span>
<span id="cb79-31"><a href="#cb79-31" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"    - </span><span class="sc">{</span>caption<span class="sc">:&lt;30}</span><span class="ss">: </span><span class="sc">{</span>sim<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="实验结果" class="level4">
<h4 class="anchored" data-anchor-id="实验结果">10.4.4.2 实验结果</h4>
</section>
<section id="实验结果表" class="level4">
<h4 class="anchored" data-anchor-id="实验结果表"><strong>实验结果表</strong></h4>
<table class="caption-top table">
<colgroup>
<col style="width: 12%">
<col style="width: 23%">
<col style="width: 17%">
<col style="width: 17%">
<col style="width: 11%">
<col style="width: 4%">
<col style="width: 15%">
</colgroup>
<thead>
<tr class="header">
<th>model_version</th>
<th>best_caption</th>
<th>all_similarities</th>
<th>similarity_ratio</th>
<th>similarity_gap</th>
<th>trained_well</th>
<th>similarity_ratio_rank</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>v0</td>
<td>猫睡在沙发上</td>
<td>[‘5.322’, ‘15.477’, ‘-4.509’, ‘-6.609’, ‘2.107’]</td>
<td>2.908</td>
<td>10.155</td>
<td>True</td>
<td>1</td>
</tr>
<tr class="even">
<td>v1</td>
<td>猫睡在沙发上</td>
<td>[‘3.117’, ‘18.174’, ‘-6.475’, ‘-1.825’, ‘8.705’]</td>
<td>2.088</td>
<td>9.469</td>
<td>True</td>
<td>3</td>
</tr>
<tr class="odd">
<td>v2</td>
<td>猫睡在沙发上</td>
<td>[‘3.085’, ‘12.541’, ‘-4.252’, ‘0.924’, ‘6.849’]</td>
<td>1.831</td>
<td>5.692</td>
<td>True</td>
<td>5</td>
</tr>
<tr class="even">
<td>v3</td>
<td>孩子们踢足球</td>
<td>[‘34.882’, ‘34.882’, ‘34.882’, ‘34.882’, ‘34.882’]</td>
<td>1</td>
<td>0</td>
<td>False</td>
<td>14</td>
</tr>
<tr class="odd">
<td>v4</td>
<td>猫睡在沙发上</td>
<td>[‘7.385’, ‘8.301’, ‘-1.038’, ‘-6.262’, ‘1.240’]</td>
<td>1.124</td>
<td>0.915</td>
<td>True</td>
<td>12</td>
</tr>
<tr class="even">
<td>v5</td>
<td>孩子们踢足球</td>
<td>[‘27.357’, ‘27.357’, ‘27.357’, ‘27.357’, ‘27.357’]</td>
<td>1</td>
<td>0</td>
<td>False</td>
<td>14</td>
</tr>
<tr class="odd">
<td>v6</td>
<td>猫睡在沙发上</td>
<td>[‘5.022’, ‘14.861’, ‘-5.370’, ‘-8.630’, ‘9.063’]</td>
<td>1.64</td>
<td>5.798</td>
<td>True</td>
<td>7</td>
</tr>
<tr class="even">
<td>v7</td>
<td>狗在公园里玩耍</td>
<td>[‘16.300’, ‘16.300’, ‘16.300’, ‘16.300’, ‘16.300’]</td>
<td>1</td>
<td>0</td>
<td>False</td>
<td>14</td>
</tr>
<tr class="odd">
<td>v8</td>
<td>猫睡在沙发上</td>
<td>[‘9.841’, ‘15.442’, ‘-7.350’, ‘-1.249’, ‘11.023’]</td>
<td>1.401</td>
<td>4.419</td>
<td>True</td>
<td>10</td>
</tr>
<tr class="even">
<td>v9</td>
<td>猫睡在沙发上</td>
<td>[‘10.382’, ‘15.192’, ‘-5.582’, ‘-1.594’, ‘5.953’]</td>
<td>1.463</td>
<td>4.81</td>
<td>True</td>
<td>9</td>
</tr>
<tr class="odd">
<td>v10_1</td>
<td>狗在公园里玩耍</td>
<td>[‘0.940’, ‘0.472’, ‘-0.554’, ‘0.334’, ‘-0.111’]</td>
<td>1.991</td>
<td>0.468</td>
<td>False</td>
<td>4</td>
</tr>
<tr class="even">
<td>v10_2</td>
<td>猫睡在沙发上</td>
<td>[‘17.720’, ‘17.720’, ‘17.720’, ‘17.720’, ‘17.720’]</td>
<td>1</td>
<td>0</td>
<td>True</td>
<td>14</td>
</tr>
<tr class="odd">
<td>v10_3</td>
<td>猫睡在沙发上</td>
<td>[‘5.913’, ‘10.334’, ‘-5.989’, ‘-1.024’, ‘5.151’]</td>
<td>1.748</td>
<td>4.421</td>
<td>True</td>
<td>6</td>
</tr>
<tr class="even">
<td>v10_4</td>
<td>猫睡在沙发上</td>
<td>[‘5.913’, ‘10.334’, ‘-5.989’, ‘-1.024’, ‘5.151’]</td>
<td>1.748</td>
<td>4.421</td>
<td>True</td>
<td>6</td>
</tr>
<tr class="odd">
<td>v10_5</td>
<td>猫睡在沙发上</td>
<td>[‘6.601’, ‘9.990’, ‘-5.984’, ‘-2.988’, ‘-0.070’]</td>
<td>1.513</td>
<td>3.389</td>
<td>True</td>
<td>8</td>
</tr>
<tr class="even">
<td>v10_6</td>
<td>狗在公园里玩耍</td>
<td>[‘33.967’, ‘33.302’, ‘31.580’, ‘32.710’, ‘31.384’]</td>
<td>1.02</td>
<td>0.665</td>
<td>False</td>
<td>13</td>
</tr>
<tr class="odd">
<td>v11</td>
<td>猫睡在沙发上</td>
<td>[‘11.315’, ‘15.491’, ‘-10.428’, ‘-0.004’, ‘10.014’]</td>
<td>1.369</td>
<td>4.175</td>
<td>True</td>
<td>11</td>
</tr>
</tbody>
</table>
</section>
<section id="模型结构与训练结果分析" class="level4">
<h4 class="anchored" data-anchor-id="模型结构与训练结果分析"><strong>模型结构与训练结果分析</strong></h4>
<p>根据这些实验结果，我们可以分析每个跨模态注意力版本的训练结果，并总结成功/失败的原因如下。</p>
<table class="caption-top table">
<colgroup>
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
</colgroup>
<thead>
<tr class="header">
<th>版本</th>
<th>注意力结构</th>
<th>关键特征</th>
<th>训练结果</th>
<th>详细说明</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>v0</td>
<td>独立双向注意力</td>
<td>仅使用缩放点积注意力</td>
<td>训练成功</td>
<td>最基本的结构。独立计算图像和文本的注意力。除了缩放外没有其他归一化/变换。由于缺乏单独的归一化，对输入特征尺度变化敏感。</td>
</tr>
<tr class="even">
<td>v1</td>
<td>共享注意力</td>
<td>单一注意力矩阵和转置矩阵</td>
<td>训练成功</td>
<td>图像→文本和文本→图像的注意力计算共享相同的注意力矩阵。尝试双向信息交换，但由于缺乏归一化仍对输入尺度敏感，且无法正确反映两种模态之间的非对称关系。</td>
</tr>
<tr class="odd">
<td>v2</td>
<td>共享注意力 + LN</td>
<td>对输入应用LayerNorm</td>
<td>训练成功</td>
<td>对输入特征应用LayerNorm以稳定特征尺度。解决了v1的问题（输入尺度敏感性）。注意力矩阵仍然共享。</td>
</tr>
<tr class="even">
<td>v3</td>
<td>v2 + 残差连接</td>
<td>在输出添加残差连接</td>
<td>训练失败</td>
<td>添加了在注意力计算后直接加上原始特征（image_features, text_features）的残差连接。这过度保留了原始特征，阻碍了通过模态间交互创建新特征。这种现象在浅层网络结构中尤为明显。</td>
</tr>
<tr class="odd">
<td>v4</td>
<td>v2 + 投影</td>
<td>模态特定的线性变换</td>
<td>训练成功</td>
<td>对每个模态应用独立的线性投影（self.image_proj, self.text_proj）。通过对归一化输入（image_norm, text_norm）应用单独的线性变换，更灵活地调整每个模态的特征空间，并将其转换为适合注意力计算的形式。</td>
</tr>
<tr class="even">
<td>v5</td>
<td>v2 + 混合比率</td>
<td>固定0.5混合比率</td>
<td>训练失败</td>
<td>以固定比率（0.5）混合原始特征（image_norm, text_norm）和注意力输出（image_attended, text_attended）。与残差连接（v3）类似保留原始特征，但固定的混合比率限制了模型根据数据灵活调整权重的能力。</td>
</tr>
<tr class="odd">
<td>v6</td>
<td>共享注意力 + Q/K/V</td>
<td>Q/K/V变换和单一LayerNorm</td>
<td>训练成功</td>
<td>为输入（image_norm, text_norm）添加生成查询（Q）、键（K）和值（V）的单独线性变换（self.to_q, self.to_k, self.to_v）。这使注意力机制能够学习更丰富的特征表示。仍使用共享注意力矩阵。</td>
</tr>
<tr class="even">
<td>v7</td>
<td>共享多头</td>
<td>多头 + 输出归一化</td>
<td>训练失败</td>
<td>将共享注意力矩阵扩展为多头注意力。保持输入的LayerNorm（v2）。虽然每个头可以学习不同的特征，但仍使用共享注意力，无法正确建模图像→文本和文本→图像之间的非对称关系。尽管对输出应用了LayerNorm，训练仍然失败。</td>
</tr>
<tr class="odd">
<td>v8</td>
<td>独立多头</td>
<td>独立双向多头 + 双重归一化</td>
<td>训练成功</td>
<td>将图像→文本和文本→图像注意力分离为独立的多头注意力。对输入和输出都应用LayerNorm。在保留每个模态特性的同时有效执行双向信息交换。</td>
</tr>
<tr class="even">
<td>v9</td>
<td>v8 + Pre-LN + FFN</td>
<td>添加门控FFN和dropout</td>
<td>训练成功</td>
<td>在v8结构上添加前置层归一化、门控前馈网络（FFN）和dropout。Pre-LN在注意力和FFN之前应用LayerNorm以提高训练稳定性。门控FFN使用GELU激活函数和dropout增强非线性并防止过拟合。仅对FFN输出应用残差连接以改善信息流。</td>
</tr>
<tr class="odd">
<td>v10_1</td>
<td>v9 + 模态特定Q/K/V</td>
<td>每个模态的专门变换</td>
<td>训练失败</td>
<td>基于v9，为每个模态使用单独的Q、K、V投影（self.image_to_q, self.image_to_k, …, self.text_to_v）。这大大增加了模型复杂度，但过度分离了每个模态的特性，使学习两个模态之间的交互变得困难。</td>
</tr>
<tr class="even">
<td>v10_2</td>
<td>v9 + 交叉门控</td>
<td>控制模态间信息流</td>
<td>训练失败</td>
<td>在v9上添加交叉门控机制。在连接注意力输出和原始特征后应用门控层（sigmoid）以控制模态间信息交换。然而，由于缺乏门控层的归一化且初始门控值很小（self.gate_scale = 0.1），无法有效控制信息流并阻碍学习。</td>
</tr>
<tr class="odd">
<td>v10_3</td>
<td>v9 + 上下文层</td>
<td>处理模态特定的上下文信息</td>
<td>训练成功</td>
<td>在v9上添加模态特定的上下文层（self.image_context, self.text_context）。这额外处理每个模态的特征，在注意力计算前提供更丰富的上下文信息。</td>
</tr>
<tr class="even">
<td>v10_4</td>
<td>v9 + 多查询</td>
<td>K,V共享的注意力方法</td>
<td>训练成功</td>
<td>在v9中引入多查询注意力（Multi-Query Attention）机制。每个头独立维护查询，而所有头共享键和值（self.to_kv）。这减少了参数数量，同时允许每个头从不同角度生成查询以捕获多样化特征。</td>
</tr>
<tr class="odd">
<td>v10_5</td>
<td>v9 + 分层多头</td>
<td>3级特征处理，基于权重的融合</td>
<td>训练成功</td>
<td>在v9中引入分层多头注意力结构。将输入特征分为3个级别处理（self.level_projections, self.level_norms）。在每个级别执行独立的多头注意力，并使用可学习权重（self.level_weights）融合各级别的输出。这使模型能够学习各种抽象级别的特征并有效结合它们。</td>
</tr>
<tr class="even">
<td>v10_6</td>
<td>v9 + 对比学习多头</td>
<td>基于对比学习的相似度约束，特征增强</td>
<td>训练失败</td>
<td>在v9上添加用于对比学习的单独投影层（self.contrast_proj）。计算归一化对比学习特征之间的相似度，并通过直接添加到原始特征的方式增强注意力输出。然而，这扭曲了原始特征，类似于v3，阻碍了模态间交互，导致训练失败。</td>
</tr>
<tr class="odd">
<td>v11</td>
<td>v9 + 多查询 + 分层融合</td>
<td>结合K,V共享与3级特征处理</td>
<td>训练成功</td>
<td>结合了v10_4（多查询）和v10_5（分层多头）的优势。通过多查询注意力提高参数效率，通过分层融合整合各种级别的特征。保持v9的稳定化技术，如Pre-LN、门控FFN和dropout。</td>
</tr>
</tbody>
</table>
</section>
<section id="关于注意力结构的解释" class="level4">
<h4 class="anchored" data-anchor-id="关于注意力结构的解释">10.4.4.3 关于注意力结构的解释</h4>
<p><strong>1. v0：</strong> 独立的双向注意力 - 基本结构</p>
<p>v0 实现了最基本形式的跨模式注意力（Cross-Modal Attention）。它分别为图像和文本计算独立的注意力，并且除了缩放点积注意力（Scaled Dot-Product Attention）以外，不使用其他归一化或变换。</p>
<div id="cell-55" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb80"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb80-1"><a href="#cb80-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb80-2"><a href="#cb80-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb80-3"><a href="#cb80-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb80-4"><a href="#cb80-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-5"><a href="#cb80-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-6"><a href="#cb80-6" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> CrossAttention(nn.Module):</span>
<span id="cb80-7"><a href="#cb80-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, dim):</span>
<span id="cb80-8"><a href="#cb80-8" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb80-9"><a href="#cb80-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.scale <span class="op">=</span> dim <span class="op">**</span> <span class="op">-</span><span class="fl">0.5</span></span>
<span id="cb80-10"><a href="#cb80-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-11"><a href="#cb80-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, image_features, text_features):</span>
<span id="cb80-12"><a href="#cb80-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Image -&gt; Text attention</span></span>
<span id="cb80-13"><a href="#cb80-13" aria-hidden="true" tabindex="-1"></a>        attn_i2t <span class="op">=</span> torch.matmul(image_features, text_features.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>)) <span class="op">*</span> <span class="va">self</span>.scale</span>
<span id="cb80-14"><a href="#cb80-14" aria-hidden="true" tabindex="-1"></a>        attn_i2t <span class="op">=</span> attn_i2t.softmax(dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb80-15"><a href="#cb80-15" aria-hidden="true" tabindex="-1"></a>        image_attended <span class="op">=</span> torch.matmul(attn_i2t, text_features)</span>
<span id="cb80-16"><a href="#cb80-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-17"><a href="#cb80-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Text -&gt; Image attention</span></span>
<span id="cb80-18"><a href="#cb80-18" aria-hidden="true" tabindex="-1"></a>        attn_t2i <span class="op">=</span> torch.matmul(text_features, image_features.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>)) <span class="op">*</span> <span class="va">self</span>.scale</span>
<span id="cb80-19"><a href="#cb80-19" aria-hidden="true" tabindex="-1"></a>        attn_t2i <span class="op">=</span> attn_t2i.softmax(dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb80-20"><a href="#cb80-20" aria-hidden="true" tabindex="-1"></a>        text_attended <span class="op">=</span> torch.matmul(attn_t2i, image_features)</span>
<span id="cb80-21"><a href="#cb80-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-22"><a href="#cb80-22" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> image_attended, text_attended</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>v0因为没有单独的归一化过程，所以对输入特征的尺度变化很敏感。如果输入数据的尺度在训练过程中发生了很大的变化，注意力权重可能会变得不稳定，训练可能无法正常进行。</p>
<p><strong>2. v2:</strong> 共享注意力 + 层归一化</p>
<p>v2是在v1的基础上，对输入特征应用层归一化（LN）以稳定特征尺度的版本。v1在计算图像到文本、文本到图像的注意力时，使用相同的注意力矩阵（权重矩阵）和其转置矩阵，但是它对输入尺度很敏感，这是一个缺点。</p>
<div id="cell-57" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb81"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb81-1"><a href="#cb81-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb81-2"><a href="#cb81-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb81-3"><a href="#cb81-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb81-4"><a href="#cb81-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-5"><a href="#cb81-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Co-attention + added LN</span></span>
<span id="cb81-6"><a href="#cb81-6" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> CrossAttention(nn.Module):</span>
<span id="cb81-7"><a href="#cb81-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, dim):</span>
<span id="cb81-8"><a href="#cb81-8" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb81-9"><a href="#cb81-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.scale <span class="op">=</span> dim <span class="op">**</span> <span class="op">-</span><span class="fl">0.5</span></span>
<span id="cb81-10"><a href="#cb81-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.norm <span class="op">=</span> nn.LayerNorm(dim)  <span class="co"># Use a single LayerNorm</span></span>
<span id="cb81-11"><a href="#cb81-11" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb81-12"><a href="#cb81-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, image_features, text_features):</span>
<span id="cb81-13"><a href="#cb81-13" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Input normalization</span></span>
<span id="cb81-14"><a href="#cb81-14" aria-hidden="true" tabindex="-1"></a>        image_norm <span class="op">=</span> <span class="va">self</span>.norm(image_features)</span>
<span id="cb81-15"><a href="#cb81-15" aria-hidden="true" tabindex="-1"></a>        text_norm <span class="op">=</span> <span class="va">self</span>.norm(text_features)</span>
<span id="cb81-16"><a href="#cb81-16" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb81-17"><a href="#cb81-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Simple attention calculation</span></span>
<span id="cb81-18"><a href="#cb81-18" aria-hidden="true" tabindex="-1"></a>        attn <span class="op">=</span> torch.matmul(image_norm, text_norm.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>)) <span class="op">*</span> <span class="va">self</span>.scale</span>
<span id="cb81-19"><a href="#cb81-19" aria-hidden="true" tabindex="-1"></a>        attn <span class="op">=</span> attn.softmax(dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb81-20"><a href="#cb81-20" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb81-21"><a href="#cb81-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Bidirectional feature fusion (without residual connection)</span></span>
<span id="cb81-22"><a href="#cb81-22" aria-hidden="true" tabindex="-1"></a>        image_out <span class="op">=</span> torch.matmul(attn, text_norm)</span>
<span id="cb81-23"><a href="#cb81-23" aria-hidden="true" tabindex="-1"></a>        text_out <span class="op">=</span> torch.matmul(attn.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>), image_norm)</span>
<span id="cb81-24"><a href="#cb81-24" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb81-25"><a href="#cb81-25" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> image_out, text_out</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><code>image_norm = self.norm(image_features)</code> 和 <code>text_norm = self.norm(text_features)</code> 中对输入特征应用层归一化。层归一化对于每个样本（在小批量内的每个图像或文本）独立地执行归一化。即，计算每个样本的特征向量的均值和方差，使其变为 0 和 1。这样，即使输入特征的尺度发生很大变化，也可以防止注意力权重发散，从而稳定训练。</p>
<p>然而，这仍然存在局限性。v2 通过层归一化解决了输入尺度问题，但使用相同的注意力矩阵对图像到文本和文本到图像注意力进行处理。这可能无法充分反映两个模态之间的非对称关系。从图像生成文本和从文本生成图像是具有不同复杂性的，因此使用相同的注意力机制来处理它们可能是低效的。</p>
<p><strong>3. v3：</strong> v2 + 残差连接（Residual Connection）- 失败案例</p>
<p>在 ResNet 模型架构之后，残差连接被广泛使用，但是在这里却成了失败的原因。残差连接通常用于缓解网络加深时可能出现的梯度消失问题，并且是有效地学习更深层次网络的常用技术。但是，在这个实验中，残差连接反而表现出降低性能的失败案例。</p>
<p>这是一个非常重要的观察。</p>
<div id="cell-59" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb82"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb82-1"><a href="#cb82-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb82-2"><a href="#cb82-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb82-3"><a href="#cb82-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb82-4"><a href="#cb82-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-5"><a href="#cb82-5" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> CrossAttention(nn.Module):</span>
<span id="cb82-6"><a href="#cb82-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, dim):</span>
<span id="cb82-7"><a href="#cb82-7" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb82-8"><a href="#cb82-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.scale <span class="op">=</span> dim <span class="op">**</span> <span class="op">-</span><span class="fl">0.5</span></span>
<span id="cb82-9"><a href="#cb82-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.norm <span class="op">=</span> nn.LayerNorm(dim)  <span class="co"># Use a single LayerNorm</span></span>
<span id="cb82-10"><a href="#cb82-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-11"><a href="#cb82-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, image_features, text_features):</span>
<span id="cb82-12"><a href="#cb82-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Input normalization</span></span>
<span id="cb82-13"><a href="#cb82-13" aria-hidden="true" tabindex="-1"></a>        image_norm <span class="op">=</span> <span class="va">self</span>.norm(image_features)</span>
<span id="cb82-14"><a href="#cb82-14" aria-hidden="true" tabindex="-1"></a>        text_norm <span class="op">=</span> <span class="va">self</span>.norm(text_features)</span>
<span id="cb82-15"><a href="#cb82-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-16"><a href="#cb82-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Simple attention calculation</span></span>
<span id="cb82-17"><a href="#cb82-17" aria-hidden="true" tabindex="-1"></a>        attn <span class="op">=</span> torch.matmul(image_norm, text_norm.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>)) <span class="op">*</span> <span class="va">self</span>.scale</span>
<span id="cb82-18"><a href="#cb82-18" aria-hidden="true" tabindex="-1"></a>        attn <span class="op">=</span> attn.softmax(dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb82-19"><a href="#cb82-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-20"><a href="#cb82-20" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Bidirectional feature fusion</span></span>
<span id="cb82-21"><a href="#cb82-21" aria-hidden="true" tabindex="-1"></a>        image_attended <span class="op">=</span> torch.matmul(attn, text_norm)</span>
<span id="cb82-22"><a href="#cb82-22" aria-hidden="true" tabindex="-1"></a>        text_attended <span class="op">=</span> torch.matmul(attn.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>), image_norm)</span>
<span id="cb82-23"><a href="#cb82-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-24"><a href="#cb82-24" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Add residual connection</span></span>
<span id="cb82-25"><a href="#cb82-25" aria-hidden="true" tabindex="-1"></a>        image_out <span class="op">=</span> image_features <span class="op">+</span> image_attended</span>
<span id="cb82-26"><a href="#cb82-26" aria-hidden="true" tabindex="-1"></a>        text_out <span class="op">=</span> text_features <span class="op">+</span> text_attended</span>
<span id="cb82-27"><a href="#cb82-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-28"><a href="#cb82-28" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> image_out, text_out</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>一般来说，残差连接在解决网络加深时学习变得更加困难的问题方面是有效的。但是在v3中，由于以下原因，反而导致了性能下降的结果。</p>
<p><strong>相对较浅的网络：</strong> v3模型具有相对较浅的网络结构。残差连接在深度网络中可以缓解梯度消失问题，但在浅层网络中，其效果甚微，甚至可能会阻碍信息流动。</p>
<p><strong>原始特征的过度保留：</strong> 跨模态注意力机制（Cross-Modal Attention）的核心是通过图像和文本之间的交互作用产生新的特征。但是在v3中，直接将原始特征向量添加到注意力运算结果中，从而稀释了注意力机制所获得的重要信息，并阻碍了两个模态之间的交互作用所产生的特征生成。也就是说，模型更专注于保持现有信息，而不是学习新的信息。</p>
<p>v3的实验结果告诉我们，残差连接并不是一个总能提高性能的万能解决方案。残差连接需要根据网络深度、应用位置以及问题特点等因素进行慎重使用。v3是由于滥用残差连接导致性能下降的典型失败案例。</p>
<p><strong>4. v8：</strong> 独立多头注意力</p>
<p>v8引入了一个重要的变化来解决之前版本（v7）的问题并提高跨模态注意力的性能。具体来说，就是将图像到文本的注意力和文本到图像的注意力分离为独立的多头注意力（Multi-Head Attention）。此外，不仅对输入进行Layer Normalization，还对注意力运算的输出进行Layer Normalization，以进一步增强训练的稳定性。</p>
<div id="cell-61" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb83"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb83-1"><a href="#cb83-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb83-2"><a href="#cb83-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb83-3"><a href="#cb83-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb83-4"><a href="#cb83-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-5"><a href="#cb83-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-6"><a href="#cb83-6" aria-hidden="true" tabindex="-1"></a><span class="co"># v8 - Independent multi-head</span></span>
<span id="cb83-7"><a href="#cb83-7" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> CrossAttention(nn.Module):</span>
<span id="cb83-8"><a href="#cb83-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, dim, num_heads<span class="op">=</span><span class="dv">8</span>):</span>
<span id="cb83-9"><a href="#cb83-9" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb83-10"><a href="#cb83-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.num_heads <span class="op">=</span> num_heads</span>
<span id="cb83-11"><a href="#cb83-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.head_dim <span class="op">=</span> dim <span class="op">//</span> num_heads</span>
<span id="cb83-12"><a href="#cb83-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.scale <span class="op">=</span> <span class="va">self</span>.head_dim <span class="op">**</span> <span class="op">-</span><span class="fl">0.5</span></span>
<span id="cb83-13"><a href="#cb83-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.norm <span class="op">=</span> nn.LayerNorm(dim)</span>
<span id="cb83-14"><a href="#cb83-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-15"><a href="#cb83-15" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Projections for multi-head attention</span></span>
<span id="cb83-16"><a href="#cb83-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.to_q <span class="op">=</span> nn.Linear(dim, dim)</span>
<span id="cb83-17"><a href="#cb83-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.to_k <span class="op">=</span> nn.Linear(dim, dim)</span>
<span id="cb83-18"><a href="#cb83-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.to_v <span class="op">=</span> nn.Linear(dim, dim)</span>
<span id="cb83-19"><a href="#cb83-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-20"><a href="#cb83-20" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Output projection</span></span>
<span id="cb83-21"><a href="#cb83-21" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.to_out <span class="op">=</span> nn.Linear(dim, dim)</span>
<span id="cb83-22"><a href="#cb83-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-23"><a href="#cb83-23" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Add output normalization</span></span>
<span id="cb83-24"><a href="#cb83-24" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.out_norm <span class="op">=</span> nn.LayerNorm(dim)</span>
<span id="cb83-25"><a href="#cb83-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-26"><a href="#cb83-26" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, image_features, text_features):</span>
<span id="cb83-27"><a href="#cb83-27" aria-hidden="true" tabindex="-1"></a>        B, N_i, _ <span class="op">=</span> image_features.shape</span>
<span id="cb83-28"><a href="#cb83-28" aria-hidden="true" tabindex="-1"></a>        _, N_t, _ <span class="op">=</span> text_features.shape</span>
<span id="cb83-29"><a href="#cb83-29" aria-hidden="true" tabindex="-1"></a>        H <span class="op">=</span> <span class="va">self</span>.num_heads</span>
<span id="cb83-30"><a href="#cb83-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-31"><a href="#cb83-31" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Input normalization</span></span>
<span id="cb83-32"><a href="#cb83-32" aria-hidden="true" tabindex="-1"></a>        image_norm <span class="op">=</span> <span class="va">self</span>.norm(image_features)</span>
<span id="cb83-33"><a href="#cb83-33" aria-hidden="true" tabindex="-1"></a>        text_norm <span class="op">=</span> <span class="va">self</span>.norm(text_features)</span>
<span id="cb83-34"><a href="#cb83-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-35"><a href="#cb83-35" aria-hidden="true" tabindex="-1"></a>        <span class="kw">def</span> split_heads(x):</span>
<span id="cb83-36"><a href="#cb83-36" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> x.reshape(B, <span class="op">-</span><span class="dv">1</span>, H, <span class="va">self</span>.head_dim).transpose(<span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb83-37"><a href="#cb83-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-38"><a href="#cb83-38" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Image -&gt; Text direction attention</span></span>
<span id="cb83-39"><a href="#cb83-39" aria-hidden="true" tabindex="-1"></a>        q_img <span class="op">=</span> split_heads(<span class="va">self</span>.to_q(image_norm))</span>
<span id="cb83-40"><a href="#cb83-40" aria-hidden="true" tabindex="-1"></a>        k_txt <span class="op">=</span> split_heads(<span class="va">self</span>.to_k(text_norm))</span>
<span id="cb83-41"><a href="#cb83-41" aria-hidden="true" tabindex="-1"></a>        v_txt <span class="op">=</span> split_heads(<span class="va">self</span>.to_v(text_norm))</span>
<span id="cb83-42"><a href="#cb83-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-43"><a href="#cb83-43" aria-hidden="true" tabindex="-1"></a>        attn_i2t <span class="op">=</span> torch.matmul(q_img, k_txt.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>)) <span class="op">*</span> <span class="va">self</span>.scale</span>
<span id="cb83-44"><a href="#cb83-44" aria-hidden="true" tabindex="-1"></a>        attn_i2t <span class="op">=</span> attn_i2t.softmax(dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb83-45"><a href="#cb83-45" aria-hidden="true" tabindex="-1"></a>        image_attended <span class="op">=</span> torch.matmul(attn_i2t, v_txt)</span>
<span id="cb83-46"><a href="#cb83-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-47"><a href="#cb83-47" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Text -&gt; Image direction attention</span></span>
<span id="cb83-48"><a href="#cb83-48" aria-hidden="true" tabindex="-1"></a>        q_txt <span class="op">=</span> split_heads(<span class="va">self</span>.to_q(text_norm))</span>
<span id="cb83-49"><a href="#cb83-49" aria-hidden="true" tabindex="-1"></a>        k_img <span class="op">=</span> split_heads(<span class="va">self</span>.to_k(image_norm))</span>
<span id="cb83-50"><a href="#cb83-50" aria-hidden="true" tabindex="-1"></a>        v_img <span class="op">=</span> split_heads(<span class="va">self</span>.to_v(image_norm))</span>
<span id="cb83-51"><a href="#cb83-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-52"><a href="#cb83-52" aria-hidden="true" tabindex="-1"></a>        attn_t2i <span class="op">=</span> torch.matmul(q_txt, k_img.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>)) <span class="op">*</span> <span class="va">self</span>.scale</span>
<span id="cb83-53"><a href="#cb83-53" aria-hidden="true" tabindex="-1"></a>        attn_t2i <span class="op">=</span> attn_t2i.softmax(dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb83-54"><a href="#cb83-54" aria-hidden="true" tabindex="-1"></a>        text_attended <span class="op">=</span> torch.matmul(attn_t2i, v_img)</span>
<span id="cb83-55"><a href="#cb83-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-56"><a href="#cb83-56" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Combine heads and output projection</span></span>
<span id="cb83-57"><a href="#cb83-57" aria-hidden="true" tabindex="-1"></a>        image_attended <span class="op">=</span> image_attended.transpose(<span class="dv">1</span>, <span class="dv">2</span>).reshape(B, N_i, <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb83-58"><a href="#cb83-58" aria-hidden="true" tabindex="-1"></a>        text_attended <span class="op">=</span> text_attended.transpose(<span class="dv">1</span>, <span class="dv">2</span>).reshape(B, N_t, <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb83-59"><a href="#cb83-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-60"><a href="#cb83-60" aria-hidden="true" tabindex="-1"></a>        image_out <span class="op">=</span> <span class="va">self</span>.out_norm(<span class="va">self</span>.to_out(image_attended))</span>
<span id="cb83-61"><a href="#cb83-61" aria-hidden="true" tabindex="-1"></a>        text_out <span class="op">=</span> <span class="va">self</span>.out_norm(<span class="va">self</span>.to_out(text_attended))</span>
<span id="cb83-62"><a href="#cb83-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-63"><a href="#cb83-63" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> image_out, text_out</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>v7引入了多头注意力机制，但在图像→文本和文本→图像的注意力计算中仍然使用相同的Q、K、V变换。也就是说，由于所有头部共享相同的Q、K、V矩阵，每个头部学习不同特征的能力受到了限制，这成为了制约模型表达能力的因素。v8为了解决这个问题，对每个方向（图像→文本、文本→图像）和每个头部应用了独立的Q、K、V变换，使模型能够学习更加灵活和丰富的特征表示。</p>
<p><strong>5. v9:</strong> v8 + Pre-LN + FFN（门控FFN + Dropout）</p>
<p>v9在v8结构的基础上，添加了三个重要机制以进一步提高训练稳定性和性能：Pre-Layer Normalization（前置层归一化）、门控前馈神经网络（Gated FFN）和Dropout（随机失活）。</p>
<div id="cell-63" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb84"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb84-1"><a href="#cb84-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb84-2"><a href="#cb84-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb84-3"><a href="#cb84-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb84-4"><a href="#cb84-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-5"><a href="#cb84-5" aria-hidden="true" tabindex="-1"></a><span class="co"># v9 - Dropout before gated FFN, pass through norm at the end -&gt; trainable</span></span>
<span id="cb84-6"><a href="#cb84-6" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> CrossAttention(nn.Module):</span>
<span id="cb84-7"><a href="#cb84-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, dim, num_heads<span class="op">=</span><span class="dv">8</span>, dropout<span class="op">=</span><span class="fl">0.1</span>, ff_dim<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb84-8"><a href="#cb84-8" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb84-9"><a href="#cb84-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.num_heads <span class="op">=</span> num_heads</span>
<span id="cb84-10"><a href="#cb84-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.head_dim <span class="op">=</span> dim <span class="op">//</span> num_heads</span>
<span id="cb84-11"><a href="#cb84-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.scale <span class="op">=</span> <span class="va">self</span>.head_dim <span class="op">**</span> <span class="op">-</span><span class="fl">0.5</span></span>
<span id="cb84-12"><a href="#cb84-12" aria-hidden="true" tabindex="-1"></a>        ff_dim <span class="op">=</span> ff_dim <span class="kw">or</span> dim <span class="op">*</span> <span class="dv">4</span></span>
<span id="cb84-13"><a href="#cb84-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-14"><a href="#cb84-14" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Normalization layers for Pre-LN</span></span>
<span id="cb84-15"><a href="#cb84-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.attn_norm <span class="op">=</span> nn.LayerNorm(dim)</span>
<span id="cb84-16"><a href="#cb84-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ff_norm <span class="op">=</span> nn.LayerNorm(dim)</span>
<span id="cb84-17"><a href="#cb84-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-18"><a href="#cb84-18" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Projections for multi-head attention</span></span>
<span id="cb84-19"><a href="#cb84-19" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.to_q <span class="op">=</span> nn.Linear(dim, dim)</span>
<span id="cb84-20"><a href="#cb84-20" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.to_k <span class="op">=</span> nn.Linear(dim, dim)</span>
<span id="cb84-21"><a href="#cb84-21" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.to_v <span class="op">=</span> nn.Linear(dim, dim)</span>
<span id="cb84-22"><a href="#cb84-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-23"><a href="#cb84-23" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Output projection</span></span>
<span id="cb84-24"><a href="#cb84-24" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.to_out <span class="op">=</span> nn.Linear(dim, dim)</span>
<span id="cb84-25"><a href="#cb84-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-26"><a href="#cb84-26" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Dropout</span></span>
<span id="cb84-27"><a href="#cb84-27" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(dropout)</span>
<span id="cb84-28"><a href="#cb84-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-29"><a href="#cb84-29" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Gated feedforward network</span></span>
<span id="cb84-30"><a href="#cb84-30" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ff_gate <span class="op">=</span> nn.Sequential(</span>
<span id="cb84-31"><a href="#cb84-31" aria-hidden="true" tabindex="-1"></a>            nn.Linear(dim, ff_dim),</span>
<span id="cb84-32"><a href="#cb84-32" aria-hidden="true" tabindex="-1"></a>            nn.GELU(),</span>
<span id="cb84-33"><a href="#cb84-33" aria-hidden="true" tabindex="-1"></a>            nn.Dropout(dropout)</span>
<span id="cb84-34"><a href="#cb84-34" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb84-35"><a href="#cb84-35" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ff_value <span class="op">=</span> nn.Sequential(</span>
<span id="cb84-36"><a href="#cb84-36" aria-hidden="true" tabindex="-1"></a>            nn.Linear(dim, ff_dim),</span>
<span id="cb84-37"><a href="#cb84-37" aria-hidden="true" tabindex="-1"></a>            nn.GELU(),</span>
<span id="cb84-38"><a href="#cb84-38" aria-hidden="true" tabindex="-1"></a>            nn.Dropout(dropout)</span>
<span id="cb84-39"><a href="#cb84-39" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb84-40"><a href="#cb84-40" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ff_out <span class="op">=</span> nn.Linear(ff_dim, dim)</span>
<span id="cb84-41"><a href="#cb84-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-42"><a href="#cb84-42" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, image_features, text_features):</span>
<span id="cb84-43"><a href="#cb84-43" aria-hidden="true" tabindex="-1"></a>        B, N_i, _ <span class="op">=</span> image_features.shape</span>
<span id="cb84-44"><a href="#cb84-44" aria-hidden="true" tabindex="-1"></a>        _, N_t, _ <span class="op">=</span> text_features.shape</span>
<span id="cb84-45"><a href="#cb84-45" aria-hidden="true" tabindex="-1"></a>        H <span class="op">=</span> <span class="va">self</span>.num_heads</span>
<span id="cb84-46"><a href="#cb84-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-47"><a href="#cb84-47" aria-hidden="true" tabindex="-1"></a>        <span class="kw">def</span> split_heads(x):</span>
<span id="cb84-48"><a href="#cb84-48" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> x.reshape(B, <span class="op">-</span><span class="dv">1</span>, H, <span class="va">self</span>.head_dim).transpose(<span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb84-49"><a href="#cb84-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-50"><a href="#cb84-50" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Pre-LN: Normalize before attention</span></span>
<span id="cb84-51"><a href="#cb84-51" aria-hidden="true" tabindex="-1"></a>        image_norm <span class="op">=</span> <span class="va">self</span>.attn_norm(image_features)</span>
<span id="cb84-52"><a href="#cb84-52" aria-hidden="true" tabindex="-1"></a>        text_norm <span class="op">=</span> <span class="va">self</span>.attn_norm(text_features)</span>
<span id="cb84-53"><a href="#cb84-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-54"><a href="#cb84-54" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Image -&gt; Text direction attention</span></span>
<span id="cb84-55"><a href="#cb84-55" aria-hidden="true" tabindex="-1"></a>        q_img <span class="op">=</span> split_heads(<span class="va">self</span>.to_q(image_norm))</span>
<span id="cb84-56"><a href="#cb84-56" aria-hidden="true" tabindex="-1"></a>        k_txt <span class="op">=</span> split_heads(<span class="va">self</span>.to_k(text_norm))</span>
<span id="cb84-57"><a href="#cb84-57" aria-hidden="true" tabindex="-1"></a>        v_txt <span class="op">=</span> split_heads(<span class="va">self</span>.to_v(text_norm))</span>
<span id="cb84-58"><a href="#cb84-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-59"><a href="#cb84-59" aria-hidden="true" tabindex="-1"></a>        attn_i2t <span class="op">=</span> torch.matmul(q_img, k_txt.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>)) <span class="op">*</span> <span class="va">self</span>.scale</span>
<span id="cb84-60"><a href="#cb84-60" aria-hidden="true" tabindex="-1"></a>        attn_i2t <span class="op">=</span> attn_i2t.softmax(dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb84-61"><a href="#cb84-61" aria-hidden="true" tabindex="-1"></a>        attn_i2t <span class="op">=</span> <span class="va">self</span>.dropout(attn_i2t)  <span class="co"># Apply dropout to attention weights</span></span>
<span id="cb84-62"><a href="#cb84-62" aria-hidden="true" tabindex="-1"></a>        image_attended <span class="op">=</span> torch.matmul(attn_i2t, v_txt)</span>
<span id="cb84-63"><a href="#cb84-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-64"><a href="#cb84-64" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Text -&gt; Image direction attention</span></span>
<span id="cb84-65"><a href="#cb84-65" aria-hidden="true" tabindex="-1"></a>        q_txt <span class="op">=</span> split_heads(<span class="va">self</span>.to_q(text_norm))</span>
<span id="cb84-66"><a href="#cb84-66" aria-hidden="true" tabindex="-1"></a>        k_img <span class="op">=</span> split_heads(<span class="va">self</span>.to_k(image_norm))</span>
<span id="cb84-67"><a href="#cb84-67" aria-hidden="true" tabindex="-1"></a>        v_img <span class="op">=</span> split_heads(<span class="va">self</span>.to_v(image_norm))</span>
<span id="cb84-68"><a href="#cb84-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-69"><a href="#cb84-69" aria-hidden="true" tabindex="-1"></a>        attn_t2i <span class="op">=</span> torch.matmul(q_txt, k_img.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>)) <span class="op">*</span> <span class="va">self</span>.scale</span>
<span id="cb84-70"><a href="#cb84-70" aria-hidden="true" tabindex="-1"></a>        attn_t2i <span class="op">=</span> attn_t2i.softmax(dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb84-71"><a href="#cb84-71" aria-hidden="true" tabindex="-1"></a>        attn_t2i <span class="op">=</span> <span class="va">self</span>.dropout(attn_t2i)  <span class="co"># Apply dropout to attention weights</span></span>
<span id="cb84-72"><a href="#cb84-72" aria-hidden="true" tabindex="-1"></a>        text_attended <span class="op">=</span> torch.matmul(attn_t2i, v_img)</span>
<span id="cb84-73"><a href="#cb84-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-74"><a href="#cb84-74" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Combine heads and output projection</span></span>
<span id="cb84-75"><a href="#cb84-75" aria-hidden="true" tabindex="-1"></a>        image_attended <span class="op">=</span> image_attended.transpose(<span class="dv">1</span>, <span class="dv">2</span>).reshape(B, N_i, <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb84-76"><a href="#cb84-76" aria-hidden="true" tabindex="-1"></a>        text_attended <span class="op">=</span> text_attended.transpose(<span class="dv">1</span>, <span class="dv">2</span>).reshape(B, N_t, <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb84-77"><a href="#cb84-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-78"><a href="#cb84-78" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Output projection and dropout</span></span>
<span id="cb84-79"><a href="#cb84-79" aria-hidden="true" tabindex="-1"></a>        image_attended <span class="op">=</span> <span class="va">self</span>.dropout(<span class="va">self</span>.to_out(image_attended))</span>
<span id="cb84-80"><a href="#cb84-80" aria-hidden="true" tabindex="-1"></a>        text_attended <span class="op">=</span> <span class="va">self</span>.dropout(<span class="va">self</span>.to_out(text_attended))</span>
<span id="cb84-81"><a href="#cb84-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-82"><a href="#cb84-82" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Residual connection - connecting the original image features makes training impossible.</span></span>
<span id="cb84-83"><a href="#cb84-83" aria-hidden="true" tabindex="-1"></a>        <span class="co"># image_attended = image_attended + image_features</span></span>
<span id="cb84-84"><a href="#cb84-84" aria-hidden="true" tabindex="-1"></a>        <span class="co"># text_attended = text_attended + text_features</span></span>
<span id="cb84-85"><a href="#cb84-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-86"><a href="#cb84-86" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Pre-LN: Normalize before FFN</span></span>
<span id="cb84-87"><a href="#cb84-87" aria-hidden="true" tabindex="-1"></a>        image_ff <span class="op">=</span> <span class="va">self</span>.ff_norm(image_attended)</span>
<span id="cb84-88"><a href="#cb84-88" aria-hidden="true" tabindex="-1"></a>        text_ff <span class="op">=</span> <span class="va">self</span>.ff_norm(text_attended)</span>
<span id="cb84-89"><a href="#cb84-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-90"><a href="#cb84-90" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Gated feedforward processing</span></span>
<span id="cb84-91"><a href="#cb84-91" aria-hidden="true" tabindex="-1"></a>        <span class="kw">def</span> apply_ff(x):</span>
<span id="cb84-92"><a href="#cb84-92" aria-hidden="true" tabindex="-1"></a>            gate <span class="op">=</span> <span class="va">self</span>.ff_gate(x)</span>
<span id="cb84-93"><a href="#cb84-93" aria-hidden="true" tabindex="-1"></a>            value <span class="op">=</span> <span class="va">self</span>.ff_value(x)</span>
<span id="cb84-94"><a href="#cb84-94" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> <span class="va">self</span>.dropout(<span class="va">self</span>.ff_out(gate <span class="op">*</span> value))</span>
<span id="cb84-95"><a href="#cb84-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-96"><a href="#cb84-96" aria-hidden="true" tabindex="-1"></a>        <span class="co"># FFN output and residual connection - this type of residual connection is possible.</span></span>
<span id="cb84-97"><a href="#cb84-97" aria-hidden="true" tabindex="-1"></a>        image_out <span class="op">=</span> apply_ff(image_ff) <span class="op">+</span> image_attended</span>
<span id="cb84-98"><a href="#cb84-98" aria-hidden="true" tabindex="-1"></a>        text_out <span class="op">=</span> apply_ff(text_ff) <span class="op">+</span> text_attended</span>
<span id="cb84-99"><a href="#cb84-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-100"><a href="#cb84-100" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> image_out, text_out</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<ul>
<li><p><strong>预归一化（Pre-Layer Normalization）</strong>：在v8中，注意力运算<em>之后</em>应用层归一化（Post-LN），而在v9中，<em>之前</em>应用（Pre-LN）。<code>self.image_norm_q</code>、<code>self.image_norm_k</code>、…、<code>self.text_norm_v</code>属于此类。预归一化比后续归一化具有更高的训练稳定性，并且不需要单独的warmup，因此在最近的Transformer基于模型中被广泛使用。</p></li>
<li><p><strong>门控Feed-Forward Network（FFN）</strong>：在v8的注意力运算之后添加FFN，以增强非线性并提高模型的表达能力。</p>
<ul>
<li><code>self.image_ffn</code>和<code>self.text_ffn</code>定义了FFN。FFN由两个线性层（Linear Layer）、中间的GELU（高斯误差线性单元）激活函数以及dropout组成。</li>
<li><strong>GELU激活函数</strong>：比ReLU更为平滑的非线性函数，具有更好的性能。</li>
<li><strong>FFN后残差连接与层归一化</strong>：在FFN的输出上应用<em>残差连接</em>，并应用层归一化（<code>self.image_ffn_norm</code>、<code>self.text_ffn_norm</code>）。与v3不同的是，在<em>经过FFN之后</em>应用残差连接，从而实现非线性处理后的信息融合，而不是简单地保留原始特征，这有助于改善信息流动并提高性能。</li>
</ul></li>
<li><p><strong>dropout</strong>：<code>self.dropout</code>定义了应用于注意力权重和FFN内部的dropout。dropout是一种有效的正则化技术，通过在训练过程中随机禁用神经元来防止模型过拟合（Overfitting）。</p></li>
</ul>
<p><strong>添加机制的效果</strong></p>
<ul>
<li>v9在保持v8独立多头注意力结构的同时，添加了预归一化、门控FFN和dropout，从而进一步提高了训练稳定性和性能。</li>
<li><strong>预归一化</strong>：使得训练初期更加稳定，并且无需单独的学习率warmup即可有效地训练模型。</li>
<li><strong>门控FFN</strong>：在注意力运算之后添加非线性变换，以提高模型的表达能力。GELU激活函数和dropout进一步增强了FFN的性能。</li>
<li><strong>dropout</strong>：防止模型过拟合并提高泛化性能。</li>
</ul>
<p>v9通过这些技术的组合大大提高了跨模态注意力的性能，并成为后续版本的基础。</p>
</section>
<section id="主要结果分析" class="level4">
<h4 class="anchored" data-anchor-id="主要结果分析">10.4.4.4 主要结果分析</h4>
<ul>
<li><p><strong>v0, v1 (基本结构):</strong> 没有正则化的简单注意力机制的 v0 和 v1，在训练上是 <em>成功</em> 的。但是，v1 在训练和验证数据集上都表现出对于“猫”相关字幕的更高相似度。这表明了正则化的重要性。</p></li>
<li><p><strong>v2 (LayerNorm):</strong> 在输入中应用 LayerNorm 的 v2 成功地完成了训练。这表明，稳定输入特征的尺度是很重要的。</p></li>
<li><p><strong>v3 (残差连接):</strong> 在 v2 基础上添加残差连接的 v3 未能成功地完成训练。这表明，在多模态学习中，残差连接并不总是有帮助的。因为残差连接可能过度保持原始特征，从而阻碍了两个模态之间的交互作用。</p></li>
<li><p><strong>v4 (投影):</strong> 为每个模态添加独立线性变换（投影）的 v4 成功地完成了训练。这表明，适当地转换每个模态的特征空间是很重要的。</p></li>
<li><p><strong>v7 (共享多头):</strong> 将共享注意力矩阵扩展为多头的 v7 未能成功地完成训练。这可以解释为，每个头都没有正确反映不同模态的特性。</p></li>
<li><p><strong>v8 (独立多头):</strong> 为每个方向（图像到文本，文本到图像）使用独立的多头注意力，并在输入和输出中分别应用 LayerNorm 的 v8 成功地完成了训练。这表明，在保留每个模态特性的同时进行信息交换是很重要的。</p></li>
<li><p><strong>v10_1 (模态特异性 Q/K/V):</strong> 基于 v9 并引入每个模态专用 Q/K/V 变换的 v10_1 的训练不稳定。这可以解释为，模型复杂度增加了，过拟合的风险也增大了。</p></li>
<li><p><strong>v10_2 (Cross-Gate):</strong> 在 v9 基础上添加跨模态门控机制的 v10_2 未能成功地完成训练。这可以看作是，门控机制没有恰当地控制两个模态之间的信息流动，反而阻碍了学习。可能是过早地限制了信息交换。</p></li>
<li><p><strong>v10_3 (上下文层):</strong> 为每个模态添加独立上下文处理层的 v10_3 成功地完成了训练。这一层可以进一步提炼每个模态的特征，并提供额外的语境信息，从而有助于性能的提高。</p></li>
<li><p><strong>v10_4 (多查询注意力):</strong> 保持查询（Q）独立，同时共享键（K）和值（V）的多查询注意力机制的 v10_4 成功地完成了训练。这可以看作是，减少参数数量的同时实现了高效的信息交换，从而提高了泛化性能。</p></li>
<li><p><strong>v10_5 (分层多头):</strong> 引入三阶段分层结构，并在每个级别应用独立的多头注意力，然后通过权重融合的 v10_5 成功地完成了训练。这表明，逐步整合特征，并有效利用每个级别的信息，从而提高性能。</p></li>
<li><p><strong>v10_6 (对比学习多头):</strong> 为对比学习添加独立投影层，并直接将相似度信息添加到原始特征中的 v10_6 的训练不稳定。这可能是由于相似度信息扭曲了原始特征，从而干扰了学习。</p></li>
<li><p><strong>v11 (多查询 + 分层融合):</strong> 结合多查询注意力（v10_4）和分层多头（v10_5）的优势的 v11 成功地完成了训练。这意味着，利用参数效率和逐步特征整合这两个优点，实现了稳定的学习。</p></li>
</ul>
<p><strong>结论</strong></p>
<p>通过这些移除实验，我们可以得出以下结论。 1. <strong>标准化的重要性：</strong> 在输入特征上应用LayerNorm对于训练稳定性非常重要（v2）。 2. <strong>残差连接的双面性：</strong> 残差连接是一个有用的机制，但在多模态学习的早期阶段，它可能会带来负面影响（v3）。过度保持原始特征会阻碍两个模态之间的交互学习。 3. <strong>独立的特征变换：</strong> 在每个模态上应用独立的线性变换（投影）可以提高性能（v4）。 4. <strong>多头注意力：</strong> 使用多头注意力时，每个头应该被独立配置以反映不同模态的特性（v7，v8）。 5. <strong>适当的复杂度：</strong> 过度增加模型的复杂度可能会使训练变得不稳定（v10_1，v10_2，v10_6）。 6. <strong>高效的机制：</strong> 多查询注意力（v10_4）和分层融合（v10_5）分别提供了参数效率和渐进特征整合的优势。 7. <strong>最佳组合的重要性：</strong> 如v11所示，适当地组合有效的机制可以建立更稳定和性能更好的多模态学习模型。</p>
<p>这些去除实验对于理解多模态学习中每个组件的作用和重要性非常有用。进一步来说，它们为设计新的模型提供了重要的指导。通过系统地分析特定机制存在或不存在对性能的影响，可以了解哪些元素对多模态融合有效，以及哪种组合会带来最优结果。</p>
<p>设计更系统化的实验案例和项目框架可以使得大规模模型和各种机制的实验顺利进行。希望这能够帮助研究。</p>
</section>
</section>
</section>
<section id="视觉变换器vit" class="level2">
<h2 class="anchored" data-anchor-id="视觉变换器vit">10.5 视觉变换器（ViT）</h2>
<p>本节简要介绍了带来图像处理领域创新发展的视觉变换器（Vision Transformer，ViT）以及其扩展版ViT-22B和MAE。</p>
<section id="从cnn到vit的范式转变" class="level3">
<h3 class="anchored" data-anchor-id="从cnn到vit的范式转变">10.5.1 从CNN到ViT的范式转变</h3>
<p>2020年，谷歌研究团队通过《一张图片值16x16个词：大规模图像识别中的Transformer》这篇论文，将ViT介绍给世界。 ViT标志着长期主导图像处理领域的卷积神经网络（Convolutional Neural Network，CNN）时代的结束和基于Transformer的新时代的开始。</p>
<p>ViT的核心思想很简单：将一张图片分成多个小块（patch），然后像对待文本中的单词（token）一样处理每个patch。这样做可以将图像转换为一系列patch序列，并且Transformer可以接受这个序列作为输入进行处理。</p>
<p>ViT与CNN相比，有以下重要的不同之处：</p>
<ol type="1">
<li><strong>局部性（Locality）vs.&nbsp;全局性（Globality）：</strong> CNN使用卷积滤波器来提取图像的<em>局部</em>特征，而ViT通过注意力机制（Attention Mechanism）使每个patch能够直接考虑与图像中所有其他patch的关系。也就是说，ViT更擅长于理解整个图像的上下文。</li>
<li><strong>层次结构（Hierarchical Structure）vs.&nbsp;平坦结构（Flat Structure）：</strong> CNN具有多个卷积和池化运算层，逐渐抽象特征，这构成了一个层次结构。相反，ViT将图像分解为patch，然后将所有patch转换为相同维度的向量，以<em>单一尺度</em>处理。这使得模型的实现和优化变得更容易。</li>
<li><strong>数据依赖性：</strong> CNN倾向于能够在相对较少的数据下良好工作。但是，ViT作为基于Transformer的模型，需要足够多的数据才能发挥其最佳性能。在大型数据集上预训练的ViT显示出超越CNN的图像分类、对象检测等各种视觉任务的性能。</li>
</ol>
<p>ViT的出现完全改变了图像处理领域的研究方向。自ViT之后，以图像patch嵌入、注意力机制、大规模预训练为基础的各种后续研究大量涌现。</p>
</section>
<section id="图像patch嵌入的原理" class="level3">
<h3 class="anchored" data-anchor-id="图像patch嵌入的原理">10.5.2 图像patch嵌入的原理</h3>
<p>图像patch嵌入是ViT的第一步，过程是将二维图像转换为一维序列形式。在PyTorch中，<code>torchvision.models.vision_transformer.PatchEmbed</code>类负责这一角色。</p>
<div id="cell-67" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb85"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb85-1"><a href="#cb85-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb85-2"><a href="#cb85-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb85-3"><a href="#cb85-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb85-4"><a href="#cb85-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-5"><a href="#cb85-5" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> PatchEmbed(nn.Module):</span>
<span id="cb85-6"><a href="#cb85-6" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb85-7"><a href="#cb85-7" aria-hidden="true" tabindex="-1"></a><span class="co">    Transforms a 2D image into a sequence of patch embeddings.</span></span>
<span id="cb85-8"><a href="#cb85-8" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb85-9"><a href="#cb85-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-10"><a href="#cb85-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(</span>
<span id="cb85-11"><a href="#cb85-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>,</span>
<span id="cb85-12"><a href="#cb85-12" aria-hidden="true" tabindex="-1"></a>        img_size: <span class="bu">int</span> <span class="op">=</span> <span class="dv">224</span>,</span>
<span id="cb85-13"><a href="#cb85-13" aria-hidden="true" tabindex="-1"></a>        patch_size: <span class="bu">int</span> <span class="op">=</span> <span class="dv">16</span>,</span>
<span id="cb85-14"><a href="#cb85-14" aria-hidden="true" tabindex="-1"></a>        in_chans: <span class="bu">int</span> <span class="op">=</span> <span class="dv">3</span>,</span>
<span id="cb85-15"><a href="#cb85-15" aria-hidden="true" tabindex="-1"></a>        embed_dim: <span class="bu">int</span> <span class="op">=</span> <span class="dv">768</span>,</span>
<span id="cb85-16"><a href="#cb85-16" aria-hidden="true" tabindex="-1"></a>    ) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb85-17"><a href="#cb85-17" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb85-18"><a href="#cb85-18" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb85-19"><a href="#cb85-19" aria-hidden="true" tabindex="-1"></a><span class="co">            img_size: The size of the input image (assuming a square image)</span></span>
<span id="cb85-20"><a href="#cb85-20" aria-hidden="true" tabindex="-1"></a><span class="co">            patch_size: The patch size (assuming square patches)</span></span>
<span id="cb85-21"><a href="#cb85-21" aria-hidden="true" tabindex="-1"></a><span class="co">            in_chans: The number of input image channels (e.g., 3 for RGB images)</span></span>
<span id="cb85-22"><a href="#cb85-22" aria-hidden="true" tabindex="-1"></a><span class="co">            embed_dim: The dimension of the patch embedding vector</span></span>
<span id="cb85-23"><a href="#cb85-23" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb85-24"><a href="#cb85-24" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb85-25"><a href="#cb85-25" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.img_size <span class="op">=</span> img_size</span>
<span id="cb85-26"><a href="#cb85-26" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.patch_size <span class="op">=</span> patch_size</span>
<span id="cb85-27"><a href="#cb85-27" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.num_patches <span class="op">=</span> (img_size <span class="op">//</span> patch_size) <span class="op">*</span> (img_size <span class="op">//</span> patch_size)</span>
<span id="cb85-28"><a href="#cb85-28" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.projection <span class="op">=</span> nn.Conv2d(in_chans, embed_dim, kernel_size<span class="op">=</span>patch_size, stride<span class="op">=</span>patch_size)</span>
<span id="cb85-29"><a href="#cb85-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-30"><a href="#cb85-30" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x: torch.Tensor) <span class="op">-&gt;</span> torch.Tensor:</span>
<span id="cb85-31"><a href="#cb85-31" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb85-32"><a href="#cb85-32" aria-hidden="true" tabindex="-1"></a><span class="co">        Transforms the input image into a sequence of patch embeddings.</span></span>
<span id="cb85-33"><a href="#cb85-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-34"><a href="#cb85-34" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb85-35"><a href="#cb85-35" aria-hidden="true" tabindex="-1"></a><span class="co">            x: Input image (shape: [batch_size, in_chans, img_size, img_size])</span></span>
<span id="cb85-36"><a href="#cb85-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-37"><a href="#cb85-37" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb85-38"><a href="#cb85-38" aria-hidden="true" tabindex="-1"></a><span class="co">            Sequence of patch embeddings (shape: [batch_size, num_patches, embed_dim])</span></span>
<span id="cb85-39"><a href="#cb85-39" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb85-40"><a href="#cb85-40" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.projection(x)  <span class="co"># [batch_size, embed_dim, num_patches_h, num_patches_w]</span></span>
<span id="cb85-41"><a href="#cb85-41" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.flatten(<span class="dv">2</span>)       <span class="co"># [batch_size, embed_dim, num_patches]</span></span>
<span id="cb85-42"><a href="#cb85-42" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.transpose(<span class="dv">1</span>, <span class="dv">2</span>)  <span class="co"># [batch_size, num_patches, embed_dim]</span></span>
<span id="cb85-43"><a href="#cb85-43" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<section id="图像补丁划分" class="level4">
<h4 class="anchored" data-anchor-id="图像补丁划分">10.5.2.1 图像补丁划分</h4>
<p><code>PatchEmbed</code> 类的 <code>__init__</code> 方法中最重要的部分是 <code>self.projection = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)</code>。这一行代码同时执行图像补丁划分和嵌入。</p>
<ul>
<li><strong><code>nn.Conv2d</code></strong>：PyTorch 中进行 2D 卷积运算的层。</li>
<li><strong><code>in_chans</code></strong>：输入图像的通道数（RGB 图像为 3）。</li>
<li><strong><code>embed_dim</code></strong>：输出嵌入向量的维度（ViT-Base 模型中为 768）。</li>
<li><strong><code>kernel_size=patch_size</code></strong>：设置卷积滤器（核）的大小与补丁大小相同。</li>
<li><strong><code>stride=patch_size</code></strong>：设置滤器在图像上移动的步长（stride）与补丁大小相同。</li>
</ul>
<p>通过将 <code>kernel_size</code> 和 <code>stride</code> 设置为 <code>patch_size</code>，卷积滤器以不重叠的方式，将图像划分为补丁，每个补丁精确地对应一个嵌入向量。每个卷积滤器压缩单个补丁的信息，生成一个嵌入向量。</p>
</section>
<section id="线性投影" class="level4">
<h4 class="anchored" data-anchor-id="线性投影">10.5.2.2 线性投影</h4>
<p><code>PatchEmbed</code> 类的 <code>forward</code> 方法通过 <code>self.projection(x)</code> 实际执行图像补丁嵌入。</p>
<ol type="1">
<li><strong><code>self.projection(x)</code></strong>：对输入图像 <code>x</code>（形状为 <code>[batch_size, in_chans, img_size, img_size]</code>）应用 <code>Conv2d</code> 运算。输出的形状为 <code>[batch_size, embed_dim, num_patches_h, num_patches_w]</code>，其中 <code>num_patches_h</code> 和 <code>num_patches_w</code> 分别是图像高度和宽度除以补丁大小后的值。</li>
<li><strong><code>x.flatten(2)</code></strong>：将 <code>Conv2d</code> 输出平坦化为 <code>[batch_size, embed_dim, num_patches]</code> 形状，其中 <code>num_patches</code> 是总的补丁数量（即 <code>num_patches_h * num_patches_w</code>）。</li>
<li><strong><code>x.transpose(1, 2)</code></strong>：交换张量维度，得到 <code>[batch_size, num_patches, embed_dim]</code> 形状。这是为了将每个补丁嵌入向量视为序列中的一个元素，以便输入到 transformer 编码器中。</li>
</ol>
<p>最终结果是，<code>PatchEmbed</code> 类划分图像为补丁，并通过线性变换（Linear Projection）将每个补丁转化为 <code>embed_dim</code> 维的向量，从而形成可以输入到 transformer 编码器的序列形式。</p>
</section>
</section>
<section id="位姿编码机制" class="level3">
<h3 class="anchored" data-anchor-id="位姿编码机制">10.5.3 位姿编码机制</h3>
<p>ViT 将图像划分为补丁，并将每个补丁视同文本中的单词一样输入到 transformer 中。但是，transformer 本身不能识别输入序列的顺序信息。因此，需要告知模型每个补丁对应于图像中的哪个位置。这正是 <strong>位姿编码（Positional Encoding）</strong> 的作用。</p>
<p>在 PyTorch 的 <code>VisionTransformer</code> 类中使用的是可学习的位姿嵌入，即在训练过程中同时优化每个补丁位置对应的唯一嵌入向量。</p>
<div id="cell-69" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb86"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb86-1"><a href="#cb86-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> VisionTransformer(nn.Module):</span>
<span id="cb86-2"><a href="#cb86-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, ..., num_patches, embed_dim, ...):</span>
<span id="cb86-3"><a href="#cb86-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb86-4"><a href="#cb86-4" aria-hidden="true" tabindex="-1"></a>        <span class="co"># ...</span></span>
<span id="cb86-5"><a href="#cb86-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.cls_token <span class="op">=</span> nn.Parameter(torch.zeros(<span class="dv">1</span>, <span class="dv">1</span>, embed_dim))  <span class="co"># Class token</span></span>
<span id="cb86-6"><a href="#cb86-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.pos_embed <span class="op">=</span> nn.Parameter(torch.zeros(<span class="dv">1</span>, num_patches <span class="op">+</span> <span class="dv">1</span>, embed_dim)) <span class="co"># Positional embedding</span></span>
<span id="cb86-7"><a href="#cb86-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.pos_drop <span class="op">=</span> nn.Dropout(p<span class="op">=</span>drop_rate)</span>
<span id="cb86-8"><a href="#cb86-8" aria-hidden="true" tabindex="-1"></a>        <span class="co"># ...</span></span>
<span id="cb86-9"><a href="#cb86-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb86-10"><a href="#cb86-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _pos_embed(<span class="va">self</span>, x):</span>
<span id="cb86-11"><a href="#cb86-11" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torch.cat((<span class="va">self</span>.cls_token.expand(x.shape[<span class="dv">0</span>], <span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>), x), dim<span class="op">=</span><span class="dv">1</span>) <span class="co"># Prepend class token</span></span>
<span id="cb86-12"><a href="#cb86-12" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.pos_embed  <span class="co"># Add positional embedding</span></span>
<span id="cb86-13"><a href="#cb86-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.pos_drop(x)</span>
<span id="cb86-14"><a href="#cb86-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb86-15"><a href="#cb86-15" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb86-16"><a href="#cb86-16" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.patch_embed(x)  <span class="co"># Patch embedding</span></span>
<span id="cb86-17"><a href="#cb86-17" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>._pos_embed(x)  <span class="co"># Add positional embedding</span></span>
<span id="cb86-18"><a href="#cb86-18" aria-hidden="true" tabindex="-1"></a>        <span class="co"># ... (Transformer Encoder etc.) ...</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><strong>代码解释</strong></p>
<ol type="1">
<li><strong><code>self.pos_embed</code> (可学习参数):</strong> 由 <code>nn.Parameter</code> 定义，会在训练过程中更新。其大小为 <code>(1, num_patches + 1, embed_dim)</code>。
<ul>
<li><code>num_patches + 1</code>: 在图像 patches 之外，增加一个特殊的 <strong>类别标记 (class token)</strong> 的位置。</li>
<li><code>embed_dim</code>: 与 patch embedding 的维度相同。</li>
<li>也就是说，每个 patch（和类别标记）都有一个唯一的位置 embedding 向量，其维度为 <code>embed_dim</code>。</li>
</ul></li>
<li><strong><code>_pos_embed</code> 方法:</strong>
<ul>
<li><strong>添加类别标记：</strong> 将输入 <code>x</code> (patch embedding 序列) 的开头添加 <code>self.cls_token</code>。<code>cls_token</code> 会被复制（expand）以适应批次大小，从而对所有图像应用相同的操作。</li>
<li><strong>添加位置嵌入：</strong> 在 patch embedding（和类别标记 embedding）上添加对应位置的 <code>self.pos_embed</code> 值。根据 PyTorch 的广播规则，<code>self.pos_embed</code> 的每个位置嵌入向量都会被自动添加到相应位置的所有 patch embedding 向量中。</li>
<li><strong>dropout：</strong> 为防止过拟合，应用 dropout。</li>
</ul></li>
<li><strong><code>forward</code> 方法：</strong> 在 <code>forward</code> 方法中，通过 <code>self.patch_embed(x)</code> 将图像转换为 patch embedding，然后调用 <code>self._pos_embed(x)</code> 添加位置嵌入。</li>
</ol>
<p><strong>总结</strong></p>
<p>ViT 使用可学习的位置嵌入，并将其添加到 patch embedding 上，以此向模型注入位置信息。位置嵌入会在模型训练过程中与其他权重一起被优化，从而能够以最适合数据的形式表示位置信息。</p>
</section>
<section id="vit-的结构和主要组成部分" class="level3">
<h3 class="anchored" data-anchor-id="vit-的结构和主要组成部分">10.5.4 ViT 的结构和主要组成部分</h3>
<p>ViT (Vision Transformer) 是一种将图像处理为类似文本的方式来执行分类等视觉任务的模型。在 PyTorch 中，可以通过 <code>torchvision.models.VisionTransformer</code> 类使用 ViT 模型。</p>
<div id="cell-71" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb87"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb87-1"><a href="#cb87-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> VisionTransformer(nn.Module):</span>
<span id="cb87-2"><a href="#cb87-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, ..., embed_dim, depth, num_heads, ...):</span>
<span id="cb87-3"><a href="#cb87-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb87-4"><a href="#cb87-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-5"><a href="#cb87-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.patch_embed <span class="op">=</span> PatchEmbed(...)  <span class="co"># Image patch embedding</span></span>
<span id="cb87-6"><a href="#cb87-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.cls_token <span class="op">=</span> nn.Parameter(...)   <span class="co"># Class token</span></span>
<span id="cb87-7"><a href="#cb87-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.pos_embed <span class="op">=</span> nn.Parameter(...)   <span class="co"># Positional embedding</span></span>
<span id="cb87-8"><a href="#cb87-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.pos_drop <span class="op">=</span> nn.Dropout(...)</span>
<span id="cb87-9"><a href="#cb87-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-10"><a href="#cb87-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.blocks <span class="op">=</span> nn.Sequential(<span class="op">*</span>[</span>
<span id="cb87-11"><a href="#cb87-11" aria-hidden="true" tabindex="-1"></a>            TransformerEncoderLayer(...) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(depth) <span class="co"># Transformer Encoder blocks</span></span>
<span id="cb87-12"><a href="#cb87-12" aria-hidden="true" tabindex="-1"></a>        ])</span>
<span id="cb87-13"><a href="#cb87-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.norm <span class="op">=</span> nn.LayerNorm(embed_dim) <span class="co"># Layer Normalization</span></span>
<span id="cb87-14"><a href="#cb87-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.head <span class="op">=</span> nn.Linear(embed_dim, num_classes) <span class="co"># Classification Head</span></span>
<span id="cb87-15"><a href="#cb87-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-16"><a href="#cb87-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-17"><a href="#cb87-17" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward_features(<span class="va">self</span>, x):</span>
<span id="cb87-18"><a href="#cb87-18" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.patch_embed(x)       <span class="co"># 1. Patch embedding</span></span>
<span id="cb87-19"><a href="#cb87-19" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torch.cat((<span class="va">self</span>.cls_token.expand(x.shape[<span class="dv">0</span>], <span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>), x), dim<span class="op">=</span><span class="dv">1</span>) <span class="co"># 2. Prepend class token</span></span>
<span id="cb87-20"><a href="#cb87-20" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.pos_embed       <span class="co"># 3. Add positional embedding</span></span>
<span id="cb87-21"><a href="#cb87-21" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.pos_drop(x)</span>
<span id="cb87-22"><a href="#cb87-22" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.blocks(x)            <span class="co"># 4. Transformer Encoder</span></span>
<span id="cb87-23"><a href="#cb87-23" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.norm(x)              <span class="co"># 5. LayerNorm</span></span>
<span id="cb87-24"><a href="#cb87-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x[:, <span class="dv">0</span>]                <span class="co"># 6. Return class token</span></span>
<span id="cb87-25"><a href="#cb87-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-26"><a href="#cb87-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-27"><a href="#cb87-27" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb87-28"><a href="#cb87-28" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.forward_features(x)   <span class="co"># Feature extraction</span></span>
<span id="cb87-29"><a href="#cb87-29" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.head(x)               <span class="co"># Classification</span></span>
<span id="cb87-30"><a href="#cb87-30" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><strong>ViT的核心组成部分：</strong></p>
<ol type="1">
<li><strong><code>PatchEmbed</code> (<em>patch嵌入</em>)：</strong> 将图像分割为多个小patch，然后将每个patch转换为固定维度的向量（嵌入）。（参见10.5.2节）</li>
<li><strong><code>cls_token</code> （类别标记）：</strong> 可学习参数，在patch嵌入序列的开头添加特殊标记。经过Transformer编码器后，该类别标记将包含代表图像整体信息（特征）的内容，并用于最终分类。</li>
<li><strong><code>pos_embed</code> （位置嵌入）：</strong> 表示每个patch（和类别标记）位置信息的可学习参数。由于Transformer无法自行感知输入序列的顺序，因此需要通过位置嵌入显式提供位置信息。（参见10.5.3节）</li>
<li><strong><code>blocks</code> （Transformer编码器）：</strong> 由多个 <code>TransformerEncoderLayer</code> 组成。
<ul>
<li><strong><code>TransformerEncoderLayer</code>：</strong> ViT的核心块，由多头自注意力（Multi-Head Self-Attention）和前馈网络（Feed-Forward Network，FFN）组成。
<ul>
<li><strong>多头自注意力：</strong> 每个patch考虑与所有其他patch（包括自身）的关系，以了解图像整体的上下文信息。</li>
<li><strong>FFN：</strong> 对每个patch嵌入进行个别处理，以添加非线性并学习更复杂的特征。</li>
<li><strong>（预归一化，残差连接，丢弃等）：</strong> 如第9章和第10章初期所见，应用了多种技术以实现稳定的训练和性能提升。</li>
</ul></li>
</ul></li>
<li><strong><code>norm</code> （层归一化）：</strong> 对Transformer编码器的输出应用层归一化</li>
<li><strong><code>head</code> （分类头）：</strong> 接受Transformer编码器输出的类别标记作为输入，最后预测图像类别的全连接层。</li>
</ol>
<p><strong><code>forward</code> 方法（整个处理流程）：</strong></p>
<ol type="1">
<li><code>forward_features</code> 方法：
<ul>
<li><code>self.patch_embed(x)</code>：将输入图像转换为patch嵌入序列。</li>
<li>在patch嵌入序列开头添加类别标记（<code>self.cls_token</code>）。</li>
<li>添加位置嵌入（<code>self.pos_embed</code>）。</li>
<li>通过Transformer编码器（<code>self.blocks</code>）。</li>
<li>应用层归一化（<code>self.norm</code>）。</li>
<li>只返回对应于类别标记的部分（<code>x[:, 0]</code>）。</li>
</ul></li>
<li><code>self.head(x)</code>：将 <code>forward_features</code> 返回的类别标记传递给分类头，以获得最终预测（分类）结果。</li>
</ol>
<p><strong>总结：</strong></p>
<p>ViT将图像分割为patch，并将每个patch输入Transformer编码器以提取图像整体特征。在此过程中，使用类别标记和位置嵌入同时考虑图像的全局信息和patch位置信息。最终，利用类别标记对图像进行分类。</p>
</section>
<section id="vit训练示例" class="level3">
<h3 class="anchored" data-anchor-id="vit训练示例">10.5.5 ViT训练示例</h3>
<p>让我们看一下如何使用ViT模型在CIFAR-10数据集上进行简单的训练。下面的代码使用PyTorch来训练ViT模型，并输出每个epoch的损失和准确率。</p>
<div id="cell-73" class="cell" data-execution_count="1">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb88"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb88-1"><a href="#cb88-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb88-2"><a href="#cb88-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb88-3"><a href="#cb88-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.optim <span class="im">as</span> optim</span>
<span id="cb88-4"><a href="#cb88-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision <span class="im">import</span> datasets, transforms</span>
<span id="cb88-5"><a href="#cb88-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision.models <span class="im">import</span> vit_b_16  <span class="co"># Using vit_b_16 model as an example</span></span>
<span id="cb88-6"><a href="#cb88-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> DataLoader</span>
<span id="cb88-7"><a href="#cb88-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-8"><a href="#cb88-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Hyperparameter setup for a simple training run</span></span>
<span id="cb88-9"><a href="#cb88-9" aria-hidden="true" tabindex="-1"></a>num_epochs <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb88-10"><a href="#cb88-10" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb88-11"><a href="#cb88-11" aria-hidden="true" tabindex="-1"></a>learning_rate <span class="op">=</span> <span class="fl">1e-4</span></span>
<span id="cb88-12"><a href="#cb88-12" aria-hidden="true" tabindex="-1"></a>image_size <span class="op">=</span> <span class="dv">224</span>  <span class="co"># ViT input image size</span></span>
<span id="cb88-13"><a href="#cb88-13" aria-hidden="true" tabindex="-1"></a>num_classes <span class="op">=</span> <span class="dv">10</span>   <span class="co"># Number of classes in the CIFAR-10 dataset</span></span>
<span id="cb88-14"><a href="#cb88-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-15"><a href="#cb88-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Use GPU if available</span></span>
<span id="cb88-16"><a href="#cb88-16" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> torch.device(<span class="st">"cuda"</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">"cpu"</span>)</span>
<span id="cb88-17"><a href="#cb88-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-18"><a href="#cb88-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Data loading and preprocessing (using CIFAR-10 dataset)</span></span>
<span id="cb88-19"><a href="#cb88-19" aria-hidden="true" tabindex="-1"></a>transform <span class="op">=</span> transforms.Compose([</span>
<span id="cb88-20"><a href="#cb88-20" aria-hidden="true" tabindex="-1"></a>    transforms.Resize((image_size, image_size)),</span>
<span id="cb88-21"><a href="#cb88-21" aria-hidden="true" tabindex="-1"></a>    transforms.ToTensor(),</span>
<span id="cb88-22"><a href="#cb88-22" aria-hidden="true" tabindex="-1"></a>    transforms.Normalize((<span class="fl">0.5</span>, <span class="fl">0.5</span>, <span class="fl">0.5</span>), (<span class="fl">0.5</span>, <span class="fl">0.5</span>, <span class="fl">0.5</span>)), <span class="co"># Normalize with CIFAR-10 statistics</span></span>
<span id="cb88-23"><a href="#cb88-23" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb88-24"><a href="#cb88-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-25"><a href="#cb88-25" aria-hidden="true" tabindex="-1"></a>train_dataset <span class="op">=</span> datasets.CIFAR10(root<span class="op">=</span><span class="st">'./data'</span>, train<span class="op">=</span><span class="va">True</span>, download<span class="op">=</span><span class="va">True</span>, transform<span class="op">=</span>transform)</span>
<span id="cb88-26"><a href="#cb88-26" aria-hidden="true" tabindex="-1"></a>train_loader <span class="op">=</span> DataLoader(train_dataset, batch_size<span class="op">=</span>batch_size, shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb88-27"><a href="#cb88-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-28"><a href="#cb88-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Create ViT model (not using pretrained weights)</span></span>
<span id="cb88-29"><a href="#cb88-29" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> vit_b_16(pretrained<span class="op">=</span><span class="va">False</span>, num_classes<span class="op">=</span>num_classes).to(device)</span>
<span id="cb88-30"><a href="#cb88-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-31"><a href="#cb88-31" aria-hidden="true" tabindex="-1"></a><span class="co"># Define loss function and optimizer</span></span>
<span id="cb88-32"><a href="#cb88-32" aria-hidden="true" tabindex="-1"></a>criterion <span class="op">=</span> nn.CrossEntropyLoss()</span>
<span id="cb88-33"><a href="#cb88-33" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> optim.AdamW(model.parameters(), lr<span class="op">=</span>learning_rate)</span>
<span id="cb88-34"><a href="#cb88-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-35"><a href="#cb88-35" aria-hidden="true" tabindex="-1"></a><span class="co"># Training loop</span></span>
<span id="cb88-36"><a href="#cb88-36" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(num_epochs):</span>
<span id="cb88-37"><a href="#cb88-37" aria-hidden="true" tabindex="-1"></a>    model.train()  <span class="co"># Set the model to training mode</span></span>
<span id="cb88-38"><a href="#cb88-38" aria-hidden="true" tabindex="-1"></a>    running_loss <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb88-39"><a href="#cb88-39" aria-hidden="true" tabindex="-1"></a>    correct_predictions <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb88-40"><a href="#cb88-40" aria-hidden="true" tabindex="-1"></a>    total_samples <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb88-41"><a href="#cb88-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-42"><a href="#cb88-42" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, (images, labels) <span class="kw">in</span> <span class="bu">enumerate</span>(train_loader):</span>
<span id="cb88-43"><a href="#cb88-43" aria-hidden="true" tabindex="-1"></a>        images <span class="op">=</span> images.to(device)</span>
<span id="cb88-44"><a href="#cb88-44" aria-hidden="true" tabindex="-1"></a>        labels <span class="op">=</span> labels.to(device)</span>
<span id="cb88-45"><a href="#cb88-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-46"><a href="#cb88-46" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Forward and backward passes</span></span>
<span id="cb88-47"><a href="#cb88-47" aria-hidden="true" tabindex="-1"></a>        outputs <span class="op">=</span> model(images)</span>
<span id="cb88-48"><a href="#cb88-48" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> criterion(outputs, labels)</span>
<span id="cb88-49"><a href="#cb88-49" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()</span>
<span id="cb88-50"><a href="#cb88-50" aria-hidden="true" tabindex="-1"></a>        loss.backward()</span>
<span id="cb88-51"><a href="#cb88-51" aria-hidden="true" tabindex="-1"></a>        optimizer.step()</span>
<span id="cb88-52"><a href="#cb88-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-53"><a href="#cb88-53" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculate statistics</span></span>
<span id="cb88-54"><a href="#cb88-54" aria-hidden="true" tabindex="-1"></a>        running_loss <span class="op">+=</span> loss.item()</span>
<span id="cb88-55"><a href="#cb88-55" aria-hidden="true" tabindex="-1"></a>        _, predicted <span class="op">=</span> torch.<span class="bu">max</span>(outputs, <span class="dv">1</span>)  <span class="co"># Select the class with the highest probability</span></span>
<span id="cb88-56"><a href="#cb88-56" aria-hidden="true" tabindex="-1"></a>        total_samples <span class="op">+=</span> labels.size(<span class="dv">0</span>)</span>
<span id="cb88-57"><a href="#cb88-57" aria-hidden="true" tabindex="-1"></a>        correct_predictions <span class="op">+=</span> (predicted <span class="op">==</span> labels).<span class="bu">sum</span>().item()</span>
<span id="cb88-58"><a href="#cb88-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-59"><a href="#cb88-59" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Print every 100 batches.</span></span>
<span id="cb88-60"><a href="#cb88-60" aria-hidden="true" tabindex="-1"></a>        <span class="co"># if (i + 1) % 100 == 0:</span></span>
<span id="cb88-61"><a href="#cb88-61" aria-hidden="true" tabindex="-1"></a>        <span class="co">#     print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')</span></span>
<span id="cb88-62"><a href="#cb88-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-63"><a href="#cb88-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-64"><a href="#cb88-64" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Print epoch statistics</span></span>
<span id="cb88-65"><a href="#cb88-65" aria-hidden="true" tabindex="-1"></a>    epoch_loss <span class="op">=</span> running_loss <span class="op">/</span> <span class="bu">len</span>(train_loader)</span>
<span id="cb88-66"><a href="#cb88-66" aria-hidden="true" tabindex="-1"></a>    epoch_accuracy <span class="op">=</span> correct_predictions <span class="op">/</span> total_samples <span class="op">*</span> <span class="dv">100</span></span>
<span id="cb88-67"><a href="#cb88-67" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'Epoch [</span><span class="sc">{</span>epoch<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">/</span><span class="sc">{</span>num_epochs<span class="sc">}</span><span class="ss">], Loss: </span><span class="sc">{</span>epoch_loss<span class="sc">:.4f}</span><span class="ss">, Accuracy: </span><span class="sc">{</span>epoch_accuracy<span class="sc">:.2f}</span><span class="ss">%'</span>)</span>
<span id="cb88-68"><a href="#cb88-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-69"><a href="#cb88-69" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Training finished!'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stderr">
<pre><code>100%|██████████| 170M/170M [00:21&lt;00:00, 8.09MB/s] 
/home/sean/anaconda3/envs/DL/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/sean/anaconda3/envs/DL/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch [1/5], Step [100/1563], Loss: 2.1349
Epoch [1/5], Step [200/1563], Loss: 1.8978
Epoch [1/5], Step [300/1563], Loss: 1.9483
Epoch [1/5], Step [400/1563], Loss: 2.0783
Epoch [1/5], Step [500/1563], Loss: 1.7614
Epoch [1/5], Step [600/1563], Loss: 1.8051
Epoch [1/5], Step [700/1563], Loss: 1.7448
Epoch [1/5], Step [800/1563], Loss: 1.8347
Epoch [1/5], Step [900/1563], Loss: 1.8127
Epoch [1/5], Step [1000/1563], Loss: 1.7755
Epoch [1/5], Step [1100/1563], Loss: 1.6506
Epoch [1/5], Step [1200/1563], Loss: 1.7523
Epoch [1/5], Step [1300/1563], Loss: 1.5987
Epoch [1/5], Step [1400/1563], Loss: 1.6078
Epoch [1/5], Step [1500/1563], Loss: 1.7110
Epoch [1/5], Loss: 1.8429, Accuracy: 29.66%
Epoch [2/5], Step [100/1563], Loss: 1.4902
Epoch [2/5], Step [200/1563], Loss: 1.5161
Epoch [2/5], Step [300/1563], Loss: 1.4563
Epoch [2/5], Step [400/1563], Loss: 1.5858
Epoch [2/5], Step [500/1563], Loss: 1.6702
Epoch [2/5], Step [600/1563], Loss: 1.5833
Epoch [2/5], Step [700/1563], Loss: 1.4790
Epoch [2/5], Step [800/1563], Loss: 1.6507
Epoch [2/5], Step [900/1563], Loss: 1.6017
Epoch [2/5], Step [1000/1563], Loss: 1.5102
Epoch [2/5], Step [1100/1563], Loss: 1.2946
Epoch [2/5], Step [1200/1563], Loss: 1.3225
Epoch [2/5], Step [1300/1563], Loss: 1.9922
Epoch [2/5], Step [1400/1563], Loss: 1.3685
Epoch [2/5], Step [1500/1563], Loss: 1.4852
Epoch [2/5], Loss: 1.5410, Accuracy: 42.69%
Epoch [3/5], Step [100/1563], Loss: 1.2692
Epoch [3/5], Step [200/1563], Loss: 1.1648
Epoch [3/5], Step [300/1563], Loss: 1.2412
Epoch [3/5], Step [400/1563], Loss: 1.6217
Epoch [3/5], Step [500/1563], Loss: 1.3776
Epoch [3/5], Step [600/1563], Loss: 1.2591
Epoch [3/5], Step [700/1563], Loss: 1.4333
Epoch [3/5], Step [800/1563], Loss: 1.3301
Epoch [3/5], Step [900/1563], Loss: 1.3536
Epoch [3/5], Step [1000/1563], Loss: 1.4488
Epoch [3/5], Step [1100/1563], Loss: 1.3179
Epoch [3/5], Step [1200/1563], Loss: 1.0684
Epoch [3/5], Step [1300/1563], Loss: 1.6526
Epoch [3/5], Step [1400/1563], Loss: 1.1815
Epoch [3/5], Step [1500/1563], Loss: 1.3683
Epoch [3/5], Loss: 1.3836, Accuracy: 49.23%
Epoch [4/5], Step [100/1563], Loss: 1.2601
Epoch [4/5], Step [200/1563], Loss: 1.3277
Epoch [4/5], Step [300/1563], Loss: 1.1337
Epoch [4/5], Step [400/1563], Loss: 1.2273
Epoch [4/5], Step [500/1563], Loss: 1.7351
Epoch [4/5], Step [600/1563], Loss: 1.3826
Epoch [4/5], Step [700/1563], Loss: 1.2639
Epoch [4/5], Step [800/1563], Loss: 1.5757
Epoch [4/5], Step [900/1563], Loss: 1.0702
Epoch [4/5], Step [1000/1563], Loss: 1.3986
Epoch [4/5], Step [1100/1563], Loss: 1.1105
Epoch [4/5], Step [1200/1563], Loss: 1.2621
Epoch [4/5], Step [1300/1563], Loss: 1.4261
Epoch [4/5], Step [1400/1563], Loss: 1.3028
Epoch [4/5], Step [1500/1563], Loss: 1.9051
Epoch [4/5], Loss: 1.2850, Accuracy: 52.98%
Epoch [5/5], Step [100/1563], Loss: 0.9517
Epoch [5/5], Step [200/1563], Loss: 0.9844
Epoch [5/5], Step [300/1563], Loss: 1.2391
Epoch [5/5], Step [400/1563], Loss: 1.3588
Epoch [5/5], Step [500/1563], Loss: 0.9441
Epoch [5/5], Step [600/1563], Loss: 1.1711
Epoch [5/5], Step [700/1563], Loss: 1.1687
Epoch [5/5], Step [800/1563], Loss: 1.0097
Epoch [5/5], Step [900/1563], Loss: 0.9899
Epoch [5/5], Step [1000/1563], Loss: 1.3289
Epoch [5/5], Step [1100/1563], Loss: 1.5510
Epoch [5/5], Step [1200/1563], Loss: 0.9139
Epoch [5/5], Step [1300/1563], Loss: 0.9221
Epoch [5/5], Step [1400/1563], Loss: 1.3378
Epoch [5/5], Step [1500/1563], Loss: 1.1785
Epoch [5/5], Loss: 1.2116, Accuracy: 55.78%
Training finished!</code></pre>
</div>
</div>
<p>此代码是用于展示ViT模型工作原理的简单示例。实际上，ViT是在像ImageNet这样的大型数据集上进行预训练（pre-training）的，然后针对特定任务（例如CIFAR-10分类）进行微调（fine-tuning），这样使用时会表现出更好的性能。在这里，我们仅仅检查它是否可以简单地进行训练。</p>
<p><strong>ViT的意义和影响</strong></p>
<p>ViT在图像分类任务中展示了超越CNN的性能，引起了计算机视觉领域的巨大反响。特别是，当使用JFT-300M这样的超过3亿张图片的大型数据集进行预训练时，它表现出了真正的价值。这给我们带来了两个重要的启示。</p>
<ol type="1">
<li><p><strong>可扩展性（Scalability）：</strong> ViT展示了随着数据集大小的增加，其性能会持续提高的卓越可扩展性。这与基于CNN的模型在一定规模以上的数据集中，性能会停滞甚至下降的现象形成对比。ViT的这一特点为我们打开了未来利用更多数据构建更强大视觉模型的可能性。</p></li>
<li><p><strong>Transformer的通用性：</strong> ViT证明了广泛用于自然语言处理（NLP）领域的Transformer架构在图像处理领域也是有效的。这成为研究能够使用一种架构处理多种模态（包括文本、图像、语音等）的多模态模型的重要起点。</p></li>
</ol>
<p>ViT的成功为之后CLIP（对比语言-图像预训练）等多模态模型的开发提供了重要基础。CLIP通过结合ViT的图像编码器和基于Transformer的文本编码器，学习将图像和文本表示在一个统一的空间中。这样，使得生成图像的文本描述或基于文本描述搜索图像等各种应用成为可能。</p>
<p><strong>严格指令：</strong></p>
<ul>
<li><strong>不要翻译以 <code>$...$</code> 或 <code>$$...$$</code> 包围的 LaTeX 数学表达式。将它们保持为原样。</strong></li>
<li><strong>不要翻译表格标记语法。保持表格格式（管道 <code>|</code>、连字符 <code>-</code>、冒号 <code>：</code>）保持原样。仅翻译表格单元格内的文本内容。</strong></li>
<li><strong>不要添加任何多余的文本。</strong>这包括但不限于：
<ul>
<li>问候语或结束语。</li>
<li>请求反馈。</li>
<li>翻译解释。</li>
<li>任何对话元素。</li>
<li>除直接翻译外的任何文本。</li>
</ul></li>
<li><strong>仅执行翻译。不要进行推理、推断或其他任务。</strong></li>
</ul>
</section>
<section id="vit-22b极致的规模" class="level3">
<h3 class="anchored" data-anchor-id="vit-22b极致的规模">10.5.6 ViT-22B：极致的规模</h3>
<p><strong>Google Research</strong> 团队提出的并训练的 ViT-22B 在图像分类中超越了 CNN 的表现，同时也引起了计算机视觉领域的巨大反响。ViT-22B 证明了，模型和数据的尺寸扩大是提高性能的一个关键因素。凭借其220亿个参数和数十亿张图像组成的超大规模数据集，ViT-22B 达到了以前难以想象的性能水平，并开启了视觉 AI 的新纪元。</p>
<p><strong>诞生背景：缩放法则和大型语言模型的成功</strong></p>
<p>ViT-22B 的出现与自然语言处理（NLP）领域中大型语言模型（Large Language Model, LLM）的惊人成功密切相关。像 GPT-3 这样的 LLM 表明，随着模型尺寸（参数数量）和数据量的增加，其性能也会持续提高，这遵循 <em>缩放法则（scaling law）</em> 。这种趋势传播了“更大即更好”的信念，并促使视觉领域进行类似的尝试。</p>
<p>由于 ViT 基于Transformer架构，因此可以轻松应用LLM中验证的缩放策略。ViT 将图像patches处理为文本tokens，这意味着可以在不大幅度改变模型结构的情况下增加参数数量并使用更多数据进行训练。</p>
<p><strong>ViT-22B 的结构和特点</strong></p>
<p>ViT-22B基本上遵循了 ViT 的架构，但在 <em>规模</em> 方面有着根本的不同。</p>
<ul>
<li><p><strong>巨大的模型尺寸：</strong> 拥有220亿个参数的ViT-22B与ViT-Base（8600万个）、ViT-Large（3.7亿个）和ViT-Huge（6.32亿个）相比具有压倒性的规模优势。这意味着模型可以捕捉到更加复杂和细微的图像特征，并内含更多的知识。</p></li>
<li><p><strong>超大规模数据集：</strong> ViT-22B是在数十亿张图像组成的 <em>非公开</em> 数据集（如JFT-4B）上训练的。如此大量的数据对于模型实现最佳的泛化性能和全面的学习各种图像分布至关重要。</p></li>
<li><p><strong>提升的性能：</strong> ViT-22B在图像分类、物体检测、图像分割等多个视觉基准测试中展示出了超越现有任何模型的最优性能（SOTA, State-Of-The-Art）。这清楚地表明了模型尺寸和数据量对性能的积极影响。</p></li>
</ul>
<p><strong>ViT-22B 训练的困难和启示</strong></p>
<p>训练像 ViT-22B 这样的超大规模模型对于一般的研究环境来说几乎是不可能的。它需要数百到数千个GPU或TPU这样的高昂特殊硬件，训练时间也可能长达几周甚至几个月。此外，存储和处理大量数据所需的基础设施建设也是一个巨大的挑战。</p>
<p>ViT-22B 的出现证实了 ViT 架构的可扩展性，但同时也提出了关于 <em>效率</em> 的思考。随着模型尺寸的增长，性能会提高，但是训练和推理所需的计算资源和能耗也会呈几何级数增加。因此，未来研究将朝着在保持性能的同时提高效率的方向进行。 Meta AI （Facebook AI Research，FAIR）团队提出的MAE（Masked Autoencoder）是一种利用无标签的大规模图像数据集来学习强大图像表示的<em>自监督学习</em>方法。基于ViT的MAE通过随机遮挡图像的大部分并训练模型恢复被遮挡的部分。MAE v3是MAE的最新版本，通过多种改进进一步提高了性能和效率。</p>
<p><strong>MAE的工作原理</strong></p>
<p>MAE的核心思想是让模型像人类解决“填空”问题一样，只需看到图像的一部分信息就能理解和恢复整个图像。</p>
<ol type="1">
<li><p><strong>输入图像的随机遮挡：</strong> 随机遮挡输入图像的大部分（例如75%）。此时，以图像补丁为单位进行遮挡。</p></li>
<li><p><strong>编码（Encoding）：</strong> 只将未被遮挡的补丁，即<em>可见</em>的补丁输入到ViT编码器中，提取特征向量。</p></li>
<li><p><strong>解码（Decoding）：</strong> 使用编码器的输出（可见补丁的特征）和<em>被遮挡的补丁</em>的信息共同恢复原始图像。此时，解码器采用轻量级Transformer块以提高计算效率。</p></li>
<li><p><strong>恢复损失（Reconstruction Loss）：</strong> 计算恢复图像与原图之间的像素级差异（例如均方误差，MSE），并训练模型（编码器和解码器）以最小化这种差异。</p></li>
</ol>
<p><strong>MAE v3的结构改进</strong></p>
<p>MAE v3通过以下主要改进相比之前版本实现了更好的性能和效率：</p>
<ol type="1">
<li><p><strong>改进的遮挡策略：</strong> 초기MAE简单地随机遮挡补丁，而MAE v3使用更为精致的遮挡策略。例如，图像的<em>有意义区域</em>(物体边界等)可以被更好地保留，或者<em>不同大小</em>的补丁可以被遮挡。</p></li>
<li><p><strong>优化的编码器-解码器结构：</strong></p></li>
</ol>
<ul>
<li>编码器：使用更大的ViT模型（如ViT-Large、ViT-Huge）从可见补丁中提取更多丰富的特征。</li>
<li>解码器：采用浅薄的Transformer块，以保持计算效率同时提高恢复性能。</li>
</ul>
<ol start="3" type="1">
<li><strong>规模扩展：</strong> 模型规模从ViT-L/16、ViT-H/16扩展到ViT-g/14（参数25亿个）。</li>
</ol>
<p><strong>MAE的优势和意义</strong></p>
<p>MAE具有以下优势，在自监督学习领域受到关注：</p>
<ol type="1">
<li><p><strong>无需标签的学习：</strong> MAE可以利用无标签的大规模图像数据集进行预训练。这节省了手动创建标签的成本和时间，并允许使用更多的数据。</p></li>
<li><p><strong>强大的表示学习：</strong> MAE通过恢复被遮挡的图像，学会理解图像的<em>结构</em>、<em>意义</em>和<em>上下文</em>。这种能力有助于在图像分类、物体检测、分割等多种下游任务中实现良好的性能。</p></li>
<li><p><strong>迁移学习的便利性：</strong> 用MAE预训练的模型可以通过微调应用于各种任务。这使得即使在标签不足的任务中也能获得良好的性能。 MAE通过“填空”这一直觉的想法，提出了一种即使没有标签也能学习强大图像表示的有效方法。 MAE v3进一步发展了这种MAE的优势，实现了更高的性能和效率，并推动着自监督学习研究的进展。</p></li>
</ol>
</section>
</section>
<section id="clip多模态学习的里程碑" class="level2">
<h2 class="anchored" data-anchor-id="clip多模态学习的里程碑">10.6 CLIP：多模态学习的里程碑</h2>
<p>2021年，OpenAI通过论文“Learning Transferable Visual Models From Natural Language Supervision”发布了 <strong>CLIP（Contrastive Language-Image Pre-training）</strong> 模型。CLIP通过学习将图像和文本这两种不同的模态表示到<em>一个共享空间</em>中，从而在多模态学习领域带来了创新性的进展。</p>
<section id="clip的基本结构双编码器dual-encoder" class="level3">
<h3 class="anchored" data-anchor-id="clip的基本结构双编码器dual-encoder">10.6.1 CLIP的基本结构：双编码器（Dual Encoder）</h3>
<p>CLIP的核心是<strong>图像编码器（Image Encoder）</strong>和<strong>文本编码器（Text Encoder）</strong>，这两种独立的编码器构成了<em>双编码器</em>结构。</p>
<ul>
<li><strong>图像编码器：</strong> 将输入图像转换为固定维度的向量（图像嵌入）。</li>
<li><strong>文本编码器：</strong> 将输入文本（图像描述）转换为与图像编码器<em>相同维度</em>的向量（文本嵌入）。</li>
</ul>
<p>这两个编码器通过<em>对比学习</em>进行训练。</p>
<p><strong>CLIP 训练的核心：对比学习（Contrastive Learning）</strong></p>
<p>CLIP 训练的核心是使用<em>大规模图像-文本对数据集</em>的<em>对比学习</em>。</p>
<ol type="1">
<li><strong>数据：</strong> 使用互联网上收集的数百万个（图像，文本）对构成的数据集。每对中的文本是对应图像的描述。</li>
<li><strong>目标：</strong> 训练编码器，使得<em>同一对</em>中的图像和文本嵌入彼此<em>接近</em>，而<em>不同对</em>中的图像和文本嵌入彼此<em>远离</em>。</li>
<li><strong>损失函数：</strong> 使用对比损失（contrastive loss）函数。该损失函数以提高<em>同一对</em>嵌入之间的相似度（例如余弦相似度）的同时降低<em>不同对</em>嵌入之间的相似度为目标。（参考10.4节对比学习）</li>
</ol>
<p><strong>代码示例</strong></p>
<div id="cell-78" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb91"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb91-1"><a href="#cb91-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb91-2"><a href="#cb91-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb91-3"><a href="#cb91-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb91-4"><a href="#cb91-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb91-5"><a href="#cb91-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-6"><a href="#cb91-6" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> CLIP(nn.Module):</span>
<span id="cb91-7"><a href="#cb91-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, image_encoder, text_encoder, embed_dim):</span>
<span id="cb91-8"><a href="#cb91-8" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb91-9"><a href="#cb91-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.image_encoder <span class="op">=</span> image_encoder</span>
<span id="cb91-10"><a href="#cb91-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.text_encoder <span class="op">=</span> text_encoder</span>
<span id="cb91-11"><a href="#cb91-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.image_projection <span class="op">=</span> nn.Linear(image_encoder.output_dim, embed_dim)</span>
<span id="cb91-12"><a href="#cb91-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.text_projection <span class="op">=</span> nn.Linear(text_encoder.output_dim, embed_dim)</span>
<span id="cb91-13"><a href="#cb91-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.logit_scale <span class="op">=</span> nn.Parameter(torch.ones([]) <span class="op">*</span> np.log(<span class="dv">1</span> <span class="op">/</span> <span class="fl">0.07</span>)) <span class="co"># Learnable scale parameter</span></span>
<span id="cb91-14"><a href="#cb91-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-15"><a href="#cb91-15" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, images, texts):</span>
<span id="cb91-16"><a href="#cb91-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 1. Image encoding</span></span>
<span id="cb91-17"><a href="#cb91-17" aria-hidden="true" tabindex="-1"></a>        image_features <span class="op">=</span> <span class="va">self</span>.image_encoder(images)  <span class="co"># [batch_size, image_encoder.output_dim]</span></span>
<span id="cb91-18"><a href="#cb91-18" aria-hidden="true" tabindex="-1"></a>        image_embeddings <span class="op">=</span> <span class="va">self</span>.image_projection(image_features)  <span class="co"># [batch_size, embed_dim]</span></span>
<span id="cb91-19"><a href="#cb91-19" aria-hidden="true" tabindex="-1"></a>        image_embeddings <span class="op">=</span> F.normalize(image_embeddings, dim<span class="op">=-</span><span class="dv">1</span>) <span class="co"># L2 normalization</span></span>
<span id="cb91-20"><a href="#cb91-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-21"><a href="#cb91-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 2. Text encoding</span></span>
<span id="cb91-22"><a href="#cb91-22" aria-hidden="true" tabindex="-1"></a>        text_features <span class="op">=</span> <span class="va">self</span>.text_encoder(texts)   <span class="co"># [batch_size, text_encoder.output_dim]</span></span>
<span id="cb91-23"><a href="#cb91-23" aria-hidden="true" tabindex="-1"></a>        text_embeddings <span class="op">=</span> <span class="va">self</span>.text_projection(text_features)    <span class="co"># [batch_size, embed_dim]</span></span>
<span id="cb91-24"><a href="#cb91-24" aria-hidden="true" tabindex="-1"></a>        text_embeddings <span class="op">=</span> F.normalize(text_embeddings, dim<span class="op">=-</span><span class="dv">1</span>) <span class="co"># L2 normalization</span></span>
<span id="cb91-25"><a href="#cb91-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-26"><a href="#cb91-26" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 3. Similarity calculation</span></span>
<span id="cb91-27"><a href="#cb91-27" aria-hidden="true" tabindex="-1"></a>        logits_per_image <span class="op">=</span> <span class="va">self</span>.logit_scale.exp() <span class="op">*</span> image_embeddings <span class="op">@</span> text_embeddings.t()  <span class="co"># [batch_size, batch_size]</span></span>
<span id="cb91-28"><a href="#cb91-28" aria-hidden="true" tabindex="-1"></a>        logits_per_text <span class="op">=</span> logits_per_image.t() <span class="co"># [batch_size, batch_size]</span></span>
<span id="cb91-29"><a href="#cb91-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-30"><a href="#cb91-30" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> logits_per_image, logits_per_text</span>
<span id="cb91-31"><a href="#cb91-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-32"><a href="#cb91-32" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> contrastive_loss(logits_per_image, logits_per_text):</span>
<span id="cb91-33"><a href="#cb91-33" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb91-34"><a href="#cb91-34" aria-hidden="true" tabindex="-1"></a><span class="co">    Calculates the Contrastive Loss</span></span>
<span id="cb91-35"><a href="#cb91-35" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb91-36"><a href="#cb91-36" aria-hidden="true" tabindex="-1"></a>    batch_size <span class="op">=</span> logits_per_image.shape[<span class="dv">0</span>]</span>
<span id="cb91-37"><a href="#cb91-37" aria-hidden="true" tabindex="-1"></a>    labels <span class="op">=</span> torch.arange(batch_size).to(logits_per_image.device) <span class="co"># Correct labels (diagonal: same pair)</span></span>
<span id="cb91-38"><a href="#cb91-38" aria-hidden="true" tabindex="-1"></a>    loss_i <span class="op">=</span> F.cross_entropy(logits_per_image, labels)  <span class="co"># Loss based on image</span></span>
<span id="cb91-39"><a href="#cb91-39" aria-hidden="true" tabindex="-1"></a>    loss_t <span class="op">=</span> F.cross_entropy(logits_per_text, labels)   <span class="co"># Loss based on text</span></span>
<span id="cb91-40"><a href="#cb91-40" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (loss_i <span class="op">+</span> loss_t) <span class="op">/</span> <span class="dv">2</span>  <span class="co"># Average loss</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="图像编码器" class="level3">
<h3 class="anchored" data-anchor-id="图像编码器">10.6.2 图像编码器</h3>
<p>CLIP的图像编码器将输入图像转换为固定维度的嵌入向量。初始CLIP论文中，实验了ResNet和ViT（Vision Transformer）。</p>
<ul>
<li><strong>基于ResNet的编码器</strong>：使用传统的CNN基于图像分类模型（例如ResNet-50、ResNet-101）。</li>
<li><strong>基于ViT的编码器</strong>：将ViT（Vision Transformer）用作图像编码器（参考10.5节）。ViT通过将图像分割为patch，并将每个patch作为Transformer的输入。</li>
</ul>
<p>实验结果表明，基于ViT的编码器比基于ResNet的编码器具有更好的性能。特别是，当模型和数据大小增加时，ViT的性能提升更加显著。</p>
</section>
<section id="文本编码器" class="level3">
<h3 class="anchored" data-anchor-id="文本编码器">10.6.3 文本编码器</h3>
<p>CLIP的文本编码器将输入文本转换为与图像编码器相同维度的嵌入向量。初始CLIP论文中，使用了基于Transformer的文本编码器。</p>
<ul>
<li>文本编码器使用字节对编码（BPE）来分词，并将每个词嵌入。</li>
<li>通过堆叠多层Transformer块来捕捉文本的上下文信息，最终生成一个代表整个文本的单一嵌入向量。</li>
</ul>
</section>
<section id="零样本传递的机制" class="level3">
<h3 class="anchored">10.6.4 零样本传递的机制</h3>
<p>CLIP的一个主要特征是其在各种图像分类任务中表现出色的<strong>零样本（zero-shot）传递</strong>能力，无需额外的微调。</p>
<p><strong>零样本传递的原因</strong></p>
<p>CLIP通过对大规模图像-文本对数据集进行对比学习，在同一个语义空间中表示图像和文本。因此，CLIP能够理解图像和文本之间的语义相关性。</p>
<p><strong>零样本分类过程</strong></p>
<ol type="1">
<li>准备要分类的类别（class）的文本描述。例如，对于CIFAR-10数据集，准备诸如“一张猫的照片”、“一张狗的照片”等文本描述。</li>
<li>使用文本编码器将每个文本描述嵌入。</li>
<li>使用图像编码器将输入图像嵌入。</li>
<li>计算图像嵌入和每个文本嵌入之间的相似度（例如，余弦相似度）。</li>
<li>选择最相似的文本描述对应的类别作为图像的预测类别。</li>
</ol>
<p><strong>零样本传递的意义</strong></p>
<p>零样本传递意味着模型可以在没有任何额外训练或微调的情况下应用于新的、从未见过的类别或任务。这与传统的监督学习方法不同，后者需要特定任务的标记数据。零样本传递的核心是灵活性。例如，如果图像分类模型仅使用“猫”和“狗”类别的数据进行训练，但提供了“长颈鹿”或“大象”的自然语言描述，即使没有这些类别的训练数据，模型也可以正确地对新类别的图像进行分类。这种能力是零样本传递最大的优势。 此外，Zero-shot迁移提供了通用性。不仅限于图像分类，还可以应用于图像搜索、图像字幕、对象检测（object detection）、视觉问答（Visual Question Answering, VQA）等多种多模态任务。例如，在图像搜索系统中输入“红色跑车”这样的文本查询，模型就可以在数据库中找到对应的图像。这是因为模型理解了图像和文本之间的语义联系。这样，一個模型可以应用于多种任务，这大大节省了时间和资源，并提高了人工智能系统的利用率。</p>
<p><strong>CLIP的影响</strong></p>
<p>CLIP通过零样本迁移能力提出了多模态学习的新可能性。在此基础上，后续研究也在不断进行，像DALL-E、Stable Diffusion这样的图像生成模型，以及GPT-4V这样的大规模多模态模型的开发，都受到了CLIP思想的重大影响。</p>
<div class="callout callout-style-default callout-note callout-titled" title="点击查看内容（深度探索：对比学习和CLIP）">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
点击查看内容（深度探索：对比学习和CLIP）
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<section id="深度潜水对比学习和clip" class="level2 callout-body-container callout-body">
<h2 class="anchored" data-anchor-id="深度潜水对比学习和clip">深度潜水：对比学习和CLIP</h2>
<p>对比学习是一种强大的方法，可以在无标签数据中学习表示。特别是在连接不同模态（如图像和文本）的多模态学习中，它表现出色的成绩。本深度潜水中，我们将深入分析对比学习的基本原理、各种方法，以及基于对比学习连接图像和文本的开创性模型CLIP（对比语言-图像预训练）。</p>
<section id="对比学习的基本原理" class="level3">
<h3 class="anchored" data-anchor-id="对比学习的基本原理">1. 对比学习的基本原理</h3>
<p>对比学习的核心思想是<strong>学习表示，使得相似的样本对在嵌入空间中靠近，而不相似的样本对则远离</strong>。</p>
<ul>
<li><strong>锚点（Anchor）</strong>：参考样本。</li>
<li><strong>正样本（Positive Sample）</strong>：与锚点在语义上相似的样本。（例如，同一图像的不同增强，同一句子的不同翻译）</li>
<li><strong>负样本（Negative Sample）</strong>：与锚点在语义上不同的样本。</li>
</ul>
<p>对比学习通常遵循以下步骤：</p>
<ol type="1">
<li><strong>数据增强（Data Augmentation）</strong>：应用各种数据增强技术来生成锚点和正样本。（例如，对图像进行随机裁剪、色彩抖动、旋转等）</li>
<li><strong>编码（Encoding）</strong>：将锚点、正样本和负样本分别通过编码器转换为嵌入向量。</li>
<li><strong>对比损失（Contrastive Loss）</strong>：使用对比损失函数来训练编码器，使得正样本对的嵌入靠近，而负样本对的嵌入远离。</li>
</ol>
</section>
<section id="对比损失函数" class="level3">
<h3 class="anchored" data-anchor-id="对比损失函数">2. 对比损失函数</h3>
<p>已经提出了多种对比损失函数，以下是一些代表性的例子：</p>
<ul>
<li><p><strong>InfoNCE损失（Noise Contrastive Estimation）</strong>：与交叉熵损失类似，它最大化了正样本对的softmax概率。</p>
<p><span class="math inline">\(L = -\log \frac{\exp(\text{sim}(z_i, z_j) / \tau)}{\sum_{k=1}^{2N} \mathbb{1}_{[k \neq i]} \exp(\text{sim}(z_i, z_k) / \tau)}\)</span></p>
<ul>
<li><span class="math inline">\(z_i\)</span>：锚点的嵌入</li>
<li><span class="math inline">\(z_j\)</span>：正样本的嵌入</li>
<li><span class="math inline">\(z_k\)</span>：负样本的嵌入（k ≠ i）</li>
<li><span class="math inline">\(\text{sim}(\cdot, \cdot)\)</span>：相似度函数（例如，余弦相似度）</li>
<li><span class="math inline">\(\tau\)</span>：温度参数（控制相似度分布）</li>
<li><span class="math inline">\(N\)</span>：mini-batch大小</li>
</ul></li>
<li><p><strong>NT-Xent损失（Normalized Temperature-scaled Cross Entropy Loss）</strong>：InfoNCE损失的变体，由SimCLR论文提出。</p></li>
<li><p><strong>三元组损失（Triplet Loss）</strong>：使用锚点、正样本和负样本，训练模型使得锚点与正样本之间的距离小于锚点与负样本之间的距离。</p>
<p><span class="math inline">\(L = \max(0, d(a, p) - d(a, n) + m)\)</span></p></li>
<li><p><span class="math inline">\(a\)</span>: 锚点</p>
<ul>
<li><span class="math inline">\(p\)</span>: 正样本</li>
<li><span class="math inline">\(n\)</span>: 负样本</li>
<li><span class="math inline">\(d(\cdot, \cdot)\)</span>: 距离函数 (例如，欧几里得距离)</li>
<li><span class="math inline">\(m\)</span>: 边距（决定距离的大小）</li>
</ul></li>
</ul>
</section>
<section id="对比学习方法论" class="level3">
<h3 class="anchored" data-anchor-id="对比学习方法论">3. 对比学习方法论</h3>
<ul>
<li><strong>SimCLR (简单视觉表示对比学习框架)：</strong> 使用数据增强、较大的批量大小和投影头（MLP）来学习图像表示。</li>
<li><strong>MoCo (动量对比)：</strong> 使用动量编码器来稳定负样本，即使在没有较大批量大小的情况下也能实现良好的性能。</li>
<li><strong>SwAV (多视图之间的任务交换)：</strong> 通过在线聚类来学习表示，而无需明确定义正负样本。</li>
<li><strong>BYOL (自举潜在变量)：</strong> 无需负样本，通过目标网络和在线网络之间的预测来进行训练。</li>
</ul>
</section>
<section id="clip对比语言-图像预训练" class="level3">
<h3 class="anchored" data-anchor-id="clip对比语言-图像预训练">4. CLIP（对比语言-图像预训练）</h3>
<p>CLIP是OpenAI开发的模型，它使用对比学习来学习强大的多模态表示，以连接图像和文本。</p>
<section id="clip的训练" class="level4">
<h4 class="anchored" data-anchor-id="clip的训练">4.1 CLIP的训练</h4>
<ul>
<li><strong>数据：</strong> 大规模图像-文本对数据集（4亿个）</li>
<li><strong>模型：</strong>
<ul>
<li><strong>图像编码器：</strong> 从图像中提取特征向量（例如，ResNet，ViT）</li>
<li><strong>文本编码器：</strong> 从文本中提取特征向量（例如，Transformer）</li>
</ul></li>
<li><strong>训练：</strong></li>
</ul>
<ol type="1">
<li>分别对图像和文本进行编码，以获得嵌入向量。</li>
<li>使用对比损失（InfoNCE）来提高同一对（正对）的图像-文本嵌入之间的余弦相似度，并降低不同对（负对）的嵌入之间的余弦相似度。</li>
</ol>
<pre><code>*   在批处理中，每个图像都有一个正文本和（N-1）个负文本。
*   同样，每个文本都有一个正图像和（N-1）个负图像。</code></pre>
</section>
<section id="clip的特点" class="level4">
<h4 class="anchored" data-anchor-id="clip的特点">4.2 CLIP的特点</h4>
<ul>
<li><strong>零样本学习：</strong> 无需额外的微调，使用学习到的图像-文本表示即可执行新任务（例如图像分类、图像搜索）。
<ul>
<li>零样本图像分类示例：
<ol type="1">
<li>使用文本表示分类类别名称（例如，“一张猫的照片”，“一张狗的照片”）。</li>
<li>将每个文本编码为Text Encoder。</li>
<li>将给定的图像编码为Image Encoder。</li>
<li>计算图像嵌入和每个文本嵌入之间的余弦相似度。</li>
<li>使用具有最高相似度的文本对应的类别来分类图像。</li>
</ol></li>
</ul></li>
<li><strong>强大的表示学习：</strong> 学习可迁移的通用图像/文本表示，以适用于各种任务。</li>
<li><strong>图像分类：</strong> 零次元分类，少样本分类。</li>
<li><strong>图像检索：</strong> 使用文本查询进行图像搜索。</li>
<li><strong>图像生成：</strong> DALL-E，Stable Diffusion等基于文本的图像生成模型的基础技术。</li>
<li><strong>视觉问答（VQA）：</strong> 输入图像和问题文本来生成答案。</li>
<li><strong>物体检测：</strong> 将CLIP集成到物体检测模型中以执行开放词汇物体检测。</li>
</ul>
</section>
</section>
<section id="对比学习和clip的局限性及未来研究方向" class="level3">
<h3 class="anchored" data-anchor-id="对比学习和clip的局限性及未来研究方向">5. 对比学习和CLIP的局限性及未来研究方向</h3>
<ul>
<li><strong>数据增强依赖性：</strong> 对比学习对数据增强技术很敏感。需要研究哪种增强方法是有效的。</li>
<li><strong>负样本选择偏差：</strong> 负样本的选择会影响学习结果，例如hard negative mining等技术正在被研究。</li>
<li><strong>模式崩溃：</strong> 所有样本都收敛到一个表达的现象。</li>
<li><strong>细粒度理解：</strong> CLIP在图像和文本之间的粗粒度对齐方面表现良好，但是在细粒度理解（例如，图像中对象之间的关系，文本的微妙细节）方面可能不足。</li>
<li><strong>计算成本：</strong> 需要大规模数据集和大的批处理大小。</li>
</ul>
</section>
<section id="结论-1" class="level3">
<h3 class="anchored" data-anchor-id="结论-1">6. 结论</h3>
<p>对比学习是一种利用无标签数据来学习强大表达的有效方法。特别是CLIP成功地将对比学习应用于多模态学习，开辟了图像和文本连接的新天地。未来，对比学习和CLIP很可能在各个领域得到广泛应用。</p>
<p><strong>参考文献：</strong> * 陈天奇、科恩布利斯、诺鲁兹、欣顿（2020）。一种简单的视觉表示对比学习框架。<em>机器学习国际会议</em>。PMLR。 * 拉德福、金正文、霍拉西、拉梅什、高戈、阿加瓦尔等人（2021）。从自然语言监督中学习可迁移的视觉模型。<em>机器学习国际会议</em>。PMLR。 * 何凯明、樊航、吴云峰、谢世光、吉尔希克（2020）。无监督视觉表示学习中的动量对比。在<em>IEEE/CVF计算机视觉与模式识别会议论文集</em>（第9729-9738页）。 * 格里尔、斯特鲁布、阿尔切、塔勒克、里什蒙、布查茨卡娅等人（2020）。自举你的潜在表示：一种新的自监督学习方法。<em>神经信息处理系统进展</em>，33，21271-21284。</p>
</section>
</section>
</div>
</div>
</section>
</section>
<section id="练习题" class="level2">
<h2 class="anchored" data-anchor-id="练习题">练习题</h2>
<p><strong>基本问题</strong></p>
<ol type="1">
<li>解释什么是多模态数据，并提供三个或以上的多模态数据例子。</li>
<li>解释联合表示（Joint Representation）和协调表示（Coordinated Representation）的区别，并比较各自的优缺点。</li>
<li>定义图像字幕任务（Image Captioning），并解释用于解决此任务的深度学习模型的一般结构（编码器-解码器）。</li>
</ol>
<p><strong>应用问题</strong></p>
<ol type="1">
<li>为解决视觉问答任务（Visual Question Answering，VQA），设计一个简单的模型结构（使用块图等），该模型接受图像和问题文本作为输入并生成答案。说明各个组件的作用。</li>
<li>解释CLIP模型的学习方法，并比较其与传统图像-文本监督学习方法的优势。</li>
<li>使用Hugging Face Transformers库，编写一段代码以生成图像相关的文本字幕（例如使用<code>blip-image-captioning-base</code>模型）。</li>
</ol>
<p><strong>深化问题</strong></p>
<ol type="1">
<li>研究多模态融合（Multimodal Fusion）的各种方法（early fusion、late fusion、hybrid fusion），并解释每种方法的优缺点以及在哪些情况下适用。</li>
<li>解释跨模式注意力机制（Cross-Modal Attention）的工作原理，并结合具体例子（例如VQA、图像字幕任务）说明其在多模态学习中的作用。</li>
<li>研究基于文本描述生成图像的模型（例如DALL-E、Stable Diffusion）的工作原理，讨论这些模型对社会可能产生的积极和消极影响。（提供想法并进行讨论）</li>
</ol>
<div class="callout callout-style-default callout-note callout-titled" title="点击查看内容（练习题答案）">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
点击查看内容（练习题答案）
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<section id="练习题答案" class="level2 callout-body-container callout-body">
<h2 class="anchored" data-anchor-id="练习题答案">练习题答案</h2>
<section id="基本问题" class="level3">
<h3 class="anchored" data-anchor-id="基本问题">基本问题</h3>
<ol type="1">
<li><strong>多模态数据：</strong> 指两个或两个以上不同类型的数据（模态）组合在一起的数据。例如：
<ul>
<li>图像和文本标题</li>
<li>视频和音频轨道</li>
<li>传感器数据（例如，加速度计，陀螺仪）和文本描述</li>
</ul></li>
<li><strong>联合表示 vs 协调表示：</strong>
<ul>
<li><strong>联合表示：</strong> 将多个模态的信息表示在一个统一的向量空间中。
<ul>
<li>优点：可以直接建模不同模态之间的相关性。</li>
<li>缺点：一个模态可能会主导其他模态。</li>
</ul></li>
<li><strong>协调表示：</strong> 每个模态分别表示在不同的向量空间中，但使这些空间相互关联（例如，通过相似性约束）。
<ul>
<li>优点：保持每个模态的独特特征，同时允许它们之间的交互。</li>
<li>缺点：与联合表示相比，模态之间的交互建模更为间接。</li>
</ul></li>
</ul></li>
<li><strong>图像字幕生成：</strong> 指根据给定的图像生成文本描述的任务。
<ul>
<li><strong>常见结构（编码器-解码器）：</strong>
<ul>
<li><strong>编码器：</strong> 提取图像特征（通常使用CNN）。</li>
<li><strong>解码器：</strong> 根据编码器提取的图像特征和之前生成的词语，预测下一个词语（通常使用RNN或Transformer）。可以通过注意力机制使模型关注图像的特定区域。</li>
</ul></li>
</ul></li>
</ol>
</section>
<section id="应用问题" class="level3">
<h3 class="anchored" data-anchor-id="应用问题">应用问题</h3>
<ol type="1">
<li><p><strong>VQA模型结构：</strong></p>
<pre class="mermaid"><code>graph LR
    subgraph VQA Model
        A[图像] --&gt; B(图像编码器 - CNN)
        C[问题文本] --&gt; D(文本编码器 - RNN/Transformer)
        B --&gt; E(融合模块)
        D --&gt; E
        E --&gt; F(解码器 - RNN/Transformer)
        F --&gt; G(答案)
    end</code></pre>
<ul>
<li><strong>图像编码器（CNN）：</strong> 输入图像并提取特征向量。</li>
<li><strong>文本编码器（RNN/Transformer）：</strong> 输入问题文本并提取特征向量。</li>
<li><strong>融合模块：</strong> 结合图像特征向量和文本特征向量（例如，连接、元素级乘法、跨模态注意力）。</li>
<li><strong>解码器（RNN/Transformer）：</strong> 根据融合的特征向量生成答案。</li>
</ul></li>
<li><p><strong>CLIP训练方式及优势：</strong></p>
<ul>
<li><strong>训练方式：</strong> CLIP使用大规模图像-文本对数据集，分别对图像和文本进行编码，并通过对比损失使得同一对中的图像和文本嵌入接近，而不同对中的嵌入远离。</li>
<li><strong>优势：</strong>
<ul>
<li><strong>零样本学习：</strong> 无需额外的微调就可以应用于新的任务（例如，图像分类）。</li>
<li><strong>强大的表示学习：</strong> 学习可迁移的、通用的图像/文本表示，可以应用于多种任务。</li>
<li><strong>数据效率：</strong> 可以利用未标记的图像-文本对数据。</li>
</ul></li>
</ul></li>
<li><p><strong>Hugging Face Transformers 图片字幕代码：</strong></p></li>
</ol>
<div class="sourceCode" id="cb94"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb94-1"><a href="#cb94-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> pipeline</span>
<span id="cb94-2"><a href="#cb94-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-3"><a href="#cb94-3" aria-hidden="true" tabindex="-1"></a>captioner <span class="op">=</span> pipeline(<span class="st">"image-to-text"</span>, model<span class="op">=</span><span class="st">"nlpconnect/vit-gpt2-image-captioning"</span>) <span class="co"># 或 "blip-image-captioning-base"</span></span>
<span id="cb94-4"><a href="#cb94-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-5"><a href="#cb94-5" aria-hidden="true" tabindex="-1"></a>image_path <span class="op">=</span> <span class="st">"path/to/your/image.jpg"</span>  <span class="co"># 图片文件路径</span></span>
<span id="cb94-6"><a href="#cb94-6" aria-hidden="true" tabindex="-1"></a>caption <span class="op">=</span> captioner(image_path)[<span class="dv">0</span>][<span class="st">'generated_text'</span>]</span>
<span id="cb94-7"><a href="#cb94-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(caption)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="进阶问题" class="level3">
<h3 class="anchored" data-anchor-id="进阶问题">进阶问题</h3>
<ol type="1">
<li><p><strong>多模态融合方法：</strong></p>
<ul>
<li><strong>Early Fusion：</strong> 在输入阶段结合模态（例如，通道拼接）。
<ul>
<li>优点：可以捕获模态之间的低级交互。</li>
<li>缺点：可能导致维度非常大，并且每个模态的独特特征可能会被稀释。</li>
</ul></li>
<li><strong>Late Fusion：</strong>独立处理每个模态，然后在最后阶段结合结果（例如，平均、投票）。
<ul>
<li>优点：可以充分利用每个模态的特征，实现简单。</li>
<li>缺点：难以捕获模态之间的低级交互。</li>
</ul></li>
<li><strong>Hybrid Fusion：</strong>混合Early Fusion和Late Fusion的方法，在不同的层次上进行融合。
<ul>
<li>优点：可以同时利用Early Fusion和Late Fusion的优势。</li>
<li>缺点：模型可能变得复杂。</li>
</ul></li>
<li><strong>适用情况：</strong>
<ul>
<li><strong>Early Fusion：</strong>模态之间的紧密交互对于任务非常重要（例如，视频和音频的同步）。</li>
<li><strong>Late Fusion：</strong>每个模态独立具有意义（例如，图片标记和文本描述）。</li>
<li><strong>Hybrid Fusion：</strong>在复杂任务中需要捕获不同层次的交互。</li>
</ul></li>
</ul></li>
<li><p><strong>跨模态注意力：</strong></p>
<ul>
<li><strong>工作原理：</strong>使用一个模态的查询(query)计算另一个模态的键(key)关于注意力权重，并使用这些权重对另一个模态的值(value)进行加权求和，从而生成新的表示。</li>
<li><strong>作用：</strong>
<ul>
<li><strong>VQA：</strong>决定问题文本的每个词(query)应该关注图片的哪个区域(key, value)。</li>
<li><strong>Image Captioning：</strong>决定生成的每个词(query)与图片的哪个区域(key, value)相关。</li>
</ul></li>
</ul></li>
<li><p><strong>基于文本的图片生成模型（DALL-E、Stable Diffusion等）：</strong></p>
<ul>
<li><strong>工作原理（简化）</strong>：
<ul>
<li><strong>DALL-E（Transformer基础）：</strong>对文本和图像进行令牌化，并使用Transformer建模当给定文本令牌序列时，图像令牌序列出现的概率。</li>
<li><strong>Stable Diffusion（扩散模型基础）：</strong>学习一个正向过程，即逐渐向图像添加噪声，以及一个反向过程，即从噪声中恢复图像。文本信息作为条件在反向过程中提供，以控制生成的图像。</li>
</ul></li>
</ul></li>
</ol>
<ul>
<li><strong>正面影響：</strong>
<ul>
<li><strong>增強創造力</strong>：視覺化新想法，支持藝術作品創作。</li>
<li><strong>提高內容生產效率</strong>：設計、廣告、教育材料的自動化製作。</li>
<li><strong>改善無障礙環境</strong>：為視覺障礙者生成圖片描述。</li>
</ul></li>
<li><strong>負面影響：</strong>
<ul>
<li><strong>Deepfake、假信息傳播</strong>：扭曲現實、傷害名譽。</li>
<li><strong>侵犯版權</strong>：未經授權使用和修改既有圖像。</li>
<li><strong>減少工作機會</strong>：設計師、插畫家等職業被取代。</li>
<li><strong>偏見和歧視</strong>：反映學習數據的偏見，生成針對特定群體的歧視性圖片。</li>
</ul></li>
</ul>
</section>
</section>
</div>
</div>
</section>
<section id="参考资料" class="level2">
<h2 class="anchored" data-anchor-id="参考资料">参考资料</h2>
<ol type="1">
<li><p><strong>CLIP (从自然语言监督中学习可迁移的视觉模型)</strong>：一种将图像和文本连接起来的多模态表示学习方法，CLIP原始论文。 <a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://arxiv.org/abs/2103.00020">https://arxiv.org/abs/2103.00020</a></p></li>
<li><p><strong>ViT (一张图片值16x16个词：大规模图像识别中的Transformer)</strong>：仅使用Transformer结构而无需CNN，在图像分类中表现出色的ViT原始论文。 <a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://arxiv.org/abs/2010.11929">https://arxiv.org/abs/2010.11929</a></p></li>
<li><p><strong>DALL-E (零样本文本到图像生成)</strong>：基于文本描述生成图像的DALL-E模型论文。 <a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://arxiv.org/abs/2102.12092">https://arxiv.org/abs/2102.12092</a></p></li>
<li><p><strong>MAE (掩蔽自动编码器是可扩展的视觉学习者)</strong>：通过遮挡图像的一部分并恢复来学习视觉表示的MAE论文。 <a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://arxiv.org/abs/2111.06377">https://arxiv.org/abs/2111.06377</a></p></li>
<li><p><strong>视觉问答 (VQA)</strong>：早期VQA研究之一，提出VQA数据集和基线模型。 <a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://arxiv.org/abs/1505.00468">https://arxiv.org/abs/1505.00468</a></p></li>
<li><p><strong>展示、关注和讲述 (具有视觉注意力的神经图像字幕生成)</strong>：首次将注意力机制引入图像字幕生成的论文。 <a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://arxiv.org/abs/1502.03044">https://arxiv.org/abs/1502.03044</a></p></li>
<li><p><strong>多模态机器学习：调查和分类法</strong>：对多模态机器学习的全面综述论文。 <a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://arxiv.org/abs/1705.09406">https://arxiv.org/abs/1705.09406</a></p></li>
<li><p><strong>多模态深度学习教程，Jiquan Ngiam</strong>：NeurIPS 2011的多模态深度学习教程（视频）。 <a href="https://www.google.com/search?q=https://www.youtube.com/watch%3Fv%3DcR_ACqfF-bY%26list%3DPL_45CaSOtPzL-HWxMcnr02KvmP9Gq-xdb">https://www.youtube.com/watch?v=cR_ACqfF-bY&amp;list=PL_45CaSOtPzL-HWxMcnr02KvmP9Gq-xdb</a></p></li>
<li><p><strong>CMU Multimodal Machine Learning Course (11-777, Spring 2023), Louis-Philippe Morency:</strong> 卡内基梅隆大学多模态机器学习课程资料。 <a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://cmu-multicomp-lab.github.io/mmml-course/spring2023/">https://cmu-multicomp-lab.github.io/mmml-course/spring2023/</a></p></li>
<li><p><strong>A Comprehensive Survey on Deep Multimodal Learning:</strong> 2022年多模态深度学习survey论文。 <a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://arxiv.org/abs/2204.11984">https://arxiv.org/abs/2204.11984</a></p></li>
<li><p><strong>arXiv:</strong> 最新多模态学习研究论文搜索。使用”multimodal learning”, “vision-language”等关键词。 <a href="https://arxiv.org/">https://arxiv.org/</a></p></li>
<li><p><strong>Hugging Face Transformers Multimodal Documentation:</strong> Hugging Face Transformers库的多模态模型文档。 <a href="https://www.google.com/search?q=https://huggingface.co/docs/transformers/main/en/model_doc/auto%23multimodal-models">https://huggingface.co/docs/transformers/main/en/model_doc/auto#multimodal-models</a></p></li>
</ol>
<p>严格指令：</p>
<ul>
<li>不要翻译以 <code>$...$</code> 或 <code>$$...$$</code> 围起来的 LaTeX 数学表达式。保持它们完全不变。</li>
<li>不要翻译表格 markdown 语法。保持表格格式（管道 <code>|</code>、连字符 <code>-</code>、冒号 <code>:</code>）完全不变。仅翻译表格单元格内的文本内容。</li>
<li>不添加任何额外的文本。这包括但不限于： <em>问候或结束语。 </em>请求反馈。 <em>对翻译的解释。 </em>任何对话元素。 <em>除直接翻译以外的任何文本。 </em>仅执行翻译，不要进行推理、推断或任何其他任务。</li>
</ul>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>