<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>quarto-inputb6d21efd084a608f – Deep Learning DNA: Surviving Architectures and Essential Principles</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap-f507c7d0488cb7630e20aad62ad8c2aa.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script>window.MathJax = {loader: {load: ['[tex]/boldsymbol']},tex: {packages: {'[+]': ['boldsymbol']}}};</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../../notebooks/zh/part_1/01_深度学习的开始.html">part_1</a></li><li class="breadcrumb-item"><a href="../../../notebooks/zh/part_1/06_过拟合和解决技术的发展.html">6. 过拟合和解决技术的发展</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../../../">中文</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Language</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index_de.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Deutsch</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index_en.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">English</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index_es.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Español</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">한국어</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index_zh.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">中文</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/zh/part_1/00_引言.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">引言</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">part_1</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/zh/part_1/01_深度学习的开始.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">1. 深度学习的开始</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/zh/part_1/02_深度学习的数学.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">2. 深度学习的数学</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/zh/part_1/03_深度学习框架.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">3. 深度学习框架</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/zh/part_1/04_激活函数.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">4. 激活函数</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/zh/part_1/05_优化和可视化.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">5. 优化和可视化</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/zh/part_1/06_过拟合和解决技术的发展.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">6. 过拟合和解决技术的发展</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/zh/part_1/07_卷积神经网络的进化.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">7. 卷积神经网络的进化</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/zh/part_1/08_变压器的诞生.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">8. 变压器的诞生</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/zh/part_1/09_变压器的进化.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">9. 变压器的进化</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/zh/part_1/10_多模态深度学习：多感官融合的开始.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">10. 多模态深度学习：多感官融合的开始</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/zh/part_1/11_多模态深度学习：超越智能的极限.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">11. 多模态深度学习：超越智能的极限</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">深度学习的最前沿</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/zh/深度学习的最前沿/01_SLM: 小但强大的语言模型.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">1. SLM: 小但强大的语言模型</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/zh/深度学习的最前沿/02_自动驾驶.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">2. 自动驾驶</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#章-过拟合及其解决技术的发展" id="toc-章-过拟合及其解决技术的发展" class="nav-link active" data-scroll-target="#章-过拟合及其解决技术的发展">6章 过拟合及其解决技术的发展</a>
  <ul class="collapse">
  <li><a href="#理解过拟合" id="toc-理解过拟合" class="nav-link" data-scroll-target="#理解过拟合">6.1 理解过拟合</a></li>
  <li><a href="#正则化技术" id="toc-正则化技术" class="nav-link" data-scroll-target="#正则化技术">6.2 正则化技术</a>
  <ul class="collapse">
  <li><a href="#l1-l2-正则化" id="toc-l1-l2-正则化" class="nav-link" data-scroll-target="#l1-l2-正则化">6.2.1 L1, L2 正则化</a></li>
  <li><a href="#在pytorch中应用l1l2正则化" id="toc-在pytorch中应用l1l2正则化" class="nav-link" data-scroll-target="#在pytorch中应用l1l2正则化">6.2.2 在PyTorch中应用L1、L2正则化</a></li>
  <li><a href="#损失平面上的正则化效应分析" id="toc-损失平面上的正则化效应分析" class="nav-link" data-scroll-target="#损失平面上的正则化效应分析">6.2.3 损失平面上的正则化效应分析</a></li>
  </ul></li>
  <li><a href="#dropout丢弃法" id="toc-dropout丢弃法" class="nav-link" data-scroll-target="#dropout丢弃法">6.3 dropout（丢弃法）</a>
  <ul class="collapse">
  <li><a href="#dropout的原理" id="toc-dropout的原理" class="nav-link" data-scroll-target="#dropout的原理">6.3.1 dropout的原理</a></li>
  <li><a href="#在pytorch中实现dropout" id="toc-在pytorch中实现dropout" class="nav-link" data-scroll-target="#在pytorch中实现dropout">6.3.2 在PyTorch中实现dropout</a></li>
  </ul></li>
  <li><a href="#批归一化batch-normalization" id="toc-批归一化batch-normalization" class="nav-link" data-scroll-target="#批归一化batch-normalization">6.4 批归一化(batch normalization)</a>
  <ul class="collapse">
  <li><a href="#批归一化的概念和效果" id="toc-批归一化的概念和效果" class="nav-link" data-scroll-target="#批归一化的概念和效果">6.4.1 批归一化的概念和效果</a></li>
  <li><a href="#在-pytorch-中实现批归一化" id="toc-在-pytorch-中实现批归一化" class="nav-link" data-scroll-target="#在-pytorch-中实现批归一化">6.4.2 在 PyTorch 中实现批归一化</a></li>
  <li><a href="#统计跟踪和推理时的应用" id="toc-统计跟踪和推理时的应用" class="nav-link" data-scroll-target="#统计跟踪和推理时的应用">6.4.3 统计跟踪和推理时的应用</a></li>
  </ul></li>
  <li><a href="#超参数的优化" id="toc-超参数的优化" class="nav-link" data-scroll-target="#超参数的优化">6.5 超参数的优化</a>
  <ul class="collapse">
  <li><a href="#优化方法论比较" id="toc-优化方法论比较" class="nav-link" data-scroll-target="#优化方法论比较">6.5.1 优化方法论比较</a></li>
  <li><a href="#使用bayes-opt进行优化" id="toc-使用bayes-opt进行优化" class="nav-link" data-scroll-target="#使用bayes-opt进行优化">6.5.2 使用Bayes-Opt进行优化</a></li>
  <li><a href="#使用botorch进行优化" id="toc-使用botorch进行优化" class="nav-link" data-scroll-target="#使用botorch进行优化">6.5.3 使用BoTorch进行优化</a></li>
  </ul></li>
  <li><a href="#高斯过程" id="toc-高斯过程" class="nav-link" data-scroll-target="#高斯过程">6.6 高斯过程</a>
  <ul class="collapse">
  <li><a href="#不确定性处理的数学基础" id="toc-不确定性处理的数学基础" class="nav-link" data-scroll-target="#不确定性处理的数学基础">6.6.1 不确定性处理的数学基础</a></li>
  <li><a href="#现代应用" id="toc-现代应用" class="nav-link" data-scroll-target="#现代应用">6.6.2 现代应用</a></li>
  <li><a href="#深度核学习deep-kernel-learning" id="toc-深度核学习deep-kernel-learning" class="nav-link" data-scroll-target="#深度核学习deep-kernel-learning">6.6.3 深度核学习(Deep Kernel Learning)</a></li>
  </ul></li>
  <li><a href="#练习题" id="toc-练习题" class="nav-link" data-scroll-target="#练习题">练习题</a>
  <ul class="collapse">
  <li><a href="#基本问题" id="toc-基本问题" class="nav-link" data-scroll-target="#基本问题">基本问题</a></li>
  <li><a href="#应用问题" id="toc-应用问题" class="nav-link" data-scroll-target="#应用问题">应用问题</a></li>
  <li><a href="#深化问题" id="toc-深化问题" class="nav-link" data-scroll-target="#深化问题">深化问题</a></li>
  </ul></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../../notebooks/zh/part_1/01_深度学习的开始.html">part_1</a></li><li class="breadcrumb-item"><a href="../../../notebooks/zh/part_1/06_过拟合和解决技术的发展.html">6. 过拟合和解决技术的发展</a></li></ol></nav></header>




<p><a href="https://colab.research.google.com/github/Quantum-Intelligence-Frontier/dldna/blob/main/notebooks/zh/part_1/06_过拟合和解决技术的发展.ipynb" target="_parent"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="在 Colab 中打开"> </a></p>
<section id="章-过拟合及其解决技术的发展" class="level1">
<h1>6章 过拟合及其解决技术的发展</h1>
<blockquote class="blockquote">
<p>“简单是极致的精妙。” - 列奥纳多·达芬奇 (Leonardo da Vinci)</p>
</blockquote>
<p>深度学习模型具有通过大量参数表达复杂函数的强大能力。然而，这种能力有时也是一把双刃剑。当模型过度适应训练数据时，反而会导致对新数据的预测性能下降的<strong>过拟合(overfitting)</strong>现象。</p>
<p>1986年反向传播算法被重新关注后，过拟合一直是深度学习研究者面临的持续挑战。最初，通过减少模型规模或增加训练数据来应对过拟合。但这些方法限制了模型的表达能力，或者由于数据收集的困难而存在局限性。2012年AlexNet的出现开启了深度学习的新时代，同时也突显了过拟合问题的严重性。由于AlexNet比之前的模型具有更多的参数，因此过拟合的风险更大。随着深度学习模型规模呈指数级增长，过拟合问题成为了深度学习研究的核心课题。</p>
<p>在本章中，我们将理解过拟合的本质，并探讨为解决这一问题而发展起来的各种技术。就像探险家探索未知领域并绘制地图一样，深度学习研究者们为了克服过拟合这一难题，不断探索和改进新的方法。</p>
<section id="理解过拟合" class="level2">
<h2 class="anchored" data-anchor-id="理解过拟合">6.1 理解过拟合</h2>
<p>过拟合最早在1670年William Hopkins的著作中被提及，但在现代意义上则始于1935年的《生物学季刊》中的一段话：“用13个观测值进行六变量分析看起来像是过拟合”。随后，在20世纪50年代开始在统计学中得到正式研究，特别是在1952年“时间序列的拟合检验”论文中，在时序分析的背景下得到了重要讨论。</p>
<p>深度学习中的过拟合问题随着2012年AlexNet的出现而进入了一个新的阶段。AlexNet是一个拥有约6000万个参数的大规模神经网络，其规模远超之前的模型。随后，随着深度学习模型规模呈指数级增长，过拟合问题变得愈发严重。例如，现代大规模语言模型（LLM）包含数千亿个参数，因此防止过拟合成为了模型设计的核心任务。</p>
<p>为了应对这些挑战，提出了诸如dropout（2014年）、批归一化（2015年）等创新解决方案，并且近年来出现了利用训练历史进行过拟合检测和预防（2024年）等更加精细的方法。特别是在大规模模型中，从传统的提前停止（early stopping）到现代技术如集成学习、数据增强等，各种策略被综合运用。</p>
<p>通过一个简单的例子来直观地理解过拟合现象。我们将对包含噪声的正弦函数数据应用不同阶数(degree)的多项式(polynomial)。</p>
<div id="cell-2" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip install dldna[colab] <span class="co"># in Colab</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="co"># !pip install dldna[all] # in your local</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>load_ext autoreload</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>autoreload <span class="dv">2</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="cell-3" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Noisy sin graph</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> real_func(x):</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> np.sin(x) <span class="op">+</span> np.random.uniform(<span class="op">-</span><span class="fl">0.2</span>, <span class="fl">0.2</span>, <span class="bu">len</span>(x))</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> y</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Create x data from 40 to 320 degrees.  Use a step value to avoid making it too dense.</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.array([np.pi<span class="op">/</span><span class="dv">180</span> <span class="op">*</span> i <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">40</span>, <span class="dv">320</span>, <span class="dv">4</span>)])</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> real_func(x)</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>sns.scatterplot(x<span class="op">=</span>x, y<span class="op">=</span>y, label<span class="op">=</span><span class="st">'real function'</span>)</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot with 1st, 3rd, and 21th degree polynomials.  </span></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> deg <span class="kw">in</span> [<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">21</span>]:  </span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the coefficients for the corresponding degree using polyfit, and create the estimated function using poly1d.</span></span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>    params <span class="op">=</span> np.polyfit(x, y, deg) <span class="co"># Get the parameter values</span></span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>    <span class="co"># print(f" {deg} params = {params}")</span></span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>    p <span class="op">=</span> np.poly1d(params) <span class="co"># Get the line function</span></span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>    sns.lineplot(x<span class="op">=</span>x, y<span class="op">=</span>p(x), color<span class="op">=</span><span class="ss">f"C</span><span class="sc">{</span>deg<span class="sc">}</span><span class="ss">"</span>, label<span class="op">=</span><span class="ss">f"deg = </span><span class="sc">{</span>deg<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>The autoreload extension is already loaded. To reload it, use:
  %reload_ext autoreload</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_1362795/2136320363.py:25: RankWarning: Polyfit may be poorly conditioned
  params = np.polyfit(x, y, deg) # Get the parameter values</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="06_过拟合和解决技术的发展_files/figure-html/cell-3-output-3.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>此代码示例生成包含噪声的正弦函数数据，并使用一阶、三阶和二十一阶多项式对这些数据进行拟合。</p>
<ul>
<li><p><strong>一次函数 (deg = 1):</strong> 数据的整体趋势未能得到很好的遵循，仅表现为简单的直线形式。这表明模型处于 <em>欠拟合(underfitting)</em> 状态，无法充分表达数据的复杂性。</p></li>
<li><p><strong>三次函数 (deg = 3):</strong> 相对较好地捕捉了数据的基本模式，同时表现出不被噪声大幅影响的平滑曲线形态。</p></li>
<li><p><strong>二十一阶函数 (deg = 21)</strong>: 过度追踪训练数据中的噪声，导致模型 <em>过拟合(overfitting)</em> 状态，即仅在训练数据上过度优化。</p></li>
</ul>
<p>当模型复杂度过低（此处指多项式的阶数）时会发生欠拟合，而复杂度过高则会导致过拟合。我们最终要寻找的是不仅在训练数据上表现良好，而且能够很好地泛化到新数据的模型，即最接近实际正弦函数的近似函数。</p>
<p>当模型的复杂度（容量，capacity）相对于训练数据量过大时会发生过拟合。神经网络具有大量参数和极高的表达能力，因此特别容易发生过拟合。当训练数据不足或数据中噪声较多时，也可能会出现过拟合。过拟合表现为以下特征。</p>
<ul>
<li><strong>训练数据</strong>上的损失（loss）持续减少。</li>
<li><strong>验证数据</strong>(validation data) 上的损失先减少后，在某个时间点开始反而增加。</li>
<li>这是因为模型学习了训练数据中的噪声和细微部分，导致其在训练数据上过度特化。</li>
</ul>
<p>最终，过拟合模型在训练数据上的表现会很高，但在实际新数据上的预测性能却会下降。为了防止这种过拟合，在后续讨论中我们将详细介绍L1/L2正则化、dropout、批标准化等技术。</p>
</section>
<section id="正则化技术" class="level2">
<h2 class="anchored" data-anchor-id="正则化技术">6.2 正则化技术</h2>
<blockquote class="blockquote">
<p><strong>挑战：</strong> 如何在有效控制模型复杂度的同时提高泛化性能？</p>
<p><strong>研究者的思考：</strong> 减少模型的大小以防止过拟合可能会限制其表达能力，而仅仅增加训练数据也不总是可行。需要一种方法来约束模型结构或学习过程，防止对训练数据过度优化，并提高对新数据的预测性能。</p>
</blockquote>
<section id="l1-l2-正则化" class="level3">
<h3 class="anchored" data-anchor-id="l1-l2-正则化">6.2.1 L1, L2 正则化</h3>
<p>在神经网络中常用的正则化（regularization）技术有L1和L2正则化。L1指的是套索（Lasso），L2指的则是岭（Ridge）正则化（用于线性回归）。</p>
<p>也称为岭回归和套索回归，每种回归都通过引入一定的惩罚项来限制参数的变化。两种方法的特点差异可以总结如下表：</p>
<table class="caption-top table">
<colgroup>
<col style="width: 27%">
<col style="width: 38%">
<col style="width: 34%">
</colgroup>
<thead>
<tr class="header">
<th>特征</th>
<th>岭回归(Ridge Regression)</th>
<th>套索回归(Lasso Regression)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>惩罚类型</td>
<td>施加L2惩罚。惩罚项是对参数平方值的和乘以α值。</td>
<td>施加L1惩罚。惩罚项是对参数绝对值的和乘以α值。</td>
</tr>
<tr class="even">
<td>参数影响</td>
<td>抑制值大的参数，使其接近0但不等于0</td>
<td>当α值较大时，可将某些参数值变为0，从而生成更简洁的模型</td>
</tr>
<tr class="odd">
<td>总体影响</td>
<td>所有参数都得以保留。因此即使影响较小的参数也会被保留下来。</td>
<td>只保留相关的参数，具有选择性特征。可以更简洁地解释复杂的模型。</td>
</tr>
<tr class="even">
<td>最优化特性</td>
<td>相比套索，对理想值较为不敏感。</td>
<td>由于惩罚项是绝对值形式，因此对理想值非常敏感。</td>
</tr>
</tbody>
</table>
<p>表达式如下：</p>
<ul>
<li><p>岭目标函数 (Ridge Regression Objective Function)</p>
<p>“修改后的岭目标函数” = （未修改的线性回归函数） + <span class="math inline">\(\alpha \cdot \sum (\text{参数})^2\)</span></p>
<p><span class="math inline">\(f_{\beta} = \sum_{i=1}^{M} (y_i - \hat{y}_i)^2 + \alpha \sum_{j} \beta_{j}^2\)</span></p>
<p>其中，<span class="math inline">\(\beta\)</span>是待求的参数（权重）向量。 <span class="math inline">\(\alpha \sum_{j} \beta_{j}^2\)</span>称为惩罚项或正则化项。 <span class="math inline">\(\alpha\)</span>是调整正则化项大小的超参数。参数的求解公式如下：</p>
<p><span class="math inline">\(\beta = \underset{\beta}{\operatorname{argmin}} \left( \sum_{i=1}^{M} (y_i - \hat{y}_i)^2 + \alpha \sum_{j} \beta_{j}^2 \right)\)</span></p></li>
<li><p>套索目标函数 (Lasso Regression Objective Function)</p>
<p>“修改后的套索目标函数” = （未修改的线性回归函数） + $ || $</p>
<p><span class="math inline">\(f_{\beta} = \sum_{i=1}^{M} (y_i - \hat{y}_i)^2 + \alpha \sum_{j} |\beta_{j}|\)</span> <span class="math inline">\(\beta = \underset{\beta}{\operatorname{argmin}} \left( \sum_{i=1}^{M} (y_i - \hat{y}_i)^2 + \alpha \sum_{j} |\beta_j| \right)\)</span></p></li>
</ul>
<p>参数平方和作为惩罚项的L2在神经网络中通常被称为权重衰减（weight decay）。我们将使用岭（L2）回归来观察它与简单线性回归有何不同。为此，我们使用sklearn中实现的模型。为了实现这一点，需要将输入x数据按次数扩展维度。接下来，我们将使用以下简单的实用函数来创建。</p>
<div id="cell-6" class="cell" data-execution_count="15">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_x_powered(x, p<span class="op">=</span><span class="dv">1</span>):</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    size <span class="op">=</span> <span class="bu">len</span>(x)</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># The shape of the created x will be (data size, degree)</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>    new_x <span class="op">=</span> np.zeros((size, p))</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> s <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(x)): <span class="co"># Iterate over data size</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> d <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, p<span class="op">+</span><span class="dv">1</span>): <span class="co"># Iterate over degrees</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>            new_x[s][d<span class="op">-</span><span class="dv">1</span>] <span class="op">=</span> x[s]<span class="op">**</span>d <span class="co"># Raise x to the power of the degree.</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> new_x</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Let's take a quick look at how it works.</span></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>deg <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.array([np.pi<span class="op">/</span><span class="dv">180</span> <span class="op">*</span> i <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">20</span>, <span class="dv">35</span>, <span class="dv">5</span>)])</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> real_func(x)  <span class="co"># real_func는 이전 코드에 정의되어 있다고 가정</span></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"x = </span><span class="sc">{</span>x<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>new_x <span class="op">=</span> get_x_powered(x, p<span class="op">=</span>deg)</span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"new_x = </span><span class="sc">{</span>new_x<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>x = [0.34906585 0.43633231 0.52359878]
new_x = [[0.34906585 0.12184697 0.04253262]
 [0.43633231 0.19038589 0.08307151]
 [0.52359878 0.27415568 0.14354758]]</code></pre>
</div>
</div>
<p>因为是三次的，所以 <span class="math inline">\(x\)</span> 值会增加到 <span class="math inline">\(x^2, x^3\)</span>。例如 0.3490, 0.1218(0.3490 的平方), 0.04253(0.3490 的立方) 就是这样的例子。如果是十次的，则数据会生成到 <span class="math inline">\(x^{10}\)</span>。惩罚项的 alpha 值可以取 0 到无穷大的值。alpha 值越大，正则化强度就越大。次数固定为 13，并且我们将在改变 alpha 值的同时比较线性回归函数和岭回归。</p>
<div id="cell-8" class="cell" data-execution_count="18">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> Ridge</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> PolynomialFeatures</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.pipeline <span class="im">import</span> make_pipeline</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a noisy sine wave (increased noise)</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> real_func(x):</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.sin(x) <span class="op">+</span> np.random.normal(<span class="dv">0</span>, <span class="fl">0.4</span>, <span class="bu">len</span>(x))  <span class="co"># Increased noise</span></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Create x data (narrower range)</span></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.array([np.pi <span class="op">/</span> <span class="dv">180</span> <span class="op">*</span> i <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">40</span>, <span class="dv">280</span>, <span class="dv">8</span>)])  <span class="co"># Narrower range, larger step</span></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> real_func(x)</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Degree of the polynomial</span></span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>deg <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a><span class="co"># List of alpha values to compare (adjusted)</span></span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>alpha_list <span class="op">=</span> [<span class="fl">0.0</span>, <span class="fl">0.1</span>, <span class="dv">10</span>]  <span class="co"># Adjusted alpha values</span></span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a>cols <span class="op">=</span> <span class="bu">len</span>(alpha_list)</span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a>fig, axes_list <span class="op">=</span> plt.subplots(<span class="dv">1</span>, cols, figsize<span class="op">=</span>(<span class="dv">20</span>, <span class="dv">5</span>))  <span class="co"># Adjusted figure size</span></span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, alpha <span class="kw">in</span> <span class="bu">enumerate</span>(alpha_list):</span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a>    axes <span class="op">=</span> axes_list[i]</span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Plot the original data</span></span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a>    sns.scatterplot(ax<span class="op">=</span>axes, x<span class="op">=</span>x, y<span class="op">=</span>y, label<span class="op">=</span><span class="st">'real function'</span>, s<span class="op">=</span><span class="dv">50</span>)  <span class="co"># Increased marker size</span></span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-32"><a href="#cb7-32" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Plot linear regression</span></span>
<span id="cb7-33"><a href="#cb7-33" aria-hidden="true" tabindex="-1"></a>    params <span class="op">=</span> np.polyfit(x, y, deg)</span>
<span id="cb7-34"><a href="#cb7-34" aria-hidden="true" tabindex="-1"></a>    p <span class="op">=</span> np.poly1d(params)</span>
<span id="cb7-35"><a href="#cb7-35" aria-hidden="true" tabindex="-1"></a>    sns.lineplot(ax<span class="op">=</span>axes, x<span class="op">=</span>x, y<span class="op">=</span>p(x), label<span class="op">=</span><span class="ss">f"LR deg = </span><span class="sc">{</span>deg<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb7-36"><a href="#cb7-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-37"><a href="#cb7-37" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Ridge regression (using Pipeline, solver='auto')</span></span>
<span id="cb7-38"><a href="#cb7-38" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> make_pipeline(PolynomialFeatures(degree<span class="op">=</span>deg), Ridge(alpha<span class="op">=</span>alpha, solver<span class="op">=</span><span class="st">'auto'</span>))</span>
<span id="cb7-39"><a href="#cb7-39" aria-hidden="true" tabindex="-1"></a>    model.fit(x.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>), y)  <span class="co"># Reshape x for pipeline</span></span>
<span id="cb7-40"><a href="#cb7-40" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> model.predict(x.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)) <span class="co"># Reshape x for prediction</span></span>
<span id="cb7-41"><a href="#cb7-41" aria-hidden="true" tabindex="-1"></a>    sns.lineplot(ax<span class="op">=</span>axes, x<span class="op">=</span>x, y<span class="op">=</span>y_pred, label<span class="op">=</span><span class="ss">f"Ridge alpha=</span><span class="sc">{</span>alpha<span class="sc">:0.1e}</span><span class="ss"> deg=</span><span class="sc">{</span>deg<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb7-42"><a href="#cb7-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-43"><a href="#cb7-43" aria-hidden="true" tabindex="-1"></a>    axes.set_title(<span class="ss">f"Alpha = </span><span class="sc">{</span>alpha<span class="sc">:0.1e}</span><span class="ss">"</span>)</span>
<span id="cb7-44"><a href="#cb7-44" aria-hidden="true" tabindex="-1"></a>    axes.set_ylim(<span class="op">-</span><span class="fl">1.5</span>, <span class="fl">1.5</span>)  <span class="co"># Limit y-axis range</span></span>
<span id="cb7-45"><a href="#cb7-45" aria-hidden="true" tabindex="-1"></a>    axes.legend()</span>
<span id="cb7-46"><a href="#cb7-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-47"><a href="#cb7-47" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb7-48"><a href="#cb7-48" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="06_过拟合和解决技术的发展_files/figure-html/cell-5-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>该图显示了使用10次多项式对添加了噪声的正弦函数数据进行拟合的结果，展示了使用不同<code>alpha</code>值（正则化强度）的Ridge回归结果。由于数据范围狭窄且噪声较多，在较低阶数下也容易发生过拟合。</p>
<ul>
<li><strong>Alpha = 0.0:</strong> Ridge回归与普通的最小二乘线性回归相同，10次多项式会跟随训练数据中的噪声，显示出严重的曲折型过拟合。</li>
<li><strong>Alpha = 0.1:</strong> 应用了较弱的正则化，与<code>alpha=0</code>相比弯曲减少，但仍对噪声敏感，与正弦函数有较大偏差。</li>
<li><strong>Alpha = 10:</strong> 强烈的正则化使曲线变得更加平滑，较好地反映了数据的整体趋势（正弦函数）。这表明L2正则化（Ridge回归）有效地控制了过拟合。</li>
</ul>
<p>通过选择合适的<code>alpha</code>值可以控制模型复杂度并提高泛化性能。L2正则化有助于将权重推向接近0的值，从而稳定模型。</p>
<p><code>sklearn.linear_model.Ridge</code>模型的优化方法可能因所选<code>solver</code>而异。特别是当数据范围狭窄且噪声较多时（如本例），使用<code>'svd'</code>或<code>'cholesky'</code>求解器可能会更稳定，因此在选择<code>solver</code>时应谨慎（代码中指定了<code>'cholesky'</code>）。</p>
</section>
<section id="在pytorch中应用l1l2正则化" class="level3">
<h3 class="anchored" data-anchor-id="在pytorch中应用l1l2正则化">6.2.2 在PyTorch中应用L1、L2正则化</h3>
<p>PyTorch和Keras在实现L1、L2正则化的方式上有所不同。Keras支持直接向各层（layer）添加正则项的方法（如<code>kernel_regularizer</code>，<code>bias_regularizer</code>）。</p>
<div id="cell-10" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># In Keras, you can specify regularization when declaring a layer.</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>keras.layers.Dense(<span class="dv">64</span>, activation<span class="op">=</span><span class="st">'relu'</span>,</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>                    kernel_regularizer<span class="op">=</span>regularizers.l2(<span class="fl">0.01</span>),</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>                    input_shape<span class="op">=</span>(<span class="dv">784</span>,))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>相比之下，PyTorch 通过在优化器(optimizer)中设置权重衰减(weight decay)来应用 L2 正则化，而 L1 正则化通常通过用户定义的损失函数实现。</p>
<div id="cell-12" class="cell" data-execution_count="5">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> custom_loss(outputs, targets, model, lambda_l1<span class="op">=</span><span class="fl">0.01</span>, lambda_l2<span class="op">=</span><span class="fl">0.01</span>,):</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>    mse_loss <span class="op">=</span> nn.MSELoss()(outputs, targets)</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>    l1_loss <span class="op">=</span> <span class="fl">0.</span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>    l2_loss <span class="op">=</span> <span class="fl">0.</span></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> param <span class="kw">in</span> model.parameters():</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>        l1_loss <span class="op">+=</span> torch.<span class="bu">sum</span>(torch.<span class="bu">abs</span>(param)) <span class="co"># Take the absolute value of the parameters.</span></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>        l2_loss <span class="op">+=</span> torch.<span class="bu">sum</span>(param <span class="op">**</span> <span class="dv">2</span>)  <span class="co"># Square the parameters.</span></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>    total_loss <span class="op">=</span> mse_loss <span class="op">+</span> lambda_l1 <span class="op">*</span> l1_loss <span class="op">+</span> lambda_l2 <span class="op">*</span> l2_loss <span class="co"># Add L1 and L2 penalty terms to the loss.</span></span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> total_loss</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Example usage within a training loop (not runnable as is)</span></span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a><span class="co"># for inputs, targets in dataloader:</span></span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a><span class="co">#     # ... (rest of the training loop)</span></span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a><span class="co">#     loss = custom_loss(outputs, targets, model)</span></span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a><span class="co">#     loss.backward()</span></span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ... (rest of the training loop)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>可以像上面的示例那样定义 <code>custom_loss</code> 函数来同时应用 L1 和 L2 正则化。但通常情况下，我们会通过设置优化器中的 <code>weight_decay</code> 来实现 L2 正则化。然而，在 Adam 和 SGD 优化器中，权重衰减的实现与传统的 L2 正则化略有不同。传统的 L2 正则化是通过在损失函数中添加参数平方项来实现的。</p>
<p><span class="math inline">\(L_{n+1} = L_{n} + \frac{ \lambda }{2} \sum w^2\)</span></p>
<p>对参数求导后可得：</p>
<p><span class="math inline">\(\frac{\partial L_{n+1}}{\partial w} = \frac{\partial L_{n}}{\partial w} +\lambda w\)</span></p>
<p>SGD 和 Adam 通过直接将 <span class="math inline">\(\lambda w\)</span> 项加到梯度上来实现这一点。<code>chapter_05/optimizers/ SGD</code> 代码如下。</p>
<div id="cell-14" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="va">self</span>.weight_decay <span class="op">!=</span> <span class="dv">0</span>:</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>    grad <span class="op">=</span> grad.add(p, alpha<span class="op">=</span><span class="va">self</span>.weight_decay)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>这种方式在与动量(momentum)或自适应学习率(adaptive learning rate)结合时，不会产生与在损失函数中添加L2正则项完全相同的效果。</p>
<p><strong>AdamW和权重衰减的分离 (Decoupled Weight Decay)</strong></p>
<p>2017年ICLR发表的”Fixing Weight Decay Regularization in Adam”论文(https://arxiv.org/abs/1711.05101)指出，在Adam优化器中，权重衰减与L2正则化的工作方式不同，并提出了修改后的AdamW优化器。在AdamW中，权重衰减与梯度更新分离，并直接应用于参数更新步骤。代码位于相同的basic.py文件中。</p>
<div id="cell-16" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># PyTorch AdamW weght decay</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> weight_decay <span class="op">!=</span> <span class="dv">0</span>:</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>    param.data.mul_(<span class="dv">1</span> <span class="op">-</span> lr <span class="op">*</span> weight_decay)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>AdamW将参数值乘以1 - lr * weight_decay。</p>
<ul>
<li><strong>传统Adam的权重衰减</strong>：在梯度更新步骤中，权重衰减项（<span class="math inline">\(\lambda w\)</span>）与学习率(<span class="math inline">\(lr\)</span>)和其他梯度调整项（例如动量）一起考虑。这可能导致权重衰减的效果因学习率和其他超参数设置而异。</li>
<li><strong>AdamW的权重衰减</strong>：由于权重衰减在参数更新步骤中单独应用，因此对学习率或其他超参数的依赖性较小。也就是说，权重衰减的效果更加可预测和一致。</li>
</ul>
<p>总之，AdamW的方法更接近于精确的L2正则化实现。SGD、Adam的权重衰减之所以被称为L2正则化，是出于历史原因以及类似的效果，但严格来说，将其视为单独的正则化技术更为准确，而AdamW通过明确这些差异来提供更好的性能。</p>
</section>
<section id="损失平面上的正则化效应分析" class="level3">
<h3 class="anchored">6.2.3 损失平面上的正则化效应分析</h3>
<p>为了直观地理解 L1 和 L2 正则化对模型学习的影响，我们将使用第 4 章介绍的损失平面（loss surface）可视化技术。比较没有正则化和应用 L2 正则化的情况下的损失平面变化，并观察不同正则化强度（<code>weight_decay</code>）下最优解位置的变化。</p>
<div id="cell-19" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sys</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_05.visualization.loss_surface <span class="im">import</span> xy_perturb_loss,  hessian_eigenvectors, visualize_loss_surface </span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_04.utils.data <span class="im">import</span> get_dataset, get_device   </span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_04.utils.metrics <span class="im">import</span> load_model  </span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.utils.data <span class="im">as</span> data_utils</span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span>  DataLoader</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> get_device()  <span class="co"># Get the device (CPU or CUDA)</span></span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a>train_dataset, test_dataset <span class="op">=</span> get_dataset()  <span class="co"># Load the datasets.  </span></span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a>act_name <span class="op">=</span> <span class="st">"ReLU"</span></span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a>model_file <span class="op">=</span> <span class="ss">f"SimpleNetwork-</span><span class="sc">{</span>act_name<span class="sc">}</span><span class="ss">.pth"</span></span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a>small_dataset <span class="op">=</span> data_utils.Subset(test_dataset, torch.arange(<span class="dv">0</span>, <span class="dv">256</span>))  <span class="co"># Use a subset of the test dataset</span></span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a>data_loader <span class="op">=</span> DataLoader(small_dataset, batch_size<span class="op">=</span><span class="dv">256</span>, shuffle<span class="op">=</span><span class="va">True</span>)  <span class="co"># Create a data loader</span></span>
<span id="cb12-22"><a href="#cb12-22" aria-hidden="true" tabindex="-1"></a>loss_func <span class="op">=</span> nn.CrossEntropyLoss()  <span class="co"># Define the loss function</span></span>
<span id="cb12-23"><a href="#cb12-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-24"><a href="#cb12-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the trained model.</span></span>
<span id="cb12-25"><a href="#cb12-25" aria-hidden="true" tabindex="-1"></a>trained_model, _ <span class="op">=</span> load_model(model_file<span class="op">=</span>model_file, path<span class="op">=</span><span class="st">"./tmp/opts/ReLU"</span>) <span class="co"># 4장의 load_model 사용</span></span>
<span id="cb12-26"><a href="#cb12-26" aria-hidden="true" tabindex="-1"></a>trained_model <span class="op">=</span> trained_model.to(device)  <span class="co"># Move the model to the device</span></span>
<span id="cb12-27"><a href="#cb12-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-28"><a href="#cb12-28" aria-hidden="true" tabindex="-1"></a>top_n <span class="op">=</span> <span class="dv">2</span>  <span class="co"># Number of top eigenvalues/eigenvectors to compute</span></span>
<span id="cb12-29"><a href="#cb12-29" aria-hidden="true" tabindex="-1"></a>top_eigenvalues, top_eigenvectors <span class="op">=</span>  hessian_eigenvectors(model<span class="op">=</span>trained_model, loss_func<span class="op">=</span>loss_func, data_loader<span class="op">=</span>data_loader, top_n<span class="op">=</span>top_n, is_cuda<span class="op">=</span><span class="va">True</span>)  <span class="co"># 5장의 함수 사용</span></span>
<span id="cb12-30"><a href="#cb12-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-31"><a href="#cb12-31" aria-hidden="true" tabindex="-1"></a>d_min ,d_max, d_num <span class="op">=</span> <span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">50</span>  <span class="co"># Define the range and number of points for the grid</span></span>
<span id="cb12-32"><a href="#cb12-32" aria-hidden="true" tabindex="-1"></a>lambda1, lambda2 <span class="op">=</span> np.linspace(d_min, d_max, d_num).astype(np.float32), np.linspace(d_min, d_max, d_num).astype(np.float32)  <span class="co"># Create the grid of lambda values</span></span>
<span id="cb12-33"><a href="#cb12-33" aria-hidden="true" tabindex="-1"></a>x, y, z <span class="op">=</span> xy_perturb_loss(model<span class="op">=</span>trained_model, top_eigenvectors<span class="op">=</span>top_eigenvectors, data_loader<span class="op">=</span>data_loader, loss_func<span class="op">=</span>loss_func, lambda1<span class="op">=</span>lambda1, lambda2<span class="op">=</span>lambda2, device<span class="op">=</span>device) <span class="co"># 5장의 함수 사용</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>首先通过 <code>xy_perturb_loss</code> 创建近似函数，然后再次将 (x,y) 输入该近似函数以计算新的 z 值。这样做的原因是，如果使用 <code>xy_perturb_loss</code> 计算的值绘制等高线，则最小值会略有不同，从而导致优化器收敛的点稍微偏离。现在，我们将不再表示优化器流经的所有路径，而是仅通过增加衰减值 weight_decay 来比较最终的最低点。</p>
<div id="cell-21" class="cell" data-execution_count="8">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.optim <span class="im">as</span> optim  <span class="co"># Import optim</span></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> DataLoader, Subset</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a><span class="co"># 5장, 4장 함수들 import</span></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_05.visualization.loss_surface <span class="im">import</span> (</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>    hessian_eigenvectors,</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>    xy_perturb_loss,</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>    visualize_loss_surface</span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_04.utils.data <span class="im">import</span> get_dataset, get_device</span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_04.utils.metrics <span class="im">import</span> load_model</span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_05.visualization.gaussian_loss_surface <span class="im">import</span> (</span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>    get_opt_params,</span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>    train_loss_surface,</span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a>    gaussian_func <span class="co"># gaussian_func 추가.</span></span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> get_device()</span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a>_, test_dataset <span class="op">=</span> get_dataset(dataset<span class="op">=</span><span class="st">"FashionMNIST"</span>) </span>
<span id="cb13-25"><a href="#cb13-25" aria-hidden="true" tabindex="-1"></a>small_dataset <span class="op">=</span> Subset(test_dataset, torch.arange(<span class="dv">0</span>, <span class="dv">256</span>))</span>
<span id="cb13-26"><a href="#cb13-26" aria-hidden="true" tabindex="-1"></a>data_loader <span class="op">=</span> DataLoader(small_dataset, batch_size<span class="op">=</span><span class="dv">256</span>, shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb13-27"><a href="#cb13-27" aria-hidden="true" tabindex="-1"></a>loss_func <span class="op">=</span> nn.CrossEntropyLoss()</span>
<span id="cb13-28"><a href="#cb13-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-29"><a href="#cb13-29" aria-hidden="true" tabindex="-1"></a>act_name <span class="op">=</span> <span class="st">"ReLU"</span> <span class="co"># Tanh로 실험하려면 이 부분을 변경</span></span>
<span id="cb13-30"><a href="#cb13-30" aria-hidden="true" tabindex="-1"></a>model_file <span class="op">=</span> <span class="ss">f"SimpleNetwork-</span><span class="sc">{</span>act_name<span class="sc">}</span><span class="ss">.pth"</span></span>
<span id="cb13-31"><a href="#cb13-31" aria-hidden="true" tabindex="-1"></a>trained_model, _ <span class="op">=</span> load_model(model_file<span class="op">=</span>model_file, path<span class="op">=</span><span class="st">"./tmp/opts/ReLU"</span>) </span>
<span id="cb13-32"><a href="#cb13-32" aria-hidden="true" tabindex="-1"></a>trained_model <span class="op">=</span> trained_model.to(device)</span>
<span id="cb13-33"><a href="#cb13-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-34"><a href="#cb13-34" aria-hidden="true" tabindex="-1"></a>top_n <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb13-35"><a href="#cb13-35" aria-hidden="true" tabindex="-1"></a>top_eigenvalues, top_eigenvectors <span class="op">=</span> hessian_eigenvectors(</span>
<span id="cb13-36"><a href="#cb13-36" aria-hidden="true" tabindex="-1"></a>    model<span class="op">=</span>trained_model,</span>
<span id="cb13-37"><a href="#cb13-37" aria-hidden="true" tabindex="-1"></a>    loss_func<span class="op">=</span>loss_func,</span>
<span id="cb13-38"><a href="#cb13-38" aria-hidden="true" tabindex="-1"></a>    data_loader<span class="op">=</span>data_loader,</span>
<span id="cb13-39"><a href="#cb13-39" aria-hidden="true" tabindex="-1"></a>    top_n<span class="op">=</span>top_n,</span>
<span id="cb13-40"><a href="#cb13-40" aria-hidden="true" tabindex="-1"></a>    is_cuda<span class="op">=</span><span class="va">True</span></span>
<span id="cb13-41"><a href="#cb13-41" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb13-42"><a href="#cb13-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-43"><a href="#cb13-43" aria-hidden="true" tabindex="-1"></a>d_min, d_max, d_num <span class="op">=</span> <span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">30</span> <span class="co"># 5장의 30을 사용</span></span>
<span id="cb13-44"><a href="#cb13-44" aria-hidden="true" tabindex="-1"></a>lambda1 <span class="op">=</span> np.linspace(d_min, d_max, d_num).astype(np.float32)</span>
<span id="cb13-45"><a href="#cb13-45" aria-hidden="true" tabindex="-1"></a>lambda2 <span class="op">=</span> np.linspace(d_min, d_max, d_num).astype(np.float32)</span>
<span id="cb13-46"><a href="#cb13-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-47"><a href="#cb13-47" aria-hidden="true" tabindex="-1"></a>x, y, z <span class="op">=</span> xy_perturb_loss(</span>
<span id="cb13-48"><a href="#cb13-48" aria-hidden="true" tabindex="-1"></a>    model<span class="op">=</span>trained_model,</span>
<span id="cb13-49"><a href="#cb13-49" aria-hidden="true" tabindex="-1"></a>    top_eigenvectors<span class="op">=</span>top_eigenvectors,</span>
<span id="cb13-50"><a href="#cb13-50" aria-hidden="true" tabindex="-1"></a>    data_loader<span class="op">=</span>data_loader,</span>
<span id="cb13-51"><a href="#cb13-51" aria-hidden="true" tabindex="-1"></a>    loss_func<span class="op">=</span>loss_func,</span>
<span id="cb13-52"><a href="#cb13-52" aria-hidden="true" tabindex="-1"></a>    lambda1<span class="op">=</span>lambda1,</span>
<span id="cb13-53"><a href="#cb13-53" aria-hidden="true" tabindex="-1"></a>    lambda2<span class="op">=</span>lambda2,</span>
<span id="cb13-54"><a href="#cb13-54" aria-hidden="true" tabindex="-1"></a>    device<span class="op">=</span>device <span class="co"># device 추가</span></span>
<span id="cb13-55"><a href="#cb13-55" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb13-56"><a href="#cb13-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-57"><a href="#cb13-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-58"><a href="#cb13-58" aria-hidden="true" tabindex="-1"></a><span class="co"># --- Optimization and Visualization ---</span></span>
<span id="cb13-59"><a href="#cb13-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-60"><a href="#cb13-60" aria-hidden="true" tabindex="-1"></a><span class="co"># Find the parameters that best fit the data.</span></span>
<span id="cb13-61"><a href="#cb13-61" aria-hidden="true" tabindex="-1"></a>popt, _, offset <span class="op">=</span> get_opt_params(x, y, z)  <span class="co"># offset 사용</span></span>
<span id="cb13-62"><a href="#cb13-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-63"><a href="#cb13-63" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Optimal parameters: </span><span class="sc">{</span>popt<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb13-64"><a href="#cb13-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-65"><a href="#cb13-65" aria-hidden="true" tabindex="-1"></a><span class="co"># Get a new z using the optimized surface function (Gaussian).</span></span>
<span id="cb13-66"><a href="#cb13-66" aria-hidden="true" tabindex="-1"></a><span class="co"># No need for global g_offset, we can use the returned offset.</span></span>
<span id="cb13-67"><a href="#cb13-67" aria-hidden="true" tabindex="-1"></a>z_fitted <span class="op">=</span> gaussian_func((x, y), <span class="op">*</span>popt,offset) <span class="co"># offset을 더해야 함.</span></span>
<span id="cb13-68"><a href="#cb13-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-69"><a href="#cb13-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-70"><a href="#cb13-70" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> [(x, y, z_fitted)]  <span class="co"># Use z_fitted</span></span>
<span id="cb13-71"><a href="#cb13-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-72"><a href="#cb13-72" aria-hidden="true" tabindex="-1"></a>axes <span class="op">=</span> visualize_loss_surface(data, act_name<span class="op">=</span>act_name, color<span class="op">=</span><span class="st">"C0"</span>, size<span class="op">=</span><span class="dv">6</span>, levels<span class="op">=</span><span class="dv">80</span>, alpha<span class="op">=</span><span class="fl">0.7</span>, plot_3d<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb13-73"><a href="#cb13-73" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> axes[<span class="dv">0</span>]</span>
<span id="cb13-74"><a href="#cb13-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-75"><a href="#cb13-75" aria-hidden="true" tabindex="-1"></a><span class="co"># Train with different weight decays and plot trajectories.</span></span>
<span id="cb13-76"><a href="#cb13-76" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> n, weight_decay <span class="kw">in</span> <span class="bu">enumerate</span>([<span class="fl">0.0</span>, <span class="fl">6.0</span>, <span class="fl">10.0</span>, <span class="fl">18.0</span>, <span class="fl">20.0</span>]):</span>
<span id="cb13-77"><a href="#cb13-77" aria-hidden="true" tabindex="-1"></a><span class="co"># for n, weight_decay in enumerate([0.0]):  # For faster testing</span></span>
<span id="cb13-78"><a href="#cb13-78" aria-hidden="true" tabindex="-1"></a>    points_sgd_m <span class="op">=</span> train_loss_surface(</span>
<span id="cb13-79"><a href="#cb13-79" aria-hidden="true" tabindex="-1"></a>        <span class="kw">lambda</span> params: optim.SGD(params, lr<span class="op">=</span><span class="fl">0.1</span>, momentum<span class="op">=</span><span class="fl">0.7</span>, weight_decay<span class="op">=</span>weight_decay),</span>
<span id="cb13-80"><a href="#cb13-80" aria-hidden="true" tabindex="-1"></a>        [d_min, d_max],</span>
<span id="cb13-81"><a href="#cb13-81" aria-hidden="true" tabindex="-1"></a>        <span class="dv">200</span>,</span>
<span id="cb13-82"><a href="#cb13-82" aria-hidden="true" tabindex="-1"></a>        (<span class="op">*</span>popt, offset) <span class="co"># unpack popt and offset</span></span>
<span id="cb13-83"><a href="#cb13-83" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb13-84"><a href="#cb13-84" aria-hidden="true" tabindex="-1"></a>    ax.plot(</span>
<span id="cb13-85"><a href="#cb13-85" aria-hidden="true" tabindex="-1"></a>        points_sgd_m[<span class="op">-</span><span class="dv">1</span>, <span class="dv">0</span>],</span>
<span id="cb13-86"><a href="#cb13-86" aria-hidden="true" tabindex="-1"></a>        points_sgd_m[<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>],</span>
<span id="cb13-87"><a href="#cb13-87" aria-hidden="true" tabindex="-1"></a>        color<span class="op">=</span><span class="ss">f"C</span><span class="sc">{</span>n<span class="sc">}</span><span class="ss">"</span>,</span>
<span id="cb13-88"><a href="#cb13-88" aria-hidden="true" tabindex="-1"></a>        marker<span class="op">=</span><span class="st">"o"</span>,</span>
<span id="cb13-89"><a href="#cb13-89" aria-hidden="true" tabindex="-1"></a>        markersize<span class="op">=</span><span class="dv">10</span>,</span>
<span id="cb13-90"><a href="#cb13-90" aria-hidden="true" tabindex="-1"></a>        zorder<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb13-91"><a href="#cb13-91" aria-hidden="true" tabindex="-1"></a>        label<span class="op">=</span><span class="ss">f"wd=</span><span class="sc">{</span>weight_decay<span class="sc">:0.1f}</span><span class="ss">"</span></span>
<span id="cb13-92"><a href="#cb13-92" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb13-93"><a href="#cb13-93" aria-hidden="true" tabindex="-1"></a>    ax.ticklabel_format(axis<span class="op">=</span><span class="st">'both'</span>, style<span class="op">=</span><span class="st">'scientific'</span>, scilimits<span class="op">=</span>(<span class="dv">0</span>, <span class="dv">0</span>))</span>
<span id="cb13-94"><a href="#cb13-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-95"><a href="#cb13-95" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb13-96"><a href="#cb13-96" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Function parameters = [ 4.59165436  0.34582255 -0.03204057 -1.09810435  1.54530407]
Optimal parameters: [ 4.59165436  0.34582255 -0.03204057 -1.09810435  1.54530407]

train_loss_surface: SGD
SGD: Iter=1 loss=4.7671 w=[-0.8065, 0.9251]
SGD: Iter=200 loss=1.9090 w=[0.3458, -0.0320]

train_loss_surface: SGD
SGD: Iter=1 loss=4.7671 w=[-0.2065, 0.3251]
SGD: Iter=200 loss=1.9952 w=[0.1327, -0.0077]

train_loss_surface: SGD
SGD: Iter=1 loss=4.7671 w=[0.1935, -0.0749]
SGD: Iter=200 loss=2.0293 w=[0.0935, -0.0051]

train_loss_surface: SGD
SGD: Iter=1 loss=4.7671 w=[0.9935, -0.8749]
SGD: Iter=200 loss=2.0641 w=[0.0587, -0.0030]

train_loss_surface: SGD
SGD: Iter=1 loss=4.7671 w=[1.1935, -1.0749]
SGD: Iter=200 loss=2.0694 w=[0.0537, -0.0027]</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="06_过拟合和解决技术的发展_files/figure-html/cell-11-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>从图中可以看出，L2正则化（weight decay）越大，优化器达到的最终点就越远离损失函数的最低点。这是因为L2正则化防止权重变得过大，从而帮助模型避免过拟合。</p>
<p>L1正则化通过将一些权重设为0来生成稀疏模型（sparse model）。当希望降低模型复杂度和去除不必要的特征时非常有用。相比之下，L2正则化不会完全将权重设为0，而是保持所有权重较小。L2正则化通常表现出更稳定的收敛性，并且由于逐渐减少权重而被称为“平滑的正则化”。</p>
<p>L1正则化和L2正则化根据问题特性、数据和模型目的的不同而应用不同。尽管一般情况下L2正则化使用更为广泛，但最好尝试两种正则化方法，查看哪种表现更好。此外，还可以考虑结合了L1正则化和L2正则化的Elastic Net正则化。</p>
<div class="callout callout-style-default callout-note callout-titled" title="点击查看内容（深入探讨：Elastic Net 正则化 - L1 和 L2 的结合）">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
点击查看内容（深入探讨：Elastic Net 正则化 - L1 和 L2 的结合）
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<section id="弹性网络正则化---l1和l2的和谐" class="level2 callout-body-container callout-body">
<h2 class="anchored" data-anchor-id="弹性网络正则化---l1和l2的和谐">弹性网络正则化 - L1和L2的和谐</h2>
<p>弹性网络是一种结合了L1正则化和L2正则化的正则化方法。它吸取了每种正则化的优点，并弥补了它们的缺点，从而可以创建更灵活、更有效的模型。</p>
<p><strong>核心:</strong></p>
<ul>
<li><strong>L1正则化 (Lasso):</strong> 限制权重绝对值之和。使某些权重 <em>精确为0</em>，生成稀疏(sparse)模型。具有特征选择(feature selection)效果，可以消除不必要的特征并简化模型。</li>
<li><strong>L2正则化 (Ridge):</strong> 限制权重平方之和。保持所有权重 <em>较小</em>，防止模型过拟合(overfitting)。收敛稳定且平滑地减少权重。</li>
<li><strong>弹性网络:</strong> 同时应用 L1正则化和L2正则化。可以获得两种正则化的效果。</li>
</ul>
<p><strong>公式:</strong></p>
<p>弹性网络的成本函数可以表示为：</p>
<p><span class="math inline">\(Cost = Loss + \lambda_1 \sum_{i} |w_i| + \lambda_2 \sum_{i} (w_i)^2\)</span></p>
<ul>
<li><code>Loss</code>: 原始模型的损失函数（例如：MSE, Cross-Entropy）</li>
<li><code>λ₁</code>: 调整L1正则化强度的超参数</li>
<li><code>λ₂</code>: 调整L2正则化强度的超参数</li>
<li><code>wᵢ</code>: 模型的权重</li>
</ul>
<p><strong>优点:</strong></p>
<ul>
<li><strong>特征选择 + 过拟合预防:</strong> 可以同时获得L1正则化的特征选择效果和L2正则化的过拟合预防效果。</li>
<li><strong>处理高相关性特征:</strong> L1正则化倾向于从高度相关的特征中仅选择一个，而将其他设置为0。弹性网络通过L2正则化缓解这一问题，显示出同时 <em>选择</em> 或 <em>消除</em> 高度相关特征的倾向。</li>
<li><strong>灵活性:</strong> 通过调整<code>λ₁</code>和<code>λ₂</code>来控制L1正则化和L2正则化的权重比例。当<code>λ₁=0</code>时为L2正则化(Ridge)，当<code>λ₂=0</code>时为L1正则化(Lasso)。</li>
</ul>
<p><strong>缺点:</strong></p>
<ul>
<li><strong>超参数调优:</strong> 需要调整两个超参数<code>λ₁</code>和<code>λ₂</code>，因此可能比单独使用L1或L2正则化更复杂。</li>
<li><strong>计算成本:</strong> 由于需要同时计算L1和L2，计算成本可能会略有增加（在现代机器学习中这通常不是大问题）。</li>
</ul>
<p><strong>适用情况</strong></p>
<ul>
<li>特征(feature)数量多，并且预计其中只有少数重要时（需要特征选择）</li>
<li>特征之间相关性高</li>
<li>不确定L1正则化和L2正则化哪个更好时（希望尝试两种方法）</li>
<li>想要在防止过拟合的同时，创建具有一定稀疏性的模型</li>
</ul>
<p><strong>总结:</strong> 弹性网络是一种结合了L1和L2优点的强大正则化方法。虽然需要进行超参数调优，但在各种问题中可以表现出良好的性能。</p>
</section>
</div>
</div>
</section>
</section>
<section id="dropout丢弃法" class="level2">
<h2 class="anchored" data-anchor-id="dropout丢弃法">6.3 dropout（丢弃法）</h2>
<section id="dropout的原理" class="level3">
<h3 class="anchored" data-anchor-id="dropout的原理">6.3.1 dropout的原理</h3>
<p>dropout是防止神经网络过拟合的强大正则化方法之一。在训练过程中，随机使一些神经元失活(dropout)，以防止特定神经元或神经元组合过度依赖于训练数据。这类似于多个个体各自学习不同部分后合力解决问题的集成学习效果。通过引导每个神经元独立地学习重要特征，提高模型的泛化性能。通常应用于全连接层(fully connected layer)，失活率设定在20%到50%之间。dropout仅在训练时应用，在推理(inference)时使用所有神经元。</p>
</section>
<section id="在pytorch中实现dropout" class="level3">
<h3 class="anchored" data-anchor-id="在pytorch中实现dropout">6.3.2 在PyTorch中实现dropout</h3>
<p>在PyTorch中，可以如下简单地实现dropout。</p>
<div id="cell-25" class="cell" data-execution_count="9">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Dropout(nn.Module):</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, dropout_rate):</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(Dropout, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout_rate <span class="op">=</span> dropout_rate</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.training:</span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>            mask <span class="op">=</span> torch.bernoulli(torch.ones_like(x) <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> <span class="va">self</span>.dropout_rate)) <span class="op">/</span> (<span class="dv">1</span> <span class="op">-</span> <span class="va">self</span>.dropout_rate)</span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> x <span class="op">*</span> mask</span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> x</span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Usage example.  Drops out 0.5 (50%).</span></span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a>dropout <span class="op">=</span> Dropout(dropout_rate<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Example input data</span></span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a>inputs <span class="op">=</span> torch.randn(<span class="dv">1000</span>, <span class="dv">100</span>)</span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Forward pass (during training)</span></span>
<span id="cb15-22"><a href="#cb15-22" aria-hidden="true" tabindex="-1"></a>dropout.train()</span>
<span id="cb15-23"><a href="#cb15-23" aria-hidden="true" tabindex="-1"></a>outputs_train <span class="op">=</span> dropout(inputs)</span>
<span id="cb15-24"><a href="#cb15-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-25"><a href="#cb15-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Forward pass (during inference)</span></span>
<span id="cb15-26"><a href="#cb15-26" aria-hidden="true" tabindex="-1"></a>dropout.<span class="bu">eval</span>()</span>
<span id="cb15-27"><a href="#cb15-27" aria-hidden="true" tabindex="-1"></a>outputs_test <span class="op">=</span> dropout(inputs)</span>
<span id="cb15-28"><a href="#cb15-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-29"><a href="#cb15-29" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Input shape:"</span>, inputs.shape)</span>
<span id="cb15-30"><a href="#cb15-30" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Training output shape:"</span>, outputs_train.shape)</span>
<span id="cb15-31"><a href="#cb15-31" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Test output shape"</span>, outputs_test.shape)</span>
<span id="cb15-32"><a href="#cb15-32" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Dropout rate (should be close to 0.5):"</span>, <span class="dv">1</span> <span class="op">-</span> torch.count_nonzero(outputs_train) <span class="op">/</span> outputs_train.numel())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Input shape: torch.Size([1000, 100])
Training output shape: torch.Size([1000, 100])
Test output shape torch.Size([1000, 100])
Dropout rate (should be close to 0.5): tensor(0.4997)</code></pre>
</div>
</div>
<p>实现非常简单。将 <code>mask</code> 值乘以输入张量，以停用一定比例的神经元。Dropout 层没有单独的学习参数，只是随机地将输入的一部分设置为 0。在实际的神经网络中，通常会在其他层（如线性层、卷积层）之间插入 Dropout 层。 在训练时，Dropout 随机移除一些神经元，但在推理时使用所有神经元。这时，为了使训练和推理时的输出值保持一致，使用 <em>inverted dropout</em> 方法。Inverted dropout 通过在训练时用 (1 - dropout_rate) 进行缩放，使得在推理时无需额外计算即可直接使用。这样可以在推理时获得类似于集成学习的效果，即相当于平均多个子网络(sub-network)，同时提高计算效率。</p>
<p>我们使用简单的数据来观察 Dropout 的效果，并通过图表展示。源代码位于 <code>chapter_06/plot_dropout.py</code>，由于不是重点，这里不再详细介绍。代码中注释详细，阅读不会困难。从图中可以看出，应用了 Dropout 的模型（蓝色）测试准确率明显更高。</p>
<div id="cell-27" class="cell" data-execution_count="11">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_06.plot_dropout <span class="im">import</span> plot_dropout_effect</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>plot_dropout_effect()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="06_过拟合和解决技术的发展_files/figure-html/cell-13-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>应用了dropout的模型（With Dropout）的训练准确度低于未应用dropout的模型（Without Dropout），但验证准确度更高。这表明dropout减少了对训练数据的过拟合，并提高了模型的泛化性能。</p>
</section>
</section>
<section id="批归一化batch-normalization" class="level2">
<h2 class="anchored" data-anchor-id="批归一化batch-normalization">6.4 批归一化(batch normalization)</h2>
<section id="批归一化的概念和效果" class="level3">
<h3 class="anchored" data-anchor-id="批归一化的概念和效果">6.4.1 批归一化的概念和效果</h3>
<p>批归一化是同时具有正则化作用并提高训练时数据稳定性的方法。批归一化最初在2015年Ioffe和Szegedy的论文[参考2]中提出。在深度学习中，数据通过每一层时激活值的分布会发生变化（内部协变量偏移），这会导致训练速度变慢并且模型变得不稳定（由于分布变化需要更多的计算步骤）。尤其是当层数较多时，这个问题会更加严重。批归一化为了缓解这一问题，以小批量为单位对数据进行归一化。</p>
<p>批归一化的核心思想是以小批量为单位对数据进行归一化。下面的代码可以帮助理解这一点。</p>
<div id="cell-30" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the mean and variance of the mini-batch</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>batch_mean <span class="op">=</span> x.mean(dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>batch_var <span class="op">=</span> x.var(dim<span class="op">=</span><span class="dv">0</span>, unbiased<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform normalization</span></span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>x_norm <span class="op">=</span> (x <span class="op">-</span> batch_mean) <span class="op">/</span> torch.sqrt(batch_var <span class="op">+</span> epsilon)</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Apply scale and shift parameters</span></span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> gamma <span class="op">*</span> x_norm <span class="op">+</span> beta</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>通常，批归一化会在一个 mini-batch 内使用数据的方差和均值来适当地改变整体数据的分布。首先进行归一化，然后应用一定量的比例参数和移动参数。上面的 gamma 是比例参数，beta 是移动参数。可以简单地将其视为 <span class="math inline">\(y = ax + b\)</span>。在执行归一化过程中使用的 epsilon 是数值分析中常见的非常小的常数值（1e-5 或 1e-7）。这个值用于提高数值稳定性。批归一化提供了以下额外效果。</p>
<ul>
<li><strong>加快学习速度</strong>：通过稳定各层的激活值分布，减轻梯度消失/爆炸问题，并允许使用更大的学习率。</li>
<li><strong>减少对初始化的依赖性</strong>：降低对权重初始化的敏感性，使学习更容易开始。</li>
<li><strong>正则化效果</strong>：由于是在 mini-batch 单位上计算统计信息，因此具有添加少量噪声的效果，有助于防止过拟合。（与 Dropout 一起使用时效果更好。）</li>
</ul>
<p>我们将随机生成一个有两个特征的数据，并将纯归一化应用的情况和应用比例、移动参数的情况用图表进行比较。通过可视化可以很容易地理解 mini-batch 的归一化具有什么样的数值意义。</p>
<div id="cell-32" class="cell" data-execution_count="21">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate data</span></span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.random.rand(<span class="dv">50</span>, <span class="dv">2</span>) <span class="op">*</span> <span class="dv">10</span></span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Batch normalization (including scaling parameters)</span></span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> batch_normalize(x, epsilon<span class="op">=</span><span class="fl">1e-5</span>, gamma<span class="op">=</span><span class="fl">1.0</span>, beta<span class="op">=</span><span class="fl">0.0</span>):</span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a>    mean <span class="op">=</span> x.mean(axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a>    var <span class="op">=</span> x.var(axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a>    x_norm <span class="op">=</span> (x <span class="op">-</span> mean) <span class="op">/</span> np.sqrt(var <span class="op">+</span> epsilon)</span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a>    x_scaled <span class="op">=</span> gamma <span class="op">*</span> x_norm <span class="op">+</span> beta</span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x_norm, mean, x_scaled</span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform normalization (gamma=1.0, beta=0.0 is pure normalization)</span></span>
<span id="cb19-18"><a href="#cb19-18" aria-hidden="true" tabindex="-1"></a>x_norm, mean, x_norm_scaled <span class="op">=</span> batch_normalize(x, gamma<span class="op">=</span><span class="fl">1.0</span>, beta<span class="op">=</span><span class="fl">0.0</span>)</span>
<span id="cb19-19"><a href="#cb19-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform normalization and scaling (apply gamma=2.0, beta=1.0)</span></span>
<span id="cb19-20"><a href="#cb19-20" aria-hidden="true" tabindex="-1"></a>_, _, x_scaled <span class="op">=</span> batch_normalize(x, gamma<span class="op">=</span><span class="fl">2.0</span>, beta<span class="op">=</span><span class="fl">1.0</span>)</span>
<span id="cb19-21"><a href="#cb19-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-22"><a href="#cb19-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Set Seaborn style</span></span>
<span id="cb19-23"><a href="#cb19-23" aria-hidden="true" tabindex="-1"></a>sns.set_style(<span class="st">"whitegrid"</span>)</span>
<span id="cb19-24"><a href="#cb19-24" aria-hidden="true" tabindex="-1"></a>sns.set_context(<span class="st">"notebook"</span>, font_scale<span class="op">=</span><span class="fl">1.2</span>)</span>
<span id="cb19-25"><a href="#cb19-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-26"><a href="#cb19-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualization</span></span>
<span id="cb19-27"><a href="#cb19-27" aria-hidden="true" tabindex="-1"></a>fig, (ax1, ax2, ax3) <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">3</span>, figsize<span class="op">=</span>(<span class="dv">18</span>, <span class="dv">5</span>))</span>
<span id="cb19-28"><a href="#cb19-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-29"><a href="#cb19-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Original data</span></span>
<span id="cb19-30"><a href="#cb19-30" aria-hidden="true" tabindex="-1"></a>sns.scatterplot(x<span class="op">=</span>x[:, <span class="dv">0</span>], y<span class="op">=</span>x[:, <span class="dv">1</span>], ax<span class="op">=</span>ax1, color<span class="op">=</span><span class="st">'royalblue'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb19-31"><a href="#cb19-31" aria-hidden="true" tabindex="-1"></a>ax1.scatter(mean[<span class="dv">0</span>], mean[<span class="dv">1</span>], color<span class="op">=</span><span class="st">'red'</span>, marker<span class="op">=</span><span class="st">'*'</span>, s<span class="op">=</span><span class="dv">200</span>, label<span class="op">=</span><span class="st">'Mean'</span>)</span>
<span id="cb19-32"><a href="#cb19-32" aria-hidden="true" tabindex="-1"></a>ax1.<span class="bu">set</span>(title<span class="op">=</span><span class="st">'Original Data'</span>,</span>
<span id="cb19-33"><a href="#cb19-33" aria-hidden="true" tabindex="-1"></a>        xlabel<span class="op">=</span><span class="st">'Feature 1'</span>,</span>
<span id="cb19-34"><a href="#cb19-34" aria-hidden="true" tabindex="-1"></a>        ylabel<span class="op">=</span><span class="st">'Feature 2'</span>,</span>
<span id="cb19-35"><a href="#cb19-35" aria-hidden="true" tabindex="-1"></a>        xlim<span class="op">=</span>(<span class="op">-</span><span class="dv">2</span>, <span class="dv">12</span>),</span>
<span id="cb19-36"><a href="#cb19-36" aria-hidden="true" tabindex="-1"></a>        ylim<span class="op">=</span>(<span class="op">-</span><span class="dv">2</span>, <span class="dv">12</span>))</span>
<span id="cb19-37"><a href="#cb19-37" aria-hidden="true" tabindex="-1"></a>ax1.legend()</span>
<span id="cb19-38"><a href="#cb19-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-39"><a href="#cb19-39" aria-hidden="true" tabindex="-1"></a><span class="co"># After normalization (gamma=1, beta=0)</span></span>
<span id="cb19-40"><a href="#cb19-40" aria-hidden="true" tabindex="-1"></a>sns.scatterplot(x<span class="op">=</span>x_norm[:, <span class="dv">0</span>], y<span class="op">=</span>x_norm[:, <span class="dv">1</span>], ax<span class="op">=</span>ax2, color<span class="op">=</span><span class="st">'crimson'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb19-41"><a href="#cb19-41" aria-hidden="true" tabindex="-1"></a>ax2.scatter(<span class="dv">0</span>, <span class="dv">0</span>, color<span class="op">=</span><span class="st">'blue'</span>, marker<span class="op">=</span><span class="st">'*'</span>, s<span class="op">=</span><span class="dv">200</span>, label<span class="op">=</span><span class="st">'Mean (0,0)'</span>)</span>
<span id="cb19-42"><a href="#cb19-42" aria-hidden="true" tabindex="-1"></a>ax2.axhline(y<span class="op">=</span><span class="dv">0</span>, color<span class="op">=</span><span class="st">'k'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb19-43"><a href="#cb19-43" aria-hidden="true" tabindex="-1"></a>ax2.axvline(x<span class="op">=</span><span class="dv">0</span>, color<span class="op">=</span><span class="st">'k'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb19-44"><a href="#cb19-44" aria-hidden="true" tabindex="-1"></a>ax2.<span class="bu">set</span>(title<span class="op">=</span><span class="st">'After Normalization (γ=1, β=0)'</span>,</span>
<span id="cb19-45"><a href="#cb19-45" aria-hidden="true" tabindex="-1"></a>        xlabel<span class="op">=</span><span class="st">'Normalized Feature 1'</span>,</span>
<span id="cb19-46"><a href="#cb19-46" aria-hidden="true" tabindex="-1"></a>        ylabel<span class="op">=</span><span class="st">'Normalized Feature 2'</span>,</span>
<span id="cb19-47"><a href="#cb19-47" aria-hidden="true" tabindex="-1"></a>        xlim<span class="op">=</span>(<span class="op">-</span><span class="dv">2</span>, <span class="dv">12</span>),</span>
<span id="cb19-48"><a href="#cb19-48" aria-hidden="true" tabindex="-1"></a>        ylim<span class="op">=</span>(<span class="op">-</span><span class="dv">2</span>, <span class="dv">12</span>))</span>
<span id="cb19-49"><a href="#cb19-49" aria-hidden="true" tabindex="-1"></a>ax2.legend()</span>
<span id="cb19-50"><a href="#cb19-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-51"><a href="#cb19-51" aria-hidden="true" tabindex="-1"></a><span class="co"># After scaling and shifting (gamma=2, beta=1)</span></span>
<span id="cb19-52"><a href="#cb19-52" aria-hidden="true" tabindex="-1"></a>sns.scatterplot(x<span class="op">=</span>x_scaled[:, <span class="dv">0</span>], y<span class="op">=</span>x_scaled[:, <span class="dv">1</span>], ax<span class="op">=</span>ax3, color<span class="op">=</span><span class="st">'green'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb19-53"><a href="#cb19-53" aria-hidden="true" tabindex="-1"></a>ax3.scatter(<span class="dv">1</span>, <span class="dv">1</span>, color<span class="op">=</span><span class="st">'purple'</span>, marker<span class="op">=</span><span class="st">'*'</span>, s<span class="op">=</span><span class="dv">200</span>, label<span class="op">=</span><span class="st">'New Mean'</span>)</span>
<span id="cb19-54"><a href="#cb19-54" aria-hidden="true" tabindex="-1"></a>ax3.axhline(y<span class="op">=</span><span class="dv">1</span>, color<span class="op">=</span><span class="st">'k'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb19-55"><a href="#cb19-55" aria-hidden="true" tabindex="-1"></a>ax3.axvline(x<span class="op">=</span><span class="dv">1</span>, color<span class="op">=</span><span class="st">'k'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb19-56"><a href="#cb19-56" aria-hidden="true" tabindex="-1"></a>ax3.<span class="bu">set</span>(title<span class="op">=</span><span class="st">'After Scale &amp; Shift (γ=2, β=1)'</span>,</span>
<span id="cb19-57"><a href="#cb19-57" aria-hidden="true" tabindex="-1"></a>        xlabel<span class="op">=</span><span class="st">'Scaled Feature 1'</span>,</span>
<span id="cb19-58"><a href="#cb19-58" aria-hidden="true" tabindex="-1"></a>        ylabel<span class="op">=</span><span class="st">'Scaled Feature 2'</span>,</span>
<span id="cb19-59"><a href="#cb19-59" aria-hidden="true" tabindex="-1"></a>        xlim<span class="op">=</span>(<span class="op">-</span><span class="dv">2</span>, <span class="dv">12</span>),</span>
<span id="cb19-60"><a href="#cb19-60" aria-hidden="true" tabindex="-1"></a>        ylim<span class="op">=</span>(<span class="op">-</span><span class="dv">2</span>, <span class="dv">12</span>))</span>
<span id="cb19-61"><a href="#cb19-61" aria-hidden="true" tabindex="-1"></a>ax3.legend()</span>
<span id="cb19-62"><a href="#cb19-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-63"><a href="#cb19-63" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb19-64"><a href="#cb19-64" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb19-65"><a href="#cb19-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-66"><a href="#cb19-66" aria-hidden="true" tabindex="-1"></a><span class="co"># Print statistics</span></span>
<span id="cb19-67"><a href="#cb19-67" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Original Data Statistics:"</span>)</span>
<span id="cb19-68"><a href="#cb19-68" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Mean: </span><span class="sc">{</span>mean<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb19-69"><a href="#cb19-69" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Variance: </span><span class="sc">{</span>x<span class="sc">.</span>var(axis<span class="op">=</span><span class="dv">0</span>)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb19-70"><a href="#cb19-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-71"><a href="#cb19-71" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Normalized Data Statistics (γ=1, β=0):"</span>)</span>
<span id="cb19-72"><a href="#cb19-72" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Mean: </span><span class="sc">{</span>x_norm<span class="sc">.</span>mean(axis<span class="op">=</span><span class="dv">0</span>)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb19-73"><a href="#cb19-73" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Variance: </span><span class="sc">{</span>x_norm<span class="sc">.</span>var(axis<span class="op">=</span><span class="dv">0</span>)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb19-74"><a href="#cb19-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-75"><a href="#cb19-75" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Scaled Data Statistics (γ=2, β=1):"</span>)</span>
<span id="cb19-76"><a href="#cb19-76" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Mean: </span><span class="sc">{</span>x_scaled<span class="sc">.</span>mean(axis<span class="op">=</span><span class="dv">0</span>)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb19-77"><a href="#cb19-77" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Variance: </span><span class="sc">{</span>x_scaled<span class="sc">.</span>var(axis<span class="op">=</span><span class="dv">0</span>)<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="06_过拟合和解决技术的发展_files/figure-html/cell-15-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Original Data Statistics:
Mean: [4.40716778 4.99644709]
Variance: [8.89458134 8.45478364]

Normalized Data Statistics (γ=1, β=0):
Mean: [-2.70894418e-16 -3.59712260e-16]
Variance: [0.99999888 0.99999882]

Scaled Data Statistics (γ=2, β=1):
Mean: [1. 1.]
Variance: [3.9999955  3.99999527]</code></pre>
</div>
</div>
<p>在 <code>seed(42)</code> 中，我们经常看到将随机初始值设置为 42。这是一种编程惯例，实际上可以使用其他数字。42 这个数字出自道格拉斯·亚当斯的小说《银河系漫游指南》，在那里它是“生命、宇宙及一切的终极答案”。因此，在程序员之间，它常被用作示例代码中的惯用表达。</p>
</section>
<section id="在-pytorch-中实现批归一化" class="level3">
<h3 class="anchored">6.4.2 在 PyTorch 中实现批归一化</h3>
<p>在 PyTorch 中的实现通常是在神经网络层中插入批归一化层。以下是一个示例。</p>
<div id="cell-35" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SimpleNet(nn.Module):</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.network <span class="op">=</span> nn.Sequential(</span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">784</span>, <span class="dv">256</span>),</span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>            nn.BatchNorm1d(<span class="dv">256</span>),  <span class="co"># 배치 정규화 층</span></span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">256</span>, <span class="dv">10</span>)</span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.network(x)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>在 PyTorch 中，基于原始源代码简化后的批量归一化实现如下。正如前一章所述，这是为了简洁和学习目的而实现的。</p>
<div id="cell-37" class="cell" data-execution_count="23">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> BatchNorm1d(nn.Module):</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, num_features, eps<span class="op">=</span><span class="fl">1e-5</span>, momentum<span class="op">=</span><span class="fl">0.1</span>):</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.num_features <span class="op">=</span> num_features</span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.eps <span class="op">=</span> eps</span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.momentum <span class="op">=</span> momentum</span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Trainable parameters</span></span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.gamma <span class="op">=</span> nn.Parameter(torch.ones(num_features))  <span class="co"># scale</span></span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.beta <span class="op">=</span> nn.Parameter(torch.zeros(num_features))  <span class="co"># shift</span></span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-15"><a href="#cb22-15" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Running statistics to be tracked</span></span>
<span id="cb22-16"><a href="#cb22-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.register_buffer(<span class="st">'running_mean'</span>, torch.zeros(num_features))</span>
<span id="cb22-17"><a href="#cb22-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.register_buffer(<span class="st">'running_var'</span>, torch.ones(num_features))</span>
<span id="cb22-18"><a href="#cb22-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-19"><a href="#cb22-19" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb22-20"><a href="#cb22-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.training:</span>
<span id="cb22-21"><a href="#cb22-21" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Calculate mini-batch statistics</span></span>
<span id="cb22-22"><a href="#cb22-22" aria-hidden="true" tabindex="-1"></a>            batch_mean <span class="op">=</span> x.mean(dim<span class="op">=</span><span class="dv">0</span>)  <span class="co"># Mean per channel</span></span>
<span id="cb22-23"><a href="#cb22-23" aria-hidden="true" tabindex="-1"></a>            batch_var <span class="op">=</span> x.var(dim<span class="op">=</span><span class="dv">0</span>, unbiased<span class="op">=</span><span class="va">False</span>)  <span class="co"># Variance per channel</span></span>
<span id="cb22-24"><a href="#cb22-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-25"><a href="#cb22-25" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Update running statistics (important)</span></span>
<span id="cb22-26"><a href="#cb22-26" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.running_mean <span class="op">=</span> (<span class="dv">1</span> <span class="op">-</span> <span class="va">self</span>.momentum) <span class="op">*</span> <span class="va">self</span>.running_mean <span class="op">+</span> <span class="va">self</span>.momentum <span class="op">*</span> batch_mean</span>
<span id="cb22-27"><a href="#cb22-27" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.running_var <span class="op">=</span> (<span class="dv">1</span> <span class="op">-</span> <span class="va">self</span>.momentum) <span class="op">*</span> <span class="va">self</span>.running_var <span class="op">+</span> <span class="va">self</span>.momentum <span class="op">*</span> batch_var</span>
<span id="cb22-28"><a href="#cb22-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-29"><a href="#cb22-29" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Normalize</span></span>
<span id="cb22-30"><a href="#cb22-30" aria-hidden="true" tabindex="-1"></a>            x_norm <span class="op">=</span> (x <span class="op">-</span> batch_mean) <span class="op">/</span> torch.sqrt(batch_var <span class="op">+</span> <span class="va">self</span>.eps)</span>
<span id="cb22-31"><a href="#cb22-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-32"><a href="#cb22-32" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb22-33"><a href="#cb22-33" aria-hidden="true" tabindex="-1"></a>            <span class="co"># During inference, use the stored statistics</span></span>
<span id="cb22-34"><a href="#cb22-34" aria-hidden="true" tabindex="-1"></a>            x_norm <span class="op">=</span> (x <span class="op">-</span> <span class="va">self</span>.running_mean) <span class="op">/</span> torch.sqrt(<span class="va">self</span>.running_var <span class="op">+</span> <span class="va">self</span>.eps)</span>
<span id="cb22-35"><a href="#cb22-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-36"><a href="#cb22-36" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Apply scale and shift</span></span>
<span id="cb22-37"><a href="#cb22-37" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.gamma <span class="op">*</span> x_norm <span class="op">+</span> <span class="va">self</span>.beta</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>前面的基本实现与主要不同之处在于更新运行中的统计信息。在训练时，累积每个小批量的统计值（均值和方差）以最终得知整体均值和方差。跟踪这种移动是通过使用动量（默认值为0.1）的指数移动平均(Exponential Moving Average)来实现的。利用训练中获得的这些均值和方差在推理时可以对推理数据应用准确的方差、偏差，从而保证学习和推理的一致性。</p>
<p>当然，这种实现是为了学习目的而极大简化了的。参考的代码位置是 (https://github.com/pytorch/pytorch/blob/main/torch/nn/modules/batchnorm.py)。BatchNorm1d的实际实现要复杂得多。这是因为通常在PyTorch、TensorFlow等框架中，除了基本逻辑外，还包含了CUDA优化、梯度优化、各种设置处理以及与C/C++的互操作等多种逻辑。</p>
<div class="callout callout-style-default callout-note callout-titled" title="点击查看内容（深入探讨：批归一化的公式推导及反向传播过程详细分析）">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
点击查看内容（深入探讨：批归一化的公式推导及反向传播过程详细分析）
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<section id="批归一化的公式推导及反向传播过程详细分析" class="level2 callout-body-container callout-body">
<h2 class="anchored" data-anchor-id="批归一化的公式推导及反向传播过程详细分析">批归一化的公式推导及反向传播过程详细分析</h2>
<p>批归一化（Batch Normalization，BN）自2015年被Ioffe &amp; Szegedy提出以来，已成为深度学习模型训练的核心技术之一。BN通过对每一层的输入进行归一化来加速学习速度，缓解梯度消失/爆炸问题，并提供一定程度的正则化（regularization）效果。在本次深入探讨中，我们将详细分析BN的前向传播及反向传播过程，并对其效果进行数学上的分析。</p>
<section id="批归一化的前向传播forward-pass公式推导" class="level3">
<h3 class="anchored" data-anchor-id="批归一化的前向传播forward-pass公式推导">批归一化的前向传播（Forward Pass）公式推导</h3>
<p>批归一化以小批量（mini-batch）为单位执行。设小批量的大小为<span class="math inline">\(B\)</span>，特征(feature)的维度为<span class="math inline">\(D\)</span>时，小批量输入数据可以表示为<span class="math inline">\(B \times D\)</span>矩阵<span class="math inline">\(\mathbf{X}\)</span>。由于BN在每个特征维度上独立进行，为了说明方便，我们仅考虑一个特征维度上的运算。</p>
<ol type="1">
<li><p><strong>计算小批量平均值:</strong></p>
<p><span class="math inline">\(\mu_B = \frac{1}{B} \sum_{i=1}^{B} x_i\)</span></p>
<p>其中<span class="math inline">\(x_i\)</span>表示小批量的第<span class="math inline">\(i\)</span>个样本在该特征上的值。</p></li>
<li><p><strong>计算小批量方差:</strong></p>
<p><span class="math inline">\(\sigma_B^2 = \frac{1}{B} \sum_{i=1}^{B} (x_i - \mu_B)^2\)</span></p></li>
<li><p><strong>归一化(Normalization):</strong></p>
<p><span class="math inline">\(\hat{x_i} = \frac{x_i - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}}\)</span></p>
<p>其中<span class="math inline">\(\epsilon\)</span>是一个小常数，用于防止分母为0。</p></li>
<li><p><strong>缩放及平移(Scale and Shift):</strong></p>
<p><span class="math inline">\(y_i = \gamma \hat{x_i} + \beta\)</span></p>
<p>其中<span class="math inline">\(\gamma\)</span>和<span class="math inline">\(\beta\)</span>是学习参数，分别负责缩放和平移。这些参数的作用是恢复归一化数据的表示能力。</p></li>
</ol>
</section>
<section id="批归一化的反向传播backward-pass公式推导---包含计算图" class="level3">
<h3 class="anchored" data-anchor-id="批归一化的反向传播backward-pass公式推导---包含计算图">批归一化的反向传播（Backward Pass）公式推导 - 包含计算图</h3>
<p>批归一化的反向传播过程使用链式法则(chain rule)来计算损失函数(loss function)对各参数的梯度。通过计算图可以直观地表达这一过程，如下所示。（此处用ASCII艺术简要表示）</p>
<pre><code>     x_i   --&gt;   [-]   --&gt;   [/]   --&gt;   [*]   --&gt;   [+]   --&gt;   y_i
      |          ^          ^          ^          ^
      |          |          |          |          |
      |          |          |          |          +---&gt; beta
      |          |          |          +---&gt; gamma
      |          |          +---&gt; sqrt(...) + epsilon
      |          +---&gt; mu_B, sigma_B^2</code></pre>
<ul>
<li><span class="math inline">\(x_i\)</span>: 输入</li>
<li><span class="math inline">\([-]\)</span>: 减法 (<span class="math inline">\(x_i - \mu_B\)</span>)</li>
<li><span class="math inline">\([/]\)</span>: 除法 (<span class="math inline">\((x_i - \mu_B) / \sqrt{\sigma_B^2 + \epsilon}\)</span>)</li>
<li><span class="math inline">\([*]\)</span>: 乘法 (<span class="math inline">\(\gamma \hat{x_i}\)</span>)</li>
<li><span class="math inline">\([+]\)</span>: 加法 (<span class="math inline">\(\gamma \hat{x_i} + \beta\)</span>)</li>
<li><span class="math inline">\(y_i\)</span>: 输出</li>
<li><span class="math inline">\(\mu_B\)</span>: 平均值</li>
<li><span class="math inline">\(\sigma_B^2\)</span>: 方差</li>
<li><span class="math inline">\(\epsilon\)</span>: 用于防止分母为0的小数</li>
<li><span class="math inline">\(\gamma, \beta\)</span>: 学习参数 现在，我们按步骤计算反向传播。假设损失函数为<span class="math inline">\(\mathcal{L}\)</span>，并且给定了<span class="math inline">\(\frac{\partial \mathcal{L}}{\partial y_i}\)</span>。</li>
</ul>
<ol type="1">
<li><p><strong>计算 <span class="math inline">\(\frac{\partial \mathcal{L}}{\partial \beta}\)</span> 和 <span class="math inline">\(\frac{\partial \mathcal{L}}{\partial \gamma}\)</span>：</strong></p>
<p><span class="math inline">\(\frac{\partial \mathcal{L}}{\partial \beta} = \sum_{i=1}^{B} \frac{\partial \mathcal{L}}{\partial y_i} \cdot \frac{\partial y_i}{\partial \beta} = \sum_{i=1}^{B} \frac{\partial \mathcal{L}}{\partial y_i}\)</span></p>
<p><span class="math inline">\(\frac{\partial \mathcal{L}}{\partial \gamma} = \sum_{i=1}^{B} \frac{\partial \mathcal{L}}{\partial y_i} \cdot \frac{\partial y_i}{\partial \gamma} = \sum_{i=1}^{B} \frac{\partial \mathcal{L}}{\partial y_i} \cdot \hat{x_i}\)</span></p></li>
<li><p><strong>计算 <span class="math inline">\(\frac{\partial \mathcal{L}}{\partial \hat{x_i}}\)</span>：</strong></p>
<p><span class="math inline">\(\frac{\partial \mathcal{L}}{\partial \hat{x_i}} = \frac{\partial \mathcal{L}}{\partial y_i} \cdot \frac{\partial y_i}{\partial \hat{x_i}} = \frac{\partial \mathcal{L}}{\partial y_i} \cdot \gamma\)</span></p></li>
<li><p><strong>计算 <span class="math inline">\(\frac{\partial \mathcal{L}}{\partial \sigma_B^2}\)</span>：</strong></p>
<p><span class="math inline">\(\frac{\partial \mathcal{L}}{\partial \sigma_B^2} = \sum_{i=1}^{B} \frac{\partial \mathcal{L}}{\partial \hat{x_i}} \cdot \frac{\partial \hat{x_i}}{\partial \sigma_B^2} = \sum_{i=1}^{B} \frac{\partial \mathcal{L}}{\partial \hat{x_i}} \cdot (x_i - \mu_B) \cdot (-\frac{1}{2})(\sigma_B^2 + \epsilon)^{-3/2}\)</span></p></li>
<li><p><strong>计算 <span class="math inline">\(\frac{\partial \mathcal{L}}{\partial \mu_B}\)</span>：</strong></p>
<p><span class="math inline">\(\frac{\partial \mathcal{L}}{\partial \mu_B} = \sum_{i=1}^{B} \frac{\partial \mathcal{L}}{\partial \hat{x_i}} \cdot \frac{\partial \hat{x_i}}{\partial \mu_B} + \frac{\partial \mathcal{L}}{\partial \sigma_B^2} \cdot \frac{\partial \sigma_B^2}{\partial \mu_B}  = \sum_{i=1}^{B} \frac{\partial \mathcal{L}}{\partial \hat{x_i}} \cdot \frac{-1}{\sqrt{\sigma_B^2 + \epsilon}} + \frac{\partial \mathcal{L}}{\partial \sigma_B^2} \cdot (-2)\frac{1}{B}\sum_{i=1}^B (x_i-\mu_B)\)</span></p>
<p>由于 <span class="math inline">\(\sum_{i=1}^B (x_i - \mu_B) = 0\)</span> <span class="math inline">\(\frac{\partial \mathcal{L}}{\partial \mu_B} =  \sum_{i=1}^{B} \frac{\partial \mathcal{L}}{\partial \hat{x_i}} \cdot \frac{-1}{\sqrt{\sigma_B^2 + \epsilon}}\)</span></p></li>
<li><p><strong><span class="math inline">\(\frac{\partial \mathcal{L}}{\partial x_i}\)</span> 计算:</strong></p>
<p><span class="math inline">\(\frac{\partial \mathcal{L}}{\partial x_i} = \frac{\partial \mathcal{L}}{\partial \hat{x_i}} \cdot \frac{\partial \hat{x_i}}{\partial x_i} + \frac{\partial \mathcal{L}}{\partial \mu_B} \cdot \frac{\partial \mu_B}{\partial x_i}  + \frac{\partial \mathcal{L}}{\partial \sigma_B^2} \cdot \frac{\partial \sigma_B^2}{\partial x_i} = \frac{\partial \mathcal{L}}{\partial \hat{x_i}} \cdot \frac{1}{\sqrt{\sigma_B^2 + \epsilon}} + \frac{\partial \mathcal{L}}{\partial \mu_B} \cdot \frac{1}{B} +  \frac{\partial \mathcal{L}}{\partial \sigma_B^2} \cdot \frac{2}{B}(x_i - \mu_B)\)</span></p></li>
</ol>
</section>
<section id="批归一化缓解梯度消失爆炸问题的原理说明" class="level3">
<h3 class="anchored" data-anchor-id="批归一化缓解梯度消失爆炸问题的原理说明">批归一化缓解梯度消失/爆炸问题的原理说明</h3>
<p>批归一化通过规范化每一层的输入，防止激活函数的输入值偏向极端。这有助于缓解在sigmoid或tanh等激活函数中出现的梯度消失/爆炸问题。</p>
<ul>
<li><p><strong>梯度消失问题:</strong> 当激活函数的输入非常大或非常小时，该函数的梯度会接近0，导致反向传播过程中梯度消失的现象。批归一化通过将输入规范化为均值0、方差1来保持激活函数的输入在合适的范围内，从而缓解梯度消失问题。</p></li>
<li><p><strong>梯度爆炸问题:</strong> 当激活函数的输入非常大时，梯度会变得极其大。批归一化通过限制输入范围，也可以缓解梯度爆炸问题。</p></li>
</ul>
</section>
<section id="批归一化的移动平均running-mean-running-variance计算及推理时的应用" class="level3">
<h3 class="anchored" data-anchor-id="批归一化的移动平均running-mean-running-variance计算及推理时的应用">批归一化的移动平均（running mean, running variance）计算及推理时的应用</h3>
<p>在训练期间，批归一化以小批量为单位计算均值和方差，但在推理（inference）时需要使用整个训练数据的均值和方差估计。为此，批归一化在训练过程中计算移动平均（running mean）和移动方差（running variance）。</p>
<ul>
<li><p><strong>移动平均计算:</strong></p>
<p><span class="math inline">\(\text{running\_mean} = (1 - \text{momentum}) \times \text{running\_mean} + \text{momentum} \times \mu_B\)</span></p></li>
<li><p><strong>移动方差计算:</strong></p>
<p><span class="math inline">\(\text{running\_var} = (1 - \text{momentum}) \times \text{running\_var} + \text{momentum} \times \sigma_B^2\)</span></p></li>
</ul>
<p>其中 <code>momentum</code> 是一个通常设置为0.1或0.01等小值的超参数。</p>
<p>在推理时，使用训练过程中计算出的 <code>running_mean</code> 和 <code>running_var</code> 对输入进行归一化。</p>
</section>
<section id="批归一化与其他正则化技术layer-normalization-instance-normalization-group-normalization的比较" class="level3">
<h3 class="anchored" data-anchor-id="批归一化与其他正则化技术layer-normalization-instance-normalization-group-normalization的比较">批归一化与其他正则化技术（Layer Normalization, Instance Normalization, Group Normalization）的比较</h3>
<ul>
<li><p><strong>批归一化 (Batch Normalization, BN):</strong> 使用小批量内的样本之间的统计信息。受批量大小的影响，在RNN中应用较为困难。</p></li>
<li><p><strong>层归一化 (Layer Normalization, LN):</strong> 在每个样本内使用特征维度的统计信息。不受批量大小影响，易于在RNN中应用。</p></li>
<li><p><strong>实例归一化 (Instance Normalization, IN):</strong> 独立计算每个样本、每个通道的统计信息。主要用于风格迁移(style transfer)等图像生成任务。</p></li>
<li><p><strong>组归一化 (Group Normalization, GN):</strong> 将通道分成组，在每组内计算统计信息。在批量大小较小时可作为BN的替代方案使用。</p></li>
</ul>
<p>每种归一化技术在不同的情况下各有优缺点，因此应根据问题的特点和模型架构选择合适的技术。</p>
</section>
</section>
</div>
</div>
</section>
<section id="统计跟踪和推理时的应用" class="level3">
<h3 class="anchored" data-anchor-id="统计跟踪和推理时的应用">6.4.3 统计跟踪和推理时的应用</h3>
</section>
</section>
<section id="超参数的优化" class="level2">
<h2 class="anchored" data-anchor-id="超参数的优化">6.5 超参数的优化</h2>
<p>超参数优化对模型性能有非常重要的影响。这种重要性从1990年代开始为人所知。1990年代后期，发现即使在相同的模型中，支持向量机(SVM)中的核函数参数(C, gamma等)也会对性能起决定性作用。2015年左右，证明了贝叶斯优化比手动调参能产生更好的结果，这成为自动化调参方法（如Google AutoML (2017年)）的核心基础。</p>
<section id="优化方法论比较" class="level3">
<h3 class="anchored" data-anchor-id="优化方法论比较">6.5.1 优化方法论比较</h3>
<p>有许多方法可以优化超参数。典型的方法包括：</p>
<ol type="1">
<li><p><strong>网格搜索 (Grid Search):</strong> 最基本的方法，指定每个超参数可能的值列表，并尝试所有这些值的组合。当超参数数量较少且每个参数可取的值范围有限时，这种方法是有用的，但由于需要测试所有的组合，计算成本非常高。适用于测试简单模型或探索空间非常小的情况。</p></li>
<li><p><strong>随机搜索 (Random Search):</strong> 随机选择每个超参数的值生成组合，并使用这些组合训练模型以评估性能。当部分超参数对性能有较大影响时，这种方法可能比网格搜索更有效。(Bergstra &amp; Bengio, 2012)</p></li>
<li><p><strong>贝叶斯优化 (Bayesian Optimization):</strong> 基于之前的探索结果，使用概率模型（通常是高斯过程）智能地选择下一个尝试的超参数组合。选择使获取函数(acquisition function)最大化的点作为下一次探索的位置。由于能高效地探索超参数空间，因此与网格搜索或随机搜索相比，在较少次数的尝试中就能找到更好的组合。</p></li>
</ol>
<p>除了上述方法外，还有利用遗传算法的进化算法(Evolutionary Algorithms)，基于梯度的优化(Gradient-based Optimization)等其他方法。</p>
<p>以下是一个使用贝叶斯优化来优化简单神经网络模型超参数的例子。</p>
</section>
<section id="使用bayes-opt进行优化" class="level3">
<h3 class="anchored" data-anchor-id="使用bayes-opt进行优化">6.5.2 使用Bayes-Opt进行优化</h3>
<p>贝叶斯优化从2010年代开始受到关注。特别是2012年发表了《机器学习算法的实用贝叶斯优化》论文后，它成为了深度学习模型超参数优化的主要方法之一。与网格搜索或随机搜索不同的是，它可以基于之前的尝试结果智能地选择下一个探索的参数，这是它的主要优势。</p>
<p>贝叶斯优化大致重复以下三个步骤：</p>
<ol type="1">
<li><strong>初始采样 (Initialization):</strong> 随机选择指定次数（<code>init_points</code>）的超参数组合训练模型并评估性能。</li>
<li><strong>代理模型 (Surrogate Model) 构建:</strong> 基于迄今为止的实验结果，构建一个描述超参数与性能之间关系的代理模型（通常是高斯过程）。</li>
<li><strong>获取函数 (Acquisition Function) 优化:</strong> 在代理模型的基础上选择下一个最有可能成功的超参数组合。此时使用获取函数(acquisition function)，它基于当前的信息（代理模型），在“探索(exploration)”和“利用(exploitation)”之间找到平衡，从而决定下一个探索点。</li>
<li>重复步骤2~3</li>
</ol>
<div id="cell-42" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.optim <span class="im">as</span> optim</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_04.models.base <span class="im">import</span> SimpleNetwork  </span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_04.utils.data <span class="im">import</span> get_data_loaders, get_device  </span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> bayes_opt <span class="im">import</span> BayesianOptimization</span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_04.experiments.model_training <span class="im">import</span> train_model, eval_loop  </span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train_simple_net(hidden_layers, learning_rate, batch_size, epochs):</span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Trains a SimpleNetwork model with given hyperparameters.</span></span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true" tabindex="-1"></a><span class="co">       Uses CIFAR100 dataset and train_model from Chapter 4.</span></span>
<span id="cb24-13"><a href="#cb24-13" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb24-14"><a href="#cb24-14" aria-hidden="true" tabindex="-1"></a>    device <span class="op">=</span> get_device()  <span class="co"># Use the utility function to get device</span></span>
<span id="cb24-15"><a href="#cb24-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-16"><a href="#cb24-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get data loaders for CIFAR100</span></span>
<span id="cb24-17"><a href="#cb24-17" aria-hidden="true" tabindex="-1"></a>    train_loader, test_loader <span class="op">=</span> get_data_loaders(dataset<span class="op">=</span><span class="st">"CIFAR100"</span>, batch_size<span class="op">=</span>batch_size)</span>
<span id="cb24-18"><a href="#cb24-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-19"><a href="#cb24-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Instantiate the model with specified activation and hidden layers.</span></span>
<span id="cb24-20"><a href="#cb24-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># CIFAR100 images are 3x32x32, so the input size is 3*32*32 = 3072.</span></span>
<span id="cb24-21"><a href="#cb24-21" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> SimpleNetwork(act_func<span class="op">=</span>nn.ReLU(), input_shape<span class="op">=</span><span class="dv">3</span><span class="op">*</span><span class="dv">32</span><span class="op">*</span><span class="dv">32</span>, hidden_shape<span class="op">=</span>hidden_layers, num_labels<span class="op">=</span><span class="dv">100</span>).to(device)</span>
<span id="cb24-22"><a href="#cb24-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-23"><a href="#cb24-23" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Optimizer: Use Adam</span></span>
<span id="cb24-24"><a href="#cb24-24" aria-hidden="true" tabindex="-1"></a>    optimizer <span class="op">=</span> optim.Adam(model.parameters(), lr<span class="op">=</span>learning_rate)</span>
<span id="cb24-25"><a href="#cb24-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-26"><a href="#cb24-26" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Train the model using the training function from Chapter 4</span></span>
<span id="cb24-27"><a href="#cb24-27" aria-hidden="true" tabindex="-1"></a>    results <span class="op">=</span> train_model(model, train_loader, test_loader, device, optimizer<span class="op">=</span>optimizer, epochs<span class="op">=</span>epochs, save_dir<span class="op">=</span><span class="st">"./tmp/tune"</span>,</span>
<span id="cb24-28"><a href="#cb24-28" aria-hidden="true" tabindex="-1"></a>                         retrain<span class="op">=</span><span class="va">True</span>) <span class="co"># retrain=True로 설정</span></span>
<span id="cb24-29"><a href="#cb24-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-30"><a href="#cb24-30" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Return the final test accuracy</span></span>
<span id="cb24-31"><a href="#cb24-31" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> results[<span class="st">'test_accuracies'</span>][<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb24-32"><a href="#cb24-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-33"><a href="#cb24-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-34"><a href="#cb24-34" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train_wrapper(learning_rate, batch_size, hidden1, hidden2):</span>
<span id="cb24-35"><a href="#cb24-35" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Wrapper function for Bayesian optimization."""</span></span>
<span id="cb24-36"><a href="#cb24-36" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> train_simple_net(</span>
<span id="cb24-37"><a href="#cb24-37" aria-hidden="true" tabindex="-1"></a>        hidden_layers<span class="op">=</span>[<span class="bu">int</span>(hidden1), <span class="bu">int</span>(hidden2)],</span>
<span id="cb24-38"><a href="#cb24-38" aria-hidden="true" tabindex="-1"></a>        learning_rate<span class="op">=</span>learning_rate,</span>
<span id="cb24-39"><a href="#cb24-39" aria-hidden="true" tabindex="-1"></a>        batch_size<span class="op">=</span><span class="bu">int</span>(batch_size),</span>
<span id="cb24-40"><a href="#cb24-40" aria-hidden="true" tabindex="-1"></a>        epochs<span class="op">=</span><span class="dv">10</span></span>
<span id="cb24-41"><a href="#cb24-41" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb24-42"><a href="#cb24-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-43"><a href="#cb24-43" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> optimize_hyperparameters():</span>
<span id="cb24-44"><a href="#cb24-44" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Runs hyperparameter optimization."""</span></span>
<span id="cb24-45"><a href="#cb24-45" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Set the parameter ranges to be optimized.</span></span>
<span id="cb24-46"><a href="#cb24-46" aria-hidden="true" tabindex="-1"></a>    pbounds <span class="op">=</span> {</span>
<span id="cb24-47"><a href="#cb24-47" aria-hidden="true" tabindex="-1"></a>        <span class="st">"learning_rate"</span>: (<span class="fl">1e-4</span>, <span class="fl">1e-2</span>),</span>
<span id="cb24-48"><a href="#cb24-48" aria-hidden="true" tabindex="-1"></a>        <span class="st">"batch_size"</span>: (<span class="dv">64</span>, <span class="dv">256</span>),</span>
<span id="cb24-49"><a href="#cb24-49" aria-hidden="true" tabindex="-1"></a>        <span class="st">"hidden1"</span>: (<span class="dv">64</span>, <span class="dv">512</span>),  <span class="co"># First hidden layer</span></span>
<span id="cb24-50"><a href="#cb24-50" aria-hidden="true" tabindex="-1"></a>        <span class="st">"hidden2"</span>: (<span class="dv">32</span>, <span class="dv">256</span>)   <span class="co"># Second hidden layer</span></span>
<span id="cb24-51"><a href="#cb24-51" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb24-52"><a href="#cb24-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-53"><a href="#cb24-53" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create a Bayesian optimization object.</span></span>
<span id="cb24-54"><a href="#cb24-54" aria-hidden="true" tabindex="-1"></a>    optimizer <span class="op">=</span> BayesianOptimization(</span>
<span id="cb24-55"><a href="#cb24-55" aria-hidden="true" tabindex="-1"></a>        f<span class="op">=</span>train_wrapper,</span>
<span id="cb24-56"><a href="#cb24-56" aria-hidden="true" tabindex="-1"></a>        pbounds<span class="op">=</span>pbounds,</span>
<span id="cb24-57"><a href="#cb24-57" aria-hidden="true" tabindex="-1"></a>        random_state<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb24-58"><a href="#cb24-58" aria-hidden="true" tabindex="-1"></a>        allow_duplicate_points<span class="op">=</span><span class="va">True</span></span>
<span id="cb24-59"><a href="#cb24-59" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb24-60"><a href="#cb24-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-61"><a href="#cb24-61" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Run optimization</span></span>
<span id="cb24-62"><a href="#cb24-62" aria-hidden="true" tabindex="-1"></a>    optimizer.maximize(</span>
<span id="cb24-63"><a href="#cb24-63" aria-hidden="true" tabindex="-1"></a>        init_points<span class="op">=</span><span class="dv">4</span>,</span>
<span id="cb24-64"><a href="#cb24-64" aria-hidden="true" tabindex="-1"></a>        n_iter<span class="op">=</span><span class="dv">10</span>,</span>
<span id="cb24-65"><a href="#cb24-65" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb24-66"><a href="#cb24-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-67"><a href="#cb24-67" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Print the best parameters and accuracy</span></span>
<span id="cb24-68"><a href="#cb24-68" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Best parameters found:"</span>)</span>
<span id="cb24-69"><a href="#cb24-69" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Learning Rate: </span><span class="sc">{</span>optimizer<span class="sc">.</span><span class="bu">max</span>[<span class="st">'params'</span>][<span class="st">'learning_rate'</span>]<span class="sc">:.6f}</span><span class="ss">"</span>)</span>
<span id="cb24-70"><a href="#cb24-70" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Batch Size: </span><span class="sc">{</span><span class="bu">int</span>(optimizer.<span class="bu">max</span>[<span class="st">'params'</span>][<span class="st">'batch_size'</span>])<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb24-71"><a href="#cb24-71" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Hidden Layer 1: </span><span class="sc">{</span><span class="bu">int</span>(optimizer.<span class="bu">max</span>[<span class="st">'params'</span>][<span class="st">'hidden1'</span>])<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb24-72"><a href="#cb24-72" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Hidden Layer 2: </span><span class="sc">{</span><span class="bu">int</span>(optimizer.<span class="bu">max</span>[<span class="st">'params'</span>][<span class="st">'hidden2'</span>])<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb24-73"><a href="#cb24-73" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Best accuracy: </span><span class="sc">{</span>optimizer<span class="sc">.</span><span class="bu">max</span>[<span class="st">'target'</span>]<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb24-74"><a href="#cb24-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-75"><a href="#cb24-75" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="va">__name__</span> <span class="op">==</span> <span class="st">"__main__"</span>:</span>
<span id="cb24-76"><a href="#cb24-76" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Starting hyperparameter optimization..."</span>)</span>
<span id="cb24-77"><a href="#cb24-77" aria-hidden="true" tabindex="-1"></a>    optimize_hyperparameters()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>上述示例使用 <code>BayesOpt</code> 包进行超参数优化。以 <code>SimpleNetwork</code>（第4章中定义）为训练目标，并使用 CIFAR100 数据集。<code>train_wrapper</code> 函数充当 <code>BayesOpt</code> 使用的目标函数，它根据给定的超参数组合训练模型并返回最终测试准确率。</p>
<p><code>pbounds</code> 指定了每个超参数的搜索范围。在 <code>optimizer.maximize</code> 中，<code>init_points</code> 是初始随机搜索次数，<code>n_iter</code> 是贝叶斯优化迭代次数。因此，总实验次数是 <code>init_points + n_iter</code>。</p>
<p>在进行超参数搜索时需要注意以下几点：</p>
<ol type="1">
<li><strong>参数范围：</strong> 范围太宽会增加搜索时间，而范围太窄可能会错过最优解。通常情况下，学习率以对数尺度（1e-4 ~ 1e-2），神经元数量以2的幂次单位设置是常见的做法。</li>
<li><strong>迭代次数：</strong> 总尝试次数大致设定为（参数个数）x 20 经验上可以得到良好的结果。在上述示例中，有4个参数，因此总共14次尝试（init_points=4, n_iter=10）可能略显不足。为了获得更好的结果，可以考虑增加 <code>n_iter</code>。</li>
</ol>
</section>
<section id="使用botorch进行优化" class="level3">
<h3 class="anchored" data-anchor-id="使用botorch进行优化">6.5.3 使用BoTorch进行优化</h3>
<p>最近，一个名为BoTorch的框架在深度学习超参数优化领域受到了关注。BoTorch是由FAIR（Facebook AI Research，现为Meta AI）于2019年开发的一个基于PyTorch的贝叶斯优化框架。Bayes-Opt是一个自2016年开始开发的、更早的贝叶斯优化库，它提供了直观且简单的接口（scikit-learn风格的API），因此被广泛使用。</p>
<p>这两个库的优缺点是明确的。</p>
<ul>
<li><strong>BoTorch:</strong>
<ul>
<li><strong>优点:</strong> 提供了与深度学习模型集成、GPU加速、高采样效率、各种高级贝叶斯优化技术（多保真度、多任务、约束优化等）、支持自动微分等功能，这些功能特别适用于深度学习模型的超参数优化。特别是对于大规模模型、高维参数空间和计算成本高的实验非常适合。</li>
<li><strong>缺点:</strong> 相比Bayes-Opt需要更多的学习，并且初始设置可能较为复杂。</li>
</ul></li>
<li><strong>Bayes-Opt:</strong>
<ul>
<li><strong>优点:</strong> 提供简单直观的API，使用起来非常方便。安装简便，教程和示例代码丰富。</li>
<li><strong>缺点:</strong> 与BoTorch相比缺乏高级功能，与深度学习模型的集成相对不流畅。在处理大规模/高维问题时性能可能会下降。</li>
</ul></li>
</ul>
<p>因此，对于简单的任务或快速原型开发建议使用Bayes-Opt；而对于复杂的深度学习模型超参数优化、大规模/高维问题以及需要高级贝叶斯优化技术（如多任务、约束优化等）的情况，则建议使用BoTorch。</p>
<p>要使用BoTorch，与Bayes-Opt不同的是，需要理解一些初始设置中的关键概念（代理模型、输入数据标准化、获取函数）。</p>
<ol type="1">
<li><p><strong>代理(Surrogate) 模型:</strong></p>
<p>代理模型是用于近似实际目标函数（在这里是指深度学习模型的验证准确率）的模型。通常使用高斯过程(GP)。GP可以快速且廉价地预测结果，而不需要计算成本高昂的实际目标函数。BoTorch提供了以下几种GP模型。</p>
<ul>
<li><code>SingleTaskGP</code>: 最基础的高斯过程模型。适用于单目标优化问题，在1000个或更少的数据点上效果良好。</li>
<li><code>MultiTaskGP</code>: 用于同时优化多个目标函数（多目标优化）的情况。例如，可以同时优化模型的准确率和推理时间。</li>
<li><code>SAASBO</code> (Sparsity-Aware Adaptive Subspace Bayesian Optimization): 专门针对高维参数空间设计的模型。假设在高维空间中存在稀疏性，并进行高效的探索。</li>
</ul></li>
<li><p><strong>输入数据标准化:</strong></p>
<p>高斯过程对数据的尺度敏感，因此对输入数据（超参数）进行标准化是非常重要的。通常将所有超参数转换为[0, 1]范围内的值。BoTorch提供了<code>Normalize</code>和<code>Standardize</code>转换。</p></li>
<li><p><strong>获取函数 (Acquisition Function):</strong> 获取函数基于代理模型（GP），用于确定下一个要实验的超参数组合。获取函数在“探索（exploration）”和“利用（exploitation）”之间找到平衡。BoTorch提供了以下各种获取函数。</p></li>
</ol>
<ul>
<li><code>ExpectedImprovement (EI)</code>：最常用的获取函数之一。考虑获得比目前最优值更好的结果的可能性及其改进程度。</li>
<li><code>LogExpectedImprovement (LogEI)</code>：EI的对数变换版本。数值上更稳定，并且对小变化更加敏感。</li>
<li><code>UpperConfidenceBound (UCB)</code>：更侧重于探索的获取函数。积极地探索不确定性较高的区域。</li>
<li><code>ProbabilityOfImprovement (PI)</code>：表示比当前最优值更好的概率。</li>
<li><code>qExpectedImprovement (qEI)</code>：也称为q-batch EI，用于并行优化。一次选择多个候选点。</li>
<li><code>qNoisyExpectedImprovement (qNEI)</code>：q-batch Noisy EI。在有噪声的环境中使用。</li>
</ul>
<p>完整代码位于<code>package/botorch_optimization.py</code>中。可以直接通过命令行执行。由于整个代码包含详细的注释，这里仅说明每个代码的重要部分。</p>
<div id="cell-45" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, max_trials: <span class="bu">int</span> <span class="op">=</span> <span class="dv">80</span>, init_samples: <span class="bu">int</span> <span class="op">=</span> <span class="dv">10</span>):</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.param_bounds <span class="op">=</span> torch.tensor([</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>        [<span class="fl">1e-4</span>, <span class="fl">64.0</span>, <span class="fl">32.0</span>, <span class="fl">32.0</span>],      <span class="co"># 최소값</span></span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>        [<span class="fl">1e-2</span>, <span class="fl">256.0</span>, <span class="fl">512.0</span>, <span class="fl">512.0</span>]    <span class="co"># 최대값</span></span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>    ], dtype<span class="op">=</span>torch.float64)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>初始化部分设置每个超参数的最小值和最大值。<code>max_trials</code> 是总尝试次数，<code>init_samples</code> 是初始随机实验次数（与 Bayes-Opt 的 <code>init_points</code> 相同）。通常将 <code>init_samples</code> 设置为参数数量的 2~3 倍。在上面的例子中，超参数有 4 个，因此 8~12 次左右是合适的。使用 <code>torch.float64</code> 是为了数值稳定性。贝叶斯优化，特别是在高斯过程计算核矩阵时会使用乔莱斯基分解（Cholesky decomposition），这一过程中 float32 可能因精度问题导致错误。</p>
<div id="cell-47" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> tune(<span class="va">self</span>):</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 가우시안 프로세스 모델 학습</span></span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> SingleTaskGP(configs, accuracies)</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>    mll <span class="op">=</span> ExactMarginalLogLikelihood(model.likelihood, model)</span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>    fit_gpytorch_mll(mll)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>使用基于高斯过程的代理模型 <code>SingleTaskGP</code>。<code>ExactMarginalLogLikelihood</code> 是用于模型训练的损失函数，而 <code>fit_gpytorch_mll</code> 使用此损失函数来训练模型。</p>
<div id="cell-49" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>acq_func <span class="op">=</span> LogExpectedImprovement(</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>    model, </span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>    best_f<span class="op">=</span>accuracies.<span class="bu">max</span>().item()</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>使用获取函数 <code>LogExpectedImprovement</code>。由于使用了对数，因此数值稳定性较高，并且对微小变化也很敏感。</p>
<div id="cell-51" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>candidate, _ <span class="op">=</span> optimize_acqf(                                   <span class="co"># 획득 함수 최적화로 다음 실험할 파라미터 선택</span></span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>    acq_func, bounds<span class="op">=</span>bounds,                                    <span class="co"># 획득 함수와 파라미터 범위 지정</span></span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>    q<span class="op">=</span><span class="dv">1</span>,                                                        <span class="co"># 한 번에 하나의 설정만 선택</span></span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>    num_restarts<span class="op">=</span><span class="dv">10</span>,                                            <span class="co"># 최적화 재시작 횟수</span></span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a>    raw_samples<span class="op">=</span><span class="dv">512</span>                                             <span class="co"># 초기 샘플링 수</span></span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><code>optimize_acqf</code> 函数通过优化获取函数来选择下一个实验的超参数组合（<code>candidate</code>）。</p>
<ul>
<li><code>q=1</code>: 每次仅选择一个候选者 (不是 q-batch 优化)。</li>
<li><code>num_restarts=10</code>: 在每个优化步骤中，从不同起点重复 10 次以避免陷入局部最优解 (local optima)。</li>
<li><code>raw_samples=512</code>: 从高斯过程中抽取 512 个样本以估计获取函数值。</li>
</ul>
<p><code>num_restarts</code> 和 <code>raw_samples</code> 对贝叶斯优化的探索-利用 (exploration-exploitation) 权衡有重要影响。<code>num_restarts</code> 决定了优化的彻底性，而 <code>raw_samples</code> 则决定了获取函数估计的准确性。这两个值越大，计算成本会增加，但获得更好结果的可能性也会提高。通常可以使用以下值：</p>
<ul>
<li>快速执行: <code>num_restarts=5</code>, <code>raw_samples=256</code></li>
<li>平衡设置: <code>num_restarts=10</code>, <code>raw_samples=512</code></li>
<li>准确性优先: <code>num_restarts=20</code>, <code>raw_samples=1024</code></li>
</ul>
<div id="cell-53" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_06.botorch_optimizer <span class="im">import</span> run_botorch_optimization</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>run_botorch_optimization(max_trials<span class="op">=</span><span class="dv">80</span>, init_samples<span class="op">=</span><span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>结果 数据集 : FashionMNIST epoch : 20 初始实验 : 5次 重复实验 : 80次</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>最优参数</th>
<th>Bayes-Opt</th>
<th>Botorch</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>学习率</td>
<td>6e-4</td>
<td>1e-4</td>
</tr>
<tr class="even">
<td>批大小</td>
<td>173</td>
<td>158</td>
</tr>
<tr class="odd">
<td>hid 1</td>
<td>426</td>
<td>512</td>
</tr>
<tr class="even">
<td>hid 2</td>
<td>197</td>
<td>512</td>
</tr>
<tr class="odd">
<td>准确度</td>
<td>0.7837</td>
<td>0.8057</td>
</tr>
</tbody>
</table>
<p>这只是一个简单的比较，但BoTorch的准确性更高。在简单的优化搜索中推荐使用Bayes-Opt，在专业的搜索中推荐使用BoTorch。</p>
</section>
</section>
<section id="高斯过程" class="level2">
<h2 class="anchored" data-anchor-id="高斯过程">6.6 高斯过程</h2>
<blockquote class="blockquote">
<p><strong>挑战：</strong> 如何量化模型的预测不确定性，并利用这些信息主动学习？</p>
<p><strong>研究者的思考：</strong> 传统的深度学习模型提供的是点估计（point estimate）预测结果，但在实际应用中了解预测的不确定性非常重要。例如，自动驾驶汽车在预测行人的下一个位置时，必须知道这一预测的不确定性才能安全驾驶。高斯过程是基于贝叶斯概率论量化预测不确定性的强大工具，但其计算复杂度高，难以应用于大规模数据。</p>
</blockquote>
<p>高斯过程（Gaussian Process, GP）是贝叶斯机器学习中提供包含不确定性预测的核心模型。之前我们简要介绍了在贝叶斯优化中将高斯过程作为代理模型（surrogate model）使用的情况，在这里我们将更详细地探讨高斯过程本身的基本原理及其重要性。</p>
<p>GP 被定义为“函数值集合的概率分布”。与确定性函数（deterministic function），如 <span class="math inline">\(y = f(x)\)</span> 不同，GP 并不是针对给定的输入 <span class="math inline">\(x\)</span> 预测一个输出值 <span class="math inline">\(y\)</span>，而是预测可能的输出值的 <em>分布</em>。例如，与其确定地预测“明天最高气温为25度”，不如说“有95%的概率明天的最高气温在23到27度之间”。如果骑自行车回家，虽然大致路线已经确定，但每次实际路径都会有所不同。因此，在这种情况下需要包含不确定性的预测而非确定性预测。</p>
<p>处理包含不确定性预测的数学工具的基础是19世纪数学家高斯提出的正态分布（高斯分布）。基于此理论，GP在20世纪40年代得到发展。当时正值第二次世界大战期间，科学家们必须处理比以往任何时候都要多的不确定数据，如雷达信号处理、密码破解、气象信息处理等。一个典型的例子是诺伯特·维纳（Norbert Wiener）为了提高高射炮精度而预测飞机未来位置的努力。他将飞机运动视为一种概率过程（当前位置 - 稍后的位置可以一定程度上被预测 - 随着时间的推移不确定性增大），即“维纳过程”，这为GP的发展奠定了重要基础。同时，哈罗德·克拉默（Harald Cramér）在时间序列分析中、安德烈·柯尔莫戈洛夫（Andrey Kolmogorov）在概率论中也奠定了GP的数学基础。1951年，丹尼尔·克里金（Daniel Krige）创建了用于矿脉分布预测的实际应用GP模型。随后到20世纪70年代，统计学家们将其系统化为适用于空间统计学、计算机实验设计和机器学习中贝叶斯优化等领域的工具。如今，高斯过程在几乎所有处理不确定性的领域，如人工智能、机器人技术、气候预测等方面发挥着核心作用。特别是在深度学习领域，通过元学习（meta-learning）实现的深度核GP近年来受到了关注，并在分子特性预测等领域表现出色。</p>
<p>今天，GP 已经应用于许多领域，包括人工智能、机器人工程和气候建模等。尤其是在深度学习中，通过元学习实现的深度核 GP 和分子特性预测等方面表现出了卓越的性能。</p>
<div class="callout callout-style-default callout-note callout-titled" title="点击查看内容（深入探讨：高斯过程的数学基础及其在机器学习中的应用）">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
点击查看内容（深入探讨：高斯过程的数学基础及其在机器学习中的应用）
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<section id="高斯过程的数学基础及其在机器学习中的应用" class="level2 callout-body-container callout-body">
<h2 class="anchored" data-anchor-id="高斯过程的数学基础及其在机器学习中的应用">高斯过程的数学基础及其在机器学习中的应用</h2>
<p>高斯过程（Gaussian Process, GP）是一种基于核方法（kernel method）的概率模型（probabilistic model），广泛用于回归（regression）和分类（classification）问题。GP 通过定义函数本身的分布，能够量化预测的不确定性，这是它的主要优势之一。在这次深入探讨中，我们将从多元正态分布（multivariate normal distribution）开始，详细讨论高斯过程的数学基础，直到概率过程（stochastic process）的角度，并探索其在各种机器学习应用中的应用。</p>
<section id="多元正态分布-multivariate-normal-distribution" class="level3">
<h3 class="anchored" data-anchor-id="多元正态分布-multivariate-normal-distribution">1. 多元正态分布 (Multivariate Normal Distribution)</h3>
<p>理解高斯过程的第一步是理解多元正态分布。如果 <span class="math inline">\(d\)</span> 维随机向量 <span class="math inline">\(\mathbf{x} = (x_1, x_2, ..., x_d)^T\)</span> 服从多元正态分布，这意味着它具有以下概率密度函数（probability density function）：</p>
<p><span class="math inline">\(p(\mathbf{x}) = \frac{1}{(2\pi)^{d/2}|\mathbf{\Sigma}|^{1/2}} \exp\left(-\frac{1}{2}(\mathbf{x} - \boldsymbol{\mu})^T \mathbf{\Sigma}^{-1} (\mathbf{x} - \boldsymbol{\mu})\right)\)</span></p>
<p>其中 <span class="math inline">\(\boldsymbol{\mu} \in \mathbb{R}^d\)</span> 是均值向量，<span class="math inline">\(\mathbf{\Sigma} \in \mathbb{R}^{d \times d}\)</span> 是协方差矩阵（covariance matrix）。协方差矩阵必须是正定的。</p>
<p><strong>核心属性:</strong></p>
<ul>
<li><p><strong>线性变换:</strong> 服从多元正态分布的随机变量的线性变换仍然服从多元正态分布。即，如果 <span class="math inline">\(\mathbf{x} \sim \mathcal{N}(\boldsymbol{\mu}, \mathbf{\Sigma})\)</span> 并且 <span class="math inline">\(\mathbf{y} = \mathbf{A}\mathbf{x} + \mathbf{b}\)</span>，则有 <span class="math inline">\(\mathbf{y} \sim \mathcal{N}(\mathbf{A}\boldsymbol{\mu} + \mathbf{b}, \mathbf{A}\mathbf{\Sigma}\mathbf{A}^T)\)</span>。</p></li>
<li><p><strong>条件分布 (Conditional Distribution):</strong> 多元正态分布的条件分布也服从正态分布。如果将 <span class="math inline">\(\mathbf{x}\)</span> 分解为 <span class="math inline">\(\mathbf{x} = (\mathbf{x}_1, \mathbf{x}_2)^T\)</span>，并将均值和协方差矩阵分解如下：</p>
<p><span class="math inline">\(\boldsymbol{\mu} = \begin{pmatrix} \boldsymbol{\mu}_1 \\ \boldsymbol{\mu}_2 \end{pmatrix}, \quad \mathbf{\Sigma} = \begin{pmatrix} \mathbf{\Sigma}_{11} &amp; \mathbf{\Sigma}_{12} \\ \mathbf{\Sigma}_{21} &amp; \mathbf{\Sigma}_{22} \end{pmatrix}\)</span></p>
<p>当给定 <span class="math inline">\(\mathbf{x}_1\)</span> 时，<span class="math inline">\(\mathbf{x}_2\)</span> 的条件分布如下：</p>
<p><span class="math inline">\(p(\mathbf{x}_2 | \mathbf{x}_1) = \mathcal{N}(\boldsymbol{\mu}_{2|1}, \mathbf{\Sigma}_{2|1})\)</span></p>
<p><span class="math inline">\(\boldsymbol{\mu}_{2|1} = \boldsymbol{\mu}_2 + \mathbf{\Sigma}_{21}\mathbf{\Sigma}_{11}^{-1}(\mathbf{x}_1 - \boldsymbol{\mu}_1)\)</span> <span class="math inline">\(\mathbf{\Sigma}_{2|1} = \mathbf{\Sigma}_{22} - \mathbf{\Sigma}_{21}\mathbf{\Sigma}_{11}^{-1}\mathbf{\Sigma}_{12}\)</span></p></li>
<li><p><strong>边缘分布 (Marginal Distribution):</strong> 多元正态分布的边缘分布也遵循正态分布。在上述划分中，<span class="math inline">\(\mathbf{x}_1\)</span> 的边缘分布如下。 <span class="math inline">\(p(\mathbf{x}_1) = \mathcal{N}(\boldsymbol{\mu_1}, \mathbf{\Sigma}_{11})\)</span></p></li>
</ul>
</section>
<section id="高斯过程的定义及其从概率过程角度的解释" class="level3">
<h3 class="anchored" data-anchor-id="高斯过程的定义及其从概率过程角度的解释">2. 高斯过程的定义及其从概率过程角度的解释</h3>
<p>高斯过程是 <em>函数</em> 的概率分布。即，如果一个函数 <span class="math inline">\(f(x)\)</span> 服从高斯过程，则意味着对于任意有限输入点集 <span class="math inline">\(\{x_1, x_2, ..., x_n\}\)</span>，对应的函数值向量 <span class="math inline">\((f(x_1), f(x_2), ..., f(x_n))^T\)</span> 遵循多元正态分布。</p>
<p><strong>定义:</strong> 高斯过程由均值函数(mean function) <span class="math inline">\(m(x)\)</span> 和协方差函数(covariance function, 或核函数) <span class="math inline">\(k(x, x')\)</span> 定义。</p>
<p><span class="math inline">\(f(x) \sim \mathcal{GP}(m(x), k(x, x'))\)</span></p>
<ul>
<li><strong>均值函数:</strong> <span class="math inline">\(m(x) = \mathbb{E}[f(x)]\)</span></li>
<li><strong>协方差函数:</strong> <span class="math inline">\(k(x, x') = \mathbb{E}[(f(x) - m(x))(f(x') - m(x'))]\)</span></li>
</ul>
<p><strong>概率过程(Stochastic Process) 角度:</strong> 高斯过程是一种概率过程，为索引集(index set, 在此即输入空间)中的每个元素分配一个随机变量。在高斯过程中，这些随机变量构成联合正态分布(joint Gaussian distribution)。</p>
</section>
<section id="核函数kernel-function的作用及各种核函数介绍" class="level3">
<h3 class="anchored" data-anchor-id="核函数kernel-function的作用及各种核函数介绍">3. 核函数(Kernel Function)的作用及各种核函数介绍</h3>
<p>核函数是高斯过程中最关键的组成部分之一。核函数表示两个输入 <span class="math inline">\(x\)</span> 和 <span class="math inline">\(x'\)</span> 之间的相似度，并决定了高斯过程的性质。</p>
<p><strong>核心作用:</strong></p>
<ul>
<li><strong>协方差定义:</strong> 核函数定义了函数值之间的协方差。即，<span class="math inline">\(k(x, x')\)</span> 表示 <span class="math inline">\(f(x)\)</span> 和 <span class="math inline">\(f(x')\)</span> 之间的协方差。</li>
<li><strong>决定函数的平滑度(Smoothness):</strong> 核函数决定了生成函数的平滑度。例如，RBF 核可以生成无限可微的函数，而 Matern 核则可以根据需要调整其可微性。</li>
<li><strong>正定性 (Positive Definiteness):</strong> 为了成为有效的协方差函数，核函数必须满足正定性。即，对于任意输入点，生成的核矩阵(kernel matrix, Gram 矩阵)必须是正定矩阵。</li>
</ul>
<p><strong>各种核函数:</strong></p>
<ul>
<li><p><strong>RBF (Radial Basis Function) 核 (或 Squared Exponential 核):</strong></p>
<p><span class="math inline">\(k(x, x') = \sigma^2 \exp\left(-\frac{\|x - x'\|^2}{2l^2}\right)\)</span></p>
<ul>
<li><span class="math inline">\(\sigma^2\)</span>: 方差(variance)</li>
<li><span class="math inline">\(l\)</span>: 长度尺度(length scale)</li>
<li>可生成非常平滑的函数。</li>
</ul></li>
<li><p><strong>Matern 核:</strong></p>
<p><span class="math inline">\(k(x, x') = \sigma^2 \frac{2^{1-\nu}}{\Gamma(\nu)}\left(\sqrt{2\nu}\frac{\|x - x'\|}{l}\right)^\nu K_\nu\left(\sqrt{2\nu}\frac{\|x - x'\|}{l}\right)\)</span></p></li>
<li><p><span class="math inline">\(\nu\)</span>: 平滑参数 (smoothness parameter)</p></li>
<li><p><span class="math inline">\(K_\nu\)</span>: 修正贝塞尔函数(modified Bessel function)</p></li>
<li><p>常用的值为半整数(half-integer)，如<span class="math inline">\(\nu = 1/2, 3/2, 5/2\)</span>。</p></li>
<li><p><span class="math inline">\(\nu\)</span>越大，越接近RBF核。</p></li>
<li><p><strong>周期 (Periodic) 核:</strong></p>
<p><span class="math inline">\(k(x, x') = \sigma^2 \exp\left(-\frac{2\sin^2(\pi|x-x'|/p)}{l^2}\right)\)</span></p>
<ul>
<li><span class="math inline">\(p\)</span> : 周期</li>
</ul></li>
<li><p><strong>线性 (Linear) 核:</strong></p>
<p><span class="math inline">\(k(x,x') = \sigma_b^2 + \sigma_v^2(x - c)(x' -c)\)</span></p></li>
</ul>
</section>
<section id="利用高斯过程进行回归regression及分类classification问题解决" class="level3">
<h3 class="anchored" data-anchor-id="利用高斯过程进行回归regression及分类classification问题解决">4. 利用高斯过程进行回归(Regression)及分类(Classification)问题解决</h3>
<p><strong>回归 (Regression):</strong></p>
<p>高斯过程回归是基于给定的训练数据 <span class="math inline">\(\mathcal{D} = \{(\mathbf{x}_i, y_i)\}_{i=1}^n\)</span> 预测新输入 <span class="math inline">\(\mathbf{x}_*\)</span> 的输出 <span class="math inline">\(f(\mathbf{x}_*)\)</span> 的问题。通过结合高斯过程的先验分布(prior distribution)和训练数据来计算后验分布(posterior distribution)，从而获得预测分布(predictive distribution)。</p>
<ul>
<li><strong>先验分布:</strong> <span class="math inline">\(f(\mathbf{x}) \sim \mathcal{GP}(0, k(\mathbf{x}, \mathbf{x}'))\)</span> (为了方便，假设均值函数为0)</li>
<li><strong>训练数据:</strong> <span class="math inline">\(\mathbf{y} = (y_1, y_2, ..., y_n)^T\)</span></li>
<li><strong>核矩阵:</strong> <span class="math inline">\(\mathbf{K} = k(\mathbf{X}, \mathbf{X})\)</span>，其中 <span class="math inline">\(\mathbf{X} = [\mathbf{x}_1, \mathbf{x}_2, ..., \mathbf{x}_n]\)</span></li>
<li><strong>预测分布:</strong> <span class="math inline">\(p(f(\mathbf{x}_*) | \mathbf{y}, \mathbf{X}, \mathbf{x}_*) = \mathcal{N}(\mu_*, \sigma_*^2)\)</span>
<ul>
<li><span class="math inline">\(\mu_* = \mathbf{k}_*^T \mathbf{K}^{-1} \mathbf{y}\)</span></li>
<li><span class="math inline">\(\sigma_*^2 = k(\mathbf{x}_*, \mathbf{x}_*) - \mathbf{k}_*^T \mathbf{K}^{-1} \mathbf{k}_*\)</span></li>
<li><span class="math inline">\(\mathbf{k}_* = [k(\mathbf{x}_*, \mathbf{x}_1), k(\mathbf{x}_*, \mathbf{x}_2), ..., k(\mathbf{x}_*, \mathbf{x}_n)]^T\)</span></li>
</ul></li>
</ul>
<p><strong>分类 (Classification):</strong></p>
<p>高斯过程分类是将潜在函数(latent function) <span class="math inline">\(f(\mathbf{x})\)</span> 建模为高斯过程，并通过该潜在函数定义分类概率。例如，在二元分类(binary classification)问题中，使用逻辑函数(logistic function)或普罗比特函数(probit function)将潜在函数值转换为概率。</p>
<ul>
<li><strong>潜在函数:</strong> <span class="math inline">\(f(\mathbf{x}) \sim \mathcal{GP}(0, k(\mathbf{x}, \mathbf{x}'))\)</span></li>
<li><strong>二元分类:</strong> <span class="math inline">\(p(y = 1 | f(\mathbf{x})) = \sigma(f(\mathbf{x}))\)</span> (其中 <span class="math inline">\(\sigma\)</span> 是逻辑函数)</li>
</ul>
<p>在分类问题中，由于后验分布没有闭合形式(closed form)，因此使用拉普拉斯近似(Laplace approximation)或变分推断(variational inference)等近似推理方法。 ### 5. 高斯过程的优缺点及与深度学习的比较</p>
<p><strong>优点:</strong></p>
<ul>
<li><strong>不确定性量化:</strong> 提供预测不确定性的形式为预测方差(predictive variance)。</li>
<li><strong>数据效率:</strong> 即使数据量相对较少也能表现出良好的性能。</li>
<li><strong>内核选择的灵活性:</strong> 可以使用各种内核函数来设计符合问题特性的模型。</li>
<li><strong>贝叶斯解释:</strong> 在贝叶斯框架中可以自然地进行解释。</li>
</ul>
<p><strong>缺点:</strong></p>
<ul>
<li><strong>计算复杂度:</strong> 对于大小为 <span class="math inline">\(n\)</span> 的训练数据，具有 <span class="math inline">\(O(n^3)\)</span> 的计算复杂度。（由于逆矩阵的计算）</li>
<li><strong>模型选择:</strong> 选择合适的内核函数和超参数非常重要，这可能很困难。</li>
<li><strong>高维输入:</strong> 在高维输入空间中性能可能会下降。</li>
</ul>
<p><strong>与深度学习的比较:</strong></p>
<ul>
<li><strong>数据需求量:</strong> 深度学习模型通常比高斯过程需要更多的数据量。</li>
<li><strong>计算成本:</strong> 深度学习模型在训练时需要大量的计算成本，但推理(inference)相对较快。高斯过程在（数据较少时）训练较快，但推理的计算成本随数据大小而增加。</li>
<li><strong>不确定性:</strong> 深度学习模型通常不提供预测的不确定性。（当然，贝叶斯深度学习(Bayesian deep learning)等例外情况除外。）</li>
<li><strong>表达能力:</strong> 深度学习模型能够表示非常复杂的函数，而高斯过程的表达能力受内核函数限制。</li>
<li><strong>可解释性</strong>: 高斯过程可以通过内核函数明确模型假设，并提供预测结果的不确定性量化。</li>
</ul>
<p>近年来，结合深度学习和高斯过程的模型（如：Deep Kernel Learning）也在研究中。</p>
</section>
</section>
</div>
</div>
<section id="不确定性处理的数学基础" class="level3">
<h3 class="anchored" data-anchor-id="不确定性处理的数学基础">6.6.1 不确定性处理的数学基础</h3>
<p>通常我们把函数看作一条线，但高斯过程将其视为“可能的多条线的集合”。从数学上讲，它是这样的：</p>
<p><span class="math inline">\(f(t) \sim \mathcal{GP}(m(t), k(t,t'))\)</span></p>
<p>以自行车位置为例，<span class="math inline">\(m(t)\)</span> 是均值函数，表示“大致会遵循这种路径”的预测。<span class="math inline">\(k(t,t')\)</span> 是协方差函数（或核），表示“不同时间点的位置之间有多相关？”存在几种典型的核函数。最常用的核函数之一是 RBF（径向基函数）。</p>
<p><span class="math inline">\(k(t,t') = \sigma^2 \exp\left(-\frac{(t-t')^2}{2l^2}\right)\)</span></p>
<p>这个公式非常直观。两个时间点 <span class="math inline">\(t\)</span> 和 <span class="math inline">\(t'\)</span> 越近，值越大；越远，值越小。这类似于“知道当前的位置后，可以大致预测片刻后的位罝，但遥远未来的位罝则难以确定”。</p>
<p>假设核(<span class="math inline">\(K\)</span>)为RBF，并考虑一个实际例子。想象你在经营自行车共享服务（或自动驾驶汽车也可以）。我们仅通过GPS观测到的几个数据点来估计自行车的整体移动路径。</p>
<p><strong>预测的基本公式</strong></p>
<p><span class="math inline">\(f_* | X, y, X_* \sim \mathcal{N}(\mu_*, \Sigma_*)\)</span></p>
<p>该公式表示“基于我们拥有的GPS记录(<span class="math inline">\(X\)</span>, <span class="math inline">\(y\)</span>)，未知时间点(<span class="math inline">\(X_*\)</span>)的自行车位置遵循均值为 <span class="math inline">\(\mu_*\)</span>、不确定性为 <span class="math inline">\(\Sigma_*\)</span> 的正态分布”。</p>
<p><strong>位置预测计算</strong></p>
<p><span class="math inline">\(\mu_* = K_*K^{-1}y\)</span></p>
<p>该公式展示了如何预测自行车的位置。<span class="math inline">\(K_*\)</span> 表示待预测时间点与GPS记录时间点之间的“时间相关性”，<span class="math inline">\(K^{-1}\)</span> 考虑了GPS记录之间关系的“权重调整”，<span class="math inline">\(y\)</span> 表示实际由GPS记录的位置。例如，预测下午2点15分的位置时： 1. 参考2点10分和2点20分的GPS记录(<span class="math inline">\(y\)</span>) 2. 考虑各时间点的时间差(<span class="math inline">\(K_*\)</span>) 3. 也反映GPS记录之间的时间连续性(<span class="math inline">\(K^{-1}\)</span>)</p>
<p><strong>不确定性估计</strong></p>
<p><span class="math inline">\(\Sigma_* = K_{**} - K_*K^{-1}K_*^T\)</span></p>
<p>该公式计算了位置预测的不确定性。<span class="math inline">\(K_{**}\)</span> 表示待预测时间点自身的“基本不确定性”，而 <span class="math inline">\(K_*K^{-1}K_*^T\)</span> 计算了由于GPS记录导致的“减少的不确定性”。因为矩阵 <span class="math inline">\(K\)</span> 表示现有观测数据之间的关系，所以数据越密集，值就越大。<span class="math inline">\(K_*\)</span> 表示新预测点与现有数据之间的关系，因此数据越密集，就越能考虑更多的周围数据。</p>
<p>以实际情况解释： 1. 初始时假设自行车可以去任何地方(<span class="math inline">\(K_{**}\)</span> 很大) 2. GPS记录越多(<span class="math inline">\(K_*\)</span> 越大) 3. 且记录越一致(<span class="math inline">\(K^{-1}\)</span> 越稳定) 4. 位置估计的不确定性就越低</p>
<p><strong>根据数据量对公式的效应</strong> 随着GPS数据量的变化，包含不确定性的预测将如下所示： 1. GPS记录频繁的区域：低不确定性 - <span class="math inline">\(K_*\)</span> 大且数据多导致 <span class="math inline">\(K_*K^{-1}K_*^T\)</span> 很大 - 因此 <span class="math inline">\(\Sigma_*\)</span> 小，路径估计准确 2. GPS记录稀少的区域：高不确定性 - <span class="math inline">\(K_*\)</span> 小且数据少导致 <span class="math inline">\(K_*K^{-1}K_*^T\)</span> 很小 - 因此 <span class="math inline">\(\Sigma_*\)</span> 大，路径估计的不确定性大</p>
<p>简单来说，随着密集时间间隔的数据增多，<span class="math inline">\(K\)</span> 变得更大，因此不确定性降低。</p>
<p>以预测自行车路径为例来理解这一点。</p>
<div id="cell-58" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a><span class="co"># 시각화 스타일 설정</span></span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a>sns.set_style(<span class="st">"whitegrid"</span>)</span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a>plt.rcParams[<span class="st">'font.size'</span>] <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-9"><a href="#cb30-9" aria-hidden="true" tabindex="-1"></a><span class="co"># 데이터셋 1: 5개 관측점</span></span>
<span id="cb30-10"><a href="#cb30-10" aria-hidden="true" tabindex="-1"></a>time1 <span class="op">=</span> np.array([<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">5</span>, <span class="dv">8</span>, <span class="dv">10</span>]).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb30-11"><a href="#cb30-11" aria-hidden="true" tabindex="-1"></a>position1 <span class="op">=</span> np.array([<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">1</span>, <span class="dv">4</span>])</span>
<span id="cb30-12"><a href="#cb30-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-13"><a href="#cb30-13" aria-hidden="true" tabindex="-1"></a><span class="co"># 데이터셋 2: 8개 관측점</span></span>
<span id="cb30-14"><a href="#cb30-14" aria-hidden="true" tabindex="-1"></a>time2 <span class="op">=</span> np.array([<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">4</span>, <span class="dv">5</span>, <span class="dv">6</span>, <span class="dv">8</span>, <span class="dv">10</span>]).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb30-15"><a href="#cb30-15" aria-hidden="true" tabindex="-1"></a>position2 <span class="op">=</span> np.array([<span class="dv">0</span>, <span class="dv">1</span>, <span class="fl">2.5</span>, <span class="fl">1.5</span>, <span class="dv">3</span>, <span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">4</span>])  <span class="co"># 더 큰 변동성 추가</span></span>
<span id="cb30-16"><a href="#cb30-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-17"><a href="#cb30-17" aria-hidden="true" tabindex="-1"></a><span class="co"># 예측할 시간점 생성: 0~10분 구간을 100개로 분할</span></span>
<span id="cb30-18"><a href="#cb30-18" aria-hidden="true" tabindex="-1"></a>time_pred <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">10</span>, <span class="dv">100</span>).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb30-19"><a href="#cb30-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-20"><a href="#cb30-20" aria-hidden="true" tabindex="-1"></a><span class="co"># RBF 커널 함수 정의</span></span>
<span id="cb30-21"><a href="#cb30-21" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> kernel(T1, T2, l<span class="op">=</span><span class="fl">2.0</span>):</span>
<span id="cb30-22"><a href="#cb30-22" aria-hidden="true" tabindex="-1"></a>    sqdist <span class="op">=</span> np.<span class="bu">sum</span>(T1<span class="op">**</span><span class="dv">2</span>, <span class="dv">1</span>).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>) <span class="op">+</span> np.<span class="bu">sum</span>(T2<span class="op">**</span><span class="dv">2</span>, <span class="dv">1</span>) <span class="op">-</span> <span class="dv">2</span> <span class="op">*</span> np.dot(T1, T2.T)</span>
<span id="cb30-23"><a href="#cb30-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.exp(<span class="op">-</span><span class="fl">0.5</span> <span class="op">*</span> sqdist <span class="op">/</span> l<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb30-24"><a href="#cb30-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-25"><a href="#cb30-25" aria-hidden="true" tabindex="-1"></a><span class="co"># 가우시안 프로세스 예측 함수</span></span>
<span id="cb30-26"><a href="#cb30-26" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> predict_gp(time, position, time_pred):</span>
<span id="cb30-27"><a href="#cb30-27" aria-hidden="true" tabindex="-1"></a>    K <span class="op">=</span> kernel(time, time)</span>
<span id="cb30-28"><a href="#cb30-28" aria-hidden="true" tabindex="-1"></a>    K_star <span class="op">=</span> kernel(time_pred, time)</span>
<span id="cb30-29"><a href="#cb30-29" aria-hidden="true" tabindex="-1"></a>    K_star_star <span class="op">=</span> kernel(time_pred, time_pred)</span>
<span id="cb30-30"><a href="#cb30-30" aria-hidden="true" tabindex="-1"></a>    mu_star <span class="op">=</span> K_star.dot(np.linalg.inv(K)).dot(position)</span>
<span id="cb30-31"><a href="#cb30-31" aria-hidden="true" tabindex="-1"></a>    sigma_star <span class="op">=</span> K_star_star <span class="op">-</span> K_star.dot(np.linalg.inv(K)).dot(K_star.T)</span>
<span id="cb30-32"><a href="#cb30-32" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> mu_star, sigma_star</span>
<span id="cb30-33"><a href="#cb30-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-34"><a href="#cb30-34" aria-hidden="true" tabindex="-1"></a><span class="co"># 두 데이터셋에 대한 예측 수행</span></span>
<span id="cb30-35"><a href="#cb30-35" aria-hidden="true" tabindex="-1"></a>mu1, sigma1 <span class="op">=</span> predict_gp(time1, position1, time_pred)</span>
<span id="cb30-36"><a href="#cb30-36" aria-hidden="true" tabindex="-1"></a>mu2, sigma2 <span class="op">=</span> predict_gp(time2, position2, time_pred)</span>
<span id="cb30-37"><a href="#cb30-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-38"><a href="#cb30-38" aria-hidden="true" tabindex="-1"></a><span class="co"># 2개의 서브플롯 생성</span></span>
<span id="cb30-39"><a href="#cb30-39" aria-hidden="true" tabindex="-1"></a>fig, (ax1, ax2) <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">16</span>, <span class="dv">4</span>))</span>
<span id="cb30-40"><a href="#cb30-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-41"><a href="#cb30-41" aria-hidden="true" tabindex="-1"></a><span class="co"># 첫 번째 그래프 (5개 데이터)</span></span>
<span id="cb30-42"><a href="#cb30-42" aria-hidden="true" tabindex="-1"></a>ax1.fill_between(time_pred.flatten(),</span>
<span id="cb30-43"><a href="#cb30-43" aria-hidden="true" tabindex="-1"></a>                mu1 <span class="op">-</span> <span class="dv">2</span><span class="op">*</span>np.sqrt(np.diag(sigma1)),</span>
<span id="cb30-44"><a href="#cb30-44" aria-hidden="true" tabindex="-1"></a>                mu1 <span class="op">+</span> <span class="dv">2</span><span class="op">*</span>np.sqrt(np.diag(sigma1)),</span>
<span id="cb30-45"><a href="#cb30-45" aria-hidden="true" tabindex="-1"></a>                color<span class="op">=</span><span class="st">'blue'</span>, alpha<span class="op">=</span><span class="fl">0.2</span>, label<span class="op">=</span><span class="st">'95</span><span class="sc">% c</span><span class="st">onfidence interval'</span>)</span>
<span id="cb30-46"><a href="#cb30-46" aria-hidden="true" tabindex="-1"></a>ax1.plot(time_pred, mu1, <span class="st">'b-'</span>, linewidth<span class="op">=</span><span class="fl">1.5</span>, label<span class="op">=</span><span class="st">'Predicted path'</span>)</span>
<span id="cb30-47"><a href="#cb30-47" aria-hidden="true" tabindex="-1"></a>ax1.plot(time1, position1, <span class="st">'ro'</span>, markersize<span class="op">=</span><span class="dv">6</span>, label<span class="op">=</span><span class="st">'GPS records'</span>)</span>
<span id="cb30-48"><a href="#cb30-48" aria-hidden="true" tabindex="-1"></a>ax1.set_xlabel(<span class="st">'Time (min)'</span>)</span>
<span id="cb30-49"><a href="#cb30-49" aria-hidden="true" tabindex="-1"></a>ax1.set_ylabel(<span class="st">'Position (km)'</span>)</span>
<span id="cb30-50"><a href="#cb30-50" aria-hidden="true" tabindex="-1"></a>ax1.set_title(<span class="st">'Route Estimation (5 GPS points)'</span>)</span>
<span id="cb30-51"><a href="#cb30-51" aria-hidden="true" tabindex="-1"></a>ax1.legend(fontsize<span class="op">=</span><span class="dv">8</span>)</span>
<span id="cb30-52"><a href="#cb30-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-53"><a href="#cb30-53" aria-hidden="true" tabindex="-1"></a><span class="co"># 두 번째 그래프 (8개 데이터)</span></span>
<span id="cb30-54"><a href="#cb30-54" aria-hidden="true" tabindex="-1"></a>ax2.fill_between(time_pred.flatten(),</span>
<span id="cb30-55"><a href="#cb30-55" aria-hidden="true" tabindex="-1"></a>                mu2 <span class="op">-</span> <span class="dv">2</span><span class="op">*</span>np.sqrt(np.diag(sigma2)),</span>
<span id="cb30-56"><a href="#cb30-56" aria-hidden="true" tabindex="-1"></a>                mu2 <span class="op">+</span> <span class="dv">2</span><span class="op">*</span>np.sqrt(np.diag(sigma2)),</span>
<span id="cb30-57"><a href="#cb30-57" aria-hidden="true" tabindex="-1"></a>                color<span class="op">=</span><span class="st">'blue'</span>, alpha<span class="op">=</span><span class="fl">0.2</span>, label<span class="op">=</span><span class="st">'95</span><span class="sc">% c</span><span class="st">onfidence interval'</span>)</span>
<span id="cb30-58"><a href="#cb30-58" aria-hidden="true" tabindex="-1"></a>ax2.plot(time_pred, mu2, <span class="st">'b-'</span>, linewidth<span class="op">=</span><span class="fl">1.5</span>, label<span class="op">=</span><span class="st">'Predicted path'</span>)</span>
<span id="cb30-59"><a href="#cb30-59" aria-hidden="true" tabindex="-1"></a>ax2.plot(time2, position2, <span class="st">'ro'</span>, markersize<span class="op">=</span><span class="dv">6</span>, label<span class="op">=</span><span class="st">'GPS records'</span>)</span>
<span id="cb30-60"><a href="#cb30-60" aria-hidden="true" tabindex="-1"></a>ax2.set_xlabel(<span class="st">'Time (min)'</span>)</span>
<span id="cb30-61"><a href="#cb30-61" aria-hidden="true" tabindex="-1"></a>ax2.set_ylabel(<span class="st">'Position (km)'</span>)</span>
<span id="cb30-62"><a href="#cb30-62" aria-hidden="true" tabindex="-1"></a>ax2.set_title(<span class="st">'Route Estimation (8 GPS points)'</span>)</span>
<span id="cb30-63"><a href="#cb30-63" aria-hidden="true" tabindex="-1"></a>ax2.legend(fontsize<span class="op">=</span><span class="dv">8</span>)</span>
<span id="cb30-64"><a href="#cb30-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-65"><a href="#cb30-65" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb30-66"><a href="#cb30-66" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="06_过拟合和解决技术的发展_files/figure-html/cell-24-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>此代码示例展示了在两种场景（5个观测点，8个观测点）中使用GP估计自行车路径。每个图中的蓝色实线表示预测的平均路径，蓝色阴影区域表示95%置信区间(confidence interval)。</p>
<ul>
<li><strong>数据较少时</strong>（左图）：由于GPS记录稀疏，预测的不确定性（置信区间的宽度）较大。</li>
<li><strong>数据较多时</strong>（右图）：随着GPS记录变得更加密集，预测的不确定性减小，预测路径更接近实际路径。</li>
</ul>
<p>GP不仅提供预测结果，还提供了预测的不确定性信息，因此在需要考虑不确定性的决策过程中，如自动驾驶、机器人控制、医疗诊断等领域非常有用。</p>
</section>
<section id="现代应用" class="level3">
<h3 class="anchored" data-anchor-id="现代应用">6.6.2 现代应用</h3>
<p>高斯过程已应用于机器人控制、传感器网络优化、分子结构预测、气候建模、天体物理学数据分析等多个科学/工程领域。机器学习中一个典型的应用是已经讨论过的超参数优化。另一个需要包含不确定性的预测的典型领域是自动驾驶汽车，通过预测其他车辆的未来位置，在不确定性较大的区间内采取更加保守的驾驶策略。此外，在医疗领域预测患者状态变化、在资产市场预测股价并根据不确定性进行风险管理等方面也得到了广泛应用。近年来，GP的应用研究在强化学习(reinforcement learning)、深度学习中的生成模型(generative model)结合、因果关系推断(causal inference)、元学习(meta-learning)等领域非常活跃。</p>
</section>
<section id="深度核学习deep-kernel-learning" class="level3">
<h3 class="anchored" data-anchor-id="深度核学习deep-kernel-learning">6.6.3 深度核学习(Deep Kernel Learning)</h3>
<p>高斯过程中最关键的部分是核（协方差函数）。深度学习在从数据中学习表示方面具有优势。将GP的预测能力和深度学习的表示学习能力高效结合的研究方向是自然的发展趋势。一种典型的方法是在定义核之前，使用神经网络直接从数据中学习核，即深度核学习(Deep Kernel Learning, DKL)。</p>
<p>DKL的一般结构如下：</p>
<ol type="1">
<li><strong>特征提取 (Feature Extraction)</strong>: 输入数据首先通过深层神经网络（通常是CNN或Transformer）转换为低维的特征向量(feature vector)。</li>
<li><strong>核计算 (Kernel Computation)</strong>: 接收提取出的特征向量作为输入，使用高斯过程的核函数（例如：RBF核）来计算核矩阵。</li>
<li><strong>高斯过程 (Gaussian Process)</strong>: 使用计算出的核矩阵和训练数据学习高斯过程模型，并对新输入进行预测（平均值及方差）。</li>
</ol>
<p>DKL通过神经网络从数据中同时学习有用的特征表示和数据之间的相似度，具有这一优势。这使得它能够处理复杂的数据（如：图像、图、文本），并考虑不确定性来进行预测。</p>
<p>DKL在许多领域得到了应用。 * <strong>图像分类 (Image Classification)</strong>: 使用CNN提取图像特征，并使用GP进行分类。 * <strong>图分类 (Graph Classification)</strong>: 使用图神经网络(Graph Neural Network, GNN)从图结构中提取特征，并使用GP进行图分类。 * <strong>分子特性预测 (Molecular Property Prediction)</strong>: 接收分子图为输入，预测分子的特性（如：溶解度、毒性）。 * <strong>时间序列预测 (Time Series Forecasting)</strong>: 使用RNN提取时序数据的特征，并使用GP预测未来值。 在这里，我们将运行一个DKL的简单示例，在第二部分中将详细探讨内容和应用案例。</p>
<p><strong>深度核网络</strong></p>
<p>首先定义深度核网络。核网络是一种学习核函数的神经网络。该神经网络接收输入数据并输出特征表示。这些特征表示用于计算核矩阵。</p>
<div id="cell-60" class="cell" data-execution_count="2">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.optim <span class="im">as</span> optim</span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.distributions <span class="im">import</span> Normal</span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Set seed for reproducibility</span></span>
<span id="cb31-10"><a href="#cb31-10" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">42</span>)</span>
<span id="cb31-11"><a href="#cb31-11" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb31-12"><a href="#cb31-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-13"><a href="#cb31-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Define a neural network to learn the kernel</span></span>
<span id="cb31-14"><a href="#cb31-14" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> DeepKernel(nn.Module):</span>
<span id="cb31-15"><a href="#cb31-15" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, input_dim, hidden_dim, output_dim):</span>
<span id="cb31-16"><a href="#cb31-16" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(DeepKernel, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb31-17"><a href="#cb31-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc1 <span class="op">=</span> nn.Linear(input_dim, hidden_dim)</span>
<span id="cb31-18"><a href="#cb31-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc2 <span class="op">=</span> nn.Linear(hidden_dim, hidden_dim)</span>
<span id="cb31-19"><a href="#cb31-19" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc3 <span class="op">=</span> nn.Linear(hidden_dim, output_dim)</span>
<span id="cb31-20"><a href="#cb31-20" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.activation <span class="op">=</span> nn.ReLU()</span>
<span id="cb31-21"><a href="#cb31-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-22"><a href="#cb31-22" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb31-23"><a href="#cb31-23" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.activation(<span class="va">self</span>.fc1(x))</span>
<span id="cb31-24"><a href="#cb31-24" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.activation(<span class="va">self</span>.fc2(x))</span>
<span id="cb31-25"><a href="#cb31-25" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.fc3(x)  <span class="co"># No activation on the final layer</span></span>
<span id="cb31-26"><a href="#cb31-26" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>深度核神经网络的输入通常为2D张量，其中第一维是批大小，第二维是输入数据的维度。输出是一个形状为（批大小，特征表示维度）的2D张量。</p>
<p><strong>GP层定义</strong></p>
<p>GP层接收深度核网络的输出，计算核矩阵，并计算预测分布。</p>
<div id="cell-62" class="cell" data-execution_count="3">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the Gaussian Process layer</span></span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> GaussianProcessLayer(nn.Module):</span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, num_dim, num_data):</span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(GaussianProcessLayer, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.num_dim <span class="op">=</span> num_dim</span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.num_data <span class="op">=</span> num_data</span>
<span id="cb32-11"><a href="#cb32-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.lengthscale <span class="op">=</span> nn.Parameter(torch.ones(num_dim))  <span class="co"># Length-scale for each dimension</span></span>
<span id="cb32-12"><a href="#cb32-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.noise_var <span class="op">=</span> nn.Parameter(torch.ones(<span class="dv">1</span>))  <span class="co"># Noise variance</span></span>
<span id="cb32-13"><a href="#cb32-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.outputscale <span class="op">=</span> nn.Parameter(torch.ones(<span class="dv">1</span>))  <span class="co"># Output scale</span></span>
<span id="cb32-14"><a href="#cb32-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-15"><a href="#cb32-15" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x, y):</span>
<span id="cb32-16"><a href="#cb32-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculate the kernel matrix (using RBF kernel)</span></span>
<span id="cb32-17"><a href="#cb32-17" aria-hidden="true" tabindex="-1"></a>        dist_matrix <span class="op">=</span> torch.cdist(x, x)  <span class="co"># Pairwise distances between inputs</span></span>
<span id="cb32-18"><a href="#cb32-18" aria-hidden="true" tabindex="-1"></a>        kernel_matrix <span class="op">=</span> <span class="va">self</span>.outputscale <span class="op">*</span> torch.exp(<span class="op">-</span><span class="fl">0.5</span> <span class="op">*</span> dist_matrix<span class="op">**</span><span class="dv">2</span> <span class="op">/</span> <span class="va">self</span>.lengthscale<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb32-19"><a href="#cb32-19" aria-hidden="true" tabindex="-1"></a>        kernel_matrix <span class="op">+=</span> <span class="va">self</span>.noise_var <span class="op">*</span> torch.eye(<span class="va">self</span>.num_data)</span>
<span id="cb32-20"><a href="#cb32-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-21"><a href="#cb32-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculate the predictive distribution (using Cholesky decomposition)</span></span>
<span id="cb32-22"><a href="#cb32-22" aria-hidden="true" tabindex="-1"></a>        L <span class="op">=</span> torch.linalg.cholesky(kernel_matrix)</span>
<span id="cb32-23"><a href="#cb32-23" aria-hidden="true" tabindex="-1"></a>        alpha <span class="op">=</span> torch.cholesky_solve(y.unsqueeze(<span class="op">-</span><span class="dv">1</span>), L)  <span class="co"># Add unsqueeze for correct shape</span></span>
<span id="cb32-24"><a href="#cb32-24" aria-hidden="true" tabindex="-1"></a>        predictive_mean <span class="op">=</span> torch.matmul(kernel_matrix, alpha).squeeze(<span class="op">-</span><span class="dv">1</span>) <span class="co"># Remove extra dimension</span></span>
<span id="cb32-25"><a href="#cb32-25" aria-hidden="true" tabindex="-1"></a>        v <span class="op">=</span> torch.linalg.solve_triangular(L, kernel_matrix, upper<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb32-26"><a href="#cb32-26" aria-hidden="true" tabindex="-1"></a>        predictive_var <span class="op">=</span> kernel_matrix <span class="op">-</span> torch.matmul(v.T, v)</span>
<span id="cb32-27"><a href="#cb32-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-28"><a href="#cb32-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-29"><a href="#cb32-29" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> predictive_mean, predictive_var</span>
<span id="cb32-30"><a href="#cb32-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-31"><a href="#cb32-31" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> predictive_mean, predictive_var</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>GP 层的输入是一个形状为 (批量大小, 特征表示维度) 的 2D 张量。输出是一个包含预测均值和方差的元组。内核矩阵计算使用 RBF 内核，通过利用乔莱斯基分解(Cholesky decomposition)提高计算效率以计算预测分布。<code>y.unsqueeze(-1)</code> 和 <code>.squeeze(-1)</code> 是用于匹配 y 和内核矩阵之间的维度。</p>
<div id="cell-64" class="cell" data-execution_count="4">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 데이터를 생성</span></span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">10</span>, <span class="dv">10</span>, <span class="dv">100</span>)</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.sin(x) <span class="op">+</span> <span class="fl">0.1</span> <span class="op">*</span> np.random.randn(<span class="dv">100</span>)</span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a><span class="co"># 데이터를 텐서로 변환</span></span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a>x_tensor <span class="op">=</span> torch.tensor(x, dtype<span class="op">=</span>torch.float32).unsqueeze(<span class="op">-</span><span class="dv">1</span>)  <span class="co"># (100,) -&gt; (100, 1)</span></span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a>y_tensor <span class="op">=</span> torch.tensor(y, dtype<span class="op">=</span>torch.float32)  <span class="co"># (100,)</span></span>
<span id="cb33-8"><a href="#cb33-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-9"><a href="#cb33-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-10"><a href="#cb33-10" aria-hidden="true" tabindex="-1"></a><span class="co"># 딥 커널과 GP 레이어를 초기화</span></span>
<span id="cb33-11"><a href="#cb33-11" aria-hidden="true" tabindex="-1"></a>deep_kernel <span class="op">=</span> DeepKernel(input_dim<span class="op">=</span><span class="dv">1</span>, hidden_dim<span class="op">=</span><span class="dv">50</span>, output_dim<span class="op">=</span><span class="dv">1</span>)  <span class="co"># output_dim=1로 수정</span></span>
<span id="cb33-12"><a href="#cb33-12" aria-hidden="true" tabindex="-1"></a>gp_layer <span class="op">=</span> GaussianProcessLayer(num_dim<span class="op">=</span><span class="dv">1</span>, num_data<span class="op">=</span><span class="bu">len</span>(x))</span>
<span id="cb33-13"><a href="#cb33-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-14"><a href="#cb33-14" aria-hidden="true" tabindex="-1"></a><span class="co"># 손실 함수와 최적화기를 정의</span></span>
<span id="cb33-15"><a href="#cb33-15" aria-hidden="true" tabindex="-1"></a>loss_fn <span class="op">=</span> nn.MSELoss()  <span class="co"># Use MSE loss</span></span>
<span id="cb33-16"><a href="#cb33-16" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> optim.Adam(<span class="bu">list</span>(deep_kernel.parameters()) <span class="op">+</span> <span class="bu">list</span>(gp_layer.parameters()), lr<span class="op">=</span><span class="fl">0.01</span>)</span>
<span id="cb33-17"><a href="#cb33-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-18"><a href="#cb33-18" aria-hidden="true" tabindex="-1"></a>num_epochs <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb33-19"><a href="#cb33-19" aria-hidden="true" tabindex="-1"></a><span class="co"># 모델을 학습</span></span>
<span id="cb33-20"><a href="#cb33-20" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(num_epochs):</span>
<span id="cb33-21"><a href="#cb33-21" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()</span>
<span id="cb33-22"><a href="#cb33-22" aria-hidden="true" tabindex="-1"></a>    kernel_output <span class="op">=</span> deep_kernel(x_tensor)</span>
<span id="cb33-23"><a href="#cb33-23" aria-hidden="true" tabindex="-1"></a>    predictive_mean, _ <span class="op">=</span> gp_layer(kernel_output, y_tensor) <span class="co"># predictive_var는 사용 안함</span></span>
<span id="cb33-24"><a href="#cb33-24" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> loss_fn(predictive_mean, y_tensor)  <span class="co"># Use predictive_mean here</span></span>
<span id="cb33-25"><a href="#cb33-25" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb33-26"><a href="#cb33-26" aria-hidden="true" tabindex="-1"></a>    optimizer.step()</span>
<span id="cb33-27"><a href="#cb33-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span>(epoch <span class="op">%</span> <span class="dv">10</span> <span class="op">==</span> <span class="dv">0</span>):</span>
<span id="cb33-28"><a href="#cb33-28" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f'Epoch </span><span class="sc">{</span>epoch<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">, Loss: </span><span class="sc">{</span>loss<span class="sc">.</span>item()<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb33-29"><a href="#cb33-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-30"><a href="#cb33-30" aria-hidden="true" tabindex="-1"></a><span class="co"># 예측을 수행</span></span>
<span id="cb33-31"><a href="#cb33-31" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb33-32"><a href="#cb33-32" aria-hidden="true" tabindex="-1"></a>    kernel_output <span class="op">=</span> deep_kernel(x_tensor)</span>
<span id="cb33-33"><a href="#cb33-33" aria-hidden="true" tabindex="-1"></a>    predictive_mean, predictive_var <span class="op">=</span> gp_layer(kernel_output, y_tensor)</span>
<span id="cb33-34"><a href="#cb33-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-35"><a href="#cb33-35" aria-hidden="true" tabindex="-1"></a><span class="co"># 결과를 시각화</span></span>
<span id="cb33-36"><a href="#cb33-36" aria-hidden="true" tabindex="-1"></a>sns.<span class="bu">set</span>()</span>
<span id="cb33-37"><a href="#cb33-37" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb33-38"><a href="#cb33-38" aria-hidden="true" tabindex="-1"></a>plt.plot(x, y, <span class="st">'bo'</span>, label<span class="op">=</span><span class="st">'Training Data'</span>)</span>
<span id="cb33-39"><a href="#cb33-39" aria-hidden="true" tabindex="-1"></a>plt.plot(x, predictive_mean.numpy(), <span class="st">'r-'</span>, label<span class="op">=</span><span class="st">'Predictive Mean'</span>)</span>
<span id="cb33-40"><a href="#cb33-40" aria-hidden="true" tabindex="-1"></a>plt.fill_between(x, predictive_mean.numpy() <span class="op">-</span> <span class="fl">1.96</span> <span class="op">*</span> np.sqrt(predictive_var.numpy().diagonal()),</span>
<span id="cb33-41"><a href="#cb33-41" aria-hidden="true" tabindex="-1"></a>                 predictive_mean.numpy() <span class="op">+</span> <span class="fl">1.96</span> <span class="op">*</span> np.sqrt(predictive_var.numpy().diagonal()),</span>
<span id="cb33-42"><a href="#cb33-42" aria-hidden="true" tabindex="-1"></a>                 alpha<span class="op">=</span><span class="fl">0.2</span>, label<span class="op">=</span><span class="st">'95% CI'</span>)</span>
<span id="cb33-43"><a href="#cb33-43" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb33-44"><a href="#cb33-44" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1, Loss: 4.3467857893837725e-13
Epoch 11, Loss: 3.1288711313699757e-13
Epoch 21, Loss: 3.9212150236903054e-13
Epoch 31, Loss: 4.184870765894244e-13
Epoch 41, Loss: 2.9785689973499396e-13
Epoch 51, Loss: 3.8607078688482344e-13
Epoch 61, Loss: 3.9107123572454383e-13
Epoch 71, Loss: 2.359286811054462e-13
Epoch 81, Loss: 3.4729958167147024e-13
Epoch 91, Loss: 2.7600995490886793e-13</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_1408185/2425174321.py:40: RuntimeWarning: invalid value encountered in sqrt
  plt.fill_between(x, predictive_mean.numpy() - 1.96 * np.sqrt(predictive_var.numpy().diagonal()),
/tmp/ipykernel_1408185/2425174321.py:41: RuntimeWarning: invalid value encountered in sqrt
  predictive_mean.numpy() + 1.96 * np.sqrt(predictive_var.numpy().diagonal()),</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="06_过拟合和解决技术的发展_files/figure-html/cell-27-output-3.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>模型训练使用均方误差（Mean Squared Error, MSE）损失函数，并使用Adam优化器同时学习深度核网络和GP层的参数。</p>
<section id="dkl的可能性与局限性" class="level4">
<h4 class="anchored" data-anchor-id="dkl的可能性与局限性">6.6.4 DKL的可能性与局限性</h4>
<p>前面的例子展示了深度核学习（Deep Kernel Learning, DKL）的基本思想。通过使用深度学习模型（<code>DeepKernel</code>类）提取输入数据的特征，并利用这些特征计算高斯过程（GP）的核，再使用GP计算预测的均值和方差（不确定性）。这样，DKL结合了深度学习的表示学习能力和GP的不确定性估计能力，即使在复杂的数据中也能做出可靠的预测。</p>
<p><strong>DKL的可能性:</strong></p>
<ul>
<li><strong>数据效率:</strong> GP在数据较少时仍能表现出良好的性能。DKL将深度学习的强大特征提取能力与GP的数据效率相结合，使得即使少量数据也能达到较好的效果。</li>
<li><strong>不确定性估计:</strong> DKL能够量化预测的不确定性。这对于安全性要求高的应用领域（如：医疗诊断、自动驾驶）非常有用。</li>
<li><strong>灵活性:</strong> DKL可以应用于各种类型的数据（图像、文本、图等）。可以通过自由选择神经网络架构来设计适合问题的特征提取器。</li>
<li><strong>与贝叶斯优化的结合:</strong> DKL可以与贝叶斯优化相结合，高效地调优模型的超参数。</li>
</ul>
<p><strong>DKL的局限性:</strong></p>
<ul>
<li><strong>计算成本:</strong> GP仍然具有较高的计算成本。特别是当训练数据量大时，核矩阵的大小会增加，使得计算变得困难。</li>
<li><strong>神经网络设计:</strong> DKL的性能很大程度上依赖于特征提取器（神经网络）的设计。选择合适的神经网络架构仍然是一个难题。</li>
<li><strong>理论理解不足:</strong> DKL的理论分析仍处于初期阶段。需要更多的研究来理解DKL为何有效以及在什么条件下能表现出良好性能。</li>
</ul>
</section>
<section id="结语" class="level4">
<h4 class="anchored" data-anchor-id="结语">结语</h4>
<p>本章探讨了为解决深度学习模型过拟合问题的各种技术。从传统的正则化方法（如L1/L2正则化、dropout、批标准化）到高级技术（如贝叶斯优化、高斯过程、深度核学习），深度学习研究一直在不断进步，以提高模型的泛化性能。</p>
<p>过拟合是指深度学习模型过于专门针对训练数据，导致对新数据的预测性能下降的现象。这可能发生在模型复杂度过高、训练数据不足或数据含有大量噪声时。防止过拟合是将深度学习模型应用于实际问题中的重要任务。</p>
<p>本章介绍的各种技术以不同的方式应对过拟合问题。</p>
<ul>
<li><strong>正则化:</strong> 通过对模型复杂度施加惩罚来防止权重过大。（L1, L2, Elastic Net）</li>
<li><strong>Dropout:</strong> 在训练过程中随机删除神经元，以防止模型过度依赖特定的神经元或神经元组合。</li>
<li><strong>批标准化:</strong> 对每层的输入进行归一化处理，以稳定和加速学习过程。</li>
<li><strong>超参数优化:</strong> 使用贝叶斯优化等方法寻找最优性能的超参数组合。</li>
<li><strong>高斯过程、深度核学习</strong>: 显式建模不确定性，从而做出更可靠的预测。 这些技术的适当组合，并根据问题的特点进行调整，是深度学习工程师的重要能力之一。没有一种“在所有情况下都完美”的单一解决方案，必须通过实验和分析来寻找最优方法。深度学习研究正在快速发展，新的解决过拟合的技术将会不断出现。</li>
</ul>
</section>
</section>
</section>
<section id="练习题" class="level2">
<h2 class="anchored" data-anchor-id="练习题">练习题</h2>
<section id="基本问题" class="level3">
<h3 class="anchored" data-anchor-id="基本问题">基本问题</h3>
<ol type="1">
<li>解释过拟合(overfitting)和欠拟合(underfitting)的概念，并说明当这些现象发生时对模型性能的影响。</li>
<li>解释L1正则化和L2正则化的区别，以及它们如何影响模型的权重。</li>
<li>解释dropout的工作原理，以及它如何帮助防止过拟合。</li>
<li>解释批归一化(batch normalization)的概念，并说明批归一化为深度学习模型的学习提供了哪些优点。</li>
<li>描述在给定代码中更改custom_loss函数中的每个lambda时，图形会发生什么变化。</li>
<li>定义L1、L2范数(norm)。</li>
<li>解释批归一化的公式中如何计算均值和方差，以及它们是如何用于归一化过程的。</li>
</ol>
</section>
<section id="应用问题" class="level3">
<h3 class="anchored" data-anchor-id="应用问题">应用问题</h3>
<ol type="1">
<li>对给定的数据集训练多项式回归(polynomial regression)模型，并通过改变次数(degree)观察过拟合和欠拟合现象。（编写Python代码）</li>
<li>构建一个简单的神经网络模型，应用L1或L2正则化以观察模型权重的变化，并比较不同正则化强度下的性能变化。（编写Python代码）</li>
<li>设置不同的dropout率来训练神经网络模型，并比较每个比率下的训练/验证损失和准确度。（编写Python代码）</li>
<li>比较添加或移除批归一化层时神经网络模型的学习速度和收敛稳定性。（编写Python代码）</li>
<li>使用拉格朗日乘数法(Lagrange multiplier method)推导应用了L1、L2正则化的损失函数的最优解条件。</li>
<li>推导批归一化的反向传播(backpropagation)过程，并说明批归一化如何帮助缓解梯度消失问题。</li>
</ol>
</section>
<section id="深化问题" class="level3">
<h3 class="anchored">深化问题</h3>
<ol type="1">
<li>可视化L1正则化和L2正则化对损失平面(loss surface)的影响，并解释每种正则化的几何意义。（编写Python代码）</li>
<li>从集成(ensemble)学习的角度解读dropout，并说明如何利用dropout来估计模型的不确定性(uncertainty)。</li>
<li>比较不同的超参数优化技术（网格搜索、随机搜索、贝叶斯优化等），并解释每种技术的优缺点。</li>
<li>不使用BoTorch，实现贝叶斯优化的核心思想以找到简单函数的最优值。（编写Python代码，可以使用库）</li>
<li>解释高斯过程(Gaussian Process)的基本原理，并说明高斯过程如何进行包含不确定性的预测。</li>
<li>解释高斯过程的核函数(kernel function)必须满足的条件，并证明RBF核满足这些条件。</li>
<li>解释贝叶斯优化中获取函数(acquisition function)的作用，推导Expected Improvement (EI) 获取函数的公式并解释其意义。</li>
</ol>
<div class="callout callout-style-default callout-note callout-titled" title="点击查看内容（练习题答案）">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
点击查看内容（练习题答案）
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<section id="练习题解答" class="level2 callout-body-container callout-body">
<h2 class="anchored" data-anchor-id="练习题解答">练习题解答</h2>
<section id="基本问题-1" class="level3">
<h3 class="anchored" data-anchor-id="基本问题-1">基本问题</h3>
<ol type="1">
<li><strong>过拟合/欠拟合:</strong>
<ul>
<li><strong>过拟合 (Overfitting):</strong> 模型对训练数据过度拟合，导致在新数据（验证/测试数据）上的性能（泛化性能）下降。虽然在训练数据上表现良好，但在验证/测试数据上的性能较差。</li>
<li><strong>欠拟合 (Underfitting):</strong> 模型过于简单，无法充分学习训练数据的模式。在训练、验证和测试数据中都表现出较低的性能。</li>
</ul></li>
<li><strong>L1/L2 正则化:</strong>
<ul>
<li><strong>L1 正则化 (Lasso):</strong> 在损失函数中添加权重绝对值之和。 <span class="math inline">\(\lambda \sum_{i} |w_i|\)</span> 可以使部分权重精确为0，产生特征选择（feature selection）的效果。</li>
<li><strong>L2 正则化 (Ridge):</strong> 在损失函数中添加权重平方的和。 <span class="math inline">\(\lambda \sum_{i} (w_i)^2\)</span> 会使权重接近于0，但不会完全变为0。</li>
</ul></li>
<li><strong>Dropout:</strong>
<ul>
<li><strong>工作原理:</strong> 在训练过程中随机使一些神经元失活（输出为0）。</li>
<li><strong>防止过拟合:</strong> 每次用不同的神经元组合进行学习，从而避免依赖特定的神经元，并产生集成学习的效果。</li>
</ul></li>
<li><strong>批标准化:</strong>
<ul>
<li><strong>概念:</strong> 对每一层的输入进行标准化，使其均值为0，方差为1。</li>
<li><strong>优点:</strong> 提高学习速度、缓解梯度消失/爆炸问题、可以使用更高的学习率、有轻微的正则化效果。</li>
</ul></li>
<li><strong><code>custom_loss</code> lambda 变化:</strong>
<ul>
<li><strong><code>lambda</code> 增加:</strong> 正则项的影响增大。权重变小，模型变得更简单，可能会导致欠拟合。</li>
<li><strong><code>lambda</code> 减少:</strong> 正则项的影响减小。权重变大，模型变得更复杂，可能会导致过拟合。</li>
</ul></li>
<li><strong>L1/L2 范数:</strong>
<ul>
<li><strong>L1 范数:</strong> 向量元素的绝对值之和。 <span class="math inline">\(\| \mathbf{x} \|_1 = |x_1| + |x_2| + \dots + |x_n|\)</span></li>
<li><strong>L2 范数:</strong> 向量元素平方和的平方根（欧几里得距离）。 <span class="math inline">\(\| \mathbf{x} \|_2 = \sqrt{|x_1|^2 + |x_2|^2 + \dots + |x_n|^2}\)</span></li>
</ul></li>
<li><strong>批标准化公式:</strong>
<ul>
<li><strong>均值 (μ):</strong> 小批量内样本的平均值。 <span class="math inline">\(\mu = \frac{1}{m} \sum_{i=1}^{m} x_i\)</span></li>
<li><strong>方差 (σ²):</strong> 小批量内样本的方差。 <span class="math inline">\(\sigma^2 = \frac{1}{m} \sum_{i=1}^{m} (x_i - \mu)^2\)</span></li>
<li><strong>标准化:</strong> <span class="math inline">\(x_{i\_\text{norm}} = \frac{x_i - \mu}{\sqrt{\sigma^2 + \epsilon}}\)</span> （ε是一个防止除以0的小常数）</li>
</ul></li>
</ol>
</section>
<section id="应用问题-1" class="level3">
<h3 class="anchored" data-anchor-id="应用问题-1">应用问题</h3>
<ol type="1">
<li><p><strong>多项式回归:</strong> (代码省略) 如果次数过高会导致过拟合，如果次数过低会导致欠拟合。</p></li>
<li><p><strong>L1/L2 正则化:</strong> (代码省略) 当正则强度（<code>lambda</code>）较高时，权重会变小，并观察性能变化。</p></li>
<li><p><strong>Dropout 比率:</strong> (代码省略) 合适的 Dropout 比率可以防止过拟合并提高性能。比率过高可能会导致欠拟合。</p></li>
<li><p><strong>批标准化:</strong> (代码省略) 添加批标准化后，学习速度加快，并且收敛更稳定。</p></li>
<li><p><strong>拉格朗日乘数法:</strong></p>
<ul>
<li><strong>L2正则化:</strong> <span class="math inline">\(L(\mathbf{w}, \lambda) = \text{Loss}(\mathbf{w}) + \lambda (\|\mathbf{w}\|_2^2 - c)  \rightarrow  \nabla_\mathbf{w}L = \nabla_\mathbf{w}\text{Loss}(\mathbf{w}) + 2\lambda\mathbf{w} = 0\)</span></li>
<li><strong>L1正则化:</strong> <span class="math inline">\(L(\mathbf{w}, \lambda) = \text{Loss}(\mathbf{w}) + \lambda (\|\mathbf{w}\|_1 - c)  \rightarrow  \nabla_\mathbf{w}L = \nabla_\mathbf{w}\text{Loss}(\mathbf{w}) + \lambda \cdot \text{sign}(\mathbf{w}) = 0\)</span> (sign(w)是w的符号)</li>
</ul></li>
<li><p><strong>批归一化反向传播:</strong> (略去推导) 批归一化通过规范化每一层的输入来缓解梯度消失/爆炸问题，并使学习更加稳定。</p></li>
</ol>
</section>
<section id="深入问题" class="level3">
<h3 class="anchored" data-anchor-id="深入问题">深入问题</h3>
<ol type="1">
<li><p><strong>损失平面可视化:</strong> (略去代码) L1正则化创建菱形约束，L2正则化创建圆形约束条件，从而使最优解在不同的位置形成。</p></li>
<li><p><strong>Dropout集成:</strong> Dropout会产生类似集成学习的效果，因为它每次训练时都会使用不同的网络结构。预测时会使用所有神经元进行(无Dropout的)平均预测。通过蒙特卡洛Dropout (Monte Carlo dropout) 可以估计预测的不确定性。</p></li>
<li><p><strong>超参数优化技术:</strong></p>
<ul>
<li><strong>网格搜索:</strong> 尝试所有的组合。计算成本非常高。</li>
<li><strong>随机搜索:</strong> 尝试随机组合。可能比网格搜索更有效。</li>
<li><strong>贝叶斯优化:</strong> 基于之前的探索结果使用概率模型来决定下一个探索点。非常高效。</li>
</ul></li>
<li><p><strong>贝叶斯优化实现:</strong> (略去代码) 使用代理模型(surrogate model, 如：高斯过程)和获取函数(acquisition function, 如：预期改进)来实现。</p></li>
<li><p><strong>高斯过程:</strong> 是关于函数的概率分布。使用核函数定义函数值之间的协方差。基于给定的数据计算后验分布，提供预测的均值和方差(不确定性)。</p></li>
<li><p><strong>核函数条件:</strong> 必须满足正半定性(positive semi-definiteness)。对于任意输入点生成的核矩阵(Gram matrix)必须是正半定(positive semi-definite)矩阵。RBF核满足这一条件。(略去证明)</p></li>
<li><p><strong>获取函数:</strong> 在贝叶斯优化中用于决定下一个探索点。预期改进(Expected Improvement, EI)考虑了超过迄今为止最优值的可能性及其改进程度来选择下一个探索点。(略去公式推导)</p></li>
</ol>
</section>
</section>
</div>
</div>
<ol type="1">
<li><strong>Dropout: A Simple Way to Prevent Neural Networks from Overfitting</strong> (Srivastava et al., 2014): 解释了 dropout 的概念和效果的原始论文。(<a href="https://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf">https://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf</a>)</li>
<li><strong>Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</strong> (Ioffe &amp; Szegedy, 2015): 解释了批归一化的概念和效果的原始论文。(<a href="https://arxiv.org/abs/1502.03167">https://arxiv.org/abs/1502.03167</a>)</li>
<li><strong>Deep Learning</strong> (Goodfellow et al., 2016): 深度学习教科书。第 7 章 “Regularization for Deep Learning” 详细讨论了过拟合及正则化技术。(<a href="http://www.deeplearningbook.org/">http://www.deeplearningbook.org/</a>)</li>
<li><strong>Understanding the difficulty of training deep feedforward neural networks</strong> (Glorot &amp; Bengio, 2010): 解释了早期深度学习模型的训练难度和权重初始化方法的重要性。(<a href="http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf">http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf</a>)</li>
<li><strong>Regularization techniques for deep learning: A survey</strong> (Kukacka et al., 2017): 综合比较分析了各种正则化技术的论文。</li>
<li><strong>A Tutorial on Bayesian Optimization</strong> (Frazier, 2018): 解释贝叶斯优化的基本概念和应用的教程。(<a href="https://arxiv.org/abs/1807.02811">https://arxiv.org/abs/1807.02811</a>)</li>
<li><strong>Bayesian Optimization</strong> (Garnett, 2023): 综合性的贝叶斯优化教科书 (<a href="https://www.bayesoptbook.com/">https://www.bayesoptbook.com/</a>)</li>
<li><strong>Gaussian Processes for Machine Learning</strong> (Rasmussen &amp; Williams, 2006): 讨论了高斯过程的基本原理和机器学习应用的教科书。(<a href="http://www.gaussianprocess.org/gpml/">http://www.gaussianprocess.org/gpml/</a>)</li>
<li><strong>Deep Kernel Learning</strong> (Wilson et al., 2016): 解释了深度核学习的概念和方法的论文。(<a href="https://arxiv.org/abs/1511.02222">https://arxiv.org/abs/1511.02222</a>)</li>
<li><strong>Hands-On Machine Learning with Scikit-Learn, Keras &amp; TensorFlow</strong> (Aurélien Géron, 2019): 实践性的机器学习及深度学习教科书。通过实际代码示例详细解释了过拟合及正则化技术。</li>
<li><strong>Adam: A Method for Stochastic Optimization</strong> (Kingma &amp; Ba, 2014) (<a href="https://arxiv.org/abs/1412.6980">https://arxiv.org/abs/1412.6980</a>): 关于 Adam 优化器的论文</li>
<li><strong>Decoupled Weight Decay Regularization</strong> (Loshchilov &amp; Hutter, 2017) (<a href="https://arxiv.org/abs/1711.05101">https://arxiv.org/abs/1711.05101</a>): 关于 AdamW 的论文</li>
<li><strong>Dropout: A Simple Way to Prevent Neural Networks from Overfitting</strong> (Srivastava et al., 2014): 解释了dropout的概念和效果的原始论文。(<a href="https://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf">https://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf</a>) 2. <strong>Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</strong> (Ioffe &amp; Szegedy, 2015): 解释了批归一化概念和效果的原始论文。(<a href="https://arxiv.org/abs/1502.03167">https://arxiv.org/abs/1502.03167</a>) 3. <strong>Deep Learning</strong> (Goodfellow et al., 2016): 深度学习教科书。第7章 “Regularization for Deep Learning” 详细讨论了过拟合及正则化技术。(<a href="http://www.deeplearningbook.org/">http://www.deeplearningbook.org/</a>) 4. <strong>Understanding the difficulty of training deep feedforward neural networks</strong> (Glorot &amp; Bengio, 2010): 解释了早期深度学习模型的训练难度和权重初始化方法的重要性。(<a href="http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf">http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf</a>) 5. <strong>Regularization techniques for deep learning: A survey</strong> (Kukacka et al., 2017): 综合比较分析了各种正则化技术的论文。 6. <strong>A Tutorial on Bayesian Optimization</strong> (Frazier, 2018): 解释贝叶斯优化的基本概念和应用的教程。(<a href="https://arxiv.org/abs/1807.02811">https://arxiv.org/abs/1807.02811</a>) 7. <strong>Bayesian Optimization</strong> (Garnett, 2023): 综合性的贝叶斯优化教科书。(<a href="https://www.bayesoptbook.com/">https://www.bayesoptbook.com/</a>) 8. <strong>Gaussian Processes for Machine Learning</strong> (Rasmussen &amp; Williams, 2006): 讨论高斯过程基本原理及其在机器学习中的应用的教科书。(<a href="http://www.gaussianprocess.org/gpml/">http://www.gaussianprocess.org/gpml/</a>) 9. <strong>Deep Kernel Learning</strong> (Wilson et al., 2016): 解释了深度核学习的概念和方法的论文。(<a href="https://arxiv.org/abs/1511.02222">https://arxiv.org/abs/1511.02222</a>) 10. <strong>Hands-On Machine Learning with Scikit-Learn, Keras &amp; TensorFlow</strong> (Aurélien Géron, 2019): 实战机器学习及深度学习教科书。结合实际代码示例解释了过拟合及正则化技术。 11. <strong>Adam: A Method for Stochastic Optimization</strong> (Kingma &amp; Ba, 2014) (<a href="https://arxiv.org/abs/1412.6980">https://arxiv.org/abs/1412.6980</a>): 关于Adam优化器的论文 12. <strong>分离权重衰减正则化</strong> (Loshchilov &amp; Hutter, 2017) (<a href="https://arxiv.org/abs/1711.05101">https://arxiv.org/abs/1711.05101</a>): 关于AdamW的论文</li>
</ol>
<p>翻译后的文本：</p>
<p>原始文本未提供，无法进行翻译。请提供需要翻译的韩文文本。</p>


</section>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>