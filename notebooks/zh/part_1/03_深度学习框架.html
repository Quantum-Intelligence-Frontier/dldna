<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>quarto-inputfe38b716e2a340a0 – Deep Learning DNA: Surviving Architectures and Essential Principles</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap-f507c7d0488cb7630e20aad62ad8c2aa.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>
<script src="https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js" crossorigin="anonymous"></script>
<script>window.MathJax = {loader: {load: ['[tex]/boldsymbol']},tex: {packages: {'[+]': ['boldsymbol']}}};</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../../notebooks/zh/part_1/01_深度学习的开始.html">part_1</a></li><li class="breadcrumb-item"><a href="../../../notebooks/zh/part_1/03_深度学习框架.html">3. 深度学习框架</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../../../">中文</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Language</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index_de.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Deutsch</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index_en.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">English</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index_es.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Español</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">한국어</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index_zh.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">中文</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/zh/part_1/00_引言.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">引言</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">part_1</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/zh/part_1/01_深度学习的开始.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">1. 深度学习的开始</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/zh/part_1/02_深度学习的数学.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">2. 深度学习的数学</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/zh/part_1/03_深度学习框架.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">3. 深度学习框架</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/zh/part_1/04_激活函数.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">4. 激活函数</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/zh/part_1/05_优化和可视化.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">5. 优化和可视化</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/zh/part_1/06_过拟合和解决技术的发展.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">6. 过拟合和解决技术的发展</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/zh/part_1/07_卷积神经网络的进化.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">7. 卷积神经网络的进化</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/zh/part_1/08_变压器的诞生.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">8. 变压器的诞生</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/zh/part_1/09_变压器的进化.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">9. 变压器的进化</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/zh/part_1/10_多模态深度学习：多感官融合的开始.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">10. 多模态深度学习：多感官融合的开始</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/zh/part_1/11_多模态深度学习：超越智能的极限.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">11. 多模态深度学习：超越智能的极限</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">深度学习的最前沿</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/zh/深度学习的最前沿/01_SLM: 小但强大的语言模型.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">1. SLM: 小但强大的语言模型</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/zh/深度学习的最前沿/02_自动驾驶.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">2. 自动驾驶</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#深度学习框架" id="toc-深度学习框架" class="nav-link active" data-scroll-target="#深度学习框架">3. 深度学习框架</a>
  <ul class="collapse">
  <li><a href="#pytorch" id="toc-pytorch" class="nav-link" data-scroll-target="#pytorch">3.1 PyTorch</a>
  <ul class="collapse">
  <li><a href="#张量对象" id="toc-张量对象" class="nav-link" data-scroll-target="#张量对象">3.1.1 张量对象</a></li>
  <li><a href="#运算" id="toc-运算" class="nav-link" data-scroll-target="#运算">3.1.2 运算</a></li>
  <li><a href="#计算图用于梯度计算" id="toc-计算图用于梯度计算" class="nav-link" data-scroll-target="#计算图用于梯度计算">3.1.3 计算图用于梯度计算</a></li>
  <li><a href="#数据加载" id="toc-数据加载" class="nav-link" data-scroll-target="#数据加载">3.1.4 数据加载</a></li>
  <li><a href="#数据转换transform" id="toc-数据转换transform" class="nav-link" data-scroll-target="#数据转换transform">3.1.5 数据转换(Transform)</a></li>
  <li><a href="#模型" id="toc-模型" class="nav-link" data-scroll-target="#模型">3.1.6 模型</a></li>
  <li><a href="#训练" id="toc-训练" class="nav-link" data-scroll-target="#训练">3.1.7 训练</a></li>
  <li><a href="#模型保存读取" id="toc-模型保存读取" class="nav-link" data-scroll-target="#模型保存读取">3.1.8 模型保存、读取</a></li>
  </ul></li>
  <li><a href="#张量板" id="toc-张量板" class="nav-link" data-scroll-target="#张量板">3.2 张量板</a>
  <ul class="collapse">
  <li><a href="#张量板基本用法" id="toc-张量板基本用法" class="nav-link" data-scroll-target="#张量板基本用法">3.2.1 张量板基本用法</a></li>
  <li><a href="#张量板的主要可视化功能" id="toc-张量板的主要可视化功能" class="nav-link" data-scroll-target="#张量板的主要可视化功能">3.2.2 张量板的主要可视化功能</a></li>
  <li><a href="#张量板示例" id="toc-张量板示例" class="nav-link" data-scroll-target="#张量板示例">3.2.3 张量板示例</a></li>
  </ul></li>
  <li><a href="#hugging-face-transformers" id="toc-hugging-face-transformers" class="nav-link" data-scroll-target="#hugging-face-transformers">3.3 Hugging Face Transformers</a>
  <ul class="collapse">
  <li><a href="#transformers库介绍" id="toc-transformers库介绍" class="nav-link" data-scroll-target="#transformers库介绍">3.3.1 Transformers库介绍</a></li>
  <li><a href="#主要应用案例" id="toc-主要应用案例" class="nav-link" data-scroll-target="#主要应用案例">3.3.2 主要应用案例</a></li>
  </ul></li>
  <li><a href="#练习题" id="toc-练习题" class="nav-link" data-scroll-target="#练习题">练习题</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../../notebooks/zh/part_1/01_深度学习的开始.html">part_1</a></li><li class="breadcrumb-item"><a href="../../../notebooks/zh/part_1/03_深度学习框架.html">3. 深度学习框架</a></li></ol></nav></header>




<p><a href="https://colab.research.google.com/github/Quantum-Intelligence-Frontier/dldna/blob/main/notebooks/zh/part_1/03_深度学习框架.ipynb" target="_parent"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="在Colab中打开"> </a></p>
<section id="深度学习框架" class="level1">
<h1>3. 深度学习框架</h1>
<blockquote class="blockquote">
<p>“工具与制造它的工匠一样出色。” - <em>匿名，但经常被引用为冯·诺伊曼</em></p>
</blockquote>
<p>在深度学习的历史中，框架的发展是非常重要的。自2012年AlexNet成功之后，各种框架相继出现。经过Caffe、Theano、Torch7等阶段，目前主要是PyTorch和TensorFlow占据了主流。</p>
<p>2010年代初，深度学习开始在图像识别、语音识别等多个领域展现出超越现有技术的惊人成果。然而，训练和部署深度学习模型仍然是一个难题。因为需要直接实现神经网络构建、梯度计算、GPU加速等。这种复杂性提高了进入深度学习研究的门槛，并减缓了研究速度。为此，出现了深度学习框架，提供了用于构建、训练和部署神经网络模型的高级API和工具，简化并加速了开发过程。最初出现的是Theano、Caffe、Torch等框架，在学术界和工业界得到了广泛应用。</p>
<p>2015年，谷歌将TensorFlow开源，给深度学习框架生态系统带来了重大变化。TensorFlow凭借灵活的架构、强大的可视化工具、大规模分布式训练支持等功能迅速获得 popularity。2017年，Facebook发布了PyTorch，树立了另一个重要的里程碑。PyTorch提供了动态计算图、直观的接口和出色的调试功能，在研究者中迅速普及。</p>
<p>目前，深度学习框架已经不仅仅是一个简单的工具，而是成为了深度学习研究和开发的核心基础设施。它们提供自动微分、GPU加速、模型并行化、分布式训练等核心功能，加速了新模型和算法的发展。此外，不同框架之间的竞争与合作也在推动着深度学习生态系统的进一步发展。</p>
<section id="pytorch" class="level2">
<h2 class="anchored" data-anchor-id="pytorch">3.1 PyTorch</h2>
<p>PyTorch 是基于 Torch 库的开源机器学习框架，用于计算机视觉和自然语言处理等应用。2016年，Facebook的人工智能研究实验室（FAIR）将Torch7用Python重新实现，并开发为核心框架。由于动态计算图和直观的调试功能，它在研究者中迅速受欢迎。尽管还有TensorFlow、JAX、Caffe等其他框架，但PyTorch已经成为了研究领域的事实标准。许多新模型通常与PyTorch实现一起发布。</p>
<p>熟悉一个框架后，利用另一个框架的优势也是一个好策略。例如，可以将TensorFlow的数据预处理管道或JAX的功能转换功能与PyTorch结合使用。</p>
<div id="cell-2" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip install dldna[colab] <span class="co"># in Colab</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="co"># !pip install dldna[all] # in your local</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="cell-3" class="cell" data-execution_count="1">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Print PyTorch version</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"PyTorch version: </span><span class="sc">{</span>torch<span class="sc">.</span>__version__<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Set the random seed for reproducibility</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">7</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>PyTorch version: 2.6.0+cu124</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="1">
<pre><code>&lt;torch._C.Generator at 0x7352f02b33f0&gt;</code></pre>
</div>
</div>
<p>在生成随机数时，设置初始种子值可以每次都获得相同的随机数。这是为了在重复的训练中保证一致的结果，研究中经常使用这种方法。</p>
<section id="张量对象" class="level3">
<h3 class="anchored" data-anchor-id="张量对象">3.1.1 张量对象</h3>
<blockquote class="blockquote">
<p><strong>挑战任务</strong>: 如何利用GPU高效地进行大规模矩阵运算？</p>
<p><strong>研究者的苦恼</strong>: 随着深度学习模型的规模增大，仅使用CPU进行训练和推理所需的时间变得非常长。虽然GPU在并行计算方面具有优势，适合深度学习，但GPU编程复杂且困难。为了使深度学习研究人员更容易利用GPU，需要一种工具来抽象化和自动化GPU运算。</p>
</blockquote>
<p>张量是PyTorch的基本数据结构。自2006年CUDA出现以来，GPU运算成为了深度学习的核心，而张量则被设计用于高效地执行这些GPU运算。张量是一般的多维数组，它概括了标量、向量和矩阵。在深度学习中，数据的维度（张量秩）非常多样。例如，图像表示为(批次, 通道, 高度, 宽度)的四维张量，而自然语言则表示为(批次, 序列长度, 嵌入维度)的三维张量。正如我们在第2章所讨论的那样，能够灵活地变换和处理这些维度非常重要。</p>
<p>可以如下声明张量：</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>属性</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>torch.tensor(data)</code></td>
<td>从数据创建一个张量</td>
</tr>
<tr class="even">
<td><code>torch.zeros(size)</code></td>
<td>创建一个全为0的张量</td>
</tr>
<tr class="odd">
<td><code>torch.ones(size)</code></td>
<td>创建一个全为1的张量</td>
</tr>
<tr class="even">
<td><code>torch.rand(size)</code></td>
<td>创建一个在[0, 1)区间内的随机值张量</td>
</tr>
</tbody>
</table>
<div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="co"># 从列表创建张量</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> torch.tensor([[<span class="dv">1</span>, <span class="dv">2</span>], [<span class="dv">3</span>, <span class="dv">4</span>]])</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(a)</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="co"># 创建全为0的张量</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> torch.zeros((<span class="dv">2</span>, <span class="dv">3</span>))</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(b)</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a><span class="co"># 创建全为1的张量</span></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>c <span class="op">=</span> torch.ones((<span class="dv">2</span>, <span class="dv">3</span>))</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(c)</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a><span class="co"># 创建随机值张量</span></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>d <span class="op">=</span> torch.rand((<span class="dv">2</span>, <span class="dv">3</span>))</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(d)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div id="cell-6" class="cell" data-execution_count="2">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a 3x2x4 tensor with random values</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> torch.Tensor(<span class="dv">3</span>, <span class="dv">2</span>, <span class="dv">4</span>)</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(a)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>tensor([[[ 1.1210e-44,  0.0000e+00,  0.0000e+00,  4.1369e-41],
         [ 1.8796e-17,  0.0000e+00,  2.8026e-45,  0.0000e+00]],

        [[ 0.0000e+00,  0.0000e+00,         nan,         nan],
         [ 6.3058e-44,  4.7424e+30,  1.4013e-45,  1.3563e-19]],

        [[ 1.0089e-43,  0.0000e+00,  1.1210e-44,  0.0000e+00],
         [-8.8105e+09,  4.1369e-41,  1.8796e-17,  0.0000e+00]]])</code></pre>
</div>
</div>
<p>可以从现有数据初始化张量。</p>
<div id="cell-8" class="cell" data-execution_count="4">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># From a Python list</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>d <span class="op">=</span> [[<span class="dv">1</span>, <span class="dv">2</span>], [<span class="dv">3</span>, <span class="dv">4</span>]]</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Type of d: </span><span class="sc">{</span><span class="bu">type</span>(d)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> torch.Tensor(d)  <span class="co"># Creates a *copy*</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Tensor a:</span><span class="ch">\n</span><span class="sc">{</span>a<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Type of a: </span><span class="sc">{</span><span class="bu">type</span>(a)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a><span class="co"># From a NumPy array</span></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>d_np <span class="op">=</span> np.array(d)</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Type of d_np: </span><span class="sc">{</span><span class="bu">type</span>(d_np)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> torch.from_numpy(d_np) <span class="co"># Shares memory with d_np (zero-copy)</span></span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Tensor b (from_numpy):</span><span class="ch">\n</span><span class="sc">{</span>b<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>c <span class="op">=</span> torch.Tensor(d_np)  <span class="co"># Creates a *copy*</span></span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Tensor c (from np array using torch.Tensor):</span><span class="ch">\n</span><span class="sc">{</span>c<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Example of memory sharing with torch.from_numpy</span></span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a>d_np[<span class="dv">0</span>, <span class="dv">0</span>] <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Modified d_np:</span><span class="ch">\n</span><span class="sc">{</span>d_np<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Tensor b (from_numpy) after modifying d_np:</span><span class="ch">\n</span><span class="sc">{</span>b<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Tensor c (copy) after modifying d_np:</span><span class="ch">\n</span><span class="sc">{</span>c<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Type of d: &lt;class 'list'&gt;
Tensor a:
tensor([[1., 2.],
        [3., 4.]])
Type of a: &lt;class 'torch.Tensor'&gt;
Type of d_np: &lt;class 'numpy.ndarray'&gt;
Tensor b (from_numpy):
tensor([[1, 2],
        [3, 4]])
Tensor c (from np array using torch.Tensor):
tensor([[1., 2.],
        [3., 4.]])
Modified d_np:
[[100   2]
 [  3   4]]
Tensor b (from_numpy) after modifying d_np:
tensor([[100,   2],
        [  3,   4]])
Tensor c (copy) after modifying d_np:
tensor([[1., 2.],
        [3., 4.]])</code></pre>
</div>
</div>
<p>输出相同并不意味着是相同的对象。<code>d</code> 是 Python 列表对象，而张量可以从各种数据结构中创建。特别是与 NumPy 数组的交互非常高效。然而，列表对象和 NumPy 数组不支持 GPU，因此对于大规模运算，转换为张量是必要的。<em>重要的是</em> 理解 <code>torch.Tensor(data)</code> 和 <code>torch.from_numpy(data)</code> 之间的区别。前者 <em>总是</em> 创建一个副本，而后者创建一个与原 NumPy 数组共享内存的 <em>视图</em>（如果可能的话 - 零拷贝）。修改 NumPy 数组也会改变通过 <code>from_numpy</code> 创建的张量，反之亦然。</p>
<p>初始化张量的方法非常多样。2006 年 Hinton 的论文之后，初始化方法的重要性得到了强调，并且开发了各种初始化策略。最基本的初始化函数如下：</p>
<ul>
<li><code>torch.zeros</code>: 用 0 初始化。</li>
<li><code>torch.ones</code>: 用 1 初始化。</li>
<li><code>torch.rand</code>: 从 0 和 1 之间的均匀分布中随机数初始化。</li>
<li><code>torch.randn</code>: 从标准正态分布（均值 0，方差 1）中随机数初始化。</li>
<li><code>torch.arange</code>: 按 n, n+1, n+2, … 的顺序初始化。</li>
</ul>
<div id="cell-10" class="cell" data-execution_count="5">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>shape <span class="op">=</span> (<span class="dv">2</span>, <span class="dv">3</span>)</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>rand_t <span class="op">=</span> torch.rand(shape)     <span class="co"># Uniform distribution [0, 1)</span></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>randn_t <span class="op">=</span> torch.randn(shape)   <span class="co"># Standard normal distribution</span></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>ones_t <span class="op">=</span> torch.ones(shape)</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>zeros_t <span class="op">=</span> torch.zeros(shape)</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Random tensor (uniform):</span><span class="ch">\n</span><span class="sc">{</span>rand_t<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Random tensor (normal):</span><span class="ch">\n</span><span class="sc">{</span>randn_t<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Ones tensor:</span><span class="ch">\n</span><span class="sc">{</span>ones_t<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Zeros tensor:</span><span class="ch">\n</span><span class="sc">{</span>zeros_t<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Random tensor (uniform):
tensor([[0.5349, 0.1988, 0.6592],
        [0.6569, 0.2328, 0.4251]])
Random tensor (normal):
tensor([[-1.2514, -1.8841,  0.4457],
        [-0.7068, -1.5750, -0.6318]])
Ones tensor:
tensor([[1., 1., 1.],
        [1., 1., 1.]])
Zeros tensor:
tensor([[0., 0., 0.],
        [0., 0., 0.]])</code></pre>
</div>
</div>
<p>PyTorch 支持 100 多种张量运算，这些都可以在 GPU 上运行。张量默认是在 CPU 内存中创建的，因此如果要使用 GPU，则需要使用 <code>to()</code> 函数显式地进行移动。在 CPU 和 GPU 之间移动大型张量会产生相当大的成本，因此谨慎的内存管理是必不可少的。在实际的深度学习训练中，GPU 的内存带宽对性能有决定性的影响。例如，在训练变压器模型时，GPU 内存越大，可以使用更大的批处理大小，从而提高训练效率。然而，高带宽内存的生产成本非常高，占 GPU 价格的很大一部分。CPU 和 GPU 张量运算的性能差异在矩阵乘法等可并行化的运算中尤为显著。正因为如此，在现代深度学习中，GPU、TPU、NPU 等专用加速器是必不可少的。</p>
<div id="cell-12" class="cell" data-execution_count="6">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Device setting</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> torch.cuda.is_available():</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>    tensor <span class="op">=</span> zeros_t.to(<span class="st">"cuda"</span>)</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>    device <span class="op">=</span> <span class="st">"cuda:0"</span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>    device <span class="op">=</span> <span class="st">"cpu"</span></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'GPU not available'</span>)</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a><span class="co"># CPU/GPU performance comparison</span></span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> time</span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a><span class="co"># CPU operation</span></span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.rand(<span class="dv">10000</span>, <span class="dv">10000</span>)</span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>start <span class="op">=</span> time.time()</span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a>torch.matmul(x, x)</span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a>cpu_time <span class="op">=</span> time.time() <span class="op">-</span> start</span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"CPU computation time = </span><span class="sc">{</span>cpu_time<span class="sc">:3.2f}</span><span class="ss"> seconds"</span>)</span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a><span class="co"># GPU operation</span></span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> device <span class="op">!=</span> <span class="st">"cpu"</span>:</span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> x.to(device)</span>
<span id="cb12-22"><a href="#cb12-22" aria-hidden="true" tabindex="-1"></a>    start <span class="op">=</span> torch.cuda.Event(enable_timing<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb12-23"><a href="#cb12-23" aria-hidden="true" tabindex="-1"></a>    end <span class="op">=</span> torch.cuda.Event(enable_timing<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb12-24"><a href="#cb12-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-25"><a href="#cb12-25" aria-hidden="true" tabindex="-1"></a>    start.record()</span>
<span id="cb12-26"><a href="#cb12-26" aria-hidden="true" tabindex="-1"></a>    torch.matmul(x, x)</span>
<span id="cb12-27"><a href="#cb12-27" aria-hidden="true" tabindex="-1"></a>    end.record()</span>
<span id="cb12-28"><a href="#cb12-28" aria-hidden="true" tabindex="-1"></a>    torch.cuda.synchronize()  <span class="co"># Wait for all operations to complete</span></span>
<span id="cb12-29"><a href="#cb12-29" aria-hidden="true" tabindex="-1"></a>    gpu_time <span class="op">=</span> start.elapsed_time(end) <span class="op">/</span> <span class="dv">1000</span>  <span class="co"># Convert milliseconds to seconds</span></span>
<span id="cb12-30"><a href="#cb12-30" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"GPU computation time = </span><span class="sc">{</span>gpu_time<span class="sc">:3.2f}</span><span class="ss"> seconds"</span>)</span>
<span id="cb12-31"><a href="#cb12-31" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"GPU is </span><span class="sc">{</span>cpu_time <span class="op">/</span> gpu_time<span class="sc">:3.1f}</span><span class="ss"> times faster."</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>CPU computation time = 2.34 seconds
GPU computation time = 0.14 seconds
GPU is 16.2 times faster.</code></pre>
</div>
</div>
<p>NumPy和张量之间的转换实现得非常高效。特别是，如上所述，使用<code>torch.from_numpy()</code>可以在不进行内存复制的情况下共享内存。</p>
<div id="cell-14" class="cell" data-execution_count="7">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>np_a <span class="op">=</span> np.array([[<span class="dv">1</span>, <span class="dv">1</span>], [<span class="dv">2</span>, <span class="dv">3</span>]])</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>tensor_a <span class="op">=</span> torch.from_numpy(np_a)</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>np_b <span class="op">=</span> tensor_a.numpy() <span class="co"># Shares memory.  If tensor_a is on CPU.</span></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"NumPy array: </span><span class="sc">{</span>np_a<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Tensor: </span><span class="sc">{</span>tensor_a<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"NumPy array from Tensor: </span><span class="sc">{</span>np_b<span class="sc">}</span><span class="ss">"</span>) <span class="co">#if tensor_a is on CPU.</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>NumPy array: [[1 1]
 [2 3]]
Tensor: tensor([[1, 1],
        [2, 3]])
NumPy array from Tensor: [[1 1]
 [2 3]]</code></pre>
</div>
</div>
<p>在将张量转换为NumPy时，张量必须位于CPU上。如果张量在GPU上，则必须先使用<code>.cpu()</code>将其移动到CPU上。张量的基本属性是<code>shape</code>、<code>dtype</code>和<code>device</code>，通过这些属性可以查看张量的形状和存储位置。</p>
<div id="cell-16" class="cell" data-execution_count="18">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> torch.rand(<span class="dv">2</span>, <span class="dv">3</span>)</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Shape = </span><span class="sc">{</span>a<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Data type = </span><span class="sc">{</span>a<span class="sc">.</span>dtype<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Device = </span><span class="sc">{</span>a<span class="sc">.</span>device<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Shape = torch.Size([2, 3])
Data type = torch.float32
Device = cpu</code></pre>
</div>
</div>
<p>索引和切片使用与NumPy相同的语法。</p>
<div id="cell-18" class="cell" data-execution_count="19">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> torch.rand(<span class="dv">3</span>, <span class="dv">3</span>)</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Tensor a:</span><span class="ch">\n</span><span class="sc">{</span>a<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"First row: </span><span class="sc">{</span>a[<span class="dv">0</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"First column: </span><span class="sc">{</span>a[:, <span class="dv">0</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Last column: </span><span class="sc">{</span>a[..., <span class="op">-</span><span class="dv">1</span>]<span class="sc">}</span><span class="ss">"</span>)  <span class="co"># Equivalent to a[:, -1]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Tensor a:
tensor([[0.2069, 0.8296, 0.4973],
        [0.9265, 0.8386, 0.6611],
        [0.5329, 0.7822, 0.0975]])
First row: tensor([0.2069, 0.8296, 0.4973])
First column: tensor([0.2069, 0.9265, 0.5329])
Last column: tensor([0.4973, 0.6611, 0.0975])</code></pre>
</div>
</div>
</section>
<section id="运算" class="level3">
<h3 class="anchored">3.1.2 运算</h3>
<p>PyTorch 支持 NumPy 的几乎所有运算。自 1964 年 APL 语言开始的多维数组运算传统，已经从 NumPy 延续到了 PyTorch。可以在 PyTorch 官方文档(<a href="[https://pytorch.org/docs/stable/tensors.html">PyTorch documentation</a>)中查看支持的所有运算列表。</p>
<p>张量形状的改变是神经网络中最常用的运算之一。通过 <code>view()</code> 函数可以更改张量的维度，同时必须保持元素总数不变。<code>permute()</code> 函数用于重新排列维度顺序。</p>
<div id="cell-20" class="cell" data-execution_count="20">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> torch.arange(<span class="dv">12</span>)</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"a: </span><span class="sc">{</span>a<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> a.view(<span class="dv">3</span>, <span class="dv">4</span>)  <span class="co"># Reshape to 3x4</span></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"x: </span><span class="sc">{</span>x<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> x.permute(<span class="dv">1</span>, <span class="dv">0</span>)  <span class="co"># Swap dimensions 0 and 1</span></span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"y: </span><span class="sc">{</span>y<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> torch.randn(<span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">5</span>)</span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"b shape: </span><span class="sc">{</span>b<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a>z <span class="op">=</span> b.permute(<span class="dv">2</span>, <span class="dv">0</span>, <span class="dv">1</span>)  <span class="co"># Change dimension order to (2, 0, 1)</span></span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"z shape: </span><span class="sc">{</span>z<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>a: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])
x: tensor([[ 0,  1,  2,  3],
        [ 4,  5,  6,  7],
        [ 8,  9, 10, 11]])
y: tensor([[ 0,  4,  8],
        [ 1,  5,  9],
        [ 2,  6, 10],
        [ 3,  7, 11]])
b shape: torch.Size([2, 3, 5])
z shape: torch.Size([5, 2, 3])</code></pre>
</div>
</div>
<p>矩阵运算是深度学习的核心，PyTorch提供了多种矩阵运算函数。</p>
<ol type="1">
<li><code>torch.matmul</code>: 执行一般的矩阵运算。根据维度的不同，它的工作方式如下：
<ul>
<li>一维 × 一维: 内积(dot product)</li>
<li>二维 × 二维: 矩阵乘法</li>
<li>一维 × 二维: 在第一个张量上增加一个维度后进行矩阵乘法</li>
<li>N维 × M维: 广播后进行矩阵乘法</li>
</ul></li>
<li><code>torch.mm</code>: 纯粹的矩阵乘法运算（不支持广播）</li>
<li><code>torch.bmm</code>: 包含批处理维度的矩阵乘法 ((b, i, k) × (b, k, j) → (b, i, j))</li>
<li><code>torch.einsum</code>: 使用爱因斯坦求和约定进行张量运算。可以简洁地表示复杂的张量运算。（详情参见“理论深入”）
<ul>
<li><code>torch.einsum('ij,jk-&gt;ik', a, b)</code>: 矩阵a和b的乘积</li>
</ul></li>
</ol>
<div id="cell-22" class="cell" data-execution_count="21">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> torch.arange(<span class="dv">6</span>)</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> torch.arange(<span class="dv">12</span>)</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> a.view(<span class="dv">2</span>, <span class="dv">3</span>)</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>Y <span class="op">=</span> b.view(<span class="dv">3</span>, <span class="dv">4</span>)</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"X: </span><span class="sc">{</span>X<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Y: </span><span class="sc">{</span>Y<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a><span class="co"># matmul (2,3) X (3,4) -&gt; (2, 4)</span></span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"X @ Y = </span><span class="sc">{</span>torch<span class="sc">.</span>matmul(X, Y)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Using torch.einsum for matrix multiplication</span></span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a>einsum_result <span class="op">=</span> torch.einsum(<span class="st">'ij,jk-&gt;ik'</span>, X, Y)</span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"X @ Y (using einsum) = </span><span class="sc">{</span>einsum_result<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb22-15"><a href="#cb22-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-16"><a href="#cb22-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-17"><a href="#cb22-17" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> torch.arange(<span class="dv">2</span>)</span>
<span id="cb22-18"><a href="#cb22-18" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> torch.arange(<span class="dv">2</span>)</span>
<span id="cb22-19"><a href="#cb22-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"a: </span><span class="sc">{</span>a<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb22-20"><a href="#cb22-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"b: </span><span class="sc">{</span>b<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb22-21"><a href="#cb22-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-22"><a href="#cb22-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Vector x Vector operation</span></span>
<span id="cb22-23"><a href="#cb22-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"a @ b = </span><span class="sc">{</span>torch<span class="sc">.</span>matmul(a, b)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb22-24"><a href="#cb22-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-25"><a href="#cb22-25" aria-hidden="true" tabindex="-1"></a><span class="co"># 1D tensor (vector), 2D tensor (matrix) operation</span></span>
<span id="cb22-26"><a href="#cb22-26" aria-hidden="true" tabindex="-1"></a><span class="co"># (2) x (2,2) is treated as (1,2) x (2,2) for matrix multiplication.</span></span>
<span id="cb22-27"><a href="#cb22-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Result: (1,2) x (2,2) -&gt; (1,2)</span></span>
<span id="cb22-28"><a href="#cb22-28" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> torch.arange(<span class="dv">4</span>)</span>
<span id="cb22-29"><a href="#cb22-29" aria-hidden="true" tabindex="-1"></a>B <span class="op">=</span> b.view(<span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb22-30"><a href="#cb22-30" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"a: </span><span class="sc">{</span>a<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb22-31"><a href="#cb22-31" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"B: </span><span class="sc">{</span>B<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb22-32"><a href="#cb22-32" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"a @ B = </span><span class="sc">{</span>torch<span class="sc">.</span>matmul(a, B)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb22-33"><a href="#cb22-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-34"><a href="#cb22-34" aria-hidden="true" tabindex="-1"></a><span class="co"># Matrix x Vector operation</span></span>
<span id="cb22-35"><a href="#cb22-35" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> torch.randn(<span class="dv">3</span>, <span class="dv">4</span>)</span>
<span id="cb22-36"><a href="#cb22-36" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> torch.randn(<span class="dv">4</span>)</span>
<span id="cb22-37"><a href="#cb22-37" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"X @ b shape = </span><span class="sc">{</span>torch<span class="sc">.</span>matmul(X, b)<span class="sc">.</span>size()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb22-38"><a href="#cb22-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-39"><a href="#cb22-39" aria-hidden="true" tabindex="-1"></a><span class="co"># Batched matrix x Batched matrix</span></span>
<span id="cb22-40"><a href="#cb22-40" aria-hidden="true" tabindex="-1"></a><span class="co"># The leading batch dimension is maintained.</span></span>
<span id="cb22-41"><a href="#cb22-41" aria-hidden="true" tabindex="-1"></a><span class="co"># The 2nd and 3rd dimensions are treated as matrices for multiplication.</span></span>
<span id="cb22-42"><a href="#cb22-42" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> torch.arange(<span class="dv">18</span>).view(<span class="dv">3</span>, <span class="dv">2</span>, <span class="dv">3</span>)</span>
<span id="cb22-43"><a href="#cb22-43" aria-hidden="true" tabindex="-1"></a>Y <span class="op">=</span> torch.arange(<span class="dv">18</span>).view(<span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">2</span>)</span>
<span id="cb22-44"><a href="#cb22-44" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"X: </span><span class="sc">{</span>X<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb22-45"><a href="#cb22-45" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Y: </span><span class="sc">{</span>Y<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb22-46"><a href="#cb22-46" aria-hidden="true" tabindex="-1"></a><span class="co"># Batch dimension remains the same, and (2,3)x(3,2) -&gt; (2,2)</span></span>
<span id="cb22-47"><a href="#cb22-47" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"X @ Y shape: </span><span class="sc">{</span>torch<span class="sc">.</span>matmul(X, Y)<span class="sc">.</span>size()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb22-48"><a href="#cb22-48" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"X @ Y: </span><span class="sc">{</span>torch<span class="sc">.</span>matmul(X, Y)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb22-49"><a href="#cb22-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-50"><a href="#cb22-50" aria-hidden="true" tabindex="-1"></a><span class="co"># Batched matrix x Broadcasted matrix</span></span>
<span id="cb22-51"><a href="#cb22-51" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> torch.arange(<span class="dv">18</span>).view(<span class="dv">3</span>, <span class="dv">2</span>, <span class="dv">3</span>)</span>
<span id="cb22-52"><a href="#cb22-52" aria-hidden="true" tabindex="-1"></a>Y <span class="op">=</span> torch.arange(<span class="dv">6</span>).view(<span class="dv">3</span>, <span class="dv">2</span>)</span>
<span id="cb22-53"><a href="#cb22-53" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"X: </span><span class="sc">{</span>X<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb22-54"><a href="#cb22-54" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Y: </span><span class="sc">{</span>Y<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb22-55"><a href="#cb22-55" aria-hidden="true" tabindex="-1"></a><span class="co"># The second matrix lacks a batch dimension.</span></span>
<span id="cb22-56"><a href="#cb22-56" aria-hidden="true" tabindex="-1"></a><span class="co"># It's broadcasted to match the batch dimension of the first matrix (repeated 3 times).</span></span>
<span id="cb22-57"><a href="#cb22-57" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"X @ Y shape: </span><span class="sc">{</span>torch<span class="sc">.</span>matmul(X, Y)<span class="sc">.</span>size()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb22-58"><a href="#cb22-58" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"X @ Y: </span><span class="sc">{</span>torch<span class="sc">.</span>matmul(X, Y)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb22-59"><a href="#cb22-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-60"><a href="#cb22-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-61"><a href="#cb22-61" aria-hidden="true" tabindex="-1"></a><span class="co"># Using torch.einsum for matrix multiplication</span></span>
<span id="cb22-62"><a href="#cb22-62" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> torch.arange(<span class="dv">6</span>).view(<span class="dv">2</span>, <span class="dv">3</span>)</span>
<span id="cb22-63"><a href="#cb22-63" aria-hidden="true" tabindex="-1"></a>Y <span class="op">=</span> torch.arange(<span class="dv">12</span>).view(<span class="dv">3</span>, <span class="dv">4</span>)</span>
<span id="cb22-64"><a href="#cb22-64" aria-hidden="true" tabindex="-1"></a>einsum_result <span class="op">=</span> torch.einsum(<span class="st">'ij,jk-&gt;ik'</span>, X, Y)  <span class="co"># Equivalent to torch.matmul(X, Y)</span></span>
<span id="cb22-65"><a href="#cb22-65" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"X @ Y (using einsum) = </span><span class="sc">{</span>einsum_result<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>X: tensor([[0, 1, 2],
        [3, 4, 5]])
Y: tensor([[ 0,  1,  2,  3],
        [ 4,  5,  6,  7],
        [ 8,  9, 10, 11]])
X @ Y = tensor([[20, 23, 26, 29],
        [56, 68, 80, 92]])
X @ Y (using einsum) = tensor([[20, 23, 26, 29],
        [56, 68, 80, 92]])
a: tensor([0, 1])
b: tensor([0, 1])
a @ b = 1
a: tensor([0, 1])
B: tensor([[0, 1],
        [2, 3]])
a @ B = tensor([2, 3])
X @ b shape = torch.Size([3])
X: tensor([[[ 0,  1,  2],
         [ 3,  4,  5]],

        [[ 6,  7,  8],
         [ 9, 10, 11]],

        [[12, 13, 14],
         [15, 16, 17]]])
Y: tensor([[[ 0,  1],
         [ 2,  3],
         [ 4,  5]],

        [[ 6,  7],
         [ 8,  9],
         [10, 11]],

        [[12, 13],
         [14, 15],
         [16, 17]]])
X @ Y shape: torch.Size([3, 2, 2])
X @ Y: tensor([[[ 10,  13],
         [ 28,  40]],

        [[172, 193],
         [244, 274]],

        [[550, 589],
         [676, 724]]])
X: tensor([[[ 0,  1,  2],
         [ 3,  4,  5]],

        [[ 6,  7,  8],
         [ 9, 10, 11]],

        [[12, 13, 14],
         [15, 16, 17]]])
Y: tensor([[0, 1],
        [2, 3],
        [4, 5]])
X @ Y shape: torch.Size([3, 2, 2])
X @ Y: tensor([[[ 10,  13],
         [ 28,  40]],

        [[ 46,  67],
         [ 64,  94]],

        [[ 82, 121],
         [100, 148]]])
X @ Y (using einsum) = tensor([[20, 23, 26, 29],
        [56, 68, 80, 92]])</code></pre>
</div>
</div>
<p><code>torch.einsum</code> 使用爱因斯坦求和约定来表示张量运算。<code>'ij,jk-&gt;ik'</code> 表示将 <code>X</code> 张量的 <code>(i, j)</code> 维与 <code>Y</code> 张量的 <code>(j, k)</code> 维相乘，生成 <code>(i, k)</code> 维的结果。这与矩阵乘法 <code>torch.matmul(X, Y)</code> 产生相同的结果。<code>einsum</code> 还支持其他多种运算，包括转置、求和、内积、外积、批矩阵乘等。更多详细信息请参阅 PyTorch 文档。</p>
<div id="cell-24" class="cell" data-execution_count="22">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Other einsum examples</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Transpose</span></span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> torch.randn(<span class="dv">2</span>, <span class="dv">3</span>)</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> torch.einsum(<span class="st">'ij-&gt;ji'</span>, a)  <span class="co"># Swap dimensions</span></span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Sum of all elements</span></span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> torch.randn(<span class="dv">2</span>, <span class="dv">3</span>)</span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> torch.einsum(<span class="st">'ij-&gt;'</span>, a)  <span class="co"># Sum all elements</span></span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Batch matrix multiplication</span></span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> torch.randn(<span class="dv">3</span>, <span class="dv">2</span>, <span class="dv">5</span>)</span>
<span id="cb24-13"><a href="#cb24-13" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> torch.randn(<span class="dv">3</span>, <span class="dv">5</span>, <span class="dv">3</span>)</span>
<span id="cb24-14"><a href="#cb24-14" aria-hidden="true" tabindex="-1"></a>c <span class="op">=</span> torch.einsum(<span class="st">'bij,bjk-&gt;bik'</span>, a, b) <span class="co"># Batch matrix multiplication</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="callout callout-style-default callout-note callout-titled" title="点击查看内容（深入探讨：爱因斯坦求和约定与torch.einsum）">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
点击查看内容（深入探讨：爱因斯坦求和约定与torch.einsum）
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<section id="爱因斯坦记号法与-torch.einsum" class="level2">
<h2 class="anchored" data-anchor-id="爱因斯坦记号法与-torch.einsum">爱因斯坦记号法与 torch.einsum</h2>
<section id="爱因斯坦记号法-einstein-notation" class="level3">
<h3 class="anchored" data-anchor-id="爱因斯坦记号法-einstein-notation">爱因斯坦记号法 (Einstein Notation)</h3>
<p>爱因斯坦记号法（Einstein Notation，或 Einstein Summation Convention）是阿尔伯特·爱因斯坦在1916年描述广义相对论时引入的一种记号法。最初是为了简洁地表示物理学中的公式，尤其是相对论的公式而设计的，但由于其便利性和表达力，在处理张量运算的各种领域中得到了广泛应用。</p>
<p><strong>核心思想:</strong></p>
<ul>
<li><strong>重复的索引表示求和:</strong> 在一个项（term）中相同的索引出现两次时，暗示对该索引的所有可能值进行求和(summation)，省略显式的求和符号(<span class="math inline">\(\sum\)</span>)以简化记号。</li>
<li><strong>自由索引与哑索引:</strong>
<ul>
<li><strong>自由索引 (free index):</strong> 出现在结果张量中的索引。在每个项中仅出现一次。</li>
<li><strong>哑索引 (dummy index):</strong> 被求和的索引。在一个项中出现两次。(summation index, bound index)</li>
</ul></li>
</ul>
<p><strong>基本规则</strong></p>
<ol type="1">
<li><strong>如果一个索引在同一项中出现两次，则对该索引进行求和。</strong></li>
<li><strong>自由索引决定了结果张量的维度。</strong></li>
<li><strong>哑索引仅用于内部计算，不显示在结果中。</strong></li>
<li><strong>索引字符可以任意选择，但为了防止混淆，最好保持一致性</strong>。（通常使用<span class="math inline">\(i, j, k, l, m, n\)</span>等）</li>
<li><strong>箭头(<span class="math inline">\(\rightarrow\)</span>)的左边</strong>表示输入张量，<strong>右边</strong>表示输出张量。</li>
</ol>
<p><strong>示例</strong></p>
<ul>
<li><strong>向量点积 (dot product):</strong> <span class="math inline">\(a_i b_i\)</span> (<span class="math inline">\(\sum_i a_i b_i\)</span> 的同义词)</li>
<li><strong>矩阵乘法 (matrix multiplication):</strong> <span class="math inline">\(A_{ij} B_{jk} = C_{ik}\)</span> (<span class="math inline">\(\sum_j A_{ij}B_{jk}\)</span> 的同义词)</li>
<li><strong>转置 (transpose):</strong> <span class="math inline">\(A_{ij} = B_{ji}\)</span> （B 是 A 的转置矩阵）</li>
<li><strong>迹 (trace):</strong> <span class="math inline">\(A_{ii}\)</span> (<span class="math inline">\(\sum_i A_{ii}\)</span> 的同义词)</li>
<li><strong>外积 (outer product):</strong> <span class="math inline">\(a_i b_j = C_{ij}\)</span></li>
<li><strong>元素乘法(element-wise multiplication):</strong> <span class="math inline">\(A_{ij}B_{ij} = C_{ij}\)</span> （Hadamard 积）</li>
</ul>
<p><strong>深度学习中的应用示例</strong> * <strong>批处理矩阵乘法 (batched matrix multiplication):</strong> <span class="math inline">\(A_{bij} B_{bjk} = C_{bik}\)</span> (<span class="math inline">\(b\)</span>: 批次维度) * <strong>注意力机制 (attention mechanism):</strong> <span class="math inline">\(e_{ij} = Q_{ik} K_{jk}\)</span>, <span class="math inline">\(a_{ij} = \text{softmax}(e_{ij})\)</span>, <span class="math inline">\(v_{i} = a_{ij} V_{j}\)</span> (<span class="math inline">\(Q\)</span>: 查询, <span class="math inline">\(K\)</span>: 键, <span class="math inline">\(V\)</span>: 值) * <strong>双线性变换 (bilinear transformation):</strong> <span class="math inline">\(x_i W_{ijk} y_j = z_k\)</span> * <strong>多维卷积 (convolution):</strong> <span class="math inline">\(I_{b,c,i,j} * F_{o,c,k,l} = O_{b,o,i',j'}\)</span> (<span class="math inline">\(b\)</span>: 批次, <span class="math inline">\(c\)</span>: 输入通道, <span class="math inline">\(o\)</span>: 输出通道, <span class="math inline">\(i, j\)</span>: 输入空间维度, <span class="math inline">\(k, l\)</span>: 滤波器空间维度) * <strong>批归一化 (Batch Normalization):</strong> <span class="math inline">\(\gamma_c * \frac{x_{b,c,h,w} - \mu_c}{\sigma_c} + \beta_c\)</span> (<span class="math inline">\(c\)</span>: 通道维度, <span class="math inline">\(b\)</span>: 批次, <span class="math inline">\(h\)</span>: 高度, <span class="math inline">\(w\)</span>: 宽度) * <strong>RNN隐状态更新</strong>: <span class="math inline">\(h_t = \tanh(W_{ih}x_t + b_{ih} + W_{hh}h_{t-1} + b_{hh})\)</span> (<span class="math inline">\(h\)</span>: 隐藏, <span class="math inline">\(x\)</span>: 输入, <span class="math inline">\(W\)</span>: 权重, <span class="math inline">\(b\)</span>: 偏置) * <strong>LSTM单元状态更新</strong>: <span class="math inline">\(c_t = f_t * c_{t-1} + i_t * \tilde{c}_t\)</span> (<span class="math inline">\(c\)</span>: 单元状态, <span class="math inline">\(f\)</span>: 遗忘门, <span class="math inline">\(i\)</span>: 输入门, <span class="math inline">\(\tilde{c}_t\)</span>: 候选单元状态)</p>
</section>
<section id="torch.einsum" class="level3">
<h3 class="anchored" data-anchor-id="torch.einsum">torch.einsum</h3>
<p><code>torch.einsum</code> 是 PyTorch 中使用爱因斯坦求和约定执行张量运算的函数。<code>einsum</code> 是 “Einstein summation” 的缩写。</p>
<p><strong>用法:</strong></p>
<div class="sourceCode" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>torch.einsum(equation, <span class="op">*</span>operands)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li><code>equation</code>: 爱因斯坦求和约定字符串，如 <code>'ij,jk-&gt;ik'</code>。</li>
<li><code>*operands</code>: 参与运算的张量（可变参数）。</li>
</ul>
<p><strong>优点</strong></p>
<ul>
<li><strong>简洁性:</strong> 复杂的张量运算可以用一行代码表示。</li>
<li><strong>可读性:</strong> 爱因斯坦求和约定能清晰地表达张量运算的意义。</li>
<li><strong>灵活性:</strong> 可以组合各种张量运算，轻松定义新运算。</li>
<li><strong>优化:</strong> PyTorch 自动优化 <code>einsum</code> 运算，使其高效计算。（在某些情况下）可能比手动实现的运算更快。利用 BLAS、cuBLAS 等库的优化例程，或优化运算顺序。</li>
<li><strong>自动微分支持</strong>: 使用 <code>einsum</code> 定义的运算与 PyTorch 的自动微分系统完全兼容。</li>
</ul>
<p><strong>torch.einsum 示例:</strong></p>
<div class="sourceCode" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a><span class="co"># 矩阵乘法</span></span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> torch.randn(<span class="dv">3</span>, <span class="dv">4</span>)</span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>B <span class="op">=</span> torch.randn(<span class="dv">4</span>, <span class="dv">5</span>)</span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a>C <span class="op">=</span> torch.einsum(<span class="st">'ij,jk-&gt;ik'</span>, A, B)  <span class="co"># C = A @ B</span></span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a><span class="co"># 转置</span></span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> torch.randn(<span class="dv">3</span>, <span class="dv">4</span>)</span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a>B <span class="op">=</span> torch.einsum(<span class="st">'ij-&gt;ji'</span>, A)  <span class="co"># B = A.T</span></span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-12"><a href="#cb26-12" aria-hidden="true" tabindex="-1"></a><span class="co"># 对角线和</span></span>
<span id="cb26-13"><a href="#cb26-13" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> torch.randn(<span class="dv">3</span>, <span class="dv">3</span>)</span>
<span id="cb26-14"><a href="#cb26-14" aria-hidden="true" tabindex="-1"></a>trace <span class="op">=</span> torch.einsum(<span class="st">'ii-&gt;'</span>, A)  <span class="co"># trace = torch.trace(A)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
<section id="批量矩阵乘法" class="level1">
<h1>批量矩阵乘法</h1>
<p>A = torch.randn(2, 3, 4) B = torch.randn(2, 4, 5) C = torch.einsum(‘bij,bjk-&gt;bik’, A, B) # C = torch.bmm(A, B)</p>
</section>
<section id="外积" class="level1">
<h1>外积</h1>
<p>a = torch.randn(3) b = torch.randn(4) C = torch.einsum(‘i,j-&gt;ij’, a, b) # C = torch.outer(a, b)</p>
</section>
<section id="元素乘法" class="level1">
<h1>元素乘法</h1>
<p>A = torch.randn(2,3) B = torch.randn(2,3) C = torch.einsum(‘ij,ij-&gt;ij’, A, B) # C = A * B</p>
</section>
<section id="双线性变换" class="level1">
<h1>双线性变换</h1>
<p>x = torch.randn(3) W = torch.randn(5, 3, 4) y = torch.randn(4) z = torch.einsum(‘i,ijk,j-&gt;k’, x, W, y) # z_k = sum_i sum_j x_i * W_{ijk} * y_j</p>
</section>
<section id="多维张量缩减" class="level1">
<h1>多维张量缩减</h1>
<p>tensor = torch.randn(3, 4, 5, 6) result = torch.einsum(‘…ij-&gt;…i’, tensor) # 对最后两个维度求和</p>
<pre><code>
**`torch.einsum` vs. 其他操作:**

| 操作                    | `torch.einsum`           | 其他方法                                   |
| :---------------------- | :----------------------- | :------------------------------------------ |
| 矩阵乘法                | `'ij,jk-&gt;ik'`           | `torch.matmul(A, B)` 或者 `A @ B`          |
| 转置                    | `'ij-&gt;ji'`           | `torch.transpose(A, 0, 1)` 或者 `A.T`        |
| 对角和                  | `'ii-&gt;'`              | `torch.trace(A)`                            |
| 批量矩阵乘法            | `'bij,bjk-&gt;bik'`        | `torch.bmm(A, B)`                           |
| 内积                    | `'i,i-&gt;'`              | `torch.dot(a, b)`                            |
| 外积                    | `'i,j-&gt;ij'`           | `torch.outer(a, b)`                          |
| 元素乘法                | `'ij,ij-&gt;ij'`          | `A * B`                                      |
| 张量缩减 (sum, mean 等) | `'ijk-&gt;i'` (示例)      | `torch.sum(A, dim=(1, 2))`                   |

**`torch.einsum` 的局限性**

  * **初始学习曲线:** 对于不熟悉爱因斯坦记号的用户，刚开始可能会有些困难。
  * **复杂操作的可读性:** 在非常复杂的操作情况下，`einsum` 字符串可能变得很长，反而降低了可读性。这种情况下，最好将操作分解为多个步骤或使用注释。
  * **无法表示所有操作:** `einsum` 基于线性代数操作，因此无法直接表示非线性操作（如：`max`, `min`, `sort`）或条件操作。在这种情况下，需要与其他 PyTorch 函数一起使用。

**`einsum` 优化 (`torch.compile`)**
`torch.compile` (PyTorch 2.0 及以上)可以进一步优化 `einsum` 操作。`compile` 通过 JIT（即时）编译分析代码，执行各种优化，例如融合张量操作、优化内存访问模式等。

```python
import torch
# 在 PyTorch 2.0 及以上版本中可用

@torch.compile
def my_einsum_function(a, b):
    return torch.einsum('ij,jk-&gt;ik', a, b)

# 首次调用时编译，之后调用时执行优化后的代码
result = my_einsum_function(torch.randn(10, 20), torch.randn(20, 30))
</code></pre>
<p><strong>结论:</strong></p>
<p>爱因斯坦表示法和 <code>torch.einsum</code> 是深度学习中表达和计算复杂张量操作的强大工具。虽然一开始可能有些陌生，但熟悉之后可以大大提高代码的可读性和效率。特别是在处理如变压器模型等涉及大量复杂张量操作的深度学习模型时，其价值尤为显著。与 <code>torch.compile</code> 一起使用时，性能可以进一步提升。</p>
<p><strong>参考:</strong></p>
<ol type="1">
<li><strong>Einstein Notation:</strong> <a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://en.wikipedia.org/wiki/Einstein_notation">https://en.wikipedia.org/wiki/Einstein_notation</a></li>
<li><strong><code>torch.einsum</code> documentation:</strong> <a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://pytorch.org/docs/stable/generated/torch.einsum.html">https://pytorch.org/docs/stable/generated/torch.einsum.html</a></li>
<li><strong>A basic introduction to NumPy’s einsum:</strong> <a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://ajcr.net/Basic-guide-to-einsum/">https://ajcr.net/Basic-guide-to-einsum/</a></li>
<li><strong>Einsum is All You Need - Einstein Summation in Deep Learning:</strong> <a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://rockt.github.io/2018/04/30/einsum">https://rockt.github.io/2018/04/30/einsum</a></li>
</ol>
</section>
</div>
</div>
</div>
</section>
<section id="计算图用于梯度计算" class="level3">
<h3 class="anchored" data-anchor-id="计算图用于梯度计算">3.1.3 计算图用于梯度计算</h3>
<p>自动微分（Automatic Differentiation）自20世纪70年代开始研究，但在2015年后随着深度学习的发展而受到广泛关注。PyTorch通过动态计算图（dynamic computation graph）实现自动微分，这是我们在第2章中讨论的链式法则（chain rule）的实际实现。</p>
<p>PyTorch的自动微分可以在每个操作步骤中跟踪和存储梯度。为此，需要在张量上显式声明梯度跟踪。</p>
<div id="cell-27" class="cell" data-execution_count="23">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> torch.randn((<span class="dv">2</span>,))</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"a.requires_grad (default): </span><span class="sc">{</span>a<span class="sc">.</span>requires_grad<span class="sc">}</span><span class="ss">"</span>)  <span class="co"># False (default)</span></span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>a.requires_grad_(<span class="va">True</span>)  <span class="co"># In-place modification</span></span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"a.requires_grad (after setting to True): </span><span class="sc">{</span>a<span class="sc">.</span>requires_grad<span class="sc">}</span><span class="ss">"</span>)  <span class="co"># True</span></span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Declare during creation</span></span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.arange(<span class="dv">2</span>, dtype<span class="op">=</span>torch.float32, requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"x.requires_grad (declared at creation): </span><span class="sc">{</span>x<span class="sc">.</span>requires_grad<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>a.requires_grad (default): False
a.requires_grad (after setting to True): True
x.requires_grad (declared at creation): True</code></pre>
</div>
</div>
<p>例如，考虑以下简单的损失函数。 (参见图3-1, 旧版本)</p>
<p><span class="math display">\[y = \frac {1}{N}\displaystyle\sum_{i}^{N} \{(x_i - 1)^2 + 4) \}\]</span></p>
<p>对<span class="math inline">\(x_i\)</span>的操作可以依次表示为<span class="math inline">\(a_i = x_i - 1\)</span>, <span class="math inline">\(b_i = a_i^2\)</span>, <span class="math inline">\(c_i = b_i + 4\)</span>, <span class="math inline">\(y = \frac{1}{N}\sum_{i=1}^{N} c_i\)</span>。</p>
<p>我们将对该表达式执行前向(forward)和后向(backward)操作。</p>
<div id="cell-29" class="cell" data-execution_count="24">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> x <span class="op">-</span> <span class="dv">1</span></span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> a<span class="op">**</span><span class="dv">2</span></span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>c <span class="op">=</span> b <span class="op">+</span> <span class="dv">4</span></span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> c.mean()</span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"y = </span><span class="sc">{</span>y<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform backward operation</span></span>
<span id="cb30-9"><a href="#cb30-9" aria-hidden="true" tabindex="-1"></a>y.backward()</span>
<span id="cb30-10"><a href="#cb30-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-11"><a href="#cb30-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the gradient of x (x.grad)</span></span>
<span id="cb30-12"><a href="#cb30-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"x.grad = </span><span class="sc">{</span>x<span class="sc">.</span>grad<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>y = 4.5
x.grad = tensor([-1.,  0.])</code></pre>
</div>
</div>
<p>每个步骤的梯度用公式计算如下。</p>
<p><span class="math inline">\(\frac{\partial a_i}{\partial x_i} = 1, \frac{\partial b_i}{\partial a_i} = 2 \cdot a_i, \frac{\partial c_i}{\partial b_i} = 1,  \frac{\partial y}{\partial c_i} = \frac{1}{N}\)</span></p>
<p>因此，根据链式法则，</p>
<p><span class="math inline">\(\frac{\partial y}{\partial x_i} = \frac{\partial y}{\partial  c_i}\frac{\partial c_i}{\partial b_i}\frac{\partial b_i}{\partial a_i}\frac{\partial a_i}{\partial x_i} =  \frac{1}{N} \cdot 1 \cdot 2 \cdot a_i \cdot 1 = \frac{2}{N}a_i = \frac{2}{N}(x_i - 1)\)</span></p>
<p><span class="math inline">\(x_i\)</span> 在 [0, 1] 内，且 N=2 (x 的元素个数)，因此 <span class="math inline">\(\frac{\partial y}{\partial x_i}  = [-0.5, 0.5]\)</span>。这与 PyTorch 的自动求导结果一致。</p>
<p>PyTorch 实现了自 1970 年代以来研究的自动求导概念的现代版本。特别是计算图的动态生成和梯度跟踪功能非常有用。但有时需要禁用这些自动求导功能。</p>
<div id="cell-31" class="cell" data-execution_count="8">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.randn(<span class="dv">3</span>, <span class="dv">4</span>)</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>w <span class="op">=</span> torch.randn(<span class="dv">4</span>, <span class="dv">2</span>)</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> torch.randn(<span class="dv">2</span>)</span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a><span class="co"># If gradient tracking is needed</span></span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a>z <span class="op">=</span> torch.matmul(x, w) <span class="op">+</span> b</span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a>z.requires_grad_(<span class="va">True</span>)  <span class="co"># Can also be set using requires_grad_()</span></span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"z.requires_grad: </span><span class="sc">{</span>z<span class="sc">.</span>requires_grad<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Disable gradient tracking method 1: Using 'with' statement</span></span>
<span id="cb32-11"><a href="#cb32-11" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb32-12"><a href="#cb32-12" aria-hidden="true" tabindex="-1"></a>    z <span class="op">=</span> torch.matmul(x, w) <span class="op">+</span> b</span>
<span id="cb32-13"><a href="#cb32-13" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"z.requires_grad (inside no_grad): </span><span class="sc">{</span>z<span class="sc">.</span>requires_grad<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb32-14"><a href="#cb32-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-15"><a href="#cb32-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Disable gradient tracking method 2: Using detach()</span></span>
<span id="cb32-16"><a href="#cb32-16" aria-hidden="true" tabindex="-1"></a>z_det <span class="op">=</span> z.detach()</span>
<span id="cb32-17"><a href="#cb32-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"z_det.requires_grad: </span><span class="sc">{</span>z_det<span class="sc">.</span>requires_grad<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>z.requires_grad: True
z.requires_grad (inside no_grad): False
z_det.requires_grad: False</code></pre>
</div>
</div>
<p>梯度跟踪禁用在以下情况下特别有用：</p>
<ol type="1">
<li><strong>推理（Inference）时</strong>：仅需要前向传播的情况下，可以节省内存和计算成本。</li>
<li><strong>微调（Fine-tuning）</strong>：仅更新特定参数而固定其余部分时使用。</li>
<li><strong>性能优化</strong>：反向传播会产生额外的内存和计算成本，因此在不需要的情况下禁用它。</li>
</ol>
<p>特别是在大规模语言模型的微调中，通常会固定大多数参数并仅更新一部分，因此梯度跟踪的选择性激活是一个非常重要的功能。</p>
</section>
<section id="数据加载" class="level3">
<h3 class="anchored" data-anchor-id="数据加载">3.1.4 数据加载</h3>
<p>数据加载是深度学习的核心要素。直到20世纪初，各个研究团队还使用各自独立的数据处理方式，但自2009年ImageNet等大规模数据集的出现后，标准化数据加载系统的必要性变得明显。</p>
<p>PyTorch提供了两个核心类来分离数据处理和训练逻辑。</p>
<ol type="1">
<li><code>torch.utils.data.Dataset</code>: 提供对数据和标签的一致访问接口。需要实现 <code>__len__</code> 和 <code>__getitem__</code> 方法。</li>
<li><code>torch.utils.data.DataLoader</code>: 提供按批次(batch)高效加载数据的机制。通过包装 <code>Dataset</code> 来自动化生成小批量、混洗和并行数据加载等操作。</li>
</ol>
<p>以下是一个使用狄利克雷(Dirichlet)分布生成随机数据的示例。</p>
<div id="cell-34" class="cell" data-execution_count="9">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.utils.data <span class="im">as</span> data</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize with Dirichlet distribution</span></span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> np.random.dirichlet(np.ones(<span class="dv">5</span>), size<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> np.zeros_like(a)</span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate label values</span></span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> (a <span class="op">==</span> a.<span class="bu">max</span>(axis<span class="op">=</span><span class="dv">1</span>)[:, <span class="va">None</span>]).astype(<span class="bu">int</span>)</span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-10"><a href="#cb34-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Data (a):</span><span class="ch">\n</span><span class="sc">{</span>a<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb34-11"><a href="#cb34-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Labels (b):</span><span class="ch">\n</span><span class="sc">{</span>b<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb34-12"><a href="#cb34-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-13"><a href="#cb34-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-14"><a href="#cb34-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a custom Dataset class by inheriting from PyTorch's Dataset.</span></span>
<span id="cb34-15"><a href="#cb34-15" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> RandomData(data.Dataset):</span>
<span id="cb34-16"><a href="#cb34-16" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, feature, length):</span>
<span id="cb34-17"><a href="#cb34-17" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb34-18"><a href="#cb34-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.feature <span class="op">=</span> feature</span>
<span id="cb34-19"><a href="#cb34-19" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.length <span class="op">=</span> length</span>
<span id="cb34-20"><a href="#cb34-20" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.generate_data()</span>
<span id="cb34-21"><a href="#cb34-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-22"><a href="#cb34-22" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> generate_data(<span class="va">self</span>):</span>
<span id="cb34-23"><a href="#cb34-23" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> np.random.dirichlet(np.ones(<span class="va">self</span>.feature), size<span class="op">=</span><span class="va">self</span>.length)</span>
<span id="cb34-24"><a href="#cb34-24" aria-hidden="true" tabindex="-1"></a>        y <span class="op">=</span> (x <span class="op">==</span> x.<span class="bu">max</span>(axis<span class="op">=</span><span class="dv">1</span>)[:, <span class="va">None</span>]).astype(<span class="bu">int</span>)  <span class="co"># One-hot encoding</span></span>
<span id="cb34-25"><a href="#cb34-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-26"><a href="#cb34-26" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.data <span class="op">=</span> x  <span class="co"># numpy object</span></span>
<span id="cb34-27"><a href="#cb34-27" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.label <span class="op">=</span> y</span>
<span id="cb34-28"><a href="#cb34-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-29"><a href="#cb34-29" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__len__</span>(<span class="va">self</span>):</span>
<span id="cb34-30"><a href="#cb34-30" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.length</span>
<span id="cb34-31"><a href="#cb34-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-32"><a href="#cb34-32" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__getitem__</span>(<span class="va">self</span>, index):</span>
<span id="cb34-33"><a href="#cb34-33" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Return data and label as torch tensors</span></span>
<span id="cb34-34"><a href="#cb34-34" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.tensor(<span class="va">self</span>.data[index], dtype<span class="op">=</span>torch.float32), torch.tensor(<span class="va">self</span>.label[index], dtype<span class="op">=</span>torch.int64)</span>
<span id="cb34-35"><a href="#cb34-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-36"><a href="#cb34-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-37"><a href="#cb34-37" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> RandomData(feature<span class="op">=</span><span class="dv">10</span>, length<span class="op">=</span><span class="dv">100</span>)</span>
<span id="cb34-38"><a href="#cb34-38" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Number of data samples = </span><span class="sc">{</span><span class="bu">len</span>(dataset)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb34-39"><a href="#cb34-39" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Data at index 0 = </span><span class="sc">{</span>dataset[<span class="dv">0</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb34-40"><a href="#cb34-40" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Data type = </span><span class="sc">{</span><span class="bu">type</span>(dataset[<span class="dv">0</span>][<span class="dv">0</span>])<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Data (a):
[[0.46073711 0.01119455 0.28991657 0.11259078 0.12556099]
 [0.07331166 0.43554042 0.1243009  0.13339224 0.23345478]]
Labels (b):
[[1 0 0 0 0]
 [0 1 0 0 0]]
Number of data samples = 100
Data at index 0 = (tensor([1.4867e-01, 1.6088e-01, 1.2207e-02, 3.6049e-02, 1.1054e-04, 8.1160e-02,
        2.9811e-02, 1.9398e-01, 4.9448e-02, 2.8769e-01]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1]))
Data type = &lt;class 'torch.Tensor'&gt;</code></pre>
</div>
</div>
<p><code>DataLoader</code> 提供了多种用于批处理的功能。主要参数如下：</p>
<ul>
<li><code>batch_size</code>: 每批的样本数</li>
<li><code>shuffle</code>: 数据顺序随机化（训练时通常设置为 <code>True</code>）</li>
<li><code>num_workers</code>: 用于数据加载并行化的进程数</li>
<li><code>drop_last</code>: 是否处理最后一个不完整批次 (如果为 True，则丢弃)</li>
</ul>
<p>从 <code>Dataset</code> 中使用 <code>__getitem__</code> 读取数据，并将结果转换为张量对象。特别是，设置 <code>num_workers</code> 对于处理大规模图像或视频数据集时非常重要。然而，在小规模数据集中，单个进程可能更加高效。如果 <code>num_workers</code> 值设置得过大，则可能会产生额外的开销，因此找到合适的值很重要。（通常尝试核心数或者核心数 * 2 的值）。</p>
<div id="cell-36" class="cell" data-execution_count="10">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a>data_loader <span class="op">=</span> data.DataLoader(dataset, batch_size<span class="op">=</span><span class="dv">4</span>, shuffle<span class="op">=</span><span class="va">True</span>, num_workers<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Read one batch.</span></span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a>train_x, train_y <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(data_loader))</span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"1st batch training data = </span><span class="sc">{</span>train_x<span class="sc">}</span><span class="ss">, </span><span class="ch">\n</span><span class="ss"> Data shape = </span><span class="sc">{</span>train_x<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"1st batch label data = </span><span class="sc">{</span>train_y<span class="sc">}</span><span class="ss">, </span><span class="ch">\n</span><span class="ss"> Data shape = </span><span class="sc">{</span>train_y<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb36-8"><a href="#cb36-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"1st batch label data type = </span><span class="sc">{</span><span class="bu">type</span>(train_y)<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>1st batch training data = tensor([[3.3120e-02, 1.4274e-01, 9.7984e-02, 1.9628e-03, 6.8926e-02, 3.4525e-01,
         4.6966e-02, 6.0947e-02, 4.2738e-02, 1.5937e-01],
        [8.0707e-02, 4.9181e-02, 3.1863e-02, 1.4238e-02, 1.6089e-02, 1.7980e-01,
         1.7544e-01, 1.3465e-01, 1.6361e-01, 1.5442e-01],
        [4.2364e-02, 3.3635e-02, 2.0840e-01, 1.6919e-02, 4.5977e-02, 6.5791e-02,
         1.8726e-01, 1.0325e-01, 2.2029e-01, 7.6117e-02],
        [1.4867e-01, 1.6088e-01, 1.2207e-02, 3.6049e-02, 1.1054e-04, 8.1160e-02,
         2.9811e-02, 1.9398e-01, 4.9448e-02, 2.8769e-01]]), 
 Data shape = torch.Size([4, 10])
1st batch label data = tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]]), 
 Data shape = torch.Size([4, 10])
1st batch label data type = &lt;class 'torch.Tensor'&gt;</code></pre>
</div>
</div>
<p>PyTorch提供了针对特定领域数据处理的专用包。自2016年以来，随着深度学习在各个领域的扩展，对各领域特化数据处理的需求日益凸显。</p>
<ul>
<li><code>torchvision</code>: 计算机视觉</li>
<li><code>torchaudio</code>: 音频处理</li>
<li><code>torchtext</code>: 自然语言处理</li>
</ul>
<p>Fashion-MNIST是2017年Zalando Research发布的数据集，旨在替代MNIST。该数据集的构成如下。</p>
<ul>
<li>训练数据: 60,000个</li>
<li>测试数据: 10,000个</li>
<li>图像尺寸: 28x28灰度</li>
</ul>
<div id="cell-38" class="cell" data-execution_count="11">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> Dataset, DataLoader</span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision <span class="im">import</span> datasets</span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision.transforms <span class="im">import</span> ToTensor, Normalize, Compose</span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn_image <span class="im">as</span> isns</span>
<span id="cb38-6"><a href="#cb38-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt <span class="co"># Added for visualization</span></span>
<span id="cb38-7"><a href="#cb38-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-8"><a href="#cb38-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-9"><a href="#cb38-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Function to calculate mean and std of the dataset</span></span>
<span id="cb38-10"><a href="#cb38-10" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> calculate_mean_std(dataset):</span>
<span id="cb38-11"><a href="#cb38-11" aria-hidden="true" tabindex="-1"></a>    dataloader <span class="op">=</span> DataLoader(dataset, batch_size<span class="op">=</span><span class="bu">len</span>(dataset), shuffle<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb38-12"><a href="#cb38-12" aria-hidden="true" tabindex="-1"></a>    data, _ <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(dataloader))</span>
<span id="cb38-13"><a href="#cb38-13" aria-hidden="true" tabindex="-1"></a>    mean <span class="op">=</span> data.mean(axis<span class="op">=</span>(<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">3</span>))  <span class="co"># Calculate mean across channel dimension</span></span>
<span id="cb38-14"><a href="#cb38-14" aria-hidden="true" tabindex="-1"></a>    std <span class="op">=</span> data.std(axis<span class="op">=</span>(<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">3</span>))    <span class="co"># Calculate std across channel dimension</span></span>
<span id="cb38-15"><a href="#cb38-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> mean, std</span>
<span id="cb38-16"><a href="#cb38-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-17"><a href="#cb38-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-18"><a href="#cb38-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Datasets.  Note:  We *don't* apply Normalize here yet.</span></span>
<span id="cb38-19"><a href="#cb38-19" aria-hidden="true" tabindex="-1"></a>train_dataset <span class="op">=</span> datasets.FashionMNIST(</span>
<span id="cb38-20"><a href="#cb38-20" aria-hidden="true" tabindex="-1"></a>    root<span class="op">=</span><span class="st">"data"</span>, train<span class="op">=</span><span class="va">True</span>, download<span class="op">=</span><span class="va">True</span>, transform<span class="op">=</span>ToTensor()</span>
<span id="cb38-21"><a href="#cb38-21" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb38-22"><a href="#cb38-22" aria-hidden="true" tabindex="-1"></a>test_dataset <span class="op">=</span> datasets.FashionMNIST(</span>
<span id="cb38-23"><a href="#cb38-23" aria-hidden="true" tabindex="-1"></a>    root<span class="op">=</span><span class="st">"data"</span>, train<span class="op">=</span><span class="va">False</span>, download<span class="op">=</span><span class="va">True</span>, transform<span class="op">=</span>ToTensor()</span>
<span id="cb38-24"><a href="#cb38-24" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb38-25"><a href="#cb38-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-26"><a href="#cb38-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate mean and std for normalization</span></span>
<span id="cb38-27"><a href="#cb38-27" aria-hidden="true" tabindex="-1"></a>train_mean, train_std <span class="op">=</span> calculate_mean_std(train_dataset)</span>
<span id="cb38-28"><a href="#cb38-28" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Train data mean: </span><span class="sc">{</span>train_mean<span class="sc">}</span><span class="ss">, std: </span><span class="sc">{</span>train_std<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb38-29"><a href="#cb38-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-30"><a href="#cb38-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Now define transforms *with* normalization</span></span>
<span id="cb38-31"><a href="#cb38-31" aria-hidden="true" tabindex="-1"></a>transform <span class="op">=</span> Compose([</span>
<span id="cb38-32"><a href="#cb38-32" aria-hidden="true" tabindex="-1"></a>    ToTensor(),</span>
<span id="cb38-33"><a href="#cb38-33" aria-hidden="true" tabindex="-1"></a>    Normalize(train_mean, train_std)  <span class="co"># Use calculated mean and std</span></span>
<span id="cb38-34"><a href="#cb38-34" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb38-35"><a href="#cb38-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-36"><a href="#cb38-36" aria-hidden="true" tabindex="-1"></a><span class="co"># Re-create datasets with the normalization transform</span></span>
<span id="cb38-37"><a href="#cb38-37" aria-hidden="true" tabindex="-1"></a>train_dataset <span class="op">=</span> datasets.FashionMNIST(</span>
<span id="cb38-38"><a href="#cb38-38" aria-hidden="true" tabindex="-1"></a>    root<span class="op">=</span><span class="st">"data"</span>, train<span class="op">=</span><span class="va">True</span>, download<span class="op">=</span><span class="va">True</span>, transform<span class="op">=</span>transform</span>
<span id="cb38-39"><a href="#cb38-39" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb38-40"><a href="#cb38-40" aria-hidden="true" tabindex="-1"></a>test_dataset <span class="op">=</span> datasets.FashionMNIST(</span>
<span id="cb38-41"><a href="#cb38-41" aria-hidden="true" tabindex="-1"></a>    root<span class="op">=</span><span class="st">"data"</span>, train<span class="op">=</span><span class="va">False</span>, download<span class="op">=</span><span class="va">True</span>, transform<span class="op">=</span>transform</span>
<span id="cb38-42"><a href="#cb38-42" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb38-43"><a href="#cb38-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-44"><a href="#cb38-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-45"><a href="#cb38-45" aria-hidden="true" tabindex="-1"></a><span class="co"># Check one training data sample.</span></span>
<span id="cb38-46"><a href="#cb38-46" aria-hidden="true" tabindex="-1"></a>sample_idx <span class="op">=</span> torch.randint(<span class="bu">len</span>(train_dataset), size<span class="op">=</span>(<span class="dv">1</span>,)).item()</span>
<span id="cb38-47"><a href="#cb38-47" aria-hidden="true" tabindex="-1"></a>img, label <span class="op">=</span> train_dataset[sample_idx]  <span class="co"># Use a random index</span></span>
<span id="cb38-48"><a href="#cb38-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-49"><a href="#cb38-49" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Label: </span><span class="sc">{</span>label<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb38-50"><a href="#cb38-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-51"><a href="#cb38-51" aria-hidden="true" tabindex="-1"></a><span class="co"># Manually create a label map</span></span>
<span id="cb38-52"><a href="#cb38-52" aria-hidden="true" tabindex="-1"></a>labels_map <span class="op">=</span> {</span>
<span id="cb38-53"><a href="#cb38-53" aria-hidden="true" tabindex="-1"></a>    <span class="dv">0</span>: <span class="st">"T-shirt"</span>,</span>
<span id="cb38-54"><a href="#cb38-54" aria-hidden="true" tabindex="-1"></a>    <span class="dv">1</span>: <span class="st">"Trouser"</span>,</span>
<span id="cb38-55"><a href="#cb38-55" aria-hidden="true" tabindex="-1"></a>    <span class="dv">2</span>: <span class="st">"Pullover"</span>,</span>
<span id="cb38-56"><a href="#cb38-56" aria-hidden="true" tabindex="-1"></a>    <span class="dv">3</span>: <span class="st">"Dress"</span>,</span>
<span id="cb38-57"><a href="#cb38-57" aria-hidden="true" tabindex="-1"></a>    <span class="dv">4</span>: <span class="st">"Coat"</span>,</span>
<span id="cb38-58"><a href="#cb38-58" aria-hidden="true" tabindex="-1"></a>    <span class="dv">5</span>: <span class="st">"Sandal"</span>,</span>
<span id="cb38-59"><a href="#cb38-59" aria-hidden="true" tabindex="-1"></a>    <span class="dv">6</span>: <span class="st">"Shirt"</span>,</span>
<span id="cb38-60"><a href="#cb38-60" aria-hidden="true" tabindex="-1"></a>    <span class="dv">7</span>: <span class="st">"Sneaker"</span>,</span>
<span id="cb38-61"><a href="#cb38-61" aria-hidden="true" tabindex="-1"></a>    <span class="dv">8</span>: <span class="st">"Bag"</span>,</span>
<span id="cb38-62"><a href="#cb38-62" aria-hidden="true" tabindex="-1"></a>    <span class="dv">9</span>: <span class="st">"Ankle Boot"</span>,</span>
<span id="cb38-63"><a href="#cb38-63" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb38-64"><a href="#cb38-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-65"><a href="#cb38-65" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Label map: </span><span class="sc">{</span>labels_map[label]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb38-66"><a href="#cb38-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-67"><a href="#cb38-67" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot using seaborn-image.</span></span>
<span id="cb38-68"><a href="#cb38-68" aria-hidden="true" tabindex="-1"></a>isns.imgplot(img.squeeze())  <span class="co"># Squeeze to remove channel dimension for grayscale</span></span>
<span id="cb38-69"><a href="#cb38-69" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="ss">f"Label: </span><span class="sc">{</span>labels_map[label]<span class="sc">}</span><span class="ss">"</span>) <span class="co"># Add title to plot</span></span>
<span id="cb38-70"><a href="#cb38-70" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb38-71"><a href="#cb38-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-72"><a href="#cb38-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-73"><a href="#cb38-73" aria-hidden="true" tabindex="-1"></a><span class="co"># Define data loaders</span></span>
<span id="cb38-74"><a href="#cb38-74" aria-hidden="true" tabindex="-1"></a>train_dataloader <span class="op">=</span> DataLoader(train_dataset, batch_size<span class="op">=</span><span class="dv">32</span>, shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb38-75"><a href="#cb38-75" aria-hidden="true" tabindex="-1"></a>test_dataloader <span class="op">=</span> DataLoader(test_dataset, batch_size<span class="op">=</span><span class="dv">32</span>, shuffle<span class="op">=</span><span class="va">False</span>) <span class="co"># No need to shuffle test data</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Train data mean: tensor([0.2860]), std: tensor([0.3530])
Label: 5
Label map: Sandal</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="03_深度学习框架_files/figure-html/cell-19-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="数据转换transform" class="level3">
<h3 class="anchored" data-anchor-id="数据转换transform">3.1.5 数据转换(Transform)</h3>
<p>数据转换(Data Transform)是深度学习中非常重要的预处理步骤。自2012年AlexNet取得成功后，数据增强(Data Augmentation)成为提高模型性能的关键因素。PyTorch提供了多种工具来实现这些转换。使用<code>transforms.Compose</code>可以顺序应用多个转换。此外，通过<code>Lambda</code>函数也可以轻松实现用户自定义的转换。</p>
<p>数据转换对于提高模型的泛化(generalization)性能非常重要。特别是在计算机视觉领域，通过各种转换进行的数据增强已成为标准实践。对于<code>Normalize</code>转换来说，为了确保模型训练的稳定性，将数据标准化是一个必要的步骤。</p>
<p>要应用<code>Normalize</code>转换，需要知道数据集的均值(mean)和标准差(standard deviation)。计算这些值的代码如下所示。</p>
<div id="cell-40" class="cell" data-execution_count="12">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision <span class="im">import</span> transforms</span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> PIL</span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> DataLoader</span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision <span class="im">import</span> datasets</span>
<span id="cb40-6"><a href="#cb40-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-7"><a href="#cb40-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate mean and std of the dataset</span></span>
<span id="cb40-8"><a href="#cb40-8" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> calculate_mean_std(dataset):</span>
<span id="cb40-9"><a href="#cb40-9" aria-hidden="true" tabindex="-1"></a>    dataloader <span class="op">=</span> DataLoader(dataset, batch_size<span class="op">=</span><span class="bu">len</span>(dataset), shuffle<span class="op">=</span><span class="va">False</span>) <span class="co"># Load all data at once</span></span>
<span id="cb40-10"><a href="#cb40-10" aria-hidden="true" tabindex="-1"></a>    data, _ <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(dataloader))</span>
<span id="cb40-11"><a href="#cb40-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># For grayscale images, calculate mean and std over height, width dimensions (0, 2, 3)</span></span>
<span id="cb40-12"><a href="#cb40-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># For RGB images, the calculation would be over (0, 1, 2)</span></span>
<span id="cb40-13"><a href="#cb40-13" aria-hidden="true" tabindex="-1"></a>    mean <span class="op">=</span> data.mean(dim<span class="op">=</span>(<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">3</span>))  <span class="co"># Calculate mean across batch and spatial dimensions</span></span>
<span id="cb40-14"><a href="#cb40-14" aria-hidden="true" tabindex="-1"></a>    std <span class="op">=</span> data.std(dim<span class="op">=</span>(<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">3</span>))    <span class="co"># Calculate std across batch and spatial dimensions</span></span>
<span id="cb40-15"><a href="#cb40-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> mean, std</span>
<span id="cb40-16"><a href="#cb40-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-17"><a href="#cb40-17" aria-hidden="true" tabindex="-1"></a><span class="co"># --- Example usage with FashionMNIST ---</span></span>
<span id="cb40-18"><a href="#cb40-18" aria-hidden="true" tabindex="-1"></a><span class="co"># 1.  Create dataset *without* normalization first:</span></span>
<span id="cb40-19"><a href="#cb40-19" aria-hidden="true" tabindex="-1"></a>train_dataset_for_calc <span class="op">=</span> datasets.FashionMNIST(</span>
<span id="cb40-20"><a href="#cb40-20" aria-hidden="true" tabindex="-1"></a>    root<span class="op">=</span><span class="st">"data"</span>, train<span class="op">=</span><span class="va">True</span>, download<span class="op">=</span><span class="va">True</span>, transform<span class="op">=</span>transforms.ToTensor()  <span class="co"># Only ToTensor</span></span>
<span id="cb40-21"><a href="#cb40-21" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb40-22"><a href="#cb40-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-23"><a href="#cb40-23" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Calculate mean and std:</span></span>
<span id="cb40-24"><a href="#cb40-24" aria-hidden="true" tabindex="-1"></a>train_mean, train_std <span class="op">=</span> calculate_mean_std(train_dataset_for_calc)</span>
<span id="cb40-25"><a href="#cb40-25" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Train data mean: </span><span class="sc">{</span>train_mean<span class="sc">}</span><span class="ss">, std: </span><span class="sc">{</span>train_std<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb40-26"><a href="#cb40-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-27"><a href="#cb40-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-28"><a href="#cb40-28" aria-hidden="true" tabindex="-1"></a><span class="co"># 3.  *Now* create the dataset with normalization:</span></span>
<span id="cb40-29"><a href="#cb40-29" aria-hidden="true" tabindex="-1"></a>transform <span class="op">=</span> transforms.Compose([</span>
<span id="cb40-30"><a href="#cb40-30" aria-hidden="true" tabindex="-1"></a>    transforms.ToTensor(),</span>
<span id="cb40-31"><a href="#cb40-31" aria-hidden="true" tabindex="-1"></a>    transforms.Normalize(train_mean, train_std)  <span class="co"># Use calculated mean and std</span></span>
<span id="cb40-32"><a href="#cb40-32" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb40-33"><a href="#cb40-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-34"><a href="#cb40-34" aria-hidden="true" tabindex="-1"></a><span class="co"># Example of defining a custom transform using Lambda</span></span>
<span id="cb40-35"><a href="#cb40-35" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> crop_image(image: PIL.Image.Image) <span class="op">-&gt;</span> PIL.Image.Image:</span>
<span id="cb40-36"><a href="#cb40-36" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Original image is assumed to be 28x28.</span></span>
<span id="cb40-37"><a href="#cb40-37" aria-hidden="true" tabindex="-1"></a>    left, top, width, height <span class="op">=</span> <span class="dv">5</span>, <span class="dv">5</span>, <span class="dv">18</span>, <span class="dv">18</span> <span class="co"># Example crop parameters</span></span>
<span id="cb40-38"><a href="#cb40-38" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> transforms.functional.crop(image, top<span class="op">=</span>top, left<span class="op">=</span>left, width<span class="op">=</span>width, height<span class="op">=</span>height)</span>
<span id="cb40-39"><a href="#cb40-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-40"><a href="#cb40-40" aria-hidden="true" tabindex="-1"></a><span class="co"># Compose transforms, including the custom one and normalization.</span></span>
<span id="cb40-41"><a href="#cb40-41" aria-hidden="true" tabindex="-1"></a>transform_with_crop <span class="op">=</span> transforms.Compose([</span>
<span id="cb40-42"><a href="#cb40-42" aria-hidden="true" tabindex="-1"></a>    transforms.Lambda(crop_image), <span class="co"># Custom cropping</span></span>
<span id="cb40-43"><a href="#cb40-43" aria-hidden="true" tabindex="-1"></a>    transforms.ColorJitter(),</span>
<span id="cb40-44"><a href="#cb40-44" aria-hidden="true" tabindex="-1"></a>    transforms.RandomInvert(),</span>
<span id="cb40-45"><a href="#cb40-45" aria-hidden="true" tabindex="-1"></a>    transforms.ToTensor(), <span class="co"># Must be *before* Normalize</span></span>
<span id="cb40-46"><a href="#cb40-46" aria-hidden="true" tabindex="-1"></a>    transforms.Normalize(train_mean, train_std) <span class="co"># Use calculated mean and std</span></span>
<span id="cb40-47"><a href="#cb40-47" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb40-48"><a href="#cb40-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-49"><a href="#cb40-49" aria-hidden="true" tabindex="-1"></a>train_dataset_transformed <span class="op">=</span> datasets.FashionMNIST(root<span class="op">=</span><span class="st">"data"</span>, train<span class="op">=</span><span class="va">True</span>, download<span class="op">=</span><span class="va">True</span>, transform<span class="op">=</span>transform_with_crop)</span>
<span id="cb40-50"><a href="#cb40-50" aria-hidden="true" tabindex="-1"></a><span class="co"># Get one sample to check the transformation.</span></span>
<span id="cb40-51"><a href="#cb40-51" aria-hidden="true" tabindex="-1"></a>sample_img, sample_label <span class="op">=</span> train_dataset_transformed[<span class="dv">0</span>]</span>
<span id="cb40-52"><a href="#cb40-52" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Transformed image shape: </span><span class="sc">{</span>sample_img<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb40-53"><a href="#cb40-53" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Transformed image min/max: </span><span class="sc">{</span>sample_img<span class="sc">.</span><span class="bu">min</span>()<span class="sc">}</span><span class="ss">, </span><span class="sc">{</span>sample_img<span class="sc">.</span><span class="bu">max</span>()<span class="sc">}</span><span class="ss">"</span>) <span class="co"># Check normalization</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Train data mean: tensor([0.2860]), std: tensor([0.3530])
Transformed image shape: torch.Size([1, 18, 18])
Transformed image min/max: -0.8102576732635498, 2.022408962249756</code></pre>
</div>
</div>
<p>在上述代码中，首先生成仅应用了 <code>ToTensor()</code> 转换的数据集以计算平均值和标准差。然后，使用计算出的值定义包含 <code>Normalize</code> 转换的最终转换。还包括一个示例，该示例使用 <code>Lambda</code> 函数将自定义的 <code>crop_image</code> 函数添加到转换管道中。<code>ToTensor()</code> 必须在 <code>Normalize</code> <em>之前</em>。<code>ToTensor()</code> 将范围为 [0, 255] 的图像转换为范围为 [0, 1] 的张量，而 <code>Normalize</code> 则将此 [0, 1] 范围的数据标准化为平均值 0 和标准差 1。数据增强通常只应用于训练数据，而不应用于验证/测试数据。</p>
</section>
<section id="模型" class="level3">
<h3 class="anchored" data-anchor-id="模型">3.1.6 模型</h3>
<p>自20世纪80年代以来，神经网络模型的实现方式已经多样化地发展。PyTorch 自2016年推出时就采用了面向对象的模型实现方法，这是通过 <code>nn.Module</code> 实现的。这种方法极大地提高了模型的可重用性和扩展性。</p>
<p>模型类是通过继承 <code>nn.Module</code> 来实现的，并且通常包含以下方法：</p>
<ul>
<li><code>__init__()</code>: 定义并初始化神经网络的组件（层、激活函数等）。</li>
<li><code>forward()</code>: 接收输入数据，执行模型的前向传播运算，并返回输出（对数或预测值）。</li>
<li>(可选) <code>training_step()</code>、<code>validation_step()</code>、<code>test_step()</code>：当与 PyTorch Lightning 等库一起使用时，定义每个训练/验证/测试步骤的行为。</li>
<li>(可选) 其他用户自定义方法：可以添加执行模型特定功能的方法。</li>
</ul>
<div id="cell-43" class="cell" data-execution_count="13">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch <span class="im">import</span> nn</span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SimpleNetwork(nn.Module):</span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb42-5"><a href="#cb42-5" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()  <span class="co"># Or super(SimpleNetwork, self).__init__()</span></span>
<span id="cb42-6"><a href="#cb42-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.flatten <span class="op">=</span> nn.Flatten()</span>
<span id="cb42-7"><a href="#cb42-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.network_stack <span class="op">=</span> nn.Sequential(</span>
<span id="cb42-8"><a href="#cb42-8" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">28</span> <span class="op">*</span> <span class="dv">28</span>, <span class="dv">512</span>),</span>
<span id="cb42-9"><a href="#cb42-9" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb42-10"><a href="#cb42-10" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">512</span>, <span class="dv">512</span>),</span>
<span id="cb42-11"><a href="#cb42-11" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb42-12"><a href="#cb42-12" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">512</span>, <span class="dv">10</span>),</span>
<span id="cb42-13"><a href="#cb42-13" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb42-14"><a href="#cb42-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-15"><a href="#cb42-15" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb42-16"><a href="#cb42-16" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.flatten(x)  <span class="co"># Flatten the image data into a 1D array</span></span>
<span id="cb42-17"><a href="#cb42-17" aria-hidden="true" tabindex="-1"></a>        logits <span class="op">=</span> <span class="va">self</span>.network_stack(x)</span>
<span id="cb42-18"><a href="#cb42-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> logits</span>
<span id="cb42-19"><a href="#cb42-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-20"><a href="#cb42-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Move model to the appropriate device (CPU or GPU)</span></span>
<span id="cb42-21"><a href="#cb42-21" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> SimpleNetwork().to(device)</span>
<span id="cb42-22"><a href="#cb42-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>SimpleNetwork(
  (flatten): Flatten(start_dim=1, end_dim=-1)
  (network_stack): Sequential(
    (0): Linear(in_features=784, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=512, bias=True)
    (3): ReLU()
    (4): Linear(in_features=512, out_features=10, bias=True)
  )
)</code></pre>
</div>
</div>
<p>logit（罗吉特）有几种含义。</p>
<ul>
<li>数学意义: 将 [0, 1] 范围的概率转换为 [−∞, ∞] 范围的实数的函数。</li>
<li>深度学习中的意义: 未归一化(unnormalized)的神经网络原始输出值。</li>
</ul>
<p>在多类分类(multi-class classification)问题中，通常在最后应用 <code>softmax</code> 函数将输出转换为可以与标签比较的概率值。此时，logit 是 <code>softmax</code> 函数的输入值。</p>
<p>模型从类生成并传输到 <code>device</code>。如果存在 GPU，则模型会上传到 GPU 内存中。</p>
<div id="cell-45" class="cell" data-execution_count="14">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.rand(<span class="dv">1</span>, <span class="dv">28</span>, <span class="dv">28</span>, device<span class="op">=</span>device)</span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a>logits <span class="op">=</span> model(x)  <span class="co"># Don't call forward() directly!  Call the *model* object.</span></span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a>prediction <span class="op">=</span> nn.Softmax(dim<span class="op">=</span><span class="dv">1</span>)(logits)  <span class="co"># Convert logits to probabilities</span></span>
<span id="cb44-4"><a href="#cb44-4" aria-hidden="true" tabindex="-1"></a>y_label <span class="op">=</span> prediction.argmax(<span class="dv">1</span>) <span class="co"># Get the predicted class</span></span>
<span id="cb44-5"><a href="#cb44-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-6"><a href="#cb44-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Logits: </span><span class="sc">{</span>logits<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb44-7"><a href="#cb44-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Prediction probabilities: </span><span class="sc">{</span>prediction<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb44-8"><a href="#cb44-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Predicted class: </span><span class="sc">{</span>y_label<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Logits: tensor([[ 0.0464, -0.0368,  0.0447, -0.0640, -0.0253,  0.0242,  0.0378, -0.1139,
          0.0005,  0.0299]], device='cuda:0', grad_fn=&lt;AddmmBackward0&gt;)
Prediction probabilities: tensor([[0.1052, 0.0968, 0.1050, 0.0942, 0.0979, 0.1029, 0.1043, 0.0896, 0.1005,
         0.1035]], device='cuda:0', grad_fn=&lt;SoftmaxBackward0&gt;)
Predicted class: tensor([0], device='cuda:0')</code></pre>
</div>
</div>
<p>需要注意的是，不应直接调用模型的 <code>forward()</code> 方法。相反，应该像调用函数一样调用模型对象（<code>model(x)</code>），这样会自动执行 <code>forward()</code>，并且与 PyTorch 的自动微分系统集成。模型对象的 <code>__call__</code> 方法会调用 <code>forward()</code> 并执行其他必要操作（如 hook 等）。</p>
</section>
<section id="训练" class="level3">
<h3 class="anchored" data-anchor-id="训练">3.1.7 训练</h3>
<blockquote class="blockquote">
<p><strong>挑战问题</strong>: 如何高效地训练大规模数据集和复杂模型？</p>
<p><strong>研究者的困扰</strong>: 深度学习模型的性能受数据量和质量，以及模型复杂度的影响很大。但是要训练大规模数据集和复杂模型需要大量的时间和计算资源。稳定训练过程、防止过拟合、找到最佳超参数也是难题。为了解决这些问题，需要有效的学习算法、优化技术，以及自动化的训练循环。</p>
</blockquote>
<p>准备好要训练的数据和模型后，进行实际的训练。为了使神经网络模型成为良好的近似器（approximator），必须反复更新参数。定义一个误差函数（loss function）来计算标签和预测值之间的差异，并选择一个优化器（optimizer）以持续更新参数从而减少误差。</p>
<p>训练过程如下：</p>
<ol type="1">
<li>数据集和数据加载器初始化</li>
<li>按批次加载数据</li>
<li>通过前向传播计算预测值</li>
<li>通过损失函数计算误差</li>
<li>通过反向传播计算梯度</li>
<li>通过优化器更新参数</li>
</ol>
<p>对整个数据集进行一次遍历称为一个周期（epoch），这种过程重复多个周期即为训练循环。</p>
<section id="超参数" class="level5">
<h5 class="anchored" data-anchor-id="超参数">超参数</h5>
<p>训练需要三个关键超参数：</p>
<ul>
<li>周期数：确定要重复多少次周期。通常在过拟合之前停止是最好的。</li>
<li>批量大小：每次通过模型的训练数据的数量。将所有数据一次性通过通常是不现实的，因为会受到GPU内存限制和矩阵运算时间指数增长的影响。通过部分数据逐步更新模型参数以接近最优值。如果批量太小，则变化量可能过大，难以接近最小值。</li>
<li>学习率：调整要更新值的比例。可以比喻为逐步寻找过程中每一步的步长。通常取较小的值。在下一章中我们将探讨学习率与优化器的关系。</li>
</ul>
<div id="cell-48" class="cell" data-execution_count="15">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 3가지 초매개변수</span></span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a>learning_rate <span class="op">=</span> <span class="fl">1e-3</span> <span class="co"># 최적화기를 위해 앞서 지정했음.</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="训练循环" class="level5">
<h5 class="anchored" data-anchor-id="训练循环">训练循环</h5>
<p>训练循环在每个epoch中分为两个阶段进行。 1. 训练阶段：参数优化 2. 验证阶段：性能评估</p>
<p>自2015年批归一化出现后，区分train()和eval()模式变得非常重要。在eval()模式下，通过禁用批归一化或dropout等训练专用操作来提高推理速度。</p>
</section>
<section id="损失函数" class="level5">
<h5 class="anchored" data-anchor-id="损失函数">损失函数</h5>
<p>损失函数是神经网络学习的核心要素。自从1943年的McCulloch-Pitts神经元模型之后，提出了各种各样的损失函数。特别是1989年从信息论中借用的交叉熵（Cross-Entropy）的引入成为深度学习发展的重要转折点。</p>
</section>
<section id="二元交叉熵-bce" class="level5">
<h5 class="anchored" data-anchor-id="二元交叉熵-bce">二元交叉熵 (BCE)</h5>
<p>在二分类任务中常用的BCE定义如下。</p>
<p><span class="math display">\[\mathcal{L} = - \sum_{i} [y_i \log{x_i} + (1-y_i)\log{(1-x_i)}] \]</span></p>
<p>其中，<span class="math inline">\(y\)</span>是实际标签，<span class="math inline">\(x\)</span>是模型的预测值，两者都位于[0, 1]范围内。</p>
<p>PyTorch提供了多种损失函数。</p>
<ul>
<li><code>nn.MSELoss</code>：用于回归问题（均方误差）</li>
<li><code>nn.NLLLoss</code>：负对数似然</li>
<li><code>nn.CrossEntropyLoss</code>：组合了<code>LogSoftmax</code>和<code>NLLLoss</code></li>
<li><code>nn.BCEWithLogitsLoss</code>：为了数值稳定性集成了Sigmoid层和BCE</li>
</ul>
<p>特别值得注意的是<code>nn.BCEWithLogitsLoss</code>。它通过集成Sigmoid层和BCE来提高数值稳定性。使用对数函数具有以下优点（在第2章中有更详细的描述）：</p>
<ol type="1">
<li>缓解急剧的数值变化</li>
<li>将乘法转换为加法以提高计算效率</li>
</ol>
<div id="cell-50" class="cell" data-execution_count="16">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize the loss function</span></span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a>loss_fn <span class="op">=</span> nn.CrossEntropyLoss()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="优化器optimizer" class="level5">
<h5 class="anchored" data-anchor-id="优化器optimizer">优化器(Optimizer)</h5>
<p>优化算法从1950年代的基本梯度下降法(Gradient Descent)开始，到2014年Adam的出现取得了重大进展。<code>torch.optim</code>提供了多种优化器，目前主流的是Adam和AdamW。</p>
<div id="cell-52" class="cell" data-execution_count="17">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Declare the optimizer.</span></span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.SGD(model.parameters(), lr<span class="op">=</span>learning_rate)</span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Learning rate scheduler (optional, but often beneficial)</span></span>
<span id="cb48-5"><a href="#cb48-5" aria-hidden="true" tabindex="-1"></a>scheduler <span class="op">=</span> torch.optim.lr_scheduler.StepLR(optimizer, step_size<span class="op">=</span><span class="dv">30</span>, gamma<span class="op">=</span><span class="fl">0.1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>在上述代码中，我们使用了 <code>torch.optim.lr_scheduler.StepLR</code> 添加了学习率调度器。每经过 <code>step_size</code> 个epoch，学习率会乘以 <code>gamma</code> 进行衰减。学习率调度对学习速度和稳定性有很大影响。</p>
</section>
<section id="训练循环-training-loop" class="level5">
<h5 class="anchored" data-anchor-id="训练循环-training-loop">训练循环 (Training Loop)</h5>
<p>我们将构建一个训练循环，对数据集进行反复操作。通常情况下，一个 epoch 由训练和验证两部分组成。</p>
<ol type="1">
<li><strong>训练循环</strong>：使用训练数据集优化参数。</li>
<li><strong>验证循环</strong>：使用测试（验证）数据集检查模型性能的变化。</li>
</ol>
<p>在训练时，可以将模型的模式设置为 <code>train</code> 和 <code>eval</code> 两种。这可以看作是一种开关。自2015年批归一化出现后，<code>train()</code> 和 <code>eval()</code> 模式的区分变得重要起来。在 <code>eval()</code> 模式下，会禁用批归一化和 dropout 等训练专用操作以提高推理速度。</p>
<div id="cell-55" class="cell" data-execution_count="18">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.tensorboard <span class="im">import</span> SummaryWriter</span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-3"><a href="#cb49-3" aria-hidden="true" tabindex="-1"></a><span class="co"># TensorBoard writer setup</span></span>
<span id="cb49-4"><a href="#cb49-4" aria-hidden="true" tabindex="-1"></a>writer <span class="op">=</span> SummaryWriter(<span class="st">'runs/fashion_mnist_experiment_1'</span>)</span>
<span id="cb49-5"><a href="#cb49-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-6"><a href="#cb49-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-7"><a href="#cb49-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train_loop(model, data_loader, loss_fn, optimizer, epoch):  <span class="co"># Added epoch for logging</span></span>
<span id="cb49-8"><a href="#cb49-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-9"><a href="#cb49-9" aria-hidden="true" tabindex="-1"></a>    model.train()  <span class="co"># Set the model to training mode</span></span>
<span id="cb49-10"><a href="#cb49-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-11"><a href="#cb49-11" aria-hidden="true" tabindex="-1"></a>    size <span class="op">=</span> <span class="bu">len</span>(data_loader.dataset)  <span class="co"># Total number of data samples</span></span>
<span id="cb49-12"><a href="#cb49-12" aria-hidden="true" tabindex="-1"></a>    num_batches <span class="op">=</span> <span class="bu">len</span>(data_loader)</span>
<span id="cb49-13"><a href="#cb49-13" aria-hidden="true" tabindex="-1"></a>    total_loss <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb49-14"><a href="#cb49-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-15"><a href="#cb49-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> batch_count, (input_data, label_data) <span class="kw">in</span> <span class="bu">enumerate</span>(data_loader):</span>
<span id="cb49-16"><a href="#cb49-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Move data to the GPU (if available).</span></span>
<span id="cb49-17"><a href="#cb49-17" aria-hidden="true" tabindex="-1"></a>        input_data <span class="op">=</span> input_data.to(device)</span>
<span id="cb49-18"><a href="#cb49-18" aria-hidden="true" tabindex="-1"></a>        label_data <span class="op">=</span> label_data.to(device)</span>
<span id="cb49-19"><a href="#cb49-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-20"><a href="#cb49-20" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compute predictions</span></span>
<span id="cb49-21"><a href="#cb49-21" aria-hidden="true" tabindex="-1"></a>        preds <span class="op">=</span> model(input_data)</span>
<span id="cb49-22"><a href="#cb49-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-23"><a href="#cb49-23" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compute loss</span></span>
<span id="cb49-24"><a href="#cb49-24" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> loss_fn(preds, label_data)</span>
<span id="cb49-25"><a href="#cb49-25" aria-hidden="true" tabindex="-1"></a>        total_loss <span class="op">+=</span> loss.item()</span>
<span id="cb49-26"><a href="#cb49-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-27"><a href="#cb49-27" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Backpropagation</span></span>
<span id="cb49-28"><a href="#cb49-28" aria-hidden="true" tabindex="-1"></a>        loss.backward()  <span class="co"># Perform backpropagation</span></span>
<span id="cb49-29"><a href="#cb49-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-30"><a href="#cb49-30" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Update parameters</span></span>
<span id="cb49-31"><a href="#cb49-31" aria-hidden="true" tabindex="-1"></a>        optimizer.step()</span>
<span id="cb49-32"><a href="#cb49-32" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()  <span class="co"># Zero the gradients before next iteration</span></span>
<span id="cb49-33"><a href="#cb49-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-34"><a href="#cb49-34" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> batch_count <span class="op">%</span> <span class="dv">100</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb49-35"><a href="#cb49-35" aria-hidden="true" tabindex="-1"></a>            loss, current <span class="op">=</span> loss.item(), batch_count <span class="op">*</span> batch_size <span class="op">+</span> <span class="bu">len</span>(input_data)</span>
<span id="cb49-36"><a href="#cb49-36" aria-hidden="true" tabindex="-1"></a>            <span class="co"># print(f"loss: {loss:&gt;7f}  [{current:&gt;5d}/{size:&gt;5d}]")</span></span>
<span id="cb49-37"><a href="#cb49-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-38"><a href="#cb49-38" aria-hidden="true" tabindex="-1"></a>    avg_train_loss <span class="op">=</span> total_loss <span class="op">/</span> num_batches</span>
<span id="cb49-39"><a href="#cb49-39" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> avg_train_loss</span>
<span id="cb49-40"><a href="#cb49-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-41"><a href="#cb49-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-42"><a href="#cb49-42" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> eval_loop(model, data_loader, loss_fn):</span>
<span id="cb49-43"><a href="#cb49-43" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">eval</span>()  <span class="co"># Set the model to evaluation mode</span></span>
<span id="cb49-44"><a href="#cb49-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-45"><a href="#cb49-45" aria-hidden="true" tabindex="-1"></a>    correct, test_loss <span class="op">=</span> <span class="fl">0.0</span>, <span class="fl">0.0</span></span>
<span id="cb49-46"><a href="#cb49-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-47"><a href="#cb49-47" aria-hidden="true" tabindex="-1"></a>    size <span class="op">=</span> <span class="bu">len</span>(data_loader.dataset)  <span class="co"># Total data size</span></span>
<span id="cb49-48"><a href="#cb49-48" aria-hidden="true" tabindex="-1"></a>    num_batches <span class="op">=</span> <span class="bu">len</span>(data_loader)  <span class="co"># Number of batches</span></span>
<span id="cb49-49"><a href="#cb49-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-50"><a href="#cb49-50" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():  <span class="co"># Disable gradient calculation within this block</span></span>
<span id="cb49-51"><a href="#cb49-51" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> input_data, label_data <span class="kw">in</span> data_loader:  <span class="co"># No need for enumerate as count is not used</span></span>
<span id="cb49-52"><a href="#cb49-52" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Move data to GPU (if available).</span></span>
<span id="cb49-53"><a href="#cb49-53" aria-hidden="true" tabindex="-1"></a>            input_data <span class="op">=</span> input_data.to(device)</span>
<span id="cb49-54"><a href="#cb49-54" aria-hidden="true" tabindex="-1"></a>            label_data <span class="op">=</span> label_data.to(device)</span>
<span id="cb49-55"><a href="#cb49-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-56"><a href="#cb49-56" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Compute predictions</span></span>
<span id="cb49-57"><a href="#cb49-57" aria-hidden="true" tabindex="-1"></a>            preds <span class="op">=</span> model(input_data)</span>
<span id="cb49-58"><a href="#cb49-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-59"><a href="#cb49-59" aria-hidden="true" tabindex="-1"></a>            test_loss <span class="op">+=</span> loss_fn(preds, label_data).item()</span>
<span id="cb49-60"><a href="#cb49-60" aria-hidden="true" tabindex="-1"></a>            correct <span class="op">+=</span> (preds.argmax(<span class="dv">1</span>) <span class="op">==</span> label_data).<span class="bu">type</span>(torch.<span class="bu">float</span>).<span class="bu">sum</span>().item()</span>
<span id="cb49-61"><a href="#cb49-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-62"><a href="#cb49-62" aria-hidden="true" tabindex="-1"></a>    test_loss <span class="op">/=</span> num_batches</span>
<span id="cb49-63"><a href="#cb49-63" aria-hidden="true" tabindex="-1"></a>    correct <span class="op">/=</span> size</span>
<span id="cb49-64"><a href="#cb49-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-65"><a href="#cb49-65" aria-hidden="true" tabindex="-1"></a>    <span class="co"># print(f"\n Test Result \n Accuracy: {(100 * correct):&gt;0.1f}%, Average loss: {test_loss:&gt;8f} \n")</span></span>
<span id="cb49-66"><a href="#cb49-66" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> test_loss, correct</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="整个训练过程" class="level5">
<h5 class="anchored" data-anchor-id="整个训练过程">整个训练过程</h5>
<p>整个训练过程在每个epoch中重复训练和验证。使用<code>tqdm</code>可视化显示进度，并使用TensorBoard记录学习率变化。</p>
<div id="cell-57" class="cell" data-execution_count="19">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Progress bar utility</span></span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tqdm.notebook <span class="im">import</span> tqdm</span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-4"><a href="#cb50-4" aria-hidden="true" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">5</span>  <span class="co"># Reduced for demonstration</span></span>
<span id="cb50-5"><a href="#cb50-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> tqdm(<span class="bu">range</span>(epochs)):</span>
<span id="cb50-6"><a href="#cb50-6" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Epoch </span><span class="sc">{</span>epoch<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ch">\n</span><span class="ss">-------------------------------"</span>)</span>
<span id="cb50-7"><a href="#cb50-7" aria-hidden="true" tabindex="-1"></a>    train_loss <span class="op">=</span> train_loop(model, train_dataloader, loss_fn, optimizer, epoch)</span>
<span id="cb50-8"><a href="#cb50-8" aria-hidden="true" tabindex="-1"></a>    test_loss, correct <span class="op">=</span> eval_loop(model, test_dataloader, loss_fn)</span>
<span id="cb50-9"><a href="#cb50-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-10"><a href="#cb50-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Log training and validation metrics to TensorBoard</span></span>
<span id="cb50-11"><a href="#cb50-11" aria-hidden="true" tabindex="-1"></a>    writer.add_scalar(<span class="st">'Loss/train'</span>, train_loss, epoch)</span>
<span id="cb50-12"><a href="#cb50-12" aria-hidden="true" tabindex="-1"></a>    writer.add_scalar(<span class="st">'Loss/test'</span>, test_loss, epoch)</span>
<span id="cb50-13"><a href="#cb50-13" aria-hidden="true" tabindex="-1"></a>    writer.add_scalar(<span class="st">'Accuracy/test'</span>, correct, epoch)</span>
<span id="cb50-14"><a href="#cb50-14" aria-hidden="true" tabindex="-1"></a>    writer.add_scalar(<span class="st">'Learning Rate'</span>, optimizer.param_groups[<span class="dv">0</span>][<span class="st">'lr'</span>], epoch) <span class="co"># Log learning rate</span></span>
<span id="cb50-15"><a href="#cb50-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-16"><a href="#cb50-16" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'Epoch: </span><span class="sc">{</span>epoch<span class="sc">}</span><span class="ss">, Train Loss: </span><span class="sc">{</span>train_loss<span class="sc">:.4f}</span><span class="ss">, Test Loss: </span><span class="sc">{</span>test_loss<span class="sc">:.4f}</span><span class="ss">, Test Accuracy: </span><span class="sc">{</span>correct<span class="sc">:.2f}</span><span class="ss">%, LR: </span><span class="sc">{</span>optimizer<span class="sc">.</span>param_groups[<span class="dv">0</span>][<span class="st">"lr"</span>]<span class="sc">:.6f}</span><span class="ss">'</span>)</span>
<span id="cb50-17"><a href="#cb50-17" aria-hidden="true" tabindex="-1"></a>    scheduler.step()  <span class="co"># Update learning rate.  Place *after* logging.</span></span>
<span id="cb50-18"><a href="#cb50-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-19"><a href="#cb50-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Done!"</span>)</span>
<span id="cb50-20"><a href="#cb50-20" aria-hidden="true" tabindex="-1"></a>writer.close() <span class="co"># Close TensorBoard Writer</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"84baac2d3bc14a3b960d258d62b7996a","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1
-------------------------------
Epoch: 0, Train Loss: 1.5232, Test Loss: 0.9543, Test Accuracy: 0.71%, LR: 0.001000
Epoch 2
-------------------------------
Epoch: 1, Train Loss: 0.7920, Test Loss: 0.7059, Test Accuracy: 0.76%, LR: 0.001000
Epoch 3
-------------------------------
Epoch: 2, Train Loss: 0.6442, Test Loss: 0.6208, Test Accuracy: 0.78%, LR: 0.001000
Epoch 4
-------------------------------
Epoch: 3, Train Loss: 0.5790, Test Loss: 0.5757, Test Accuracy: 0.79%, LR: 0.001000
Epoch 5
-------------------------------
Epoch: 4, Train Loss: 0.5383, Test Loss: 0.5440, Test Accuracy: 0.80%, LR: 0.001000
Done!</code></pre>
</div>
</div>
<p>这种训练-验证循环自1990年代以来已成为标准的深度学习训练方法。特别是在验证阶段，它在监控过拟合和决定提前停止（early stopping）方面发挥着重要作用。</p>
</section>
</section>
<section id="模型保存读取" class="level3">
<h3 class="anchored" data-anchor-id="模型保存读取">3.1.8 模型保存、读取</h3>
<p>模型保存是深度学习实践中非常重要的一部分。可以将训练好的模型保存，然后在需要时重新加载以供再利用，或者部署到其他环境（例如：服务器、移动设备）中。PyTorch 提供了两种主要的保存方式。</p>
<section id="仅保存权重" class="level5">
<h5 class="anchored" data-anchor-id="仅保存权重">仅保存权重</h5>
<p>模型的训练参数（权重和偏置）存储在一个称为 <code>state_dict</code> 的 Python 字典中。<code>state_dict</code> 是一个将每一层（layer）映射到该层参数张量的结构。这种方式的优点是，即使模型结构发生变化，也可以加载权重，因此通常推荐使用。</p>
<div id="cell-60" class="cell" data-execution_count="37">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Save model weights</span></span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a>torch.save(model.state_dict(), <span class="st">'model_weights.pth'</span>)</span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-4"><a href="#cb52-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Load weights</span></span>
<span id="cb52-5"><a href="#cb52-5" aria-hidden="true" tabindex="-1"></a>model_saved_weights <span class="op">=</span> SimpleNetwork()  <span class="co"># Create an empty model with the same architecture</span></span>
<span id="cb52-6"><a href="#cb52-6" aria-hidden="true" tabindex="-1"></a>model_saved_weights.load_state_dict(torch.load(<span class="st">'model_weights.pth'</span>))</span>
<span id="cb52-7"><a href="#cb52-7" aria-hidden="true" tabindex="-1"></a>model_saved_weights.to(device) <span class="co"># Don't forget to move to the correct device!</span></span>
<span id="cb52-8"><a href="#cb52-8" aria-hidden="true" tabindex="-1"></a>model_saved_weights.<span class="bu">eval</span>() <span class="co"># Set to evaluation mode</span></span>
<span id="cb52-9"><a href="#cb52-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-10"><a href="#cb52-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Check performance (assuming eval_loop is defined)</span></span>
<span id="cb52-11"><a href="#cb52-11" aria-hidden="true" tabindex="-1"></a>eval_loop(model_saved_weights, test_dataloader, loss_fn)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_112013/3522135054.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model_saved_weights.load_state_dict(torch.load('model_weights.pth'))</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="37">
<pre><code>(0.5459668265935331, 0.8036)</code></pre>
</div>
</div>
</section>
<section id="整个模型保存" class="level5">
<h5 class="anchored" data-anchor-id="整个模型保存">整个模型保存</h5>
<p>自2018年以来，随着模型架构变得更加复杂，也开始采用同时保存模型结构和权重的方法。</p>
<div id="cell-62" class="cell" data-execution_count="38">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a>torch.save(model, <span class="st">'model_trained.pth'</span>)</span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-3"><a href="#cb55-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the entire model</span></span>
<span id="cb55-4"><a href="#cb55-4" aria-hidden="true" tabindex="-1"></a>model_saved <span class="op">=</span> torch.load(<span class="st">'model_trained.pth'</span>)</span>
<span id="cb55-5"><a href="#cb55-5" aria-hidden="true" tabindex="-1"></a>model_saved.to(device)  <span class="co"># Move the loaded model to the correct device.</span></span>
<span id="cb55-6"><a href="#cb55-6" aria-hidden="true" tabindex="-1"></a>model_saved.<span class="bu">eval</span>() <span class="co">#  Set the loaded model to evaluation mode</span></span>
<span id="cb55-7"><a href="#cb55-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-8"><a href="#cb55-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Check performance</span></span>
<span id="cb55-9"><a href="#cb55-9" aria-hidden="true" tabindex="-1"></a>eval_loop(model_saved, test_dataloader, loss_fn)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_112013/3185686172.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model_saved = torch.load('model_trained.pth')</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="38">
<pre><code>(0.5459668265935331, 0.8036)</code></pre>
</div>
</div>
<p>整个模型保存方式虽然方便，但如果模型类定义发生变化时可能会出现兼容性问题。特别是在生产环境中，模型架构不经常更改，因此仅保存权重的方式可能更加稳定。此外，整个模型的保存方式使用了Python的<code>pickle</code>模块，而<code>pickle</code>存在可以执行任意代码的安全漏洞，因此可能存在安全风险。</p>
</section>
<section id="safetensors-更安全的替代方案" class="level5">
<h5 class="anchored" data-anchor-id="safetensors-更安全的替代方案">Safetensors: 更安全的替代方案</h5>
<p>近年来，出现了像<code>safetensors</code>这样的新存储格式，以提高安全性和加载速度。<code>safetensors</code>是一种用于安全高效地存储张量数据的格式。</p>
<ul>
<li><strong>安全性:</strong> <code>safetensors</code>不允许执行任意代码，因此比<code>pickle</code>更安全。</li>
<li><strong>零复制:</strong> 通过直接将数据映射到内存而无需复制，从而加快加载速度。</li>
<li><strong>惰性加载:</strong> 只加载所需部分，减少内存使用量。</li>
<li><strong>支持多种框架</strong>: PyTorch, TensorFlow, JAX 等</li>
</ul>
<div id="cell-64" class="cell" data-execution_count="39">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb58"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Install safetensors: pip install safetensors</span></span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-3"><a href="#cb58-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> safetensors.torch <span class="im">import</span> save_file, load_file</span>
<span id="cb58-4"><a href="#cb58-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-5"><a href="#cb58-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Save using safetensors</span></span>
<span id="cb58-6"><a href="#cb58-6" aria-hidden="true" tabindex="-1"></a>state_dict <span class="op">=</span> model.state_dict()</span>
<span id="cb58-7"><a href="#cb58-7" aria-hidden="true" tabindex="-1"></a>save_file(state_dict, <span class="st">"model_weights.safetensors"</span>)</span>
<span id="cb58-8"><a href="#cb58-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-9"><a href="#cb58-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Load using safetensors</span></span>
<span id="cb58-10"><a href="#cb58-10" aria-hidden="true" tabindex="-1"></a>loaded_state_dict <span class="op">=</span> load_file(<span class="st">"model_weights.safetensors"</span>, device<span class="op">=</span>device) <span class="co"># Load directly to the device.</span></span>
<span id="cb58-11"><a href="#cb58-11" aria-hidden="true" tabindex="-1"></a>model_new <span class="op">=</span> SimpleNetwork().to(device) <span class="co"># Create an instance of your model class</span></span>
<span id="cb58-12"><a href="#cb58-12" aria-hidden="true" tabindex="-1"></a>model_new.load_state_dict(loaded_state_dict)</span>
<span id="cb58-13"><a href="#cb58-13" aria-hidden="true" tabindex="-1"></a>model_new.<span class="bu">eval</span>()</span>
<span id="cb58-14"><a href="#cb58-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-15"><a href="#cb58-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Check performance</span></span>
<span id="cb58-16"><a href="#cb58-16" aria-hidden="true" tabindex="-1"></a>eval_loop(model_new, test_dataloader, loss_fn)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="39">
<pre><code>(0.5459668265935331, 0.8036)</code></pre>
</div>
</div>
</section>
</section>
</section>
<section id="张量板" class="level2">
<h2 class="anchored" data-anchor-id="张量板">3.2 张量板</h2>
<p>张量板是记录、跟踪和高效可视化在深度学习训练中生成的各种日志的工具。它是一种常见的被称为仪表盘的日志数据记录/可视化工具。最初为TensorFlow开发，但现在已与PyTorch整合。类似于张量板的仪表盘形式的可视化工具有以下几种。</p>
<ul>
<li>Weights &amp; Biases (WandB): 基于云的MLOps集成平台，提供广泛的实验跟踪、数据集版本管理、模型管理等功能。其团队协作功能特别出色，在企业环境中广泛使用。</li>
<li>Vertex AI: Google Cloud提供的完全托管ML工具，提供与BigQuery、Dataproc、Spark的原生集成。可以快速进行模型构建、部署和扩展，适合大规模ML工作流。</li>
<li>MLflow: 提供实验跟踪、模型打包、中央注册表等的开源工具。简化了ML模型的跟踪和部署，在数据科学及ML领域广泛使用。</li>
</ul>
<p>除了上述三种工具外，还有许多其他工具。这里主要将使用张量板。</p>
<section id="张量板基本用法" class="level3">
<h3 class="anchored" data-anchor-id="张量板基本用法">3.2.1 张量板基本用法</h3>
<p>张量板于2015年与TensorFlow一同出现。当时深度学习模型的复杂性急剧增加，有效监控训练过程的需求变得突出。</p>
<p>张量板的核心功能包括： 1. 标量指标跟踪：记录损失值、准确率等数值 2. 模型结构可视化：计算图的图形化表示 3. 分布跟踪：观察权重、梯度的分布变化 4. 嵌入投影：高维向量的2D/3D可视化 5. 超参数优化：比较不同设置的实验结果</p>
<p>张量板是用于可视化和分析深度学习训练过程的强大工具。使用张量板的基本步骤主要分为安装、设置日志目录、设置回调三个阶段。</p>
<section id="安装方法" class="level5">
<h5 class="anchored" data-anchor-id="安装方法">安装方法</h5>
<p>可以通过pip或conda安装张量板。</p>
<div id="cell-66" class="cell" data-vscode="{&quot;languageId&quot;:&quot;shellscript&quot;}">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb60"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip install tensorboard</span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true" tabindex="-1"></a><span class="co"># 또는</span></span>
<span id="cb60-3"><a href="#cb60-3" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>conda install <span class="op">-</span>c conda<span class="op">-</span>forge tensorboard</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="日志目录设置" class="level5">
<h5 class="anchored" data-anchor-id="日志目录设置">日志目录设置</h5>
<p>TensorBoard 读取存储在日志目录中的事件文件以进行可视化。在 Jupyter 笔记本或 Colab 中，可以如下设置。</p>
<div id="cell-68" class="cell" data-execution_count="41">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb61"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.tensorboard <span class="im">import</span> SummaryWriter</span>
<span id="cb61-2"><a href="#cb61-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-3"><a href="#cb61-3" aria-hidden="true" tabindex="-1"></a><span class="co"># 로그 디렉토리 설정</span></span>
<span id="cb61-4"><a href="#cb61-4" aria-hidden="true" tabindex="-1"></a>log_dir <span class="op">=</span> <span class="st">'logs/experiment_1'</span></span>
<span id="cb61-5"><a href="#cb61-5" aria-hidden="true" tabindex="-1"></a>writer <span class="op">=</span> SummaryWriter(log_dir)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="tensorboard运行" class="level5">
<h5 class="anchored" data-anchor-id="tensorboard运行">TensorBoard运行</h5>
<p>TensorBoard可以通过以下两种方式启动。</p>
<ol type="1">
<li>在命令行中运行</li>
</ol>
<div id="cell-70" class="cell" data-vscode="{&quot;languageId&quot;:&quot;shellscript&quot;}">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb62"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a>tensorboard <span class="op">--</span>logdir<span class="op">=</span>logs</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<ol start="2" type="1">
<li>在Jupyter笔记本中运行</li>
</ol>
<div id="cell-72" class="cell" data-vscode="{&quot;languageId&quot;:&quot;shellscript&quot;}">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb63"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>load_ext tensorboard</span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>tensorboard <span class="op">--</span>logdir<span class="op">=</span>logs</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>执行后，在网页浏览器中访问 http://localhost:6006 即可查看 TensorBoard 仪表板。</p>
</section>
<section id="在远程服务器上运行" class="level5">
<h5 class="anchored" data-anchor-id="在远程服务器上运行">在远程服务器上运行</h5>
<p>在远程服务器上运行 TensorBoard 时，使用 SSH 隧道。</p>
<div id="cell-74" class="cell" data-vscode="{&quot;languageId&quot;:&quot;shellscript&quot;}">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb64"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a>ssh <span class="op">-</span>L <span class="dv">6006</span>:<span class="fl">127.0.0.1</span>:<span class="dv">6006</span> username<span class="op">@</span>server_ip</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><code>SummaryWriter</code> 是生成记录在 TensorBoard 中的数据的核心类。主要参数如下：</p>
<ul>
<li><code>log_dir</code>: 日志文件存储的目录路径。</li>
<li><code>comment</code>: 附加到 <code>log_dir</code> 的字符串。</li>
<li><code>flush_secs</code>: 将日志写入磁盘的时间间隔（秒）。</li>
<li><code>max_queue</code>: 设置保留多少待处理的事件/步骤。</li>
</ul>
<p><strong>主要方法 (SummaryWriter)</strong></p>
<ul>
<li><code>add_scalar(tag, scalar_value, global_step=None)</code>: 记录标量值（如：损失、准确率）。</li>
<li><code>add_histogram(tag, values, global_step=None, bins='tensorflow')</code>: 记录直方图（值的分布）。</li>
<li><code>add_image(tag, img_tensor, global_step=None, dataformats='CHW')</code>: 记录图像。</li>
<li><code>add_figure(tag, figure, global_step=None, close=True)</code>: 记录 Matplotlib 图形。</li>
<li><code>add_video(tag, vid_tensor, global_step=None, fps=4, dataformats='NCHW')</code>: 记录视频。</li>
<li><code>add_audio(tag, snd_tensor, global_step=None, sample_rate=44100)</code>: 记录音频。</li>
<li><code>add_text(tag, text_string, global_step=None)</code>: 记录文本。</li>
<li><code>add_graph(model, input_to_model=None, verbose=False)</code>: 记录模型图。</li>
<li><code>add_embedding(mat, metadata=None, label_img=None, global_step=None, tag='default', metadata_header=None)</code>: 记录嵌入投影仪。</li>
<li><code>add_hparams(hparam_dict, metric_dict, hparam_domain_discrete=None, run_name=None)</code>: 记录超参数及其对应指标。</li>
<li><code>flush()</code>: 将所有待处理的事件记录到磁盘。</li>
<li><code>close()</code>: 结束日志并释放资源。</li>
</ul>
<p><strong>主要回调参数 (TensorFlow/Keras)</strong></p>
<p>在 TensorFlow/Keras 中使用 TensorBoard 时，使用 <code>tf.keras.callbacks.TensorBoard</code> 回调。主要参数如下：</p>
<ul>
<li><code>log_dir</code>: 日志存储位置。</li>
<li><code>histogram_freq</code>: 计算直方图的频率（0 表示不计算）。用于可视化权重、偏置和激活值的分布。</li>
<li><code>write_graph</code>: 是否可视化模型图。</li>
<li><code>write_images</code>: 是否以图像形式可视化模型权重。</li>
<li><code>update_freq</code>: 记录损失和指标的频率（‘batch’, ‘epoch’ 或整数）。</li>
<li><code>profile_batch</code>: 指定要配置文件化的批次范围（例如：<code>profile_batch='5, 8'</code>）。配置文件化有助于查找性能瓶颈。</li>
<li><code>embeddings_freq</code>: 可视化嵌入层的频率。</li>
<li><code>embeddings_metadata</code>: 嵌入元数据文件路径。</li>
</ul>
</section>
</section>
<section id="张量板的主要可视化功能" class="level3">
<h3 class="anchored" data-anchor-id="张量板的主要可视化功能">3.2.2 张量板的主要可视化功能</h3>
<p>张量板可以可视化模型训练过程中产生的各种指标。主要的可视化仪表盘包括标量、直方图、分布、图形、嵌入等。</p>
<section id="标量指标可视化" class="level5">
<h5 class="anchored" data-anchor-id="标量指标可视化">标量指标可视化</h5>
<p>标量仪表盘用于可视化损失值、准确度等数值型指标的变化。可以跟踪学习率、梯度范数、各层权重的均值/方差等模型训练过程中的各种统计值。还可以监控最新的生成模型中重要的FID(Fréchet Inception Distance)分数或QICE(Quantile Interval Coverage Error)等质量评估指标。通过这些指标，可以实时监控模型的训练进度，并及早发现过拟合或训练不稳定性等问题。标量值可以如下记录。</p>
<div id="cell-77" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb65"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a>writer.add_scalar(<span class="st">'Loss/train'</span>, train_loss, step)</span>
<span id="cb65-2"><a href="#cb65-2" aria-hidden="true" tabindex="-1"></a>writer.add_scalar(<span class="st">'Accuracy/train'</span>, train_acc, step)</span>
<span id="cb65-3"><a href="#cb65-3" aria-hidden="true" tabindex="-1"></a>writer.add_scalar(<span class="st">'Learning/learning_rate'</span>, current_lr, step)</span>
<span id="cb65-4"><a href="#cb65-4" aria-hidden="true" tabindex="-1"></a>writer.add_scalar(<span class="st">'Gradients/norm'</span>, grad_norm, step)</span>
<span id="cb65-5"><a href="#cb65-5" aria-hidden="true" tabindex="-1"></a>writer.add_scalar(<span class="st">'Quality/fid_score'</span>, fid_score, step)</span>
<span id="cb65-6"><a href="#cb65-6" aria-hidden="true" tabindex="-1"></a>writer.add_scalar(<span class="st">'Metrics/qice'</span>, qice_value, step)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="直方图和分布可视化" class="level5">
<h5 class="anchored" data-anchor-id="直方图和分布可视化">直方图和分布可视化</h5>
<p>可以观察权重和偏置的分布变化。直方图通过直观展示每层的权重、偏置、梯度、激活值的分布，有助于理解模型的内部状态。特别是在训练过程中，可以及早发现权重饱和于特定值或梯度消失/爆炸的问题，对模型调试非常有用。可以如下记录直方图。</p>
<div id="cell-79" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb66"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> name, param <span class="kw">in</span> model.named_parameters():</span>
<span id="cb66-2"><a href="#cb66-2" aria-hidden="true" tabindex="-1"></a>    writer.add_histogram(<span class="ss">f'Parameters/</span><span class="sc">{</span>name<span class="sc">}</span><span class="ss">'</span>, param.data, global_step)</span>
<span id="cb66-3"><a href="#cb66-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> param.grad <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb66-4"><a href="#cb66-4" aria-hidden="true" tabindex="-1"></a>        writer.add_histogram(<span class="ss">f'Gradients/</span><span class="sc">{</span>name<span class="sc">}</span><span class="ss">'</span>, param.grad, global_step)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="模型结构可视化" class="level5">
<h5 class="anchored" data-anchor-id="模型结构可视化">模型结构可视化</h5>
<p>可以直观地查看模型的结构。特别是可以直观地理解复杂神经网络的层次结构和连接。TensorBoard 通过计算图以图形形式表示数据流、各层的输入输出形状、运算顺序等，并且可以通过扩展每个节点来审查详细信息。近年来，对于 Transformer 和 Diffusion 模型中复杂的注意力机制、交叉注意层、条件分支结构等的可视化特别有用。这对于模型调试和优化非常有帮助，尤其是对于包含跳跃连接或并行结构的复杂架构的理解。可以如下记录模型图。</p>
<div id="cell-81" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb67"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a>writer.add_graph(model, input_to_model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="嵌入可视化" class="level5">
<h5 class="anchored" data-anchor-id="嵌入可视化">嵌入可视化</h5>
<p>使用 TensorBoard 的 Projector 可以将高维嵌入投影到 2D 或 3D 空间进行可视化。这有助于分析词嵌入或图像特征向量之间的关系。通过 PCA 或 UMAP 等降维技术，可以在保留复杂高维数据的聚类结构和相对距离的同时进行可视化。特别是 UMAP 可以在保持局部结构和全局结构的同时实现快速可视化。通过这种方式，可以查看具有相似特征的数据点如何聚类，以及类之间的区分是否明确，并且可以跟踪学习过程中特征空间的变化。如下所示记录嵌入。</p>
<div id="cell-83" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb68"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a>writer.add_embedding(</span>
<span id="cb68-2"><a href="#cb68-2" aria-hidden="true" tabindex="-1"></a>    features,</span>
<span id="cb68-3"><a href="#cb68-3" aria-hidden="true" tabindex="-1"></a>    metadata<span class="op">=</span>labels,</span>
<span id="cb68-4"><a href="#cb68-4" aria-hidden="true" tabindex="-1"></a>    label_img<span class="op">=</span>images,</span>
<span id="cb68-5"><a href="#cb68-5" aria-hidden="true" tabindex="-1"></a>    global_step<span class="op">=</span>step</span>
<span id="cb68-6"><a href="#cb68-6" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="超参数可视化" class="level5">
<h5 class="anchored" data-anchor-id="超参数可视化">超参数可视化</h5>
<p>可以可视化超参数调优的结果。除了学习率、批大小和 dropout 比例外，还可以分析 Transformer 模型的注意力头数、提示长度、token 嵌入维度等结构参数的影响。最新的 LLM 或扩散模型中重要的噪声调度、采样步数、CFG（无分类器引导）权重等推理参数也可以一起可视化。通过并行坐标图或散点图表示不同超参数组合下的模型性能，有助于找到最佳配置。特别是可以一目了然地比较多个实验结果，便于分析超参数之间的相互作用对模型性能的影响。如下可以记录超参数及相关指标。</p>
<div id="cell-85" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb69"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a>writer.add_hparams(</span>
<span id="cb69-2"><a href="#cb69-2" aria-hidden="true" tabindex="-1"></a>    {</span>
<span id="cb69-3"><a href="#cb69-3" aria-hidden="true" tabindex="-1"></a>        <span class="st">'lr'</span>: learning_rate, </span>
<span id="cb69-4"><a href="#cb69-4" aria-hidden="true" tabindex="-1"></a>        <span class="st">'batch_size'</span>: batch_size, </span>
<span id="cb69-5"><a href="#cb69-5" aria-hidden="true" tabindex="-1"></a>        <span class="st">'num_heads'</span>: n_heads,</span>
<span id="cb69-6"><a href="#cb69-6" aria-hidden="true" tabindex="-1"></a>        <span class="st">'cfg_scale'</span>: guidance_scale,</span>
<span id="cb69-7"><a href="#cb69-7" aria-hidden="true" tabindex="-1"></a>        <span class="st">'sampling_steps'</span>: num_steps,</span>
<span id="cb69-8"><a href="#cb69-8" aria-hidden="true" tabindex="-1"></a>        <span class="st">'prompt_length'</span>: max_length</span>
<span id="cb69-9"><a href="#cb69-9" aria-hidden="true" tabindex="-1"></a>    },</span>
<span id="cb69-10"><a href="#cb69-10" aria-hidden="true" tabindex="-1"></a>    {</span>
<span id="cb69-11"><a href="#cb69-11" aria-hidden="true" tabindex="-1"></a>        <span class="st">'accuracy'</span>: accuracy, </span>
<span id="cb69-12"><a href="#cb69-12" aria-hidden="true" tabindex="-1"></a>        <span class="st">'loss'</span>: final_loss,</span>
<span id="cb69-13"><a href="#cb69-13" aria-hidden="true" tabindex="-1"></a>        <span class="st">'fid_score'</span>: fid_score</span>
<span id="cb69-14"><a href="#cb69-14" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb69-15"><a href="#cb69-15" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="图像可视化" class="level5">
<h5 class="anchored" data-anchor-id="图像可视化">图像可视化</h5>
<p>在学习过程中可以可视化生成的图像或中间特征图。通过可视化卷积层的滤波器和激活图，可以直观地理解模型正在学习哪些特征，并且可以查看每一层关注输入图像的哪一部分。特别是在像Stable Diffusion和DALL-E这样的最新生成模型中，可以通过视觉追踪生成图像的质量变化，这非常有用。随着混合模型的出现，更加精细和逼真的图像生成成为可能。可以如下记录图像。</p>
<div id="cell-87" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb70"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 입력 이미지나 생성된 이미지 시각화</span></span>
<span id="cb70-2"><a href="#cb70-2" aria-hidden="true" tabindex="-1"></a>writer.add_images(<span class="st">'Images/generated'</span>, generated_images, global_step)</span>
<span id="cb70-3"><a href="#cb70-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-4"><a href="#cb70-4" aria-hidden="true" tabindex="-1"></a><span class="co"># 디퓨전 모델의 중간 생성 과정 시각화</span></span>
<span id="cb70-5"><a href="#cb70-5" aria-hidden="true" tabindex="-1"></a>writer.add_images(<span class="st">'Diffusion/steps'</span>, diffusion_steps, global_step)</span>
<span id="cb70-6"><a href="#cb70-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-7"><a href="#cb70-7" aria-hidden="true" tabindex="-1"></a><span class="co"># 어텐션 맵 시각화</span></span>
<span id="cb70-8"><a href="#cb70-8" aria-hidden="true" tabindex="-1"></a>writer.add_image(<span class="st">'Attention/maps'</span>, attention_visualization, global_step)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>通过TensorBoard的可视化功能，可以直观地理解模型的学习过程并快速发现问题。特别是可以实时监控学习进度，这对于早期停止训练或调整超参数非常有用。嵌入式可视化对于理解高维数据的关系尤其有用，并且有助于分析模型所学特征空间的结构。</p>
</section>
</section>
<section id="张量板示例" class="level3">
<h3 class="anchored" data-anchor-id="张量板示例">3.2.3 张量板示例</h3>
<p>在本节中，我们将详细探讨如何将前面讨论的张量板的各种功能应用于实际的深度学习模型训练。使用MNIST手写数字数据集训练一个简单的CNN（卷积神经网络）模型，并逐步说明如何通过张量板可视化训练过程中产生的主要指标和数据。</p>
<p><strong>核心可视化元素:</strong></p>
<table class="caption-top table">
<colgroup>
<col style="width: 15%">
<col style="width: 76%">
<col style="width: 8%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">可视化类型</th>
<th style="text-align: left;">可视化内容</th>
<th style="text-align: left;">张量板标签</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>标量指标</strong></td>
<td style="text-align: left;">训练/测试损失(loss)，训练/测试准确度(accuracy)，学习率(learning rate)，梯度范数(norm)</td>
<td style="text-align: left;">SCALARS</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>直方图/分布</strong></td>
<td style="text-align: left;">所有层的权重(weight)分布，所有层的梯度(gradient)分布</td>
<td style="text-align: left;">DISTRIBUTIONS, HISTOGRAMS</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>模型结构</strong></td>
<td style="text-align: left;">MNIST CNN 模型的计算图(computational graph)</td>
<td style="text-align: left;">GRAPHS</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>特征图</strong></td>
<td style="text-align: left;">Conv1 层特征图，Conv2 层特征图，输入图像网格，Conv1 过滤器可视化</td>
<td style="text-align: left;">IMAGES</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>嵌入</strong></td>
<td style="text-align: left;">FC1 层的32维特征向量，使用t-SNE进行的2D可视化，MNIST图像标签</td>
<td style="text-align: left;">PROJECTOR</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>超参数</strong></td>
<td style="text-align: left;">批次大小，学习率，dropout比率，优化器类型，权重衰减，动量，调度器步长/gamma</td>
<td style="text-align: left;">HPARAMS</td>
</tr>
</tbody>
</table>
<p><strong>可视化周期:</strong></p>
<ul>
<li>标量/直方图: 每50个批次(batch)</li>
<li>特征图/图像: 每50个批次</li>
<li>嵌入: 每个纪元(epoch)结束时</li>
<li>超参数: 训练开始和结束时</li>
</ul>
<p>代码示例</p>
<p>此示例使用 <code>dld</code> 包。导入所需的模块并开始训练。<code>train()</code> 函数使用默认超参数对 MNIST 数据集进行 CNN 模型的训练，并将训练过程记录到 TensorBoard 中。如果要尝试其他超参数，可以向 <code>train()</code> 函数传递 <code>hparams_dict</code> 参数。</p>
<div id="cell-91" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb71"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a><span class="co"># In a notebook cell:</span></span>
<span id="cb71-2"><a href="#cb71-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_03.train <span class="im">import</span> train</span>
<span id="cb71-3"><a href="#cb71-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-4"><a href="#cb71-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Run with default hyperparameters</span></span>
<span id="cb71-5"><a href="#cb71-5" aria-hidden="true" tabindex="-1"></a>train()</span>
<span id="cb71-6"><a href="#cb71-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-7"><a href="#cb71-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Run with custom hyperparameters</span></span>
<span id="cb71-8"><a href="#cb71-8" aria-hidden="true" tabindex="-1"></a>my_hparams <span class="op">=</span> {</span>
<span id="cb71-9"><a href="#cb71-9" aria-hidden="true" tabindex="-1"></a>    <span class="st">'batch_size'</span>: <span class="dv">128</span>,</span>
<span id="cb71-10"><a href="#cb71-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">'learning_rate'</span>: <span class="fl">0.01</span>,</span>
<span id="cb71-11"><a href="#cb71-11" aria-hidden="true" tabindex="-1"></a>    <span class="st">'epochs'</span>: <span class="dv">8</span>,</span>
<span id="cb71-12"><a href="#cb71-12" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb71-13"><a href="#cb71-13" aria-hidden="true" tabindex="-1"></a>train(hparams_dict<span class="op">=</span>my_hparams, log_dir<span class="op">=</span><span class="st">'runs/my_custom_run'</span>)</span>
<span id="cb71-14"><a href="#cb71-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-15"><a href="#cb71-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Start TensorBoard (in a separate cell, or from the command line)</span></span>
<span id="cb71-16"><a href="#cb71-16" aria-hidden="true" tabindex="-1"></a><span class="co"># %load_ext tensorboard</span></span>
<span id="cb71-17"><a href="#cb71-17" aria-hidden="true" tabindex="-1"></a><span class="co"># %tensorboard --logdir runs</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><strong>TensorBoard 运行:</strong></p>
<p>训练完成后，在 shell 中使用以下命令运行 TensorBoard。</p>
<div id="cell-93" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb72"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-2"><a href="#cb72-2" aria-hidden="true" tabindex="-1"></a>tensorboard <span class="op">--</span>logdir runs</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>在网页浏览器中访问 <code>http://localhost:6006</code> 可以查看 TensorBoard 仪表板。</p>
<p>可以确认每个项目分别生成了多个卡片。 <img src="../../../assets/images/03_01.png" class="img-fluid" alt="TensorBoard"></p>
<p>可以在各个项目中查看单个值的变化、图像等。 <img src="../../../assets/images/03_02.png" class="img-fluid" alt="TensorBoard"></p>
<p><strong>使用 TensorBoard 仪表板</strong></p>
<ul>
<li><strong>SCALARS 标签页:</strong> 跟踪训练/测试损失、准确度、学习率等随时间的变化。这有助于了解模型是否训练良好，是否存在过拟合(overfitting)等问题。</li>
<li><strong>GRAPHS 标签页:</strong> 可视化模型的计算图，直观展示数据流和运算过程。这对于理解复杂模型的结构非常有帮助。</li>
<li><strong>DISTRIBUTIONS/HISTOGRAMS 标签页:</strong> 可视化权重和梯度的分布。这有助于诊断权重初始化是否恰当、是否存在梯度消失(vanishing gradients)或梯度爆炸(exploding gradients)等问题。</li>
<li><strong>IMAGES 标签页:</strong> 以图像形式可视化输入图像、特征图、滤波器等。这可以直观地查看模型关注图像的哪些部分，特征提取是否有效等。</li>
<li><strong>PROJECTOR 标签页:</strong> 将高维嵌入投影到 2D/3D 并进行可视化。这有助于了解数据的聚类、异常值(outlier)等问题。</li>
<li><strong>HPARAMS 标签页:</strong> 比较使用不同超参数组合实验的结果，帮助找到最优设置。</li>
</ul>
<p>在本例中，我们探讨了如何使用 TensorBoard 可视化深度学习模型的训练过程。TensorBoard 不仅仅是一个简单的可视化工具，它对于理解模型的工作方式、诊断问题以及改进性能都是必不可少的。</p>
</section>
</section>
<section id="hugging-face-transformers" class="level2">
<h2 class="anchored" data-anchor-id="hugging-face-transformers">3.3 Hugging Face Transformers</h2>
<p>Hugging Face 是一家由法国企业家于2016年创立的公司，最初是一个面向青少年的聊天机器人应用。起初的目标是提供情感支持和娱乐的人工智能朋友，但当他们将自家聊天机器人的NLP模型开源时，迎来了一个重要的转折点。这发生在BERT、GPT等高性能语言模型出现但仍难以实际应用的时期，因此引起了巨大反响。2019年推出的Transformers库为自然语言处理领域带来了革命性的变化。如果说PyTorch提供了深度学习的基础运算和学习框架，那么Hugging Face则在此基础上专注于实现和利用真实的语言模型。特别是通过简化预训练模型的共享和重用，使大规模语言模型不再只是少数大企业的专属，而是任何人都可以使用。</p>
<p>Hugging Face 构建了一个开放的生态系统，被称为“AI 的 GitHub”。目前已有超过100万个模型和数十万的数据集被分享，这不仅超越了单纯的代码仓库，还发展成为一个促进道德和负责任的AI开发的平台。特别是通过引入模型卡系统来明确每个模型的局限性和偏差，并通过基于社区的反馈系统持续验证模型的质量和伦理性。这些努力不仅促进了AI开发的民主化，更提出了一个负责的技术发展的新范式。Hugging Face 的方法平衡了技术创新与伦理考虑，已成为现代AI开发的最佳实践之一。</p>
<section id="transformers库介绍" class="level3">
<h3 class="anchored" data-anchor-id="transformers库介绍">3.3.1 Transformers库介绍</h3>
<p>Transformers 提供了一个集成接口，可以轻松下载和使用预训练模型。它可以在PyTorch或TensorFlow等框架上运行，确保与现有深度学习生态系统的兼容性。特别是支持像JAX这样的新框架，扩大了研究人员的选择范围。Transformers 的核心组成部分主要有两个。</p>
<section id="模型中心和管道" class="level5">
<h5 class="anchored" data-anchor-id="模型中心和管道">模型中心和管道</h5>
<p>模型中心充当预训练模型的中央存储库。公开了专门用于文本生成、分类、翻译、摘要、问答等多种自然语言处理任务的模型。每个模型都附带详细的元数据，如性能指标、许可信息、学习数据来源等。特别是通过模型卡（Model Card）系统明确指出模型的局限性和偏差，鼓励负责任的AI开发。</p>
<p>管道将复杂的预处理和后处理过程抽象化，提供简单易用的接口。这在生产环境中尤其有用，并大大降低了模型集成的成本。管道内部自动配置分词器和模型，并自动执行批处理或GPU加速等优化操作。</p>
<div id="cell-96" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb73"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> pipeline</span>
<span id="cb73-2"><a href="#cb73-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-3"><a href="#cb73-3" aria-hidden="true" tabindex="-1"></a>classifier <span class="op">=</span> pipeline(<span class="st">"sentiment-analysis"</span>)</span>
<span id="cb73-4"><a href="#cb73-4" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> classifier(<span class="st">"I love this book!"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stderr">
<pre><code>No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).
Using a pipeline without specifying a model name and revision in production is not recommended.</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"c6703892f09b4ade869f16b776740536","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"b5210ba6dfe24216ab409ef41d197c2c","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"cc25443102364b8e96035a7c0218e23b","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"4e19048c8971437b82f666267ec92f21","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Device set to use cuda:0</code></pre>
</div>
</div>
</section>
<section id="分词器和模型类" class="level5">
<h5 class="anchored" data-anchor-id="分词器和模型类">分词器和模型类</h5>
<p>分词器将输入文本转换为模型可以处理的数字序列。每个模型都有专用的分词器，这反映了训练数据的特性。分词器不仅进行简单的单词分割，还能一致地处理子词分词、添加特殊标记、填充、截断等复杂的预处理任务。特别是，它综合支持WordPiece、BPE、SentencePiece等多种分词算法，可以选择最适合每种语言和领域的最佳分词方法。</p>
<p>模型类实现了执行实际计算的神经网络。支持BERT、GPT、T5等各种架构，并且可以通过AutoModel系列类自动选择模型的架构。每个模型都附带预训练权重，并可根据需要针对特定任务进行微调。此外，还可以立即应用模型并行化、量化、剪枝等优化技术。</p>
<div id="cell-98" class="cell" data-execution_count="43">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb76"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb76-1"><a href="#cb76-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoTokenizer, AutoModel</span>
<span id="cb76-2"><a href="#cb76-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-3"><a href="#cb76-3" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(<span class="st">"bert-base-uncased"</span>)</span>
<span id="cb76-4"><a href="#cb76-4" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> AutoModel.from_pretrained(<span class="st">"bert-base-uncased"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
</section>
<section id="主要应用案例" class="level3">
<h3 class="anchored" data-anchor-id="主要应用案例">3.3.2 主要应用案例</h3>
<p>Transformers 库在各种自然语言处理任务中得到了广泛应用。自 2020 年以来，随着 GPT 系列模型的发展，文本生成能力取得了飞跃性的进步，到 2024 年，Llama 3 这样的高性能开源模型的出现进一步扩展了应用范围。特别是 Llama 3 的 405B 参数模型展现出与 GPT-4 媲美的性能，在多语言处理、编程和推理能力方面取得了显著进展。这些进步使得在实际业务环境中可以实现各种应用，如客户服务、内容生成、数据分析、自动化任务处理等广泛领域都在使用。特别是代码生成和调试能力的大幅提升也为提高开发者生产力做出了贡献。</p>
<p><strong>Hugging Face Hub 的使用:</strong></p>
<p>Hugging Face Hub (<a href="https://huggingface.co/models">https://huggingface.co/models</a>) 是一个可以搜索、过滤和下载众多模型和数据集的平台。</p>
<ul>
<li><strong>模型搜索:</strong> 可以在左上角的搜索框中通过模型名称（如 “bert”, “gpt2”, “t5”）或任务（如 “text-classification”, “question-answering”）进行搜索。</li>
<li><strong>过滤:</strong> 在左侧面板中可以根据任务(Task)、库(Libraries)、语言(Languages)、数据集(Datasets)等不同标准进行过滤。</li>
<li><strong>模型页面:</strong> 每个模型页面提供了模型描述、使用示例、性能指标、模型卡等有用信息。</li>
</ul>
<p><strong>文本生成与分类</strong></p>
<p>文本生成是在给定提示的基础上生成自然文本的任务。最新的模型提供了以下高级功能： - 多模态生成：结合文本和图像的内容生成 - 代码自动生成：针对不同编程语言优化的代码编写 - 对话型代理：实现理解上下文的智能聊天机器人 - 专业领域文本：生成医疗、法律等特殊领域的文档</p>
<div id="cell-100" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb77"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb77-1"><a href="#cb77-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> pipeline</span>
<span id="cb77-2"><a href="#cb77-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-3"><a href="#cb77-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Text generation pipeline (using gpt2 model)</span></span>
<span id="cb77-4"><a href="#cb77-4" aria-hidden="true" tabindex="-1"></a>generator <span class="op">=</span> pipeline(<span class="st">'text-generation'</span>, model<span class="op">=</span><span class="st">'gpt2'</span>)  <span class="co"># Smaller model</span></span>
<span id="cb77-5"><a href="#cb77-5" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> generator(<span class="st">"Design a webpage that"</span>, max_length<span class="op">=</span><span class="dv">50</span>, num_return_sequences<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb77-6"><a href="#cb77-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(result[<span class="dv">0</span>][<span class="st">'generated_text'</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stderr">
<pre><code>Device set to use cuda:0
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Design a webpage that is compatible with your browser with our FREE SEO Service.

You read that right. By utilizing a web browser's default settings, your webpage should be free from advertisements and other types of spam. The best way to avoid this</code></pre>
</div>
</div>
<p>文本分类在2025年将更加精细，提供以下功能：</p>
<ul>
<li>零样本/少样本学习：通过Hugging Face的Transformer库可以实现对新类别的即时适应。特别是基于自然语言推理的预训练模型仅需不到8个示例即可达到90%以上的准确度，并可应用于多种领域。</li>
<li>多语种分类：如Hugging Face的ModernBERT等最新的多语种模型支持16种以上的主要语言。尤其是拥有1.5亿参数的基础模型也达到了80%以上的F1分数，在低资源语言中表现出色。</li>
<li>层次分类：Hugging Face的HiGen框架提供了专门针对层次标签分类的功能。通过基于级别的损失函数，可以有效地捕捉文本与标签之间的语义关系，特别是在数据不足的类别中也能表现出高效率。</li>
<li>实时分类：通过Hugging Face管道可以实现实时处理流数据。集成优化技术如Flash Attention可高效处理长序列，并在实时应用中提供高吞吐量。</li>
</ul>
<section id="微调和模型共享" class="level5">
<h5 class="anchored" data-anchor-id="微调和模型共享">微调和模型共享</h5>
<p>Hugging Face集成了最新的微调技术，支持大规模语言模型的高效学习。这些技术可以在保持模型性能的同时大幅减少学习成本和时间。</p>
<ul>
<li>QLoRA (量化低秩适应)：通过Hugging Face的PEFT库提供，结合4位量化和低秩适应可将内存使用量减少90%以上。特别是650亿参数模型也可以在单个48GB GPU上进行微调。</li>
<li>Spectrum：与Hugging Face TRL库集成的选择性层优化技术。通过分析各层的信噪比，仅选择重要度高的层进行学习，从而提高计算效率。</li>
<li>Flash Attention：从Hugging Face Transformer 2.2版本开始默认支持，并可以通过设置attn_implementation=“flash_attention_2”参数轻松激活。特别在处理长序列时内存效率显著提升。</li>
<li>DeepSpeed：通过Hugging Face Accelerate库完美集成，并通过ZeRO优化器高效支持大规模分布式学习。尤其在推理时也可使用，可以将大模型分布在多个GPU上加载。</li>
</ul>
<div id="cell-102" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb80"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb80-1"><a href="#cb80-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments, Trainer, DataCollatorWithPadding</span>
<span id="cb80-2"><a href="#cb80-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> datasets <span class="im">import</span> Dataset</span>
<span id="cb80-3"><a href="#cb80-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb80-4"><a href="#cb80-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb80-5"><a href="#cb80-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-6"><a href="#cb80-6" aria-hidden="true" tabindex="-1"></a><span class="co"># --- 1. Load a pre-trained model and tokenizer ---</span></span>
<span id="cb80-7"><a href="#cb80-7" aria-hidden="true" tabindex="-1"></a>model_name <span class="op">=</span> <span class="st">"distilbert-base-uncased"</span>  <span class="co"># Use a small, fast model</span></span>
<span id="cb80-8"><a href="#cb80-8" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(model_name)</span>
<span id="cb80-9"><a href="#cb80-9" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> AutoModelForSequenceClassification.from_pretrained(model_name, num_labels<span class="op">=</span><span class="dv">2</span>)  <span class="co"># Binary classification</span></span>
<span id="cb80-10"><a href="#cb80-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-11"><a href="#cb80-11" aria-hidden="true" tabindex="-1"></a><span class="co"># --- 2. Create a simple dataset (for demonstration) ---</span></span>
<span id="cb80-12"><a href="#cb80-12" aria-hidden="true" tabindex="-1"></a>raw_data <span class="op">=</span> {</span>
<span id="cb80-13"><a href="#cb80-13" aria-hidden="true" tabindex="-1"></a>    <span class="st">"text"</span>: [</span>
<span id="cb80-14"><a href="#cb80-14" aria-hidden="true" tabindex="-1"></a>        <span class="st">"This is a positive example!"</span>,</span>
<span id="cb80-15"><a href="#cb80-15" aria-hidden="true" tabindex="-1"></a>        <span class="st">"This is a negative example."</span>,</span>
<span id="cb80-16"><a href="#cb80-16" aria-hidden="true" tabindex="-1"></a>        <span class="st">"Another positive one."</span>,</span>
<span id="cb80-17"><a href="#cb80-17" aria-hidden="true" tabindex="-1"></a>        <span class="st">"And a negative one."</span></span>
<span id="cb80-18"><a href="#cb80-18" aria-hidden="true" tabindex="-1"></a>    ],</span>
<span id="cb80-19"><a href="#cb80-19" aria-hidden="true" tabindex="-1"></a>    <span class="st">"label"</span>: [<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>],  <span class="co"># 1 for positive, 0 for negative</span></span>
<span id="cb80-20"><a href="#cb80-20" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb80-21"><a href="#cb80-21" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> Dataset.from_dict(raw_data)</span>
<span id="cb80-22"><a href="#cb80-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-23"><a href="#cb80-23" aria-hidden="true" tabindex="-1"></a><span class="co"># --- 3. Tokenize the dataset ---</span></span>
<span id="cb80-24"><a href="#cb80-24" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> tokenize_function(examples):</span>
<span id="cb80-25"><a href="#cb80-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> tokenizer(examples[<span class="st">"text"</span>], truncation<span class="op">=</span><span class="va">True</span>) <span class="co">#padding is handled by data collator</span></span>
<span id="cb80-26"><a href="#cb80-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-27"><a href="#cb80-27" aria-hidden="true" tabindex="-1"></a>tokenized_dataset <span class="op">=</span> dataset.<span class="bu">map</span>(tokenize_function, batched<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb80-28"><a href="#cb80-28" aria-hidden="true" tabindex="-1"></a>tokenized_dataset <span class="op">=</span> tokenized_dataset.remove_columns([<span class="st">"text"</span>]) <span class="co"># remove text, keep label</span></span>
<span id="cb80-29"><a href="#cb80-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-30"><a href="#cb80-30" aria-hidden="true" tabindex="-1"></a><span class="co"># --- 4. Data Collator (for dynamic padding) ---</span></span>
<span id="cb80-31"><a href="#cb80-31" aria-hidden="true" tabindex="-1"></a>data_collator <span class="op">=</span> DataCollatorWithPadding(tokenizer<span class="op">=</span>tokenizer)</span>
<span id="cb80-32"><a href="#cb80-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-33"><a href="#cb80-33" aria-hidden="true" tabindex="-1"></a><span class="co"># --- 5. Training Arguments ---</span></span>
<span id="cb80-34"><a href="#cb80-34" aria-hidden="true" tabindex="-1"></a>fp16_enabled <span class="op">=</span> <span class="va">False</span></span>
<span id="cb80-35"><a href="#cb80-35" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> torch.cuda.is_available():</span>
<span id="cb80-36"><a href="#cb80-36" aria-hidden="true" tabindex="-1"></a>    <span class="cf">try</span>:</span>
<span id="cb80-37"><a href="#cb80-37" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> torch.cuda.get_device_capability()[<span class="dv">0</span>] <span class="op">&gt;=</span> <span class="dv">7</span>:</span>
<span id="cb80-38"><a href="#cb80-38" aria-hidden="true" tabindex="-1"></a>            fp16_enabled <span class="op">=</span> <span class="va">True</span></span>
<span id="cb80-39"><a href="#cb80-39" aria-hidden="true" tabindex="-1"></a>    <span class="cf">except</span>:</span>
<span id="cb80-40"><a href="#cb80-40" aria-hidden="true" tabindex="-1"></a>        <span class="cf">pass</span></span>
<span id="cb80-41"><a href="#cb80-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-42"><a href="#cb80-42" aria-hidden="true" tabindex="-1"></a>training_args <span class="op">=</span> TrainingArguments(</span>
<span id="cb80-43"><a href="#cb80-43" aria-hidden="true" tabindex="-1"></a>    output_dir<span class="op">=</span><span class="st">"./results"</span>,</span>
<span id="cb80-44"><a href="#cb80-44" aria-hidden="true" tabindex="-1"></a>    num_train_epochs<span class="op">=</span><span class="dv">1</span>,          <span class="co"># Keep it short</span></span>
<span id="cb80-45"><a href="#cb80-45" aria-hidden="true" tabindex="-1"></a>    per_device_train_batch_size<span class="op">=</span><span class="dv">2</span>,  <span class="co"># Small batch size</span></span>
<span id="cb80-46"><a href="#cb80-46" aria-hidden="true" tabindex="-1"></a>    logging_steps<span class="op">=</span><span class="dv">1</span>,           <span class="co"># Log every step</span></span>
<span id="cb80-47"><a href="#cb80-47" aria-hidden="true" tabindex="-1"></a>    save_strategy<span class="op">=</span><span class="st">"no"</span>,         <span class="co"># No saving</span></span>
<span id="cb80-48"><a href="#cb80-48" aria-hidden="true" tabindex="-1"></a>    report_to<span class="op">=</span><span class="st">"none"</span>,          <span class="co"># No reporting</span></span>
<span id="cb80-49"><a href="#cb80-49" aria-hidden="true" tabindex="-1"></a>    fp16<span class="op">=</span>fp16_enabled,  <span class="co"># Use fp16 if avail.</span></span>
<span id="cb80-50"><a href="#cb80-50" aria-hidden="true" tabindex="-1"></a>    <span class="co"># --- Optimization techniques (demonstration) ---</span></span>
<span id="cb80-51"><a href="#cb80-51" aria-hidden="true" tabindex="-1"></a>    <span class="co"># gradient_checkpointing=True,  # Enable gradient checkpointing (if needed for large models)</span></span>
<span id="cb80-52"><a href="#cb80-52" aria-hidden="true" tabindex="-1"></a>    <span class="co"># gradient_accumulation_steps=2, # Increase effective batch size</span></span>
<span id="cb80-53"><a href="#cb80-53" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb80-54"><a href="#cb80-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-55"><a href="#cb80-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-56"><a href="#cb80-56" aria-hidden="true" tabindex="-1"></a><span class="co"># --- 6. Trainer ---</span></span>
<span id="cb80-57"><a href="#cb80-57" aria-hidden="true" tabindex="-1"></a>trainer <span class="op">=</span> Trainer(</span>
<span id="cb80-58"><a href="#cb80-58" aria-hidden="true" tabindex="-1"></a>    model<span class="op">=</span>model,</span>
<span id="cb80-59"><a href="#cb80-59" aria-hidden="true" tabindex="-1"></a>    args<span class="op">=</span>training_args,</span>
<span id="cb80-60"><a href="#cb80-60" aria-hidden="true" tabindex="-1"></a>    train_dataset<span class="op">=</span>tokenized_dataset,</span>
<span id="cb80-61"><a href="#cb80-61" aria-hidden="true" tabindex="-1"></a>    <span class="co"># eval_dataset=...,  # Add an eval dataset if you have one</span></span>
<span id="cb80-62"><a href="#cb80-62" aria-hidden="true" tabindex="-1"></a>    data_collator<span class="op">=</span>data_collator,  <span class="co"># Use the data collator</span></span>
<span id="cb80-63"><a href="#cb80-63" aria-hidden="true" tabindex="-1"></a>    <span class="co"># optimizers=(optimizer, scheduler) # you could also customize optimizer</span></span>
<span id="cb80-64"><a href="#cb80-64" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb80-65"><a href="#cb80-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-66"><a href="#cb80-66" aria-hidden="true" tabindex="-1"></a><span class="co"># --- 7. Train ---</span></span>
<span id="cb80-67"><a href="#cb80-67" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Starting training..."</span>)</span>
<span id="cb80-68"><a href="#cb80-68" aria-hidden="true" tabindex="-1"></a>trainer.train()</span>
<span id="cb80-69"><a href="#cb80-69" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Training finished!"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"ad7aa580feaf4d5fa3abcd96b1bc43e3","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"24731acb95cb4dbea5d194f768b17df3","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"ee9456452ccb4910849e2973a1765462","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"688ecbdc102a4cc6b562c911f79851c0","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"62d43521fee04e07a6ce10a5488d2b38","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"c2522a9d78974c45beeb8575e3c29e85","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Starting training...</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/home/sean/anaconda3/envs/DL/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn(</code></pre>
</div>
<div class="cell-output cell-output-display">

    <div>
      
      <progress value="1" max="1" style="width:300px; height:20px; vertical-align: middle;"></progress>
      [1/1 00:00, Epoch 1/1]
    </div>
    
<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">Step</th>
<th data-quarto-table-cell-role="th">Training Loss</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>0.667500</td>
</tr>
</tbody>
</table>
<p>
</p></div>
<div class="cell-output cell-output-stdout">
<pre><code>Training finished!</code></pre>
</div>
</div>
<p>模型共享生态系统目前支持以下最新功能，截至2025年： - 模型卡片自动生成：Hugging Face 的自动化模型卡片系统可以自动分析和记录性能指标和偏见。特别是通过 Model Card Toolkit，可以以标准化格式清晰地描述模型的特性和局限性。 - 版本管理：通过 Hugging Face Hub 基于 Git 的版本管理系统，可以跟踪模型的变更历史和性能变化。可以自动生成并比较每个版本的性能度量和参数变化。 - 协作工具：提供与 Hugging Face Spaces 集成的协作环境。团队成员可以在实时共享模型开发、测试和部署过程，并互相给予反馈，同时支持与 CI/CD 管道的集成。 - 伦理 AI：通过 Hugging Face 的伦理 AI 框架，可以自动验证和评估模型的偏见。特别是可以分析不同人口统计群体之间的性能差异，并提前识别潜在风险。</p>
</section>
</section>
</section>
<section id="练习题" class="level2">
<h2 class="anchored" data-anchor-id="练习题">练习题</h2>
<p><strong>1. 基础问题</strong></p>
<ul>
<li>解释PyTorch张量和NumPy数组的区别以及相互转换的方法。</li>
<li>解释<code>torch.nn.Linear</code>层的作用和权重初始化方法。</li>
<li>解释在PyTorch中自动微分(automatic differentiation)是如何工作的，并解释<code>requires_grad</code>属性的作用。</li>
</ul>
<p><strong>2. 应用问题</strong></p>
<ul>
<li>使用<code>torch.utils.data.Dataset</code>和<code>torch.utils.data.DataLoader</code>将给定的数据集分为训练/验证/测试集，并编写代码以批处理方式加载数据。</li>
<li>继承<code>nn.Module</code>实现一个简单的CNN模型（例如：LeNet-5），并使用<code>torchsummary</code>检查模型的结构和参数数量。</li>
<li>使用MNIST或Fashion-MNIST数据集训练模型，并使用TensorBoard可视化训练过程（损失、准确率等）。</li>
</ul>
<p><strong>3. 深化问题</strong></p>
<ul>
<li>使用<code>torch.einsum</code>实现矩阵乘法、转置、批处理矩阵乘法、双线性变换(bilinear transformation)等。（提供每种操作的爱因斯坦符号表示，并用PyTorch代码实现。）</li>
<li>编写代码创建自定义数据集并使用<code>torchvision.transforms</code>进行数据增强（例如：图像旋转、裁剪、颜色转换等）。</li>
<li>解释如何使用<code>torch.autograd.grad</code>计算高阶导数(higher-order derivatives)，并编写一个简单的示例代码。（例如：海森矩阵计算）</li>
<li>解释为什么在不直接调用<code>torch.nn.Module</code>的<code>forward()</code>方法的情况下，可以像函数一样调用模型对象。（提示：<code>__call__</code>方法与自动微分系统的关联）</li>
</ul>
<div class="callout callout-style-default callout-note callout-titled" title="点击查看内容（答案）">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
点击查看内容（答案）
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<section id="练习题解答" class="level2 callout-body-container callout-body">
<h2 class="anchored" data-anchor-id="练习题解答">练习题解答</h2>
<section id="基础问题解答" class="level3">
<h3 class="anchored" data-anchor-id="基础问题解答">1. 基础问题解答</h3>
<ol type="1">
<li><strong>张量 vs.&nbsp;NumPy 数组:</strong>
<ul>
<li><strong>区别:</strong> 张量支持 GPU 加速、自动求导。NumPy 是基于 CPU 的通用数组运算。</li>
<li><strong>转换:</strong> <code>torch.from_numpy()</code>, <code>.numpy()</code>（注意，GPU 张量需要先调用 <code>.cpu()</code>）。</li>
</ul>
<div class="sourceCode" id="cb85"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb85-1"><a href="#cb85-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 示例</span></span>
<span id="cb85-2"><a href="#cb85-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb85-3"><a href="#cb85-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb85-4"><a href="#cb85-4" aria-hidden="true" tabindex="-1"></a>numpy_array <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>])</span>
<span id="cb85-5"><a href="#cb85-5" aria-hidden="true" tabindex="-1"></a>torch_tensor <span class="op">=</span> torch.from_numpy(numpy_array)  <span class="co"># 或者使用 torch.tensor()</span></span>
<span id="cb85-6"><a href="#cb85-6" aria-hidden="true" tabindex="-1"></a>numpy_back <span class="op">=</span> torch_tensor.cpu().numpy()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li><strong><code>nn.Linear</code>:</strong>
<ul>
<li><strong>作用:</strong> <code>y = xW^T + b</code>（线性变换）。对输入 <code>x</code> 进行权重 <code>W</code> 的乘法并加上偏置 <code>b</code>。</li>
<li><strong>初始化:</strong> 默认使用 Kaiming He 初始化（均匀分布）。可以通过 <code>torch.nn.init</code> 模块进行更改。</li>
</ul>
<div class="sourceCode" id="cb86"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb86-1"><a href="#cb86-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 示例</span></span>
<span id="cb86-2"><a href="#cb86-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb86-3"><a href="#cb86-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.init <span class="im">as</span> init</span>
<span id="cb86-4"><a href="#cb86-4" aria-hidden="true" tabindex="-1"></a>linear_layer <span class="op">=</span> nn.Linear(in_features<span class="op">=</span><span class="dv">10</span>, out_features<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb86-5"><a href="#cb86-5" aria-hidden="true" tabindex="-1"></a>init.xavier_uniform_(linear_layer.weight) <span class="co"># Xavier 初始化</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li><strong>自动求导 (Autograd):</strong>
<ul>
<li><strong>工作原理:</strong> 当 <code>requires_grad=True</code> 的张量进行运算时，会生成计算图。调用 <code>.backward()</code> 时，通过链式法则计算梯度。</li>
<li><strong><code>requires_grad</code>:</strong> 设置是否进行梯度计算及跟踪。</li>
</ul>
<div class="sourceCode" id="cb87"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb87-1"><a href="#cb87-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 示例</span></span>
<span id="cb87-2"><a href="#cb87-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb87-3"><a href="#cb87-3" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.tensor([<span class="fl">2.0</span>], requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb87-4"><a href="#cb87-4" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> x<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> <span class="dv">3</span><span class="op">*</span>x <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb87-5"><a href="#cb87-5" aria-hidden="true" tabindex="-1"></a>y.backward()</span>
<span id="cb87-6"><a href="#cb87-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(x.grad)  <span class="co"># 输出: tensor([7.])</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
</ol>
</section>
<section id="应用问题解答" class="level3">
<h3 class="anchored" data-anchor-id="应用问题解答">2. 应用问题解答</h3>
<ol start="4" type="1">
<li><p><strong><code>Dataset</code>, <code>DataLoader</code>:</strong></p>
<div class="sourceCode" id="cb88"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb88-1"><a href="#cb88-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> Dataset, DataLoader, random_split</span>
<span id="cb88-2"><a href="#cb88-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torchvision.transforms <span class="im">as</span> transforms</span>
<span id="cb88-3"><a href="#cb88-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision <span class="im">import</span> datasets</span>
<span id="cb88-4"><a href="#cb88-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-5"><a href="#cb88-5" aria-hidden="true" tabindex="-1"></a><span class="co"># 自定义 Dataset（示例）</span></span>
<span id="cb88-6"><a href="#cb88-6" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> CustomDataset(Dataset):</span>
<span id="cb88-7"><a href="#cb88-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, data, targets, transform<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb88-8"><a href="#cb88-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.data <span class="op">=</span> data</span>
<span id="cb88-9"><a href="#cb88-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.targets <span class="op">=</span> targets</span>
<span id="cb88-10"><a href="#cb88-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.transform <span class="op">=</span> transform</span>
<span id="cb88-11"><a href="#cb88-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__len__</span>(<span class="va">self</span>):</span>
<span id="cb88-12"><a href="#cb88-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">len</span>(<span class="va">self</span>.data)</span>
<span id="cb88-13"><a href="#cb88-13" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__getitem__</span>(<span class="va">self</span>, idx):</span>
<span id="cb88-14"><a href="#cb88-14" aria-hidden="true" tabindex="-1"></a>        sample, label <span class="op">=</span> <span class="va">self</span>.data[idx], <span class="va">self</span>.targets[idx]</span>
<span id="cb88-15"><a href="#cb88-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.transform:</span>
<span id="cb88-16"><a href="#cb88-16" aria-hidden="true" tabindex="-1"></a>            sample <span class="op">=</span> <span class="va">self</span>.transform(sample)</span>
<span id="cb88-17"><a href="#cb88-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> sample, label</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<h1 id="mnist-dataloader-示例-使用-torchvision">MNIST DataLoader 示例 (使用 torchvision)</h1>
<p>transform = transforms.ToTensor() # 将图像数据转换为张量 mnist_dataset = datasets.MNIST(root=‘./data’, train=True, download=True, transform=transform) train_size = int(0.8 * len(mnist_dataset)) val_size = len(mnist_dataset) - train_size train_dataset, val_dataset = random_split(mnist_dataset, [train_size, val_size]) train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True) val_loader = DataLoader(val_dataset, batch_size=32)</p></li>
</ol>
<pre><code>
5.  **LeNet-5, `torchsummary`, TensorBoard:** (完整代码请参见之前的回答，这里只展示核心部分)

```python
import torch.nn as nn
import torch.nn.functional as F
from torchsummary import summary
from torch.utils.tensorboard import SummaryWriter

# LeNet-5 模型
class LeNet5(nn.Module):
    def __init__(self):
        super(LeNet5, self).__init__()
        self.conv1 = nn.Conv2d(1, 6, kernel_size=5, stride=1, padding=2)
        self.pool1 = nn.AvgPool2d(kernel_size=2, stride=2)
        self.conv2 = nn.Conv2d(6, 16, kernel_size=5, stride=1)
        self.pool2 = nn.AvgPool2d(kernel_size=2, stride=2)
        self.fc1 = nn.Linear(16 * 5 * 5, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        x = self.pool1(x)
        x = F.relu(self.conv2(x))
        x = self.pool2(x)
        x = x.view(x.size(0), -1)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x

model = LeNet5()
summary(model, input_size=(1, 28, 28)) # 模型结构摘要

# ... (训练代码，请参见之前的回答) ...

writer = SummaryWriter() # TensorBoard
# ... (在训练中使用 writer.add_scalar() 等进行记录) ...
writer.close()</code></pre>
</section>
<section id="进阶问题解答" class="level3">
<h3 class="anchored" data-anchor-id="进阶问题解答">3. 进阶问题解答</h3>
<ol start="6" type="1">
<li><strong><code>torch.einsum</code>:</strong></li>
</ol>
<div class="sourceCode" id="cb90"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb90-1"><a href="#cb90-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>A = torch.randn(3, 4) B = torch.randn(4, 5) C = torch.einsum(“ij,jk-&gt;ik”, A, B) # 矩阵乘法 D = torch.einsum(“ij-&gt;ji”, A) # 转置 E = torch.einsum(“bi,bj,ijk-&gt;bk”, A, B, torch.randn(2,3,4)) # 双线性变换</p>
<pre><code>
7. **自定义数据集，数据增强:**

```python
from torch.utils.data import Dataset
from torchvision import transforms
from PIL import Image
import os

class CustomImageDataset(Dataset): # 继承自Dataset
    def __init__(self, root_dir, transform=None):
        # ... (构造函数实现) ...
        pass
    def __len__(self):
        # ... (返回数据数量) ...
        pass
    def __getitem__(self, idx):
        # ... (返回对应idx的样本) ...
        pass

# 数据增强
transform = transforms.Compose([
    transforms.RandomResizedCrop(224),  # 随机大小和比例裁剪
    transforms.RandomHorizontalFlip(),     # 水平翻转
    transforms.ToTensor(),              # 转换为张量
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # 标准化
])

# dataset = CustomImageDataset(root_dir='path/to/images', transform=transform)</code></pre>
<ol start="8" type="1">
<li><strong>高阶函数:</strong></li>
</ol>
<div class="sourceCode" id="cb92"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb92-1"><a href="#cb92-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb92-2"><a href="#cb92-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb92-3"><a href="#cb92-3" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.tensor(<span class="fl">2.0</span>, requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb92-4"><a href="#cb92-4" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> x<span class="op">**</span><span class="dv">3</span></span>
<span id="cb92-5"><a href="#cb92-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb92-6"><a href="#cb92-6" aria-hidden="true" tabindex="-1"></a><span class="co"># 一阶导数</span></span>
<span id="cb92-7"><a href="#cb92-7" aria-hidden="true" tabindex="-1"></a>first_derivative <span class="op">=</span> torch.autograd.grad(y, x, create_graph<span class="op">=</span><span class="va">True</span>)[<span class="dv">0</span>]  <span class="co"># create_graph=True</span></span>
<span id="cb92-8"><a href="#cb92-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(first_derivative)</span>
<span id="cb92-9"><a href="#cb92-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb92-10"><a href="#cb92-10" aria-hidden="true" tabindex="-1"></a><span class="co"># 二阶导数 (海森矩阵)</span></span>
<span id="cb92-11"><a href="#cb92-11" aria-hidden="true" tabindex="-1"></a>second_derivative <span class="op">=</span> torch.autograd.grad(first_derivative, x)[<span class="dv">0</span>]</span>
<span id="cb92-12"><a href="#cb92-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(second_derivative)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ol start="9" type="1">
<li><strong><code>__call__</code> 方法:</strong></li>
</ol>
<p><code>nn.Module</code> 的 <code>__call__</code> 方法在调用 <code>forward()</code> <em>前后</em> 执行额外的操作（如挂钩注册、自动求导相关设置等）。如果直接调用 <code>forward()</code>，这些功能将被忽略，可能导致梯度计算不正确或模型的其他功能（例如：<code>nn.Module</code> 的 <code>training</code> 属性设置）无法正常工作。因此，必须像调用函数一样调用模型对象 (<code>model(input)</code>)。</p>
</section>
</section>
</div>
</div>
<p><strong>参考资料</strong></p>
<ol type="1">
<li><strong>PyTorch官方教程:</strong> <a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://pytorch.org/tutorials/">https://pytorch.org/tutorials/</a></li>
<li><strong>使用PyTorch进行深度学习 (Stevens, Antiga, Viehmann, 2020):</strong> <a href="https://www.google.com/search?q=https://pytorch.org/deep-learning-with-pytorch">https://pytorch.org/deep-learning-with-pytorch</a></li>
<li><strong>编程PyTorch进行深度学习 (Delugach, 2023):</strong> <a href="https://www.google.com/search?q=https://www.oreilly.com/library/view/programming-pytorch-for/9781098142481/">https://www.oreilly.com/library/view/programming-pytorch-for/9781098142481/</a></li>
<li><strong>PyTorch菜谱 (Kalyan, 2019):</strong> <a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://pytorch.org/tutorials/recipes/recipes_index.html">https://pytorch.org/tutorials/recipes/recipes_index.html</a></li>
<li><strong>理解训练深层前馈神经网络的难度 (Glorot &amp; Bengio, 2010):</strong> <a href="http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf">http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf</a></li>
<li><strong>Fastai库:</strong> <a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://docs.fast.ai/">https://docs.fast.ai/</a></li>
<li><strong>PyTorch Lightning:</strong> <a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://www.pytorchlightning.ai/">https://www.pytorchlightning.ai/</a></li>
<li><strong>Hugging Face Transformers文档:</strong> <a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://huggingface.co/docs/transformers/index">https://huggingface.co/docs/transformers/index</a></li>
<li><strong>TensorBoard文档:</strong> <a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://www.tensorflow.org/tensorboard">https://www.tensorflow.org/tensorboard</a></li>
<li><strong>Weights &amp; Biases文档:</strong> <a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://docs.wandb.ai/">https://docs.wandb.ai/</a></li>
</ol>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>