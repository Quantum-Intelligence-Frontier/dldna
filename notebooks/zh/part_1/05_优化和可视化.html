<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>quarto-inpute26935e673c7fe46 – Deep Learning DNA: Surviving Architectures and Essential Principles</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap-f507c7d0488cb7630e20aad62ad8c2aa.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>
<script src="https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js" crossorigin="anonymous"></script>
<script>window.MathJax = {loader: {load: ['[tex]/boldsymbol']},tex: {packages: {'[+]': ['boldsymbol']}}};</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../../notebooks/zh/part_1/01_深度学习的开始.html">part_1</a></li><li class="breadcrumb-item"><a href="../../../notebooks/zh/part_1/05_优化和可视化.html">5. 优化和可视化</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../../../">中文</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Language</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index_de.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Deutsch</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index_en.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">English</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index_es.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Español</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">한국어</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index_zh.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">中文</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/zh/part_1/00_引言.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">引言</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">part_1</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/zh/part_1/01_深度学习的开始.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">1. 深度学习的开始</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/zh/part_1/02_深度学习的数学.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">2. 深度学习的数学</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/zh/part_1/03_深度学习框架.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">3. 深度学习框架</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/zh/part_1/04_激活函数.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">4. 激活函数</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/zh/part_1/05_优化和可视化.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">5. 优化和可视化</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/zh/part_1/06_过拟合和解决技术的发展.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">6. 过拟合和解决技术的发展</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/zh/part_1/07_卷积神经网络的进化.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">7. 卷积神经网络的进化</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/zh/part_1/08_变压器的诞生.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">8. 变压器的诞生</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/zh/part_1/09_变压器的进化.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">9. 变压器的进化</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/zh/part_1/10_多模态深度学习：多感官融合的开始.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">10. 多模态深度学习：多感官融合的开始</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/zh/part_1/11_多模态深度学习：超越智能的极限.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">11. 多模态深度学习：超越智能的极限</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">深度学习的最前沿</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/zh/深度学习的最前沿/01_SLM: 小但强大的语言模型.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">1. SLM: 小但强大的语言模型</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/zh/深度学习的最前沿/02_自动驾驶.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">2. 自动驾驶</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#最优化与可视化" id="toc-最优化与可视化" class="nav-link active" data-scroll-target="#最优化与可视化">5. 最优化与可视化</a>
  <ul class="collapse">
  <li><a href="#参数初始化的发展与现代方法" id="toc-参数初始化的发展与现代方法" class="nav-link" data-scroll-target="#参数初始化的发展与现代方法">5.1 参数初始化的发展与现代方法</a>
  <ul class="collapse">
  <li><a href="#初始化方法的数学原理" id="toc-初始化方法的数学原理" class="nav-link" data-scroll-target="#初始化方法的数学原理">5.1.1 初始化方法的数学原理</a></li>
  <li><a href="#初始化方法实战比较分析" id="toc-初始化方法实战比较分析" class="nav-link" data-scroll-target="#初始化方法实战比较分析">5.1.2 初始化方法：实战比较分析</a></li>
  <li><a href="#实践建议及额外考虑事项" id="toc-实践建议及额外考虑事项" class="nav-link" data-scroll-target="#实践建议及额外考虑事项">5.1.3 实践建议及额外考虑事项</a></li>
  </ul></li>
  <li><a href="#优化算法深度学习训练的核心引擎" id="toc-优化算法深度学习训练的核心引擎" class="nav-link" data-scroll-target="#优化算法深度学习训练的核心引擎">5.2 优化算法：深度学习训练的核心引擎</a>
  <ul class="collapse">
  <li><a href="#优化算法的发展与实现---持续的进化" id="toc-优化算法的发展与实现---持续的进化" class="nav-link" data-scroll-target="#优化算法的发展与实现---持续的进化">5.2.1 优化算法的发展与实现 - 持续的进化</a></li>
  <li><a href="#优化训练比较" id="toc-优化训练比较" class="nav-link" data-scroll-target="#优化训练比较">5.2.2 优化训练比较</a></li>
  </ul></li>
  <li><a href="#最优化过程的可视化与分析深度学习训练的黑箱透视" id="toc-最优化过程的可视化与分析深度学习训练的黑箱透视" class="nav-link" data-scroll-target="#最优化过程的可视化与分析深度学习训练的黑箱透视">5.3 最优化过程的可视化与分析：深度学习训练的黑箱透视</a>
  <ul class="collapse">
  <li><a href="#损失表面loss-landscape的理解深度学习模型的地图" id="toc-损失表面loss-landscape的理解深度学习模型的地图" class="nav-link" data-scroll-target="#损失表面loss-landscape的理解深度学习模型的地图">5.3.1 损失表面（Loss Landscape）的理解：深度学习模型的地图</a></li>
  <li><a href="#损失表面分析的深度技术" id="toc-损失表面分析的深度技术" class="nav-link" data-scroll-target="#损失表面分析的深度技术">5.3.2 损失表面分析的深度技术</a></li>
  </ul></li>
  <li><a href="#最优化过程可视化用高斯函数窥探深度学习的学习秘密" id="toc-最优化过程可视化用高斯函数窥探深度学习的学习秘密" class="nav-link" data-scroll-target="#最优化过程可视化用高斯函数窥探深度学习的学习秘密">5.4 最优化过程可视化：用高斯函数窥探深度学习的学习秘密</a>
  <ul class="collapse">
  <li><a href="#使用高斯函数进行近似分析简单中隐藏的洞察" id="toc-使用高斯函数进行近似分析简单中隐藏的洞察" class="nav-link" data-scroll-target="#使用高斯函数进行近似分析简单中隐藏的洞察">5.4.1 使用高斯函数进行近似分析：简单中隐藏的洞察</a></li>
  <li><a href="#路径可视化" id="toc-路径可视化" class="nav-link" data-scroll-target="#路径可视化">5.4.2 路径可视化</a></li>
  </ul></li>
  <li><a href="#最优化过程的动态分析学习轨迹的探索" id="toc-最优化过程的动态分析学习轨迹的探索" class="nav-link" data-scroll-target="#最优化过程的动态分析学习轨迹的探索">5.5 最优化过程的动态分析：学习轨迹的探索</a>
  <ul class="collapse">
  <li><a href="#训练过程的特点" id="toc-训练过程的特点" class="nav-link" data-scroll-target="#训练过程的特点">5.5.1 训练过程的特点</a></li>
  <li><a href="#学习稳定性分析及控制" id="toc-学习稳定性分析及控制" class="nav-link" data-scroll-target="#学习稳定性分析及控制">5.5.2 学习稳定性分析及控制</a></li>
  <li><a href="#结语" id="toc-结语" class="nav-link" data-scroll-target="#结语">结语</a></li>
  <li><a href="#练习题" id="toc-练习题" class="nav-link" data-scroll-target="#练习题">练习题</a></li>
  <li><a href="#参考资料" id="toc-参考资料" class="nav-link" data-scroll-target="#参考资料">参考资料</a></li>
  </ul></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../../notebooks/zh/part_1/01_深度学习的开始.html">part_1</a></li><li class="breadcrumb-item"><a href="../../../notebooks/zh/part_1/05_优化和可视化.html">5. 优化和可视化</a></li></ol></nav></header>




<p><a href="https://colab.research.google.com/github/Quantum-Intelligence-Frontier/dldna/blob/main/notebooks/zh/part_1/05_优化与可视化.ipynb" target="_parent"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="在 Colab 中打开"> </a></p>
<section id="最优化与可视化" class="level1">
<h1>5. 最优化与可视化</h1>
<blockquote class="blockquote">
<p>“理论和实践之间的差异比理论和实践本身更大。” - 扬·勒孔（Yann LeCun），2018 年图灵奖得主</p>
</blockquote>
<p>深度学习模型的成功很大程度上依赖于有效的最优化算法和恰当的权重初始化策略。本章将深入探讨深度学习模型训练的核心要素——最优化与初始化方法，并通过可视化来直观地理解这一过程。首先，我们将回顾各种权重初始化方法的发展历程及其数学原理，这些方法奠定了神经网络学习的基础。然后，从梯度下降（Gradient Descent）开始，我们比较分析了包括 Adam、Lion、Sophia 和 AdaFactor 等最新最优化算法的工作原理和性能。特别地，除了理论背景之外，我们还将通过实验验证每个算法在实际深度学习模型训练过程中的具体表现。最后，我们将介绍多种高维损失函数空间（loss landscape）的可视化与分析技术，并通过这些方法提供对深度学习模型学习动力学（learning dynamics）深入理解的洞察力。</p>
<section id="参数初始化的发展与现代方法" class="level2">
<h2 class="anchored" data-anchor-id="参数初始化的发展与现代方法">5.1 参数初始化的发展与现代方法</h2>
<p>神经网络的参数初始化是决定模型收敛性、学习效率和最终性能的关键因素之一。不正确的初始化可能是导致学习失败的主要原因。PyTorch 通过 <code>torch.nn.init</code> 模块提供了多种初始化方法，详细内容可在官方文档中查阅（<a href="https://pytorch.org/docs/stable/nn.init.html">https://pytorch.org/docs/stable/nn.init.html</a>）。初始化方法的发展反映了深度学习研究者们克服神经网络学习难题的历史。特别是不当的初始化会导致梯度消失（vanishing gradient）或梯度爆炸（exploding gradient），这些现象会妨碍深层神经网络的学习。近年来，随着 GPT-3、LaMDA 等大规模语言模型（Large Language Models, LLMs）的出现，初始化的重要性更加凸显。随着模型规模的增大，初始参数分布对学习初期阶段的影响也会放大。因此，选择与模型特性和规模相匹配的适当初始化策略已成为深度学习模型开发的必要步骤。</p>
<section id="初始化方法的数学原理" class="level3">
<h3 class="anchored" data-anchor-id="初始化方法的数学原理">5.1.1 初始化方法的数学原理</h3>
<p>神经网络初始化方法的发展是深入的数学理论和大量实验验证共同作用的结果。每种初始化方法都是为了解决特定问题情况（如：使用特定激活函数、网络深度、模型类型）或改善学习动力学而设计，并随着时代的变迁应对新的挑战不断发展。</p>
<p>以下是本书将重点比较和分析的初始化方法。（完整实现代码收录在 <code>chapter_04/initialization/base.py</code> 文件中。）</p>
<div id="cell-2" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip install dldna[colab] <span class="co"># in Colab</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="co"># !pip install dldna[all] # in your local</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>load_ext autoreload</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>autoreload <span class="dv">2</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="cell-3" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Set seed</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">7</span>)</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">7</span>)</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_05.initialization.base <span class="im">import</span> init_methods, init_weights_lecun, init_weights_scaled_orthogonal, init_weights_lmomentum <span class="co"># init_weights_emergence, init_weights_dynamic 삭제</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>init_methods <span class="op">=</span> {</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Historical/Educational Significance</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>    <span class="st">'lecun'</span>: init_weights_lecun,        <span class="co"># The first systematic initialization proposed in 1998</span></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>    <span class="st">'xavier_normal'</span>: nn.init.xavier_normal_, <span class="co"># Key to the revival of deep learning in 2010</span></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>    <span class="st">'kaiming_normal'</span>: nn.init.kaiming_normal_, <span class="co"># Standard for the ReLU era, 2015</span></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Modern Standard</span></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>    <span class="st">'orthogonal'</span>: nn.init.orthogonal_,  <span class="co"># Important in RNN/LSTM</span></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>    <span class="st">'scaled_orthogonal'</span>: init_weights_scaled_orthogonal, <span class="co"># Optimization of deep neural networks</span></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 2024 Latest Research</span></span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>    <span class="st">'l-momentum'</span>: init_weights_lmomentum <span class="co"># L-Momentum Initialization</span></span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<section id="传统初始化" class="level5">
<h5 class="anchored" data-anchor-id="传统初始化">传统初始化</h5>
<ul>
<li><p><strong>LeCun 初始化 (1998年):</strong> <span class="math inline">\(std = \sqrt{\frac{1}{n_{in}}}\)</span></p>
<ul>
<li>Yann LeCun在1998年提出的方法，只考虑输入维度(<span class="math inline">\(n_{in}\)</span>)来决定权重的标准差。目的是防止每个神经元的输出随输入数量大幅变化，但在深层网络中，随着层数加深，激活值的方差有减少的趋势。<em>这在使用tanh等sigmoid系列激活函数时尤为明显。</em></li>
</ul></li>
</ul>
</section>
<section id="现代初始化" class="level5">
<h5 class="anchored" data-anchor-id="现代初始化">现代初始化</h5>
<ul>
<li><p><strong>Xavier 初始化 (Glorot, 2010):</strong> <span class="math inline">\(std = \sqrt{\frac{2}{n_{in} + n_{out}}}\)</span></p>
<ul>
<li>Xavier Glorot和Yoshua Bengio提出的方法，考虑了输入(<span class="math inline">\(n_{in}\)</span>)和输出(<span class="math inline">\(n_{out}\)</span>)维度以缓解梯度消失/爆炸问题。核心是保持每层激活值和梯度方差的适当水平。<em>主要用于sigmoid、tanh等饱和型(saturating)激活函数时效果显著。</em></li>
</ul></li>
<li><p><strong>Kaiming 初始化 (He, 2015):</strong> <span class="math inline">\(std = \sqrt{\frac{2}{n_{in}}}\)</span></p>
<ul>
<li>Kaiming He等人提出的方法，考虑了ReLU激活函数的特性（将负输入变为0）进行设计。ReLU倾向于使激活值的方差减半，因此使用比Xavier初始化更大的方差(<span class="math inline">\(\sqrt{2}\)</span>倍)来补偿这一点。<em>这减少了“死神经元(dead neuron)”问题，并在深度网络中实现稳定的训练，在使用ReLU系列激活函数时实际上已成为标准(de facto standard)。</em></li>
</ul></li>
</ul>
</section>
<section id="最新初始化-2023年以后" class="level5">
<h5 class="anchored" data-anchor-id="最新初始化-2023年以后">最新初始化 (2023年以后)</h5>
<ul>
<li><strong>L-Momentum 初始化 (Zhuang, 2024)</strong>
<ul>
<li><p>L-Momentum 初始化是2024年提出的一种最新的初始化方法，受现有基于动量的优化算法启发，控制初始权重矩阵的L-动量。</p></li>
<li><p><strong>公式:</strong></p>
<p><span class="math inline">\(W \sim U(-\sqrt{\frac{6}{n_{in}}}, \sqrt{\frac{6}{n_{in}}})\)</span> <span class="math inline">\(W = W \cdot \sqrt{\frac{\alpha}{Var(W)}}\)</span></p>
<p>其中<span class="math inline">\(U\)</span>是均匀分布，<span class="math inline">\(\alpha\)</span>表示L-动量的值，通常使用优化器中的动量值的平方。</p></li>
<li><p>目标是在初期减少梯度波动，提供稳定的训练路径。</p></li>
<li><p>该方法适用于多种优化器及激活函数，并且实验结果显示它有助于使用较大的学习率、快速收敛和提高泛化性能。</p></li>
</ul></li>
</ul>
</section>
<section id="数学原理" class="level5">
<h5 class="anchored">数学原理</h5>
<p>大多数现代初始化方法都遵循以下三个核心原则（明确或隐含）：</p>
<ol type="1">
<li><p><strong>方差保持 (Variance Preservation):</strong> 前向传播时激活值的方差和反向传播时梯度的方差在各层中应保持恒定。</p>
<p><span class="math inline">\(Var(y) \approx Var(x)\)</span></p>
<p>这有助于防止信号变得过大或过小，从而实现稳定的训练。</p></li>
<li><p><strong>谱控制 (Spectral Control):</strong> 控制权重矩阵的奇异值(singular value)分布，以确保学习过程中的数值稳定性。</p>
<p><span class="math inline">\(\sigma_{max}(W) / \sigma_{min}(W) \leq C\)</span></p>
<p>这在循环神经网络(RNN)等需要反复乘以权重矩阵的结构中尤为重要。</p></li>
<li><p><strong>表达力优化 (Expressivity Optimization):</strong> 通过最大化权重矩阵的有效秩(effective rank)，使网络具有足够的表达能力。</p>
<p><span class="math inline">\(rank_{eff}(W) = \frac{\sum_i \sigma_i}{\max_i \sigma_i}\)</span> <em>最近的研究努力满足这些原则。</em></p></li>
</ol>
<p>总之，初始化方法应谨慎选择，需考虑模型的规模、结构、激活函数以及优化算法之间的相互作用。这是因为这对模型的学习速度、稳定性和最终性能有重大影响。</p>
<div class="callout callout-style-default callout-note callout-titled" title="点击查看内容（深入探讨：深度神经网络初始化的数学原理和最新技术）">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
点击查看内容（深入探讨：深度神经网络初始化的数学原理和最新技术）
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<section id="深度神经网络初始化的数学原理和最新技术" class="level3">
<h3 class="anchored" data-anchor-id="深度神经网络初始化的数学原理和最新技术">深度神经网络初始化的数学原理和最新技术</h3>
<section id="方差保持原则-variance-preservation-principle" class="level4">
<h4 class="anchored" data-anchor-id="方差保持原则-variance-preservation-principle">1. 方差保持原则 (Variance Preservation Principle)</h4>
<section id="理论基础" class="level5">
<h5 class="anchored" data-anchor-id="理论基础">1.1 理论基础</h5>
<p>随着神经网络深度的增加，前向传播(forward propagation)及反向传播(backpropagation)过程中保持信号的统计特性（特别是方差）非常重要。这可以防止信号消失(vanishing)或爆炸(exploding)，从而实现稳定的训练。</p>
<p>设第<span class="math inline">\(l\)</span>层的激活值为<span class="math inline">\(h_l\)</span>，权重矩阵为<span class="math inline">\(W_l\)</span>，偏置为<span class="math inline">\(b_l\)</span>，激活函数为<span class="math inline">\(f\)</span>，则前向传播可表示如下：</p>
<p><span class="math inline">\(h_l = f(W_l h_{l-1} + b_l)\)</span></p>
<p>假设输入信号<span class="math inline">\(h_{l-1} \in \mathbb{R}^{n_{in}}\)</span>的每个元素是均值0、方差<span class="math inline">\(\sigma^2_{h_{l-1}}\)</span>的独立随机变量，权重矩阵<span class="math inline">\(W_l \in \mathbb{R}^{n_{out} \times n_{in}}\)</span>的每个元素是均值0、方差<span class="math inline">\(Var(W_l)\)</span>的独立随机变量，并且偏置<span class="math inline">\(b_l = 0\)</span>。<em>假设激活函数为线性时</em>，以下成立。</p>
<p><span class="math inline">\(Var(h_l) = n_{in} Var(W_l) Var(h_{l-1})\)</span> （其中<span class="math inline">\(n_{in}\)</span>是第<span class="math inline">\(l\)</span>层的输入维度）</p>
<p>为了保持激活值的方差，需要<span class="math inline">\(Var(h_l) = Var(h_{l-1})\)</span>，因此<span class="math inline">\(Var(W_l) = 1/n_{in}\)</span>。</p>
<p>在反向传播时，对于误差信号<span class="math inline">\(\delta_l = \frac{\partial L}{\partial h_l}\)</span>（其中<span class="math inline">\(L\)</span>是损失函数），存在以下关系：</p>
<p><span class="math inline">\(\delta_{l-1} = W_l^T \delta_l\)</span> （假设激活函数为线性）</p>
<p>因此，在反向传播时为了保持方差，需要<span class="math inline">\(Var(\delta_{l-1}) = n_{out}Var(W_l)Var(\delta_l)\)</span>，所以<span class="math inline">\(Var(W_l) = 1/n_{out}\)</span>。（其中<span class="math inline">\(n_{out}\)</span>是第<span class="math inline">\(l\)</span>层的输出维度）</p>
</section>
<section id="非线性激活函数扩展" class="level5">
<h5 class="anchored" data-anchor-id="非线性激活函数扩展">1.2 非线性激活函数扩展</h5>
<p><strong>ReLU 激活函数</strong></p>
<p>ReLU 函数（<span class="math inline">\(f(x) = max(0, x)\)</span>）会使输入的一半变为0，因此激活值的方差有减少的趋势。Kaiming He为此提出了以下的方差保持公式：</p>
<p><span class="math inline">\(Var(W_l) = \frac{2}{n_{in}} \quad (\text{ReLU 专用})\)</span></p>
<p>这是为了补偿通过ReLU时发生的方差减小。</p>
<p><strong>Leaky ReLU 激活函数</strong></p>
<p>对于Leaky ReLU（<span class="math inline">\(f(x) = max(\alpha x, x)\)</span>，<span class="math inline">\(\alpha\)</span>是一个较小的常数），通用公式如下：</p>
<p><span class="math inline">\(Var(W_l) = \frac{2}{(1 + \alpha^2) n_{in}}\)</span></p>
</section>
<section id="概率论方法-参考" class="level5">
<h5 class="anchored" data-anchor-id="概率论方法-参考">1.3 概率论方法 (参考)</h5>
<p>也可以使用Fisher Information Matrix (FIM)的逆矩阵进行初始化。FIM包含了参数空间中的曲率信息，利用这一点可以实现更有效的初始化。（更多详细内容参见参考文献[4] Martens, 2020）。</p>
</section>
</section>
<section id="谱控制-spectral-control" class="level4">
<h4 class="anchored" data-anchor-id="谱控制-spectral-control">2. 谱控制 (Spectral Control)</h4>
<section id="奇异值分解与学习动力学" class="level5">
<h5 class="anchored" data-anchor-id="奇异值分解与学习动力学">2.1 奇异值分解与学习动力学</h5>
<p>权重矩阵 <span class="math inline">\(W \in \mathbb{R}^{m \times n}\)</span> 的奇异值分解 (Singular Value Decomposition, SVD) 表示为 <span class="math inline">\(W = U\Sigma V^T\)</span>。其中 <span class="math inline">\(\Sigma\)</span> 是对角矩阵，其对角元素是 <span class="math inline">\(W\)</span> 的奇异值 (<span class="math inline">\(\sigma_1 \geq \sigma_2 \geq ... \geq 0\)</span>)。权重矩阵的最大奇异值 (<span class="math inline">\(\sigma_{max}\)</span>) 过大可能会导致梯度爆炸 (exploding gradient)，而最小奇异值 (<span class="math inline">\(\sigma_{min}\)</span>) 过小则可能导致梯度消失 (vanishing gradient)。</p>
<p>因此，控制奇异值的比例（条件数, condition number）<span class="math inline">\(\kappa = \sigma_{max}/\sigma_{min}\)</span> 非常重要。当 <span class="math inline">\(\kappa\)</span> 接近 1 时，可以保证更稳定的梯度流。</p>
<p><strong>定理 2.1 (Saxe et al., 2014)</strong>: 对于使用正交初始化 (orthogonal initialization) 的深层线性神经网络，如果每层的权重矩阵 <span class="math inline">\(W_l\)</span> 是正交矩阵，则输入对输出的雅可比矩阵 (Jacobian matrix) <span class="math inline">\(J\)</span> 的弗罗贝尼乌斯范数 (Frobenius norm) 保持为 1。</p>
<p><span class="math inline">\(||J||_F = 1\)</span></p>
<p>这有助于缓解非常深的网络中的梯度消失或爆炸问题。</p>
</section>
<section id="动态谱归一化" class="level5">
<h5 class="anchored" data-anchor-id="动态谱归一化">2.2 动态谱归一化</h5>
<p>Miyato 等人 (2018) 提出了 Spectral Normalization 技术，通过限制权重矩阵的谱范数（即最大奇异值）来提高 GAN 学习的稳定性。</p>
<p><span class="math inline">\(W_{SN} = \frac{W}{\sigma_{max}(W)}\)</span></p>
<p>该方法在 GAN 学习中特别有效，并且最近已被应用于 Vision Transformer 等其他模型。</p>
</section>
</section>
<section id="表达力优化-expressivity-optimization" class="level4">
<h4 class="anchored" data-anchor-id="表达力优化-expressivity-optimization">3. 表达力优化 (Expressivity Optimization)</h4>
<section id="有效秩理论" class="level5">
<h5 class="anchored" data-anchor-id="有效秩理论">3.1 有效秩理论</h5>
<p>权重矩阵 <span class="math inline">\(W\)</span> 能够表示多少不同的特征（feature）可以通过奇异值分布的均匀性来衡量。有效秩 (effective rank) 定义如下：</p>
<p><span class="math inline">\(\text{rank}_{eff}(W) = \exp\left( -\sum_{i=1}^r p_i \ln p_i \right) \quad \text{其中 } p_i = \frac{\sigma_i}{\sum_j \sigma_j}\)</span></p>
<p>这里 <span class="math inline">\(r\)</span> 是 <span class="math inline">\(W\)</span> 的秩，<span class="math inline">\(\sigma_i\)</span> 是第 <span class="math inline">\(i\)</span> 个奇异值，<span class="math inline">\(p_i\)</span> 是归一化的奇异值。有效秩是表示奇异值分布的指标，其值越大，表明奇异值越均匀分布，这也就意味着更高的表达力。</p>
</section>
<section id="初始化策略比较表" class="level5">
<h5 class="anchored" data-anchor-id="初始化策略比较表">3.2 初始化策略比较表</h5>
<table class="caption-top table">
<colgroup>
<col style="width: 15%">
<col style="width: 66%">
<col style="width: 7%">
<col style="width: 11%">
</colgroup>
<thead>
<tr class="header">
<th>初始化方法</th>
<th>奇异值分布</th>
<th>有效秩</th>
<th>适用架构</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Xavier</td>
<td>较快减少</td>
<td>低</td>
<td>浅层MLP</td>
</tr>
<tr class="even">
<td>Kaiming</td>
<td>为ReLU激活函数调整（相对较少减少）</td>
<td>中等</td>
<td>CNN</td>
</tr>
<tr class="odd">
<td>Orthogonal</td>
<td>所有奇异值均为1</td>
<td>最高</td>
<td>RNN/Transformer</td>
</tr>
<tr class="even">
<td>Emergence-Promoting</td>
<td>根据网络大小调整，相对缓慢减少（接近重尾分布）</td>
<td>高</td>
<td>大规模语言模型</td>
</tr>
</tbody>
</table>
</section>
<section id="emergence-promoting初始化" class="level5">
<h5 class="anchored" data-anchor-id="emergence-promoting初始化">3.3 Emergence-Promoting初始化</h5>
<p>Emergence-Promoting 初始化是为促进大规模语言模型(LLM)中的突发能力(emergent abilities)而提出的新技术。此方法通过调整网络大小（尤其是层的深度）来调节初始权重的方差，从而产生增加有效秩的效果。</p>
<p>Chen et al.&nbsp;(2023) 在Transformer模型中提出了如下缩放因子 <span class="math inline">\(\nu_l\)</span>：</p>
<p><span class="math inline">\(\nu_l = \frac{1}{\sqrt{d_{in}}} \left( 1 + \frac{\ln l}{\ln d} \right)\)</span></p>
<p>其中 <span class="math inline">\(d_{in}\)</span> 是输入维度，<span class="math inline">\(l\)</span> 是层的索引，<span class="math inline">\(d\)</span> 是模型深度。通过将此缩放因子乘以权重矩阵的标准差来初始化权重。即，从标准差为 <span class="math inline">\(\nu_l\)</span> 乘以 <span class="math inline">\(\sqrt{2/n_{in}}\)</span> 的正态分布中采样。</p>
</section>
</section>
<section id="初始化与优化的相互作用" class="level4">
<h4 class="anchored" data-anchor-id="初始化与优化的相互作用">4. 初始化与优化的相互作用</h4>
<section id="ntk理论扩展" class="level5">
<h5 class="anchored" data-anchor-id="ntk理论扩展">4.1 NTK理论扩展</h5>
<p>Jacot et al.(2018) 的神经切线核 (NTK) 理论是分析“非常宽”(infinitely wide)神经网络学习动力学的有效工具。根据NTK理论，在初始化时，非常宽的神经网络的Hessian矩阵的期望值与单位矩阵成正比。即，</p>
<p><span class="math inline">\(\lim_{n_{in} \to \infty} \mathbb{E}[\nabla^2 \mathcal{L}] \propto I\)</span>（在初始化时）</p>
<p>这暗示Xavier初始化在宽神经网络中提供了接近最优的初始化。</p>
</section>
<section id="元初始化策略" class="level5">
<h5 class="anchored" data-anchor-id="元初始化策略">4.2 元初始化策略</h5>
<p>像MetaInit (2023)这样的最新研究提出了通过元学习来学习给定架构和数据集的最佳初始化分布的方法。</p>
<p><span class="math inline">\(\theta_{init} = \arg\min_\theta \mathbb{E}_{\mathcal{T}}[\mathcal{L}(\phi_{fine-tune}(\theta, \mathcal{T}))]\)</span></p>
<p>其中 <span class="math inline">\(\theta\)</span> 是初始化参数，<span class="math inline">\(\mathcal{T}\)</span> 是学习任务，<span class="math inline">\(\phi\)</span> 表示使用 <span class="math inline">\(\theta\)</span> 初始化的模型进行微调的过程。</p>
</section>
</section>
<section id="参考基于物理的初始化技术" class="level4">
<h4 class="anchored" data-anchor-id="参考基于物理的初始化技术">5. （参考）基于物理的初始化技术</h4>
<p>近年来，还研究了受物理学原理启发的初始化方法。例如，提出了模仿量子力学中的薛定谔方程或流体力学中的纳维-斯托克斯方程的方法来优化层间信息流动。然而，这些方法仍处于研究初期阶段，其实用性尚未得到验证。</p>
</section>
<section id="实用建议" class="level4">
<h4 class="anchored" data-anchor-id="实用建议">6. 实用建议</h4>
<ol type="1">
<li><strong>CNN 架构：</strong> 通常建议使用 Kaiming 初始化（He 初始化）和批量归一化（Batch Normalization）。</li>
<li><strong>Transformer：</strong> 广泛使用 Scaled Orthogonal Initialization（奇异值调整）或 Xavier 初始化。</li>
<li><strong>LLM：</strong> 需要考虑专门针对大规模模型的初始化方法，如促进 Emergence 的初始化。</li>
<li><strong>神经 ODE：</strong> 除非有特殊情况，否则通常使用一般方法。</li>
</ol>
<hr>
</section>
</section>
<section id="参考文献" class="level3">
<h3 class="anchored" data-anchor-id="参考文献">参考文献</h3>
<ol type="1">
<li>He et al.&nbsp;“Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification”, ICCV 2015</li>
<li>Saxe et al.&nbsp;“Exact solutions to the nonlinear dynamics of learning in deep linear neural networks”, ICLR 2014</li>
<li>Jacot et al.&nbsp;“Neural Tangent Kernel: Convergence and Generalization in Neural Networks”, NeurIPS 2018</li>
<li>Martens, J. “New insights and perspectives on the natural gradient method.” The Journal of Machine Learning Research, 2020.</li>
<li>Chen et al.&nbsp;“Towards Understanding Large Language Models: A Transformative Reading List”, arXiv preprint arXiv:2307.12980, 2023. (与促进 Emergence 的初始化相关)</li>
<li>Miyato et al., “Spectral Normalization for Generative Adversarial Networks”, ICLR 2018</li>
</ol>
</section>
</div>
</div>
</div>
</section>
</section>
<section id="初始化方法实战比较分析" class="level3">
<h3 class="anchored" data-anchor-id="初始化方法实战比较分析">5.1.2 初始化方法：实战比较分析</h3>
<p>为了了解前面讨论的各种初始化方法在实际模型训练中会产生什么样的影响，我们将使用一个简单的模型进行对比实验。将在相同的条件下训练应用了不同初始化方法的模型，并分析其结果。评估指标如下。</p>
<table class="caption-top table">
<colgroup>
<col style="width: 17%">
<col style="width: 45%">
<col style="width: 37%">
</colgroup>
<thead>
<tr class="header">
<th>评估指标</th>
<th>意义</th>
<th>理想特性</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>错误率(%)</td>
<td>最终模型的预测性能 (越低越好)</td>
<td>越低越好</td>
</tr>
<tr class="even">
<td>收敛速度</td>
<td>学习曲线的斜率 (学习稳定性指标)</td>
<td>越低（越陡）收敛越快</td>
</tr>
<tr class="odd">
<td>平均条件数</td>
<td>权重矩阵的数值稳定性</td>
<td>越低（接近1）越稳定</td>
</tr>
<tr class="even">
<td>谱范数</td>
<td>权重矩阵的大小 (最大奇异值)</td>
<td>需要适当，既不太大也不太小</td>
</tr>
<tr class="odd">
<td>有效秩比</td>
<td>权重矩阵的表现力 (奇异值分布的均匀性)</td>
<td>越高越好</td>
</tr>
<tr class="even">
<td>执行时间(s)</td>
<td>学习时间</td>
<td>越低越好</td>
</tr>
</tbody>
</table>
<div id="cell-7" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_04.models.base <span class="im">import</span> SimpleNetwork</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_04.utils.data <span class="im">import</span> get_data_loaders, get_device</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_05.initialization.base <span class="im">import</span> init_methods</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_05.initialization.analysis <span class="im">import</span> analyze_initialization, create_detailed_analysis_table</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> get_device()</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize data loaders</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>train_dataloader, test_dataloader <span class="op">=</span> get_data_loaders()</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Detailed analysis of initialization methods</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> analyze_initialization(</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>    model_class<span class="op">=</span><span class="kw">lambda</span>: SimpleNetwork(act_func<span class="op">=</span>nn.PReLU()),</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>    init_methods<span class="op">=</span>init_methods,</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>    train_loader<span class="op">=</span>train_dataloader,</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>    test_loader<span class="op">=</span>test_dataloader,</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>    epochs<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>    device<span class="op">=</span>device</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Print detailed analysis results table</span></span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>create_detailed_analysis_table(results)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
Initialization method: lecun</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"f7415089fb524e58a3bc0648f03072a2","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/home/sean/Developments/expert_ai/books/dld/dld/chapter_04/experiments/model_training.py:320: UserWarning: std(): degrees of freedom is &lt;= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at ../aten/src/ATen/native/ReduceOps.cpp:1823.)
  'std': param.data.std().item(),</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Initialization method: xavier_normal</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"29ce93b749b746c7a45214d8a3d878f9","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Initialization method: kaiming_normal</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"fd9357f07d154648991dcd18979512aa","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Initialization method: orthogonal</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"bdc4bd358cf64c8ea5002b02fad46550","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Initialization method: scaled_orthogonal</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"e80c4b3368c249dd8edd6b4f4d125d62","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Initialization method: l-momentum</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"622b899e8de849aba61c8cec8bd7e078","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Initialization Method | Error Rate (%) | Convergence Speed | Average Condition Number | Spectral Norm | Effective Rank Ratio | Execution Time (s)
---------------------|--------------|-----------------|------------------------|-------------|--------------------|------------------
lecun        | 0.48 | 0.33 | 5.86 | 1.42 | 0.89 | 30.5
xavier_normal | 0.49 | 0.33 | 5.53 | 1.62 | 0.89 | 30.2
kaiming_normal | 0.45 | 0.33 | 5.85 | 1.96 | 0.89 | 30.1
orthogonal   | 0.49 | 0.33 | 1.00 | 0.88 | 0.95 | 30.0
scaled_orthogonal | 2.30 | 1.00 | 1.00 | 0.13 | 0.95 | 30.0
l-momentum   | nan | 0.00 | 5.48 | 19.02 | 0.89 | 30.1</code></pre>
</div>
</div>
<p>实验结果如下表所示。</p>
<table class="caption-top table">
<colgroup>
<col style="width: 16%">
<col style="width: 11%">
<col style="width: 13%">
<col style="width: 18%">
<col style="width: 10%">
<col style="width: 15%">
<col style="width: 14%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">初始化方法</th>
<th style="text-align: center;">错误率 (%)</th>
<th style="text-align: center;">收敛速度</th>
<th style="text-align: center;">平均条件数</th>
<th style="text-align: center;">谱范数</th>
<th style="text-align: center;">有效秩比</th>
<th style="text-align: center;">执行时间 (s)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">lecun</td>
<td style="text-align: center;">0.48</td>
<td style="text-align: center;">0.33</td>
<td style="text-align: center;">5.66</td>
<td style="text-align: center;">1.39</td>
<td style="text-align: center;">0.89</td>
<td style="text-align: center;">23.3</td>
</tr>
<tr class="even">
<td style="text-align: left;">xavier_normal</td>
<td style="text-align: center;">0.48</td>
<td style="text-align: center;">0.33</td>
<td style="text-align: center;">5.60</td>
<td style="text-align: center;">1.64</td>
<td style="text-align: center;">0.89</td>
<td style="text-align: center;">23.2</td>
</tr>
<tr class="odd">
<td style="text-align: left;">kaiming_normal</td>
<td style="text-align: center;">0.45</td>
<td style="text-align: center;">0.33</td>
<td style="text-align: center;">5.52</td>
<td style="text-align: center;">1.98</td>
<td style="text-align: center;">0.89</td>
<td style="text-align: center;">23.2</td>
</tr>
<tr class="even">
<td style="text-align: left;">orthogonal</td>
<td style="text-align: center;">0.49</td>
<td style="text-align: center;">0.33</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">0.88</td>
<td style="text-align: center;">0.95</td>
<td style="text-align: center;">23.3</td>
</tr>
<tr class="odd">
<td style="text-align: left;">scaled_orthogonal</td>
<td style="text-align: center;">2.30</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">0.13</td>
<td style="text-align: center;">0.95</td>
<td style="text-align: center;">23.3</td>
</tr>
<tr class="even">
<td style="text-align: left;">l-momentum</td>
<td style="text-align: center;">nan</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">5.78</td>
<td style="text-align: center;">20.30</td>
<td style="text-align: center;">0.89</td>
<td style="text-align: center;">23.2</td>
</tr>
</tbody>
</table>
<p>实验结果中的关键点如下。</p>
<ol type="1">
<li><p><strong>Kaiming 初始化的优秀性能:</strong> Kaiming 初始化显示出最低的错误率，为 0.45%。这表明它与 ReLU 激活函数的最佳组合，再次确认了在使用 ReLU 类激活函数时 Kaiming 初始化的有效性。</p></li>
<li><p><strong>Orthogonal 系列的稳定性:</strong> Orthogonal 初始化表现出最优秀的数值稳定性，条件数为 1.00。这意味着学习过程中梯度不会失真并能良好传播，特别是对于权重矩阵重复相乘的模型（如循环神经网络 RNN）来说非常重要。<em>然而，在本实验中错误率相对较高，这可能是由于所使用的模型（简单的 MLP）的特点所致。</em></p></li>
<li><p><strong>Scaled Orthogonal 初始化的问题:</strong> Scaled Orthogonal 初始化显示出非常高的错误率 2.30%。这表明该初始化方法可能不适用于给定的模型和数据集，或者需要额外的超参数调整。<em>可能存在缩放因子 (scaling factor) 过小导致学习无法正常进行的情况。</em></p></li>
<li><p><strong>L-Momentum 初始化的不稳定性</strong>: L-Momentum 的误差率和收敛速度分别为 nan 和 0.00，表明完全没有进行学习。谱范数为 20.30 非常高，可能是由于权重初始值太大而导致发散。</p></li>
</ol>
</section>
<section id="实践建议及额外考虑事项" class="level3">
<h3 class="anchored" data-anchor-id="实践建议及额外考虑事项">5.1.3 实践建议及额外考虑事项</h3>
<p>深度学习模型初始化是一个需要仔细选择的<em>超参数</em>，它要考虑模型的架构、激活函数、优化算法以及数据集的特性。以下是在实践中选择初始化方法时应考虑的事项。</p>
<section id="基本原则" class="level5">
<h5 class="anchored" data-anchor-id="基本原则">基本原则</h5>
<ul>
<li><strong>ReLU 类激活函数:</strong>
<ul>
<li><strong>Kaiming 初始化 (He 初始化):</strong> 当前使用 ReLU 及其变体（如 Leaky ReLU, ELU, SELU 等）时最常用的初始化方法。它不仅实验结果良好，而且具有坚实的理论基础（方差保持）。</li>
<li><strong>L-Momentum Initialization</strong>: 如果使用 Adam, AdamW 等 Momentum 类优化器，则可以考虑。</li>
</ul></li>
<li><strong>Sigmoid, Tanh 激活函数:</strong>
<ul>
<li><strong>Xavier 初始化 (Glorot 初始化):</strong> 这些激活函数在输入值过大或过小时可能会出现梯度消失问题（vanishing gradient problem），因此 Xavier 初始化仍然是一个有效的选择。</li>
</ul></li>
<li><strong>循环神经网络 (RNN, LSTM, GRU):</strong>
<ul>
<li><strong>正交初始化:</strong> 对于具有循环连接的 RNN 类模型，保持权重矩阵的奇异值接近 1 是重要的。正交初始化可以保证这一点，从而缓解梯度爆炸/消失问题并帮助学习长期依赖性。</li>
<li><em>注意: 正交初始化通常应用于 RNN 的 hidden-to-hidden 权重矩阵，而 input-to-hidden 权重矩阵则使用其他初始化方法（如 Kaiming）。</em></li>
</ul></li>
</ul>
</section>
<section id="模型规模及特性" class="level5">
<h5 class="anchored" data-anchor-id="模型规模及特性">模型规模及特性</h5>
<ul>
<li><strong>一般深度神经网络 (50层以下):</strong>
<ul>
<li>通常情况下，Kaiming 初始化（ReLU 类）或 Xavier 初始化（Sigmoid/Tanh 类）就足够了。</li>
</ul></li>
<li><strong>非常深的神经网络 (50层以上):</strong>
<ul>
<li><strong>残差连接 (ResNet):</strong> 如果有残差连接，则 Kaiming 初始化效果很好。</li>
<li><strong>没有残差连接 (ResNet) 的情况</strong>: 在初始化时需要更加谨慎。可以考虑 Scaled Orthogonal, Fixup Initialization 等方法。</li>
</ul></li>
<li><strong>大规模模型 (1B+ 参数):</strong>
<ul>
<li><strong>L-Momentum Initialization</strong></li>
<li><strong>零初始化 (特定部分):</strong> 对于 Transformer 模型的某些部分（如 attention layer 的 output projection）进行零初始化可能有效。 （参考: Megatron-LM）</li>
<li><em>注意: 大规模模型容易导致学习不稳定，因此除了初始化外，还需要谨慎地组合学习率调度、梯度裁剪、正则化技术等。</em></li>
</ul></li>
</ul>
</section>
<section id="额外考虑事项" class="level5">
<h5 class="anchored" data-anchor-id="额外考虑事项">额外考虑事项</h5>
<ul>
<li><strong>批归一化 (Batch Normalization) / 层归一化 (Layer Normalization):</strong> 归一化技术 <em>稍微</em> 减少了初始化的重要性，但并不能完全替代。仍然建议选择合适的初始化。</li>
<li><strong>迁移学习 (Transfer Learning):</strong> 使用预训练(pretrained)模型时，通常会直接使用预训练的权重，或者仅对进行微调(fine-tuning)的层应用较小的学习率和Kaiming/Xavier初始化。</li>
<li><strong>优化算法:</strong> 根据所使用的优化器，有与其相匹配的良好初始化方法。例如，如果使用Adam优化器，则可以使用L-Momentum初始化。</li>
<li><strong>实验及验证:</strong> 最佳的初始化方法可能因问题和数据的不同而异。因此，尝试多种初始化方法，并选择在验证数据集(validation set)上表现最佳的方法非常重要。</li>
</ul>
<p>初始化是深度学习模型训练中的“隐形英雄”。正确的初始化可以决定模型训练的成败，对最大化模型性能和缩短训练时间起着关键作用。希望基于本节提供的指导和最新的研究趋势，您能找到最适合您的深度学习模型的初始化策略。</p>
</section>
</section>
</section>
<section id="优化算法深度学习训练的核心引擎" class="level2">
<h2 class="anchored" data-anchor-id="优化算法深度学习训练的核心引擎">5.2 优化算法：深度学习训练的核心引擎</h2>
<blockquote class="blockquote">
<p><strong>挑战:</strong> 如何解决梯度下降(Gradient Descent)陷入局部最小值(local minima)或学习速度过慢的问题？</p>
<p><strong>研究者的苦恼:</strong> 仅仅减少学习率是不够的。有些情况下，学习变得过于缓慢，耗时很长；有时则会发散，导致学习失败。寻找最优解的过程就像在雾中摸索下山一样艰难。尽管出现了动量、RMSProp、Adam 等多种优化算法，但仍然没有一种能够完美解决所有问题的万能解决方案。</p>
</blockquote>
<p>深度学习的迅猛发展不仅得益于模型结构的创新，还与<em>高效的优化算法的发展</em>密切相关。优化算法是自动寻找并加速损失函数(loss function)最小值的核心引擎。<em>这个引擎的工作效率和稳定性决定了深度学习模型的学习速度和最终性能。</em></p>
<section id="优化算法的发展与实现---持续的进化" class="level3">
<h3 class="anchored" data-anchor-id="优化算法的发展与实现---持续的进化">5.2.1 优化算法的发展与实现 - 持续的进化</h3>
<p>优化算法在过去的几十年中，<em>就像生物体一样进化</em>, 在解决三大核心任务的过程中不断发展。</p>
<ol type="1">
<li><strong>计算效率(Computational Efficiency):</strong> 需要在有限的计算资源下尽可能快速地完成学习。</li>
<li><strong>泛化性能(Generalization Performance):</strong> 不仅在训练数据上表现良好，在新数据上也应有良好的性能。</li>
<li><strong>可扩展性(Scalability):</strong> 即使模型和数据的规模扩大，也能稳定工作。</li>
</ol>
<p>每个挑战都催生了新的算法，寻找更好算法的竞争至今仍在继续。</p>
<section id="优化算法的历史" class="level5">
<h5 class="anchored" data-anchor-id="优化算法的历史">优化算法的历史</h5>
<ul>
<li><strong>1847年，柯西(Cauchy):</strong> 提出了梯度下降法(Gradient Descent)。通过沿着损失函数的梯度(gradieint)方向逐渐调整参数这一简单而强大的想法成为了现代深度学习优化的基础。</li>
<li><strong>1951年，罗宾斯(Robbins)和蒙罗(Monro):</strong> 建立了随机梯度下降法(Stochastic Gradient Descent, SGD)的数学基础。SGD 通过使用小批量(mini-batch)而非整个数据集来大大提高计算效率。</li>
<li><strong>1986年，鲁梅尔哈特(Rumelhart):</strong> 提出了动量(Momentum)方法，并与反向传播(Backpropagation)算法一起。动量为优化过程引入了惯性，缓解了SGD的振荡(oscillation)问题并提高了收敛速度。</li>
<li><strong>2011年，杜奇(Duchi):</strong> 发表了AdaGrad(自适应梯度)算法。AdaGrad 是按参数调整学习率的不同自适应学习率方法的先驱。</li>
<li><strong>2012年，辛顿(Hinton):</strong> 提出了RMSProp。 (在讲座笔记中介绍，未发表论文) RMSProp 改善了AdaGrad的学习率下降问题，使得更稳定的训练成为可能。</li>
<li><strong>2014年，金马(Kingma)和巴(Ba):</strong> 发表了Adam(自适应矩估计)算法。Adam 结合了动量和RMSProp的优点，成为了目前最广泛使用的优化算法之一。</li>
</ul>
<p>近期的优化算法正在以下三个主要方向上发展。 1. <strong>内存效率:</strong> Lion, AdaFactor 等专注于减少大规模模型（尤其是基于 Transformer 的模型）训练所需的内存使用量。 2. <strong>分布式学习优化:</strong> LAMB, LARS 等在使用多个 GPU/TPU 并行训练大型模型时提高效率。 3. <strong>领域/任务特定优化:</strong> Sophia, AdaBelief 等为特定问题领域（如：自然语言处理，计算机视觉）或特定模型结构提供优化性能。</p>
<p><em>特别是，随着大规模语言模型 (LLM) 和多模态模型的出现，有效地优化数十亿、数千亿参数，在有限内存环境下进行训练，并在分布式环境中稳定收敛变得尤为重要。这些挑战催生了8位优化、ZeRO 优化、梯度检查点等新技术的出现。</em></p>
</section>
<section id="基本优化算法" class="level5">
<h5 class="anchored" data-anchor-id="基本优化算法">基本优化算法</h5>
<p>在深度学习中，优化算法承担着寻找损失函数最小值的任务，即找到模型的最佳参数。每个算法都有其独特的特征和优缺点，根据问题的特点和模型结构选择合适的算法至关重要。</p>
<p><strong>SGD 和动量</strong></p>
<p>随机梯度下降法 (Stochastic Gradient Descent, SGD) 是最基础也是使用最广泛的优化算法之一。每一步都使用迷你批次 (mini-batch) 数据来计算损失函数的梯度，并沿着相反方向更新参数。</p>
<ul>
<li><p><strong>参数更新公式:</strong></p>
<p><span class="math display">\[w^{(t)} = w^{(t-1)} - \eta \cdot g^{(t)}\]</span></p>
<ul>
<li><span class="math inline">\(w^{(t)}\)</span>: 第 <span class="math inline">\(t\)</span> 步中的参数（权重）</li>
<li><span class="math inline">\(\eta\)</span>: 学习率 (learning rate)</li>
<li><span class="math inline">\(g^{(t)}\)</span>: 在第 <span class="math inline">\(t\)</span> 步中计算的梯度</li>
</ul></li>
</ul>
<p>动量 (Momentum) 是通过引入物理学中的动量概念来改进 SGD 的方法。使用过去梯度的指数加权平均值（exponential moving average）为优化路径赋予惯性，从而缓解 SGD 的振荡问题并提高收敛速度。</p>
<ul>
<li><p><strong>动量更新公式:</strong></p>
<p><span class="math display">\[v^{(t)} = \mu \cdot v^{(t-1)} + g^{(t)}\]</span></p>
<p><span class="math display">\[w^{(t)} = w^{(t-1)} - \eta \cdot v^{(t)}\]</span></p>
<ul>
<li><span class="math inline">\(\mu\)</span>: 动量系数（通常为 0.9 或 0.99）</li>
<li><span class="math inline">\(v^{(t)}\)</span>: 第 <span class="math inline">\(t\)</span> 步中的速度 (velocity)</li>
</ul></li>
</ul>
<p><em>用于学习的主要优化算法的实现代码包含在 <code>chapter_05/optimizer/</code> 目录中。</em> 下面是一个包括动量在内的 SGD 算法的学习用实现示例。所有优化算法类都继承自 <code>BaseOptimizer</code> 类，并为学习目的进行了简单的实现。（实际的 PyTorch 等库为了效率和通用性，实现了更为复杂的版本。）</p>
<div id="cell-11" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> typing <span class="im">import</span> Iterable, List, Optional</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_05.optimizers.basic <span class="im">import</span> BaseOptimizer</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SGD(BaseOptimizer):</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Implements SGD with momentum."""</span></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, params: Iterable[nn.Parameter], lr: <span class="bu">float</span>, </span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>                 maximize: <span class="bu">bool</span> <span class="op">=</span> <span class="va">False</span>, momentum: <span class="bu">float</span> <span class="op">=</span> <span class="fl">0.0</span>):</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(params, lr)</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.maximize <span class="op">=</span> maximize</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.momentum <span class="op">=</span> momentum</span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.momentum_buffer_list: List[Optional[torch.Tensor]] <span class="op">=</span> [<span class="va">None</span>] <span class="op">*</span> <span class="bu">len</span>(<span class="va">self</span>.params)</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>    <span class="at">@torch.no_grad</span>()</span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> step(<span class="va">self</span>) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i, p <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="va">self</span>.params):</span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a>            grad <span class="op">=</span> p.grad <span class="cf">if</span> <span class="kw">not</span> <span class="va">self</span>.maximize <span class="cf">else</span> <span class="op">-</span>p.grad</span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="va">self</span>.momentum <span class="op">!=</span> <span class="fl">0.0</span>:</span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a>                buf <span class="op">=</span> <span class="va">self</span>.momentum_buffer_list[i]</span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> buf <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a>                    buf <span class="op">=</span> torch.clone(grad).detach()</span>
<span id="cb12-22"><a href="#cb12-22" aria-hidden="true" tabindex="-1"></a>                <span class="cf">else</span>:</span>
<span id="cb12-23"><a href="#cb12-23" aria-hidden="true" tabindex="-1"></a>                    buf.mul_(<span class="va">self</span>.momentum).add_(grad, alpha<span class="op">=</span><span class="dv">1</span><span class="op">-</span><span class="va">self</span>.momentum)</span>
<span id="cb12-24"><a href="#cb12-24" aria-hidden="true" tabindex="-1"></a>                grad <span class="op">=</span> buf</span>
<span id="cb12-25"><a href="#cb12-25" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.momentum_buffer_list[i] <span class="op">=</span> buf</span>
<span id="cb12-26"><a href="#cb12-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-27"><a href="#cb12-27" aria-hidden="true" tabindex="-1"></a>            p.add_(grad, alpha<span class="op">=-</span><span class="va">self</span>.lr)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><strong>自适应学习率算法 (Adaptive Learning Rate Algorithms)</strong></p>
<p><em>深度学习模型的参数以不同的频率和重要性进行更新。自适应学习率算法是一种根据这些参数特性单独调整学习率的方法。</em></p>
<ul>
<li><p><strong>AdaGrad（自适应梯度，2011年）:</strong></p>
<ul>
<li><p><strong>核心思想:</strong> 对于经常更新的参数使用较小的学习率，对于很少更新的参数使用较大的学习率。</p></li>
<li><p><strong>公式:</strong></p>
<p><span class="math inline">\(w^{(t)} = w^{(t-1)} - \frac{\eta}{\sqrt{G^{(t)} + \epsilon}} \cdot g^{(t)}\)</span></p>
<ul>
<li><span class="math inline">\(G^{(t)}\)</span>: 过去梯度平方的累积和</li>
<li><span class="math inline">\(\epsilon\)</span>: 一个小常数，防止除以0（例如：<span class="math inline">\(10^{-8}\)</span>）</li>
</ul></li>
<li><p><strong>优点:</strong> 在处理稀疏数据(sparse data)时有效。</p></li>
<li><p><strong>缺点:</strong> 随着学习的进行，学习率单调递减，可能导致学习过早停止。</p></li>
</ul></li>
<li><p><strong>RMSProp（均方根传播，2012年）:</strong></p>
<ul>
<li><p><strong>核心思想:</strong> 为了解决AdaGrad的学习率下降问题，使用过去梯度平方的指数移动平均(exponential moving average)，而不是累积和。</p></li>
<li><p><strong>公式:</strong></p>
<p><span class="math inline">\(v^{(t)} = \beta \cdot v^{(t-1)} + (1-\beta) \cdot (g^{(t)})^2\)</span></p>
<p><span class="math inline">\(w^{(t)} = w^{(t-1)} - \frac{\eta}{\sqrt{v^{(t)} + \epsilon}} \cdot g^{(t)}\)</span></p>
<ul>
<li><span class="math inline">\(\beta\)</span>: 一个衰减率(decay rate)，用于调节过去梯度平方的影响（通常为0.9）</li>
</ul></li>
<li><p><strong>优点:</strong> 相比AdaGrad，学习率下降问题得到缓解，可以更长时间地进行有效学习。</p></li>
</ul></li>
</ul>
<p><strong>Adam（自适应矩估计，2014年）:</strong></p>
<p><em>Adam是目前使用最广泛的优化算法之一，结合了动量(Momentum)和RMSProp的思想。</em></p>
<ul>
<li><p><strong>核心思想:</strong></p>
<ul>
<li>动量: 使用过去梯度的指数移动平均(一阶矩)来提供惯性效果。</li>
<li>RMSProp: 使用过去梯度平方的指数移动平均(二阶矩)来调整每个参数的学习率。</li>
<li>偏差校正(Bias Correction): 校正在初始阶段，一阶和二阶矩向0偏移的情况。</li>
</ul></li>
<li><p><strong>公式:</strong></p>
<p><span class="math inline">\(m^{(t)} = \beta\_1 \cdot m^{(t-1)} + (1-\beta\_1) \cdot g^{(t)}\)</span></p>
<p><span class="math inline">\(v^{(t)} = \beta\_2 \cdot v^{(t-1)} + (1-\beta\_2) \cdot (g^{(t)})^2\)</span></p>
<p><span class="math inline">\(\hat{m}^{(t)} = \frac{m^{(t)}}{1-\beta\_1^t}\)</span></p>
<p><span class="math inline">\(\hat{v}^{(t)} = \frac{v^{(t)}}{1-\beta\_2^t}\)</span></p>
<p><span class="math inline">\(w^{(t)} = w^{(t-1)} - \eta \cdot \frac{\hat{m}^{(t)}}{\sqrt{\hat{v}^{(t)}} + \epsilon}\)</span></p>
<ul>
<li><span class="math inline">\(\beta_1\)</span>: 一阶矩(动量)的衰减率（通常为0.9）</li>
<li><span class="math inline">\(\beta_2\)</span>: 二阶矩(RMSProp)的衰减率（通常为0.999） 上述的优化算法各自具有独特的优缺点，需要根据问题的特点、模型结构、数据等因素选择合适的算法。Adam在许多情况下表现出色，但有时SGD + Momentum组合可能显示出更好的泛化性能，或者在特定问题中其他自适应学习率算法（如：RMSProp）可能更有效。因此，通过实验找到最佳算法非常重要。</li>
</ul></li>
</ul>
</section>
<section id="现代优化算法更快更高效更大规模模型" class="level5">
<h5 class="anchored">现代优化算法：更快、更高效、更大规模模型</h5>
<p>随着深度学习模型和数据集的规模呈爆炸性增长，对支持<em>内存效率、快速收敛速度以及大规模分布式学习</em>的新优化算法的需求日益增加。以下是最新的几种算法，旨在满足这些需求。</p>
<ul>
<li><p><strong>Lion（演化符号动量，2023）:</strong></p>
<ul>
<li><strong>核心思想：</strong> 由Google Research通过程序搜索发现的算法，与Adam类似使用动量，但只使用梯度的符号进行更新。也就是说，忽略梯度的大小，仅考虑方向。</li>
<li><strong>优点：</strong>
<ul>
<li>内存使用量少于Adam（无需存储二阶矩）。</li>
<li>对所有参数执行相同大小的更新，因此在具有稀疏梯度的问题中（如自然语言处理）效果良好。</li>
<li>可以使用比Adam更大的学习率。</li>
<li>实证上，在许多情况下性能优于AdamW。</li>
</ul></li>
<li><strong>缺点：</strong>
<ul>
<li>忽略了梯度大小信息，因此在某些问题中可能收敛速度慢于Adam或性能较低。</li>
<li>学习率调优更为敏感。</li>
<li>更详细的内容请参阅深入探讨。</li>
</ul></li>
</ul></li>
<li><p><strong>Sophia（二阶截断随机优化，2023）:</strong></p>
<ul>
<li><strong>核心思想：</strong> 利用二阶导数信息（Hessian矩阵），但为了减少计算成本，仅估计并使用Hessian的对角成分，并在更新中应用裁剪以提高稳定性。</li>
<li><strong>优点：</strong> 比Adam更快收敛和更稳定的训练</li>
<li><strong>缺点：</strong> 需要调优更多超参数（如Hessian估计频率、裁剪阈值）。</li>
<li>更详细的内容请参阅深入探讨。</li>
</ul></li>
<li><p><strong>AdaFactor（2018）:</strong></p>
<ul>
<li><strong>核心思想：</strong> 提出此算法是为了减少大规模模型（特别是Transformer）的内存使用量，通过将Adam中的二阶矩矩阵近似为低维矩阵的乘积。</li>
<li><strong>优点：</strong> 内存使用量远少于Adam。</li>
<li><strong>缺点：</strong> 由于对二阶矩信息进行了近似，在某些问题中性能可能低于Adam。</li>
<li>更详细的内容请参阅深入探讨。</li>
</ul></li>
</ul>
<p>最近的研究表明，上述介绍的算法（Lion、Sophia、AdaFactor）在特定条件下可以超过现有的Adam/AdamW的性能。</p>
<ul>
<li><strong>Lion：</strong> 在使用大批次大小的学习过程中比AdamW更快，内存使用量更少，并且往往表现出更好的泛化性能。</li>
<li><strong>Sophia：</strong> （特别是在大规模语言模型中）预训练阶段可以比Adam更快收敛，并实现更低的困惑度（或更高的准确性）。</li>
<li><strong>AdaFactor：</strong> 在内存受限环境中学习大规模Transformer模型时，可以成为Adam的一个良好替代方案。 但是，没有一种“万能”的优化算法能够保证在所有问题上都表现出最佳性能。因此，在实际应用时需要综合考虑模型的大小、学习数据的特点、可用资源（内存、计算能力）、是否进行分布式学习等因素来选择合适的算法，并且<em>必须通过实验和验证找到最优的超参数</em>。</li>
</ul>
<p>现在我们来进行一个1个epoch的实验，看看效果如何。</p>
<div id="cell-14" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_04.models.base <span class="im">import</span> SimpleNetwork</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_04.utils.data <span class="im">import</span> get_data_loaders, get_device</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_05.optimizers.basic <span class="im">import</span> Adam, SGD</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_05.optimizers.advanced <span class="im">import</span> Lion, Sophia</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_04.experiments.model_training <span class="im">import</span> train_model  <span class="co"># Corrected import</span></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> get_device()</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> SimpleNetwork(act_func<span class="op">=</span>nn.ReLU(), hidden_shape<span class="op">=</span>[<span class="dv">512</span>, <span class="dv">64</span>]).to(device)</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize SGD optimizer</span></span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> SGD(params<span class="op">=</span>model.parameters(), lr<span class="op">=</span><span class="fl">1e-3</span>, momentum<span class="op">=</span><span class="fl">0.9</span>)</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a><span class="co"># # Initialize Adam optimizer</span></span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a><span class="co"># optimizer = Adam(params=model.parameters(), lr=1e-3, beta1=0.9, beta2=0.999, eps=1e-8)</span></span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a><span class="co"># # Initialize AdaGrad optimizer</span></span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a><span class="co"># optimizer = AdaGrad(params=model.parameters(), lr=1e-2, eps=1e-10)</span></span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a><span class="co"># # Initialize Lion optimizer</span></span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a><span class="co"># optimizer = Lion(params=model.parameters(), lr=1e-4,  betas=(0.9, 0.99), weight_decay=0.0)</span></span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize Sophia optimizer</span></span>
<span id="cb13-25"><a href="#cb13-25" aria-hidden="true" tabindex="-1"></a><span class="co"># optimizer = Sophia(params=model.parameters(), lr=1e-3, betas=(0.965, 0.99), rho=0.04, weight_decay=0.0, k=10)</span></span>
<span id="cb13-26"><a href="#cb13-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-27"><a href="#cb13-27" aria-hidden="true" tabindex="-1"></a>train_dataloader, test_dataloader <span class="op">=</span> get_data_loaders()</span>
<span id="cb13-28"><a href="#cb13-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-29"><a href="#cb13-29" aria-hidden="true" tabindex="-1"></a>train_model(model, train_dataloader, test_dataloader, device, optimizer<span class="op">=</span>optimizer, epochs<span class="op">=</span><span class="dv">1</span>, batch_size<span class="op">=</span><span class="dv">256</span>, save_dir<span class="op">=</span><span class="st">"./tmp/opts/ReLU"</span>, retrain<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
Starting training for SimpleNetwork-ReLU.</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"c406d6c56f5d409e87cd8759b19fd284","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Execution completed for SimpleNetwork-ReLU, Execution time = 7.4 secs</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="5">
<pre><code>{'epochs': [1],
 'train_losses': [2.2232478597005207],
 'train_accuracies': [0.20635],
 'test_losses': [2.128580910873413],
 'test_accuracies': [0.3466]}</code></pre>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled" title="点击以查看内容（深入探讨：现代优化算法的深度分析）">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
点击以查看内容（深入探讨：现代优化算法的深度分析）
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<section id="现代优化算法的深入分析" class="level3 callout-body-container callout-body">
<h3 class="anchored" data-anchor-id="现代优化算法的深入分析">现代优化算法的深入分析</h3>
<section id="lion-进化符号动量" class="level4">
<h4 class="anchored" data-anchor-id="lion-进化符号动量">Lion (进化符号动量)</h4>
<p>Lion 是 Google Research 通过 AutoML 技术发现的优化算法。它类似于 Adam，但不使用梯度的大小信息，而只使用其符号(sign)，这是其最大的特点。</p>
<p><strong>核心思想:</strong></p>
<ul>
<li><strong>符号下降:</strong> 使用梯度的符号来决定更新方向。这强制所有参数进行相同大小的更新，在具有稀疏梯度的问题中（例如：自然语言处理）效果很好。</li>
<li><strong>动量:</strong> 考虑之前的更新方向，以提高学习的稳定性和速度。</li>
</ul>
<p><strong>数学原理:</strong></p>
<ol type="1">
<li><p><strong>更新计算:</strong></p>
<p><span class="math inline">\(c\_t = \beta\_1 m\_{t-1} + (1 - \beta\_1) g\_t\)</span></p>
<ul>
<li><span class="math inline">\(c\_t\)</span>: 当前步骤的更新向量。动量(<span class="math inline">\(m\_{t-1}\)</span>)和当前梯度(<span class="math inline">\(g\_t\)</span>)的加权平均。</li>
<li><span class="math inline">\(\beta\_1\)</span>: 动量的指数衰减率（通常为 0.9 或 0.99）。</li>
</ul></li>
<li><p><strong>权重更新:</strong></p>
<p><span class="math inline">\(w\_{t+1} = w\_t - \eta \cdot \text{sign}(c\_t)\)</span></p>
<ul>
<li><span class="math inline">\(\eta\)</span>: 学习率</li>
<li><span class="math inline">\(\text{sign}(c\_t)\)</span>: <span class="math inline">\(c\_t\)</span>的每个元素的符号 (+1 或者 -1)。如果为 0，则保持不变。</li>
</ul></li>
<li><p><strong>动量更新:</strong></p>
<p><span class="math inline">\(m\_t = c\_t\)</span></p>
<ul>
<li>直接使用计算更新时的值作为下一步的动量。</li>
</ul></li>
</ol>
<p><strong>优点:</strong></p>
<ul>
<li><strong>内存效率:</strong> 不像 Adam 需要存储二阶矩（方差），因此内存使用量较少。</li>
<li><strong>计算效率:</strong> 符号运算比乘法运算成本更低。</li>
<li><strong>对稀疏性鲁棒:</strong> 所有参数都以相同大小更新，因此在具有稀疏梯度的问题中效果很好。</li>
</ul>
<p><strong>缺点:</strong></p>
<ul>
<li>忽略了梯度的大小信息，因此在某些问题上可能收敛速度较慢或性能较低。</li>
<li>学习率调优更加敏感。</li>
</ul>
<p><strong>参考:</strong></p>
<ul>
<li>分析表明 Lion 具有类似于 L1 正则化的效果。（详细内容需进一步研究）</li>
<li><a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://arxiv.org/abs/2302.06675">Chen et al., 2023</a> 的论文报告称，在训练 BERT-Large 模型时，Lion 相比 AdamW 最多快了 2 倍的收敛速度，并且内存使用量也减少了。但这是特定实验的结果，不一定适用于所有情况。</li>
</ul>
</section>
<section id="sophia-二阶裁剪随机优化" class="level4">
<h4 class="anchored" data-anchor-id="sophia-二阶裁剪随机优化">Sophia (二阶裁剪随机优化)</h4>
<p>Sophia 是一种利用二阶导数信息（Hessian 矩阵）来提高学习速度和稳定性的优化算法。然而，直接计算 Hessian 矩阵的计算成本非常高，因此 Sophia 通过改进 <em>Hutchinson’s method</em> 来估计 Hessian 的对角成分。</p>
<p><strong>核心思想:</strong></p>
<ul>
<li><strong>轻量级 Hessian 估计:</strong> 改进 Hutchinson’s method 以高效地估计 Hessian 矩阵的对角成分。
<ul>
<li>原始 Hutchinson’s method 使用 <span class="math inline">\(h\_t = \mathbb{E}[z\_t z\_t^T H\_t] = diag(H\_t)\)</span>，其中 z 是随机向量</li>
<li>改进: 使用协方差来减少方差。</li>
</ul></li>
<li><strong>裁剪(Clipping):</strong> 在使用估计的 Hessian 更新梯度之前，限制更新大小（clip）以提高学习的稳定性。</li>
</ul>
<ol type="1">
<li><p><strong>Hessian 对角线估计：</strong></p>
<ul>
<li><p>每步，从 {-1, +1} 中均匀分布选择随机向量 <span class="math inline">\(z_t\)</span> 的每个元素。</p></li>
<li><p>计算 Hessian 对角线的估计值 <span class="math inline">\(h_t\)</span> 如下：</p>
<p><span class="math inline">\(h_t = \beta_2 h_{t-1} + (1 - \beta_2) \text{diag}(H_t z_t) z_t^T\)</span></p>
<p>（其中 <span class="math inline">\(H_t\)</span> 是 t 步的 Hessian）</p></li>
<li><p>Sophia 使用指数移动平均（EMA）利用过去的估计值 (<span class="math inline">\(h_{t-1}\)</span>) 来减少 Hutchinson’s estimator 的方差。</p></li>
</ul></li>
<li><p><strong>更新计算：</strong></p>
<ul>
<li><span class="math inline">\(m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t\)</span> （动量）</li>
<li><span class="math inline">\(u_t = \text{clip}(m_t / (h_t + \epsilon), \rho)\)</span>
<ul>
<li><span class="math inline">\(u_t\)</span>: 除以 Hessian 后裁剪的更新。</li>
<li><span class="math inline">\(\text{clip}(x, \rho) = \text{sign}(x) \cdot \min(|x|, \rho)\)</span>.</li>
<li><span class="math inline">\(\rho\)</span>: 裁剪阈值（超参数）</li>
<li><span class="math inline">\(h_t + \epsilon\)</span> 是对 <span class="math inline">\(h_t\)</span> 的每个元素加上 <span class="math inline">\(\epsilon\)</span> 的操作</li>
</ul></li>
</ul></li>
<li><p><strong>权重更新：</strong></p>
<p><span class="math inline">\(w_{t+1} = w_t - \eta \cdot u_t\)</span></p>
<ul>
<li><span class="math inline">\(\eta\)</span>: 学习率</li>
</ul></li>
</ol>
<p><strong>优点：</strong></p>
<ul>
<li><strong>快速收敛：</strong> 利用二阶导数信息，可以比 Adam 更快地收敛。</li>
<li><strong>稳定性：</strong> 通过裁剪提高学习的稳定性。</li>
</ul>
<p><strong>缺点：</strong></p>
<ul>
<li>需要调整比 Adam 更多的超参数（<span class="math inline">\(\beta_1\)</span>, <span class="math inline">\(\beta_2\)</span>, <span class="math inline">\(\rho\)</span>）。</li>
<li>性能取决于 Hessian 估计的准确性。</li>
</ul>
<p><strong>参考：</strong></p>
<ul>
<li><a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://arxiv.org/abs/2305.14342">Li et al., 2023</a> 报告称，Sophia 在语言模型预训练（pre-training）中比 Adam 达到了更低的损失（loss），所需步数更少。 （使用准确率/perplexity 等指标）</li>
</ul>
</section>
<section id="adafactor" class="level4">
<h4 class="anchored" data-anchor-id="adafactor">AdaFactor</h4>
<p>AdaFactor 是一种内存高效的优化算法，用于大规模模型，特别是 Transformer 模型的学习。它类似于 Adam 使用自适应学习率，但通过改进存储二阶矩（方差）的方式来大幅减少内存使用。</p>
<p><strong>核心思想：</strong></p>
<ul>
<li><strong>矩阵分解 (Matrix Factorization)：</strong> 将二阶矩矩阵近似为两个低秩矩阵的乘积以减少内存使用量。</li>
<li><span class="math inline">\(v\_t\)</span> 的每个行和列的总和用两个向量 <span class="math inline">\(R\_t\)</span> (<span class="math inline">\(n \times 1\)</span>) 和 <span class="math inline">\(C\_t\)</span> (<span class="math inline">\(m \times 1\)</span>) 表示，而不是使用 <span class="math inline">\(v\_t\)</span>。
<ul>
<li><span class="math inline">\(R\_t = \beta\_{2t} R\_{t-1} + (1 - \beta\_{2t}) (\text{row\_sum}(g\_t^2)/m)\)</span></li>
<li><span class="math inline">\(C\_t = \beta\_{2t} C\_{t-1} + (1 - \beta\_{2t}) (\text{col\_sum}(g\_t^2)/n)\)</span></li>
</ul></li>
<li><span class="math inline">\(R\_t\)</span> 和 <span class="math inline">\(C\_t\)</span> 分别是对 <span class="math inline">\(g\_t^2\)</span> 的行(row)和列(column)的总和进行指数移动平均(exponential moving average)的结果。(<span class="math inline">\(\beta\_{2t}\)</span> 是调度参数)</li>
<li>通过 <span class="math inline">\(\hat{v\_t} = R\_t C\_t^T / (\text{sum}(R\_t) \cdot \text{sum}(C\_t))\)</span> 进行近似</li>
</ul>
<ol start="2" type="1">
<li><p><strong>更新计算：</strong></p>
<p><span class="math inline">\(u\_t =  g\_t / \sqrt{\hat{v\_t}}\)</span></p></li>
<li><p><strong>权重更新</strong> <span class="math inline">\(w\_{t+1} = w\_t - \eta \cdot u\_t\)</span></p></li>
</ol>
<p><strong>优点:</strong></p>
<ul>
<li><strong>内存效率:</strong> 只存储大小为 <span class="math inline">\(O(n+m)\)</span> 的向量，而不是大小为 <span class="math inline">\(O(nm)\)</span> 的二阶矩矩阵，从而大大减少了内存使用。</li>
<li><strong>大规模模型训练:</strong> 因其内存效率高，适合用于大规模模型的训练。</li>
</ul>
<p><strong>缺点:</strong></p>
<ul>
<li>由于二次矩信息被近似，在某些问题上可能性能低于 Adam。</li>
<li>可能会产生额外的计算成本，因为涉及到矩阵分解。</li>
</ul>
<p><strong>参考:</strong></p>
<ul>
<li><a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://arxiv.org/abs/1804.04235">Shazeer &amp; Stern, 2018</a> 报告称 AdaFactor 在训练变压器模型时，虽然减少了内存使用量，但仍能达到与 Adam 相似的性能。</li>
</ul>
</section>
<section id="其他值得参考的最新优化算法" class="level4">
<h4 class="anchored" data-anchor-id="其他值得参考的最新优化算法">其他值得参考的最新优化算法</h4>
<ul>
<li><strong>LAMB (层自适应矩估计批量训练):</strong> 是一种专门用于大规模批处理训练的算法。通过调整各层的学习率，使在较大的批次大小下也能实现稳定的学习。(参见: <a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://arxiv.org/abs/1904.00962">You et al., 2019</a>)</li>
<li><strong>LARS (层自适应率缩放):</strong> 类似于 LAMB，使用各层学习率调整，并在大批次训练中表现良好。主要应用于像 ResNet 这样的图像分类模型。(参见: <a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://arxiv.org/abs/1708.03888">You et al., 2017</a>)</li>
</ul>
</section>
</section>
</div>
</div>
</section>
</section>
<section id="优化训练比较" class="level3">
<h3 class="anchored" data-anchor-id="优化训练比较">5.2.2 优化训练比较</h3>
<p>优化算法的性能在任务和模型结构上会有很大差异。我们通过实验来分析这些特性。</p>
<section id="基本任务分析" class="level5">
<h5 class="anchored" data-anchor-id="基本任务分析">基本任务分析</h5>
<p>使用FashionMNIST数据集进行基本性能比较。该数据集简化了实际的服装图像分类问题，适合用于分析深度学习算法的基本特性。</p>
<div id="cell-17" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_05.experiments.basic <span class="im">import</span> run_basic_experiment</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_05.visualization.optimization <span class="im">import</span> plot_training_results</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_04.utils.data <span class="im">import</span> get_data_loaders</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_05.optimizers.basic <span class="im">import</span> SGD, Adam</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_05.optimizers.advanced <span class="im">import</span> Lion</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Device configuration</span></span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> torch.device(<span class="st">"cuda:0"</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">"cpu"</span>)</span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Data loaders</span></span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a>train_loader, test_loader <span class="op">=</span> get_data_loaders()</span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Optimizer dictionary</span></span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a>optimizers <span class="op">=</span> {</span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a>    <span class="st">'SGD'</span>: SGD,</span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Adam'</span>: Adam,</span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Lion'</span>: Lion</span>
<span id="cb17-20"><a href="#cb17-20" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb17-21"><a href="#cb17-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-22"><a href="#cb17-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Optimizer configurations</span></span>
<span id="cb17-23"><a href="#cb17-23" aria-hidden="true" tabindex="-1"></a>optimizer_configs <span class="op">=</span> {</span>
<span id="cb17-24"><a href="#cb17-24" aria-hidden="true" tabindex="-1"></a>    <span class="st">'SGD'</span>: {<span class="st">'lr'</span>: <span class="fl">0.01</span>, <span class="st">'momentum'</span>: <span class="fl">0.9</span>},</span>
<span id="cb17-25"><a href="#cb17-25" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Adam'</span>: {<span class="st">'lr'</span>: <span class="fl">0.001</span>},</span>
<span id="cb17-26"><a href="#cb17-26" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Lion'</span>: {<span class="st">'lr'</span>: <span class="fl">1e-4</span>}</span>
<span id="cb17-27"><a href="#cb17-27" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb17-28"><a href="#cb17-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-29"><a href="#cb17-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Run experiments</span></span>
<span id="cb17-30"><a href="#cb17-30" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> {}</span>
<span id="cb17-31"><a href="#cb17-31" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> name, config <span class="kw">in</span> optimizer_configs.items():</span>
<span id="cb17-32"><a href="#cb17-32" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Starting experiment with </span><span class="sc">{</span>name<span class="sc">}</span><span class="ss"> optimizer..."</span>)</span>
<span id="cb17-33"><a href="#cb17-33" aria-hidden="true" tabindex="-1"></a>    results[name] <span class="op">=</span> run_basic_experiment(</span>
<span id="cb17-34"><a href="#cb17-34" aria-hidden="true" tabindex="-1"></a>        optimizer_class<span class="op">=</span>optimizers[name],</span>
<span id="cb17-35"><a href="#cb17-35" aria-hidden="true" tabindex="-1"></a>        train_loader<span class="op">=</span>train_loader,</span>
<span id="cb17-36"><a href="#cb17-36" aria-hidden="true" tabindex="-1"></a>        test_loader<span class="op">=</span>test_loader,</span>
<span id="cb17-37"><a href="#cb17-37" aria-hidden="true" tabindex="-1"></a>        config<span class="op">=</span>config,</span>
<span id="cb17-38"><a href="#cb17-38" aria-hidden="true" tabindex="-1"></a>        device<span class="op">=</span>device,</span>
<span id="cb17-39"><a href="#cb17-39" aria-hidden="true" tabindex="-1"></a>        epochs<span class="op">=</span><span class="dv">20</span></span>
<span id="cb17-40"><a href="#cb17-40" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb17-41"><a href="#cb17-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-42"><a href="#cb17-42" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize training curves</span></span>
<span id="cb17-43"><a href="#cb17-43" aria-hidden="true" tabindex="-1"></a>plot_training_results(</span>
<span id="cb17-44"><a href="#cb17-44" aria-hidden="true" tabindex="-1"></a>    results,</span>
<span id="cb17-45"><a href="#cb17-45" aria-hidden="true" tabindex="-1"></a>    metrics<span class="op">=</span>[<span class="st">'loss'</span>, <span class="st">'accuracy'</span>, <span class="st">'gradient_norm'</span>, <span class="st">'memory'</span>],</span>
<span id="cb17-46"><a href="#cb17-46" aria-hidden="true" tabindex="-1"></a>    mode<span class="op">=</span><span class="st">"train"</span>,  <span class="co"># Changed mode to "train"</span></span>
<span id="cb17-47"><a href="#cb17-47" aria-hidden="true" tabindex="-1"></a>    title<span class="op">=</span><span class="st">'Optimizer Comparison on FashionMNIST'</span></span>
<span id="cb17-48"><a href="#cb17-48" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
Starting experiment with SGD optimizer...

==================================================
Optimizer: SGD
Initial CUDA Memory Status (GPU 0):
Allocated: 23.0MB
Reserved: 48.0MB
Model Size: 283.9K parameters
==================================================
</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"ada5e53909f54c17bd9780217a7549fc","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
==================================================
Final CUDA Memory Status (GPU 0):
Peak Allocated: 27.2MB
Peak Reserved: 48.0MB
Current Allocated: 25.2MB
Current Reserved: 48.0MB
==================================================


Starting experiment with Adam optimizer...

==================================================
Optimizer: Adam
Initial CUDA Memory Status (GPU 0):
Allocated: 25.2MB
Reserved: 48.0MB
Model Size: 283.9K parameters
==================================================
</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"672f6e9f6a2c4916aa04a216cf6fa722","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
==================================================
Final CUDA Memory Status (GPU 0):
Peak Allocated: 28.9MB
Peak Reserved: 50.0MB
Current Allocated: 26.3MB
Current Reserved: 50.0MB
==================================================


Starting experiment with Lion optimizer...

==================================================
Optimizer: Lion
Initial CUDA Memory Status (GPU 0):
Allocated: 24.1MB
Reserved: 50.0MB
Model Size: 283.9K parameters
==================================================
</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"df577f7a7c8042899f6580786f12b819","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
==================================================
Final CUDA Memory Status (GPU 0):
Peak Allocated: 27.2MB
Peak Reserved: 50.0MB
Current Allocated: 25.2MB
Current Reserved: 50.0MB
==================================================
</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="05_优化和可视化_files/figure-html/cell-7-output-8.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>实验结果展示了每个算法的特点。使用 FashionMNIST 数据集和 MLP 模型进行的实验的主要观察结果如下。</p>
<ol type="1">
<li><strong>收敛速度:</strong>
<ul>
<li>Adam 和 Lion 在学习初期非常快速地收敛。（在最初的几个 epoch 内损失急剧下降，准确性迅速增加）</li>
<li>SGD 显示出相对缓慢且稳定的收敛模式。</li>
</ul></li>
<li><strong>学习曲线稳定性:</strong>
<ul>
<li>Adam 的学习曲线非常平滑和稳定。</li>
<li>Lion 类似于 Adam 保持稳定，但在准确率曲线上有轻微波动。</li>
<li>SGD 在损失和准确率曲线上都有较大的波动。</li>
</ul></li>
<li><strong>内存使用量:</strong>
<ul>
<li>Lion 比 Adam 使用稍少的内存，但差异不大（Adam：约 26.2MB，Lion：约 25.2MB）。</li>
<li>SGD 是三者中内存使用最少的。</li>
</ul></li>
<li><strong>梯度范数:</strong>
<ul>
<li>Lion：初始梯度范数非常高（约 4.0），迅速下降，并在较低值（约 1.5）处稳定。（初期大步探索，快速接近最优解附近）</li>
<li>Adam：比 Lion 更小的初始梯度范数（约 2.0），迅速下降并在更低值（约 1.0）处稳定。（自适应学习率调整）</li>
<li>SGD：初始梯度范数最小（约 0.3），表现出较大的波动，并在较高值（约 2.0-2.5）处振动。（广泛区域探索，暗示可能存在 flat minima）</li>
</ul></li>
</ol>
<p>基础实验中 Adam 和 Lion 显示了快速的初期收敛速度，Adam 表现最为稳定的学习过程，Lion 使用稍少的内存，而 SGD 则倾向于进行广泛的范围探索。</p>
</section>
<section id="高级任务评估" class="level5">
<h5 class="anchored" data-anchor-id="高级任务评估">高级任务评估</h5>
<p>在 CIFAR-100 和 CNN/变压器模型中，优化算法之间的差异变得更加明显。</p>
<div id="cell-19" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_05.experiments.advanced <span class="im">import</span> run_advanced_experiment</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_05.visualization.optimization <span class="im">import</span> plot_training_results</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_04.utils.data <span class="im">import</span> get_data_loaders</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_05.optimizers.basic <span class="im">import</span> SGD, Adam</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_05.optimizers.advanced <span class="im">import</span> Lion</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Device configuration</span></span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> torch.device(<span class="st">"cuda:0"</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">"cpu"</span>)</span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Data loaders</span></span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a>train_loader, test_loader <span class="op">=</span> get_data_loaders(dataset<span class="op">=</span><span class="st">"CIFAR100"</span>)</span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Optimizer dictionary</span></span>
<span id="cb22-15"><a href="#cb22-15" aria-hidden="true" tabindex="-1"></a>optimizers <span class="op">=</span> {</span>
<span id="cb22-16"><a href="#cb22-16" aria-hidden="true" tabindex="-1"></a>    <span class="st">'SGD'</span>: SGD,</span>
<span id="cb22-17"><a href="#cb22-17" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Adam'</span>: Adam,</span>
<span id="cb22-18"><a href="#cb22-18" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Lion'</span>: Lion</span>
<span id="cb22-19"><a href="#cb22-19" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb22-20"><a href="#cb22-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-21"><a href="#cb22-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Optimizer configurations</span></span>
<span id="cb22-22"><a href="#cb22-22" aria-hidden="true" tabindex="-1"></a>optimizer_configs <span class="op">=</span> {</span>
<span id="cb22-23"><a href="#cb22-23" aria-hidden="true" tabindex="-1"></a>    <span class="st">'SGD'</span>: {<span class="st">'lr'</span>: <span class="fl">0.01</span>, <span class="st">'momentum'</span>: <span class="fl">0.9</span>},</span>
<span id="cb22-24"><a href="#cb22-24" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Adam'</span>: {<span class="st">'lr'</span>: <span class="fl">0.001</span>},</span>
<span id="cb22-25"><a href="#cb22-25" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Lion'</span>: {<span class="st">'lr'</span>: <span class="fl">1e-4</span>}</span>
<span id="cb22-26"><a href="#cb22-26" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb22-27"><a href="#cb22-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-28"><a href="#cb22-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Run experiments</span></span>
<span id="cb22-29"><a href="#cb22-29" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> {}</span>
<span id="cb22-30"><a href="#cb22-30" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> name, config <span class="kw">in</span> optimizer_configs.items():</span>
<span id="cb22-31"><a href="#cb22-31" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Starting experiment with </span><span class="sc">{</span>name<span class="sc">}</span><span class="ss"> optimizer..."</span>)</span>
<span id="cb22-32"><a href="#cb22-32" aria-hidden="true" tabindex="-1"></a>    results[name] <span class="op">=</span> run_advanced_experiment(</span>
<span id="cb22-33"><a href="#cb22-33" aria-hidden="true" tabindex="-1"></a>        optimizer_class<span class="op">=</span>optimizers[name],</span>
<span id="cb22-34"><a href="#cb22-34" aria-hidden="true" tabindex="-1"></a>        model_type<span class="op">=</span><span class="st">'cnn'</span>,</span>
<span id="cb22-35"><a href="#cb22-35" aria-hidden="true" tabindex="-1"></a>        train_loader<span class="op">=</span>train_loader,</span>
<span id="cb22-36"><a href="#cb22-36" aria-hidden="true" tabindex="-1"></a>        test_loader<span class="op">=</span>test_loader,</span>
<span id="cb22-37"><a href="#cb22-37" aria-hidden="true" tabindex="-1"></a>        config<span class="op">=</span>config,</span>
<span id="cb22-38"><a href="#cb22-38" aria-hidden="true" tabindex="-1"></a>        device<span class="op">=</span>device,</span>
<span id="cb22-39"><a href="#cb22-39" aria-hidden="true" tabindex="-1"></a>        epochs<span class="op">=</span><span class="dv">40</span></span>
<span id="cb22-40"><a href="#cb22-40" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb22-41"><a href="#cb22-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-42"><a href="#cb22-42" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize training curves</span></span>
<span id="cb22-43"><a href="#cb22-43" aria-hidden="true" tabindex="-1"></a>plot_training_results(</span>
<span id="cb22-44"><a href="#cb22-44" aria-hidden="true" tabindex="-1"></a>    results,</span>
<span id="cb22-45"><a href="#cb22-45" aria-hidden="true" tabindex="-1"></a>    metrics<span class="op">=</span>[<span class="st">'loss'</span>, <span class="st">'accuracy'</span>, <span class="st">'gradient_norm'</span>, <span class="st">'memory'</span>],</span>
<span id="cb22-46"><a href="#cb22-46" aria-hidden="true" tabindex="-1"></a>    mode<span class="op">=</span><span class="st">"train"</span>,</span>
<span id="cb22-47"><a href="#cb22-47" aria-hidden="true" tabindex="-1"></a>    title<span class="op">=</span><span class="st">'Optimizer Comparison on CIFAR100'</span></span>
<span id="cb22-48"><a href="#cb22-48" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Files already downloaded and verified
Files already downloaded and verified

Starting experiment with SGD optimizer...

==================================================
Optimizer: SGD
Initial CUDA Memory Status (GPU 0):
Allocated: 26.5MB
Reserved: 50.0MB
Model Size: 1194.1K parameters
==================================================
</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"f9006d867491454a9db530180a287b44","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
==================================================
Final CUDA Memory Status (GPU 0):
Peak Allocated: 120.4MB
Peak Reserved: 138.0MB
Current Allocated: 35.6MB
Current Reserved: 138.0MB
==================================================

Results saved to: SGD_cnn_20250225_161620.csv

Starting experiment with Adam optimizer...

==================================================
Optimizer: Adam
Initial CUDA Memory Status (GPU 0):
Allocated: 35.6MB
Reserved: 138.0MB
Model Size: 1194.1K parameters
==================================================
</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"dcd60bbfe3b64789828b28ff1a1c305e","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
==================================================
Final CUDA Memory Status (GPU 0):
Peak Allocated: 124.9MB
Peak Reserved: 158.0MB
Current Allocated: 40.2MB
Current Reserved: 158.0MB
==================================================

Results saved to: Adam_cnn_20250225_162443.csv

Starting experiment with Lion optimizer...

==================================================
Optimizer: Lion
Initial CUDA Memory Status (GPU 0):
Allocated: 31.0MB
Reserved: 158.0MB
Model Size: 1194.1K parameters
==================================================
</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"31f67588627840bd8e354958b5028454","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
==================================================
Final CUDA Memory Status (GPU 0):
Peak Allocated: 120.4MB
Peak Reserved: 158.0MB
Current Allocated: 35.6MB
Current Reserved: 158.0MB
==================================================

Results saved to: Lion_cnn_20250225_163259.csv</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="05_优化和可视化_files/figure-html/cell-8-output-8.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>实验结果是使用CIFAR-100数据集和CNN模型比较SGD、Adam、Lion优化算法的，展示了每种算法的特点。</p>
<ol type="1">
<li><p><strong>收敛速度及准确性:</strong></p>
<ul>
<li>SGD在40个epoch后仍表现出较低的准确性（约50%以下），收敛较慢。</li>
<li>Adam在接近20个epoch时达到约50%的准确性，相对快速收敛。</li>
<li>Lion比Adam更快地收敛，在40个epoch时达到约55%，准确度最高。</li>
</ul></li>
<li><p><strong>学习曲线稳定性:</strong></p>
<ul>
<li>Adam的Loss和Accuracy曲线都很稳定。</li>
<li>Lion与Adam类似地稳定，但Accuracy曲线上有轻微波动。</li>
<li>SGD的Loss和Accuracy曲线都有较大波动。</li>
</ul></li>
<li><p><strong>内存使用量:</strong></p>
<ul>
<li>Lion（约31MB）和SGD（约31MB）比Adam（约34MB）使用的内存略少。</li>
</ul></li>
<li><p><strong>梯度范数:</strong></p>
<ul>
<li>Lion: 初始梯度范数较高（约3.56），快速增加后减少，在10附近稳定。（初期大步探索）</li>
<li>Adam: 初始梯度范数比Lion小（约3.26），缓慢增加后趋于稳定。（稳定探索）</li>
<li>SGD: 初始梯度范数最小（约3.13），波动较大，维持在其他算法较高的值。</li>
</ul></li>
</ol>
<p>给定的实验条件下，<strong>Lion</strong>表现出最快的收敛速度和最高的准确性。<strong>Adam</strong>显示出稳定的學習曲线，而<strong>SGD</strong>则较慢且波动较大。内存使用量方面，Lion和SGD比Adam略少。</p>
<div id="cell-21" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_05.experiments.advanced <span class="im">import</span> run_advanced_experiment</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_05.visualization.optimization <span class="im">import</span> plot_training_results</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_04.utils.data <span class="im">import</span> get_data_loaders</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_05.optimizers.basic <span class="im">import</span> SGD, Adam</span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_05.optimizers.advanced <span class="im">import</span> Lion</span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Device configuration</span></span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> torch.device(<span class="st">"cuda:0"</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">"cpu"</span>)</span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Data loaders</span></span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a>train_loader, test_loader <span class="op">=</span> get_data_loaders(dataset<span class="op">=</span><span class="st">"CIFAR100"</span>)</span>
<span id="cb27-13"><a href="#cb27-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-14"><a href="#cb27-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Optimizer dictionary</span></span>
<span id="cb27-15"><a href="#cb27-15" aria-hidden="true" tabindex="-1"></a>optimizers <span class="op">=</span> {</span>
<span id="cb27-16"><a href="#cb27-16" aria-hidden="true" tabindex="-1"></a>    <span class="st">'SGD'</span>: SGD,</span>
<span id="cb27-17"><a href="#cb27-17" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Adam'</span>: Adam,</span>
<span id="cb27-18"><a href="#cb27-18" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Lion'</span>: Lion</span>
<span id="cb27-19"><a href="#cb27-19" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb27-20"><a href="#cb27-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-21"><a href="#cb27-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Optimizer configurations</span></span>
<span id="cb27-22"><a href="#cb27-22" aria-hidden="true" tabindex="-1"></a>optimizer_configs <span class="op">=</span> {</span>
<span id="cb27-23"><a href="#cb27-23" aria-hidden="true" tabindex="-1"></a>    <span class="st">'SGD'</span>: {<span class="st">'lr'</span>: <span class="fl">0.01</span>, <span class="st">'momentum'</span>: <span class="fl">0.9</span>},</span>
<span id="cb27-24"><a href="#cb27-24" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Adam'</span>: {<span class="st">'lr'</span>: <span class="fl">0.001</span>},</span>
<span id="cb27-25"><a href="#cb27-25" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Lion'</span>: {<span class="st">'lr'</span>: <span class="fl">1e-4</span>}</span>
<span id="cb27-26"><a href="#cb27-26" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb27-27"><a href="#cb27-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-28"><a href="#cb27-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Run experiments</span></span>
<span id="cb27-29"><a href="#cb27-29" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> {}</span>
<span id="cb27-30"><a href="#cb27-30" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> name, config <span class="kw">in</span> optimizer_configs.items():</span>
<span id="cb27-31"><a href="#cb27-31" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Starting experiment with </span><span class="sc">{</span>name<span class="sc">}</span><span class="ss"> optimizer..."</span>)</span>
<span id="cb27-32"><a href="#cb27-32" aria-hidden="true" tabindex="-1"></a>    results[name] <span class="op">=</span> run_advanced_experiment(</span>
<span id="cb27-33"><a href="#cb27-33" aria-hidden="true" tabindex="-1"></a>        optimizer_class<span class="op">=</span>optimizers[name],</span>
<span id="cb27-34"><a href="#cb27-34" aria-hidden="true" tabindex="-1"></a>        model_type<span class="op">=</span><span class="st">'transformer'</span>,</span>
<span id="cb27-35"><a href="#cb27-35" aria-hidden="true" tabindex="-1"></a>        train_loader<span class="op">=</span>train_loader,</span>
<span id="cb27-36"><a href="#cb27-36" aria-hidden="true" tabindex="-1"></a>        test_loader<span class="op">=</span>test_loader,</span>
<span id="cb27-37"><a href="#cb27-37" aria-hidden="true" tabindex="-1"></a>        config<span class="op">=</span>config,</span>
<span id="cb27-38"><a href="#cb27-38" aria-hidden="true" tabindex="-1"></a>        device<span class="op">=</span>device,</span>
<span id="cb27-39"><a href="#cb27-39" aria-hidden="true" tabindex="-1"></a>        epochs<span class="op">=</span><span class="dv">40</span></span>
<span id="cb27-40"><a href="#cb27-40" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb27-41"><a href="#cb27-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-42"><a href="#cb27-42" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize training curves</span></span>
<span id="cb27-43"><a href="#cb27-43" aria-hidden="true" tabindex="-1"></a>plot_training_results(</span>
<span id="cb27-44"><a href="#cb27-44" aria-hidden="true" tabindex="-1"></a>    results,</span>
<span id="cb27-45"><a href="#cb27-45" aria-hidden="true" tabindex="-1"></a>    metrics<span class="op">=</span>[<span class="st">'loss'</span>, <span class="st">'accuracy'</span>, <span class="st">'gradient_norm'</span>, <span class="st">'memory'</span>],</span>
<span id="cb27-46"><a href="#cb27-46" aria-hidden="true" tabindex="-1"></a>    mode<span class="op">=</span><span class="st">"train"</span>,</span>
<span id="cb27-47"><a href="#cb27-47" aria-hidden="true" tabindex="-1"></a>    title<span class="op">=</span><span class="st">'Optimizer Comparison on CIFAR100'</span></span>
<span id="cb27-48"><a href="#cb27-48" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Files already downloaded and verified
Files already downloaded and verified

Starting experiment with SGD optimizer...</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/home/sean/anaconda3/envs/DL/lib/python3.10/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True
  warnings.warn(</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
==================================================
Optimizer: SGD
Initial CUDA Memory Status (GPU 0):
Allocated: 274.5MB
Reserved: 318.0MB
Model Size: 62099.8K parameters
==================================================
</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"0194a84568dd47cc87bcbd8b8efcf1c7","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
==================================================
Final CUDA Memory Status (GPU 0):
Peak Allocated: 836.8MB
Peak Reserved: 906.0MB
Current Allocated: 749.5MB
Current Reserved: 906.0MB
==================================================

Results saved to: SGD_transformer_20250225_164652.csv

Starting experiment with Adam optimizer...

==================================================
Optimizer: Adam
Initial CUDA Memory Status (GPU 0):
Allocated: 748.2MB
Reserved: 906.0MB
Model Size: 62099.8K parameters
==================================================
</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"a67a0697ce6e40169da7fa60aa2dd45f","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
==================================================
Final CUDA Memory Status (GPU 0):
Peak Allocated: 1073.0MB
Peak Reserved: 1160.0MB
Current Allocated: 985.1MB
Current Reserved: 1160.0MB
==================================================

Results saved to: Adam_transformer_20250225_170159.csv

Starting experiment with Lion optimizer...

==================================================
Optimizer: Lion
Initial CUDA Memory Status (GPU 0):
Allocated: 511.4MB
Reserved: 1160.0MB
Model Size: 62099.8K parameters
==================================================
</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"7cdfacd5f0ef4c57b845cc96581dcd2e","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
==================================================
Final CUDA Memory Status (GPU 0):
Peak Allocated: 985.1MB
Peak Reserved: 1160.0MB
Current Allocated: 748.2MB
Current Reserved: 1160.0MB
==================================================

Results saved to: Lion_transformer_20250225_171625.csv</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="05_优化和可视化_files/figure-html/cell-9-output-10.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>通常，变压器不是直接用于图像分类任务，而是以ViT（视觉变压器）等适应图像特征的结构形式使用。本实验作为优化算法比较的一个例子进行。变压器模型实验结果如下。</p>
<ol type="1">
<li>收敛性能：Adam表现出最快的初始收敛，其次是Lion和SGD。</li>
<li>稳定性和泛化能力：Adam达到了30.5%并表现出最稳定的性能。Lion在训练后期的测试准确率为28.88%，出现了一些性能下降。SGD以31.1%的准确性表现出了最好的泛化性能。</li>
<li>内存使用情况：Lion和SGD使用了相似的内存，而Adam相对使用了更多的内存。</li>
<li>梯度动力学：Adam的梯度范数从1.98逐渐减少到0.92。Lion从2.81开始下降至1.21，SGD从8.41开始减少到5.92，表现出最大的变化。</li>
</ol>
<p><strong>结论</strong> 在CIFAR-100数据集上的实验结果表明，尽管SGD的学习速度最慢，但其泛化性能最好。Adam虽然内存使用量大，却展示了最快的收敛和稳定的训练过程；Lion在内存效率和收敛速度方面表现出了平衡的性能。</p>
</section>
</section>
</section>
<section id="最优化过程的可视化与分析深度学习训练的黑箱透视" class="level2">
<h2 class="anchored" data-anchor-id="最优化过程的可视化与分析深度学习训练的黑箱透视">5.3 最优化过程的可视化与分析：深度学习训练的黑箱透视</h2>
<blockquote class="blockquote">
<p><strong>挑战问题:</strong> 如何在数百万、数千万维度的高维空间中有效地可视化和理解深度学习最优化过程？</p>
<p><strong>研究者的困惑:</strong> 深度学习模型的参数空间是人类难以直观想象的超高维空间。尽管研究人员开发了各种降维技术和可视化工具试图打开这个“黑箱”，但许多部分仍然笼罩在神秘之中。</p>
</blockquote>
<p>理解神经网络的学习过程对于有效的模型设计、最优化算法选择以及超参数调优至关重要。特别是，对损失函数（loss function）的几何特性（geometry）和最优化路径（optimization path）进行可视化和分析，可以为学习过程的动力学特性（dynamics）和稳定性（stability）提供重要的洞察力。<em>近年来，损失表面可视化的研究不仅为深度学习研究人员提供了揭开神经网络学习秘密的关键线索，还促进了更高效、稳定的算法及模型结构的发展。</em></p>
<p>在本节中，我们将探讨损失表面可视化的基本概念和最新技术，并通过这些技术分析深度学习训练过程中出现的各种现象（如：局部最小值、鞍点、最优化路径的特性）。特别是，我们重点讨论了模型结构（例如残差连接）对损失表面的影响，以及不同最优化算法导致的最优化路径差异。</p>
<section id="损失表面loss-landscape的理解深度学习模型的地图" class="level3">
<h3 class="anchored" data-anchor-id="损失表面loss-landscape的理解深度学习模型的地图">5.3.1 损失表面（Loss Landscape）的理解：深度学习模型的地图</h3>
<p>损失表面可视化是理解深度学习模型训练过程的关键工具。<em>就像通过地形图了解山的高度和山谷的位置一样，通过损失表面可视化可以直观地掌握参数空间中损失函数的变化。</em></p>
<p>2017年Goodfellow等人的研究表明，损失表面的平坦性（flatness）与模型的泛化（generalization）性能密切相关。（宽而平的最小值比窄而尖的最小值更有利于泛化） 2018年Li等人通过三维可视化展示了残差连接（residual connection）如何使损失表面变得平坦，从而促进学习。这些发现已成为ResNet等现代神经网络架构设计的核心基础。</p>
<section id="基本可视化技术" class="level5">
<h5 class="anchored" data-anchor-id="基本可视化技术">基本可视化技术</h5>
<ol type="1">
<li><p><strong>线性插值法 (Linear Interpolation):</strong></p>
<ul>
<li><p><em>概念:</em> 线性地结合两个不同模型（例如：训练前/后模型，收敛到不同局部最小值的模型）的权重，计算它们之间损失函数的值。</p></li>
<li><p><em>公式:</em></p>
<p><span class="math inline">\(w(\alpha) = (1-\alpha)w_1 + \alpha w_2\)</span></p>
<ul>
<li><span class="math inline">\(w_1\)</span>, <span class="math inline">\(w_2\)</span>: 两个模型的权重</li>
<li><span class="math inline">\(\alpha \in [0,1]\)</span>: 插值系数（当<span class="math inline">\(\alpha\)</span>为0时取<span class="math inline">\(w_1\)</span>，为1时取<span class="math inline">\(w_2\)</span>，介于两者之间则表示两组权重的线性组合）</li>
<li><span class="math inline">\(L(w(\alpha))\)</span>: 在插值权重<span class="math inline">\(w(\alpha)\)</span>处的损失值</li>
</ul></li>
</ul></li>
</ol>
<div id="cell-24" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> DataLoader, Subset</span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_05.visualization.loss_surface <span class="im">import</span> linear_interpolation, visualize_linear_interpolation</span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_04.utils.data <span class="im">import</span> get_dataset</span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_04.utils.metrics <span class="im">import</span> load_model</span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Linear Interpolation</span></span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-10"><a href="#cb34-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Device configuration</span></span>
<span id="cb34-11"><a href="#cb34-11" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> torch.device(<span class="st">"cuda"</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">"cpu"</span>)</span>
<span id="cb34-12"><a href="#cb34-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-13"><a href="#cb34-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the dataset</span></span>
<span id="cb34-14"><a href="#cb34-14" aria-hidden="true" tabindex="-1"></a>_, test_dataset <span class="op">=</span> get_dataset(dataset<span class="op">=</span><span class="st">"FashionMNIST"</span>)</span>
<span id="cb34-15"><a href="#cb34-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a small dataset</span></span>
<span id="cb34-16"><a href="#cb34-16" aria-hidden="true" tabindex="-1"></a>small_dataset <span class="op">=</span> Subset(test_dataset, torch.arange(<span class="dv">0</span>, <span class="dv">256</span>))</span>
<span id="cb34-17"><a href="#cb34-17" aria-hidden="true" tabindex="-1"></a>data_loader <span class="op">=</span> DataLoader(small_dataset, batch_size<span class="op">=</span><span class="dv">256</span>, shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb34-18"><a href="#cb34-18" aria-hidden="true" tabindex="-1"></a>loss_func <span class="op">=</span> nn.CrossEntropyLoss()</span>
<span id="cb34-19"><a href="#cb34-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-20"><a href="#cb34-20" aria-hidden="true" tabindex="-1"></a><span class="co"># model1, _ = load_model(model_file="SimpleNetwork-ReLU.pth", path="tmp/models/")</span></span>
<span id="cb34-21"><a href="#cb34-21" aria-hidden="true" tabindex="-1"></a><span class="co"># model2, _ = load_model(model_file="SimpleNetwork-Tanh.pth", path="tmp/models/")</span></span>
<span id="cb34-22"><a href="#cb34-22" aria-hidden="true" tabindex="-1"></a>model1, _ <span class="op">=</span> load_model(model_file<span class="op">=</span><span class="st">"SimpleNetwork-ReLU-epoch1.pth"</span>, path<span class="op">=</span><span class="st">"tmp/models/"</span>)</span>
<span id="cb34-23"><a href="#cb34-23" aria-hidden="true" tabindex="-1"></a>model2, _ <span class="op">=</span> load_model(model_file<span class="op">=</span><span class="st">"SimpleNetwork-ReLU-epoch15.pth"</span>, path<span class="op">=</span><span class="st">"tmp/models/"</span>)</span>
<span id="cb34-24"><a href="#cb34-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-25"><a href="#cb34-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-26"><a href="#cb34-26" aria-hidden="true" tabindex="-1"></a>model1 <span class="op">=</span> model1.to(device)</span>
<span id="cb34-27"><a href="#cb34-27" aria-hidden="true" tabindex="-1"></a>model2 <span class="op">=</span> model2.to(device)</span>
<span id="cb34-28"><a href="#cb34-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Linear interpolation</span></span>
<span id="cb34-29"><a href="#cb34-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-30"><a href="#cb34-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Test with a small dataset</span></span>
<span id="cb34-31"><a href="#cb34-31" aria-hidden="true" tabindex="-1"></a>_, test_dataset <span class="op">=</span> get_dataset(dataset<span class="op">=</span><span class="st">"FashionMNIST"</span>)</span>
<span id="cb34-32"><a href="#cb34-32" aria-hidden="true" tabindex="-1"></a>small_dataset <span class="op">=</span> Subset(test_dataset, torch.arange(<span class="dv">0</span>, <span class="dv">256</span>))</span>
<span id="cb34-33"><a href="#cb34-33" aria-hidden="true" tabindex="-1"></a>data_loader <span class="op">=</span> DataLoader(small_dataset, batch_size<span class="op">=</span><span class="dv">256</span>, shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb34-34"><a href="#cb34-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-35"><a href="#cb34-35" aria-hidden="true" tabindex="-1"></a>alphas, losses,  accuracies <span class="op">=</span> linear_interpolation(model1, model2, data_loader, loss_func, device)</span>
<span id="cb34-36"><a href="#cb34-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-37"><a href="#cb34-37" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> visualize_linear_interpolation(alphas, losses, accuracies,  <span class="st">"ReLU(1)-ReLU(15)"</span>,  size<span class="op">=</span>(<span class="dv">6</span>, <span class="dv">4</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="05_优化和可视化_files/figure-html/cell-10-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>线性插值中，α=0 表示第一个模型（1 个 epoch 训练），α=1 表示第二个模型（15 个 epoch 训练）的权重，中间值表示两个模型权重的线性组合。图中随着 α 值的增加，损失函数值呈现出下降的趋势，这表明随着训练的进行，模型向更好的最优解移动。然而，线性插值仅显示了高维权重空间的一个非常有限的截面，其局限性在于实际的最佳路径可能是非线性的，并且将 α 范围扩展到 [0,1] 之外会使解释变得困难。</p>
<p>使用贝塞尔曲线或样条进行非线性路径探索、通过 PCA 或 t-SNE 进行高维结构可视化可以提供更全面的信息。在实践中，线性插值可以用作初始分析工具，并且最好将 α 限制在 [0,1] 范围内或略作外推。结合其他可视化技术进行综合分析，在模型性能差异较大时需要额外的分析。</p>
<p>以下是 PCA 和 t-SNE 分析。</p>
<div id="cell-26" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_05.visualization.loss_surface <span class="im">import</span> analyze_weight_space, visualize_weight_space</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_04.utils.metrics <span class="im">import</span> load_model, load_models_by_pattern</span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a>models, labels <span class="op">=</span> load_models_by_pattern(</span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a>    activation_types<span class="op">=</span>[<span class="st">'ReLU'</span>],</span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># activation_types=['Tanh'],</span></span>
<span id="cb35-9"><a href="#cb35-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># activation_types=['GELU'],</span></span>
<span id="cb35-10"><a href="#cb35-10" aria-hidden="true" tabindex="-1"></a>    epochs<span class="op">=</span>[<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>,<span class="dv">5</span>,<span class="dv">6</span>,<span class="dv">7</span>,<span class="dv">8</span>,<span class="dv">9</span>,<span class="dv">10</span>,<span class="dv">11</span>,<span class="dv">12</span>,<span class="dv">13</span>,<span class="dv">14</span>,<span class="dv">15</span>]</span>
<span id="cb35-11"><a href="#cb35-11" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb35-12"><a href="#cb35-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-13"><a href="#cb35-13" aria-hidden="true" tabindex="-1"></a><span class="co"># PCA analysis</span></span>
<span id="cb35-14"><a href="#cb35-14" aria-hidden="true" tabindex="-1"></a>embedded_pca <span class="op">=</span> analyze_weight_space(models, labels, method<span class="op">=</span><span class="st">'pca'</span>)</span>
<span id="cb35-15"><a href="#cb35-15" aria-hidden="true" tabindex="-1"></a>visualize_weight_space(embedded_pca, labels, method<span class="op">=</span><span class="st">'PCA'</span>)</span>
<span id="cb35-16"><a href="#cb35-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"embedded_pca = </span><span class="sc">{</span>embedded_pca<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb35-17"><a href="#cb35-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-18"><a href="#cb35-18" aria-hidden="true" tabindex="-1"></a><span class="co"># t-SNE analysis</span></span>
<span id="cb35-19"><a href="#cb35-19" aria-hidden="true" tabindex="-1"></a>embedded_tsne <span class="op">=</span> analyze_weight_space(models, labels, method<span class="op">=</span><span class="st">'tsne'</span>, perplexity<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb35-20"><a href="#cb35-20" aria-hidden="true" tabindex="-1"></a>visualize_weight_space(embedded_tsne, labels, method<span class="op">=</span><span class="st">'t-SNE'</span>)</span>
<span id="cb35-21"><a href="#cb35-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"embedded_tsne = </span><span class="sc">{</span>embedded_tsne<span class="sc">}</span><span class="ss">"</span>) <span class="co"># Corrected: Print embedded_tsne, not embedded_pca</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="05_优化和可视化_files/figure-html/cell-11-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>embedded_pca = [[ 9.8299894e+00  2.1538167e+00]
 [-1.1609798e+01 -9.0169059e-03]
 [-1.1640446e+01 -1.2218434e-02]
 [-1.1667191e+01 -1.3469303e-02]
 [-1.1691980e+01 -1.5136327e-02]
 [-1.1714937e+01 -1.6765745e-02]
 [-1.1735878e+01 -1.8110925e-02]
 [ 9.9324265e+00  1.5862983e+00]
 [ 1.0126298e+01  4.7935897e-01]
 [ 1.0256655e+01 -2.8844318e-01]
 [ 1.0319887e+01 -6.6510278e-01]
 [ 1.0359785e+01 -8.9812231e-01]
 [ 1.0392080e+01 -1.0731999e+00]
 [ 1.0418671e+01 -1.2047548e+00]
 [-1.1575559e+01 -5.1336871e-03]]</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="05_优化和可视化_files/figure-html/cell-11-output-3.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>embedded_tsne = [[ 119.4719    -99.78837 ]
 [ 100.26558    66.285835]
 [  94.79294    62.795162]
 [  89.221085   59.253677]
 [  83.667984   55.70297 ]
 [  77.897224   52.022995]
 [  74.5897     49.913578]
 [ 123.20351  -100.34615 ]
 [ -70.45423   -65.66194 ]
 [ -65.55417   -68.90429 ]
 [ -60.166885  -72.466805]
 [ -54.70004   -76.077   ]
 [ -49.00131   -79.833694]
 [ -45.727974  -81.99213 ]
 [ 105.22419    69.45333 ]]</code></pre>
</div>
</div>
<p>PCA和t-SNE可视化展示了学习过程中模型权重空间的变化，将其投影到低维（2维）空间中。</p>
<ul>
<li><strong>PCA可视化:</strong>
<ul>
<li>点表示每个epoch的模型权重。 (紫色(epoch 1) -&gt; 红色(epoch 9) -&gt; 绿色系(epoch 10之后))</li>
<li>初始时广泛分布的权重随着学习的进行而聚集到特定区域。</li>
<li>特别是在从epoch 9过渡到epoch 10时，观察到了显著的变化。</li>
<li>PCA展示了权重空间中变化最大的方向（主成分）。</li>
</ul></li>
<li><strong>t-SNE可视化:</strong>
<ul>
<li>类似于PCA，点的颜色随epoch变化，显示了学习初期/中期/后期的权重分布变化。</li>
<li>t-SNE是一种<em>非线性</em>降维技术，侧重于保持高维空间中的<em>局部邻近关系</em>。</li>
<li>epoch 1-9组和epoch 10-15组相对清晰地分离，支持了PCA的结果。</li>
</ul></li>
</ul>
<p>通过这些可视化，我们可以直观地理解学习过程中模型权重的变化以及优化算法在权重空间中的探索。<em>特别是结合使用PCA和t-SNE，可以同时把握全局变化（PCA）和局部结构（t-SNE）。</em></p>
<ol start="2" type="1">
<li><strong>等高线图 (Contour Plot)</strong></li>
</ol>
<p>等高线图是在二维平面上绘制连接损失函数值相同的点（等高线），以可视化损失表面形态的方法。<em>就像地形图中的等高线一样，表示损失函数的“高低”.</em></p>
<p><em>一般步骤如下：</em></p>
<ol type="1">
<li><p><strong>设定基点:</strong> 选择作为基准的模型参数(<span class="math inline">\(w_0\)</span>)。(例如: 训练完成后的模型参数)</p></li>
<li><p><strong>选择方向向量:</strong> 选择两个方向向量(<span class="math inline">\(d_1\)</span>, <span class="math inline">\(d_2\)</span>)。<em>这些向量形成二维平面的基(basis)</em></p>
<ul>
<li><em>常见的选择:</em> 随机(random)方向，通过PCA(主成分分析)获得的主要成分方向，或者使用PyHessian等库获取的Hessian矩阵的最大特征值对应的前两个特征向量(eigenvector)。<em>在后一种情况下，表示损失函数值变化最剧烈的方向</em>。</li>
</ul></li>
<li><p><strong>参数扰动:</strong> 以基点<span class="math inline">\(w_0\)</span>为中心，沿选定的两个方向向量<span class="math inline">\(d_1\)</span>, <span class="math inline">\(d_2\)</span>扰动(perturb)参数。</p>
<p><span class="math inline">\(w(\lambda_1, \lambda_2) = w_0 + \lambda_1 d_1 + \lambda_2 d_2\)</span></p>
<ul>
<li><span class="math inline">\(\lambda_1\)</span>, <span class="math inline">\(\lambda_2\)</span>: 每个方向向量的标量系数 (例如: 从-0.2到0.2范围内以固定间隔选择值)</li>
</ul></li>
<li><p><strong>计算损失值:</strong> 对每个<span class="math inline">\((\lambda_1, \lambda_2)\)</span>组合，将扰动后的参数<span class="math inline">\(w(\lambda_1, \lambda_2)\)</span>应用于模型，并计算损失函数值。</p></li>
<li><p><strong>绘制等高线图:</strong> 使用<span class="math inline">\((\lambda_1, \lambda_2, L(w(\lambda_1, \lambda_2)))\)</span>数据绘制二维等高线图。(使用matplotlib的<code>contour</code>或<code>tricontourf</code>函数等)</p></li>
</ol>
<p><em>等高线图可以直观地显示损失表面的局部形态(local geometry)，还可以与优化算法的轨迹(trajectory)一起表示，以分析算法的工作方式。</em></p>
<div id="cell-29" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> DataLoader, Subset</span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_05.visualization.loss_surface <span class="im">import</span> hessian_eigenvectors, xy_perturb_loss, visualize_loss_surface, linear_interpolation</span>
<span id="cb38-6"><a href="#cb38-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_04.utils.data <span class="im">import</span> get_dataset</span>
<span id="cb38-7"><a href="#cb38-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_04.utils.metrics <span class="im">import</span> load_model</span>
<span id="cb38-8"><a href="#cb38-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_05.optimizers.basic <span class="im">import</span> SGD, Adam</span>
<span id="cb38-9"><a href="#cb38-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-10"><a href="#cb38-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Device configuration</span></span>
<span id="cb38-11"><a href="#cb38-11" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> torch.device(<span class="st">"cuda"</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">"cpu"</span>)</span>
<span id="cb38-12"><a href="#cb38-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-13"><a href="#cb38-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the dataset</span></span>
<span id="cb38-14"><a href="#cb38-14" aria-hidden="true" tabindex="-1"></a>_, test_dataset <span class="op">=</span> get_dataset(dataset<span class="op">=</span><span class="st">"FashionMNIST"</span>)</span>
<span id="cb38-15"><a href="#cb38-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a small dataset</span></span>
<span id="cb38-16"><a href="#cb38-16" aria-hidden="true" tabindex="-1"></a>small_dataset <span class="op">=</span> Subset(test_dataset, torch.arange(<span class="dv">0</span>, <span class="dv">256</span>))</span>
<span id="cb38-17"><a href="#cb38-17" aria-hidden="true" tabindex="-1"></a>data_loader <span class="op">=</span> DataLoader(small_dataset, batch_size<span class="op">=</span><span class="dv">256</span>, shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb38-18"><a href="#cb38-18" aria-hidden="true" tabindex="-1"></a>loss_func <span class="op">=</span> nn.CrossEntropyLoss()</span>
<span id="cb38-19"><a href="#cb38-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-20"><a href="#cb38-20" aria-hidden="true" tabindex="-1"></a>trained_model, _ <span class="op">=</span> load_model(model_file<span class="op">=</span><span class="st">"SimpleNetwork-ReLU.pth"</span>, path<span class="op">=</span><span class="st">"tmp/models/"</span>)</span>
<span id="cb38-21"><a href="#cb38-21" aria-hidden="true" tabindex="-1"></a><span class="co"># trained_model, _ = load_model(model_file="SimpleNetwork-Tanh.pth", path="tmp/models/")</span></span>
<span id="cb38-22"><a href="#cb38-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-23"><a href="#cb38-23" aria-hidden="true" tabindex="-1"></a>trained_model <span class="op">=</span> trained_model.to(device)</span>
<span id="cb38-24"><a href="#cb38-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-25"><a href="#cb38-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-26"><a href="#cb38-26" aria-hidden="true" tabindex="-1"></a><span class="co"># pyhessian</span></span>
<span id="cb38-27"><a href="#cb38-27" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> []  <span class="co"># List to store the calculated result sets</span></span>
<span id="cb38-28"><a href="#cb38-28" aria-hidden="true" tabindex="-1"></a>top_n <span class="op">=</span> <span class="dv">4</span>  <span class="co"># Must be an even number.  Each pair of eigenvectors is used.  2 is the minimum.  10 means 5 graphs.</span></span>
<span id="cb38-29"><a href="#cb38-29" aria-hidden="true" tabindex="-1"></a>top_eigenvalues, top_eignevectors <span class="op">=</span> hessian_eigenvectors(model<span class="op">=</span>trained_model, loss_func<span class="op">=</span>loss_func, data_loader<span class="op">=</span>data_loader, top_n<span class="op">=</span>top_n, is_cuda<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb38-30"><a href="#cb38-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-31"><a href="#cb38-31" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the scale with lambda.</span></span>
<span id="cb38-32"><a href="#cb38-32" aria-hidden="true" tabindex="-1"></a>lambda1, lambda2 <span class="op">=</span> np.linspace(<span class="op">-</span><span class="fl">0.2</span>, <span class="fl">0.2</span>, <span class="dv">40</span>).astype(np.float32), np.linspace(<span class="op">-</span><span class="fl">0.2</span>, <span class="fl">0.2</span>, <span class="dv">40</span>).astype(np.float32)</span>
<span id="cb38-33"><a href="#cb38-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-34"><a href="#cb38-34" aria-hidden="true" tabindex="-1"></a><span class="co"># If top_n=10, a total of 5 pairs of graphs can be drawn.</span></span>
<span id="cb38-35"><a href="#cb38-35" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(top_n <span class="op">//</span> <span class="dv">2</span>):</span>
<span id="cb38-36"><a href="#cb38-36" aria-hidden="true" tabindex="-1"></a>    x, y, z <span class="op">=</span> xy_perturb_loss(model<span class="op">=</span>trained_model, top_eigenvectors<span class="op">=</span>top_eignevectors[i<span class="op">*</span><span class="dv">2</span>:(i<span class="op">+</span><span class="dv">1</span>)<span class="op">*</span><span class="dv">2</span>], data_loader<span class="op">=</span>data_loader, loss_func<span class="op">=</span>loss_func, lambda1<span class="op">=</span>lambda1, lambda2<span class="op">=</span>lambda2, device<span class="op">=</span>device)</span>
<span id="cb38-37"><a href="#cb38-37" aria-hidden="true" tabindex="-1"></a>    data.append((x, y, z))</span>
<span id="cb38-38"><a href="#cb38-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-39"><a href="#cb38-39" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> visualize_loss_surface(data, <span class="st">"ReLU"</span>, color<span class="op">=</span><span class="st">"C0"</span>, alpha<span class="op">=</span><span class="fl">0.6</span>, plot_3d<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb38-40"><a href="#cb38-40" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> visualize_loss_surface(data, <span class="st">"ReLU"</span>, color<span class="op">=</span><span class="st">"C0"</span>, alpha<span class="op">=</span><span class="fl">0.6</span>, plot_3d<span class="op">=</span><span class="va">False</span>) <span class="co"># Changed "ReLu" to "ReLU" for consistency</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stderr">
<pre><code>/home/sean/anaconda3/envs/DL/lib/python3.10/site-packages/torch/autograd/graph.py:825: UserWarning: Using backward() with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak. (Triggered internally at ../torch/csrc/autograd/engine.cpp:1201.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="05_优化和可视化_files/figure-html/cell-12-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="05_优化和可视化_files/figure-html/cell-12-output-3.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>等高线图比简单的线性插值提供更丰富的<em>局部区域</em>信息。线性插值显示了两个模型之间的<em>一维路径</em>上的损失函数值的变化，而等高线图则在选定的两个方向(<span class="math inline">\(\lambda_1\)</span>, <span class="math inline">\(\lambda_2\)</span>)为轴的<em>二维平面</em>上可视化损失函数的变化。通过这种方式，可以观察到优化路径上的细微变化、<em>线性插值无法揭示的周围区域</em>的局部最小值(local minima)、鞍点(saddle point)的存在及其之间的障碍(barrier)等。</p>
</section>
</section>
<section id="损失表面分析的深度技术" class="level3">
<h3 class="anchored" data-anchor-id="损失表面分析的深度技术">5.3.2 损失表面分析的深度技术</h3>
<p>超越简单的可视化（线性插值、等高线图），正在研究更深入地理解深度学习模型损失表面(loss landscape)的高级分析技术。</p>
<ol type="1">
<li><p><strong>拓扑数据分析 (Topological Data Analysis, TDA):</strong></p>
<ul>
<li><em>核心思想:</em> 使用拓扑学(topology)工具来分析损失表面的连通性(connectivity)等“形状”。</li>
<li><em>主要技术:</em> 持久同调(persistent homology)，Mapper算法等。</li>
<li><em>应用:</em> 通过了解损失表面的复杂性、局部最小值(local minima)之间的连接结构、鞍点(saddle point)的特性等，可以获得关于学习动力学(learning dynamics)和泛化(generalization)性能的洞察力。<em>(详细内容请参阅“深入探讨：基于拓扑的损失表面分析”)</em></li>
</ul></li>
<li><p><strong>多尺度分析 (Multi-scale Analysis):</strong></p>
<ul>
<li><em>核心思想:</em> 在不同的尺度(scale)上分析损失表面，以掌握宏观结构和微观结构。</li>
<li><em>主要技术:</em> 小波变换(wavelet transform)，尺度空间理论(scale-space theory)等。</li>
<li><em>应用:</em> 通过分析损失表面的粗糙度(roughness)、关键特征(feature)在不同尺度上的分布等，可以理解优化算法的工作方式、学习难度等问题。<em>(详细内容请参阅“深入探讨：多尺度损失表面分析”)</em></li>
</ul></li>
</ol>
<p><em>这些高级分析技术提供了关于损失表面更抽象和定量的信息，有助于更深入地理解深度学习模型的学习过程，并为更好的模型设计及优化策略的制定做出贡献。</em></p>
<div id="cell-32" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn  <span class="co"># Import the nn module</span></span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> DataLoader, Subset  <span class="co"># Import DataLoader and Subset</span></span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_05.visualization.loss_surface <span class="im">import</span>  analyze_loss_surface_multiscale</span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_04.utils.data <span class="im">import</span> get_dataset  <span class="co"># Import get_dataset</span></span>
<span id="cb40-6"><a href="#cb40-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_04.utils.metrics <span class="im">import</span> load_model  <span class="co"># Import load_model</span></span>
<span id="cb40-7"><a href="#cb40-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-8"><a href="#cb40-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Device configuration</span></span>
<span id="cb40-9"><a href="#cb40-9" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> torch.device(<span class="st">"cuda"</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">"cpu"</span>)</span>
<span id="cb40-10"><a href="#cb40-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-11"><a href="#cb40-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Load dataset and create a small subset</span></span>
<span id="cb40-12"><a href="#cb40-12" aria-hidden="true" tabindex="-1"></a>_, test_dataset <span class="op">=</span> get_dataset(dataset<span class="op">=</span><span class="st">"FashionMNIST"</span>)</span>
<span id="cb40-13"><a href="#cb40-13" aria-hidden="true" tabindex="-1"></a>small_dataset <span class="op">=</span> Subset(test_dataset, torch.arange(<span class="dv">0</span>, <span class="dv">256</span>))</span>
<span id="cb40-14"><a href="#cb40-14" aria-hidden="true" tabindex="-1"></a>data_loader <span class="op">=</span> DataLoader(small_dataset, batch_size<span class="op">=</span><span class="dv">256</span>, shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb40-15"><a href="#cb40-15" aria-hidden="true" tabindex="-1"></a>loss_func <span class="op">=</span> nn.CrossEntropyLoss()</span>
<span id="cb40-16"><a href="#cb40-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-17"><a href="#cb40-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Load model (example: SimpleNetwork-ReLU)</span></span>
<span id="cb40-18"><a href="#cb40-18" aria-hidden="true" tabindex="-1"></a>model, _ <span class="op">=</span> load_model(model_file<span class="op">=</span><span class="st">"SimpleNetwork-ReLU.pth"</span>, path<span class="op">=</span><span class="st">"tmp/models/"</span>)</span>
<span id="cb40-19"><a href="#cb40-19" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> model.to(device)</span>
<span id="cb40-20"><a href="#cb40-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-21"><a href="#cb40-21" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> analyze_loss_surface_multiscale(model, data_loader, loss_func, device)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="05_优化和可视化_files/figure-html/cell-13-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>使用 <code>analyze_loss_surface_multiscale</code> 函数从多尺度角度分析、可视化了在 FashionMNIST 数据集上训练的 <code>SimpleNetwork-ReLU</code> 模型的损失表面。</p>
<p><strong>图形解释（基于小波变换）:</strong></p>
<ul>
<li><p><strong>Approx. Coefficients (近似系数):</strong> 表示损失表面的整体形态(global structure)。中心区域（低损失值）可能存在最小值。</p></li>
<li><p><strong>Detail Coeff Level 1/2 (细节系数):</strong> 表示更小规模的变化。“Level 1”表示中间尺度，“Level 2”显示最细微的尺度的起伏（局部最小值、鞍点、噪声等）。</p></li>
<li><p><strong>颜色:</strong> 深色（低损失），亮色（高损失）</p></li>
<li><p>根据 <code>analyze_loss_surface_multiscale</code> 函数的实现（小波函数、分解级别等），结果可能会有所不同。</p></li>
<li><p>该可视化仅显示了损失表面的<em>部分</em>，难以完全理解高维空间的复杂性。</p></li>
</ul>
<p>多尺度分析通过将损失表面分解为多个尺度，展示了单凭简单可视化难以发现的多层次结构。在大尺度上可以把握总体趋势，在小尺度上可以观察局部变化，有助于理解优化算法的行为、学习难度和泛化性能等。</p>
<div class="callout callout-style-default callout-note callout-titled" title="点击以查看内容（基于拓扑的损失表面分析）">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
点击以查看内容（基于拓扑的损失表面分析）
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<section id="基于拓扑的损失面分析" class="level3 callout-body-container callout-body">
<h3 class="anchored" data-anchor-id="基于拓扑的损失面分析">基于拓扑的损失面分析</h3>
<p>拓扑学(topology)是研究在连续变形下不变的几何性质的领域。在深度学习中，基于拓扑的分析通过分析损失面的连通性(connectivity)，孔洞(hole)，空腔(void)等拓扑特征(topological feature)，以获得关于学习动力学和泛化性能的洞察力。</p>
<p><strong>核心概念:</strong></p>
<ul>
<li><p><strong>Sublevel Set:</strong> 给定函数 <span class="math inline">\(f: \mathbb{R}^n \rightarrow \mathbb{R}\)</span> 和阈值 <span class="math inline">\(c\)</span>，定义为 <span class="math inline">\(f^{-1}((-\infty, c]) = {x \in \mathbb{R}^n | f(x) \leq c}\)</span> 的集合。在损失函数中，它表示具有特定损失值以下的参数空间区域。</p></li>
<li><p><strong>Persistent Homology:</strong> 跟踪sublevel set的变化，记录拓扑特征（0阶：连通分支，1阶：环路，2阶：空腔，…）的生成和消亡。</p>
<ul>
<li><strong>0阶特征 (Connected Components):</strong> 相互连接区域的数量。在损失面中，与局部最小值(local minima)的数量相关。</li>
<li><strong>1阶特征 (Loops):</strong> 闭合环路的数量。在损失面中，与围绕鞍点(saddle point)的路径的存在性相关。</li>
</ul></li>
<li><p><strong>Persistence Diagram:</strong> 将每个拓扑特征的生成(birth)和消亡(death)时刻的损失值以坐标平面上的点表示出来。点的 <span class="math inline">\(y\)</span> 坐标（<span class="math inline">\(\text{death} - \text{birth}\)</span>）表示该特征的“寿命(lifetime)”或“持久性(persistence)”，值越大，特征越稳定。</p></li>
<li><p><strong>Bottleneck Distance:</strong> 是测量两个persistence diagram之间距离的方法之一。通过找到两个图中的点之间的最优匹配(optimal matching)，计算匹配点之间的最大距离。</p></li>
</ul>
<p><strong>数学背景（简要）:</strong></p>
<ul>
<li><strong>Simplicial Complex:</strong> 点(vertex)、线(edge)、三角形(triangle)、四面体(tetrahedron)等的泛化概念，用于近似拓扑空间。</li>
<li><strong>Boundary Operator:</strong> 计算simplicial complex边界的操作符。</li>
<li><strong>Homology Group:</strong> 使用boundary operator定义的群(group)，表示拓扑空间中的“孔洞”。</li>
<li><strong>Persistent Homology Algorithm:</strong> 通过sublevel set filtration构建simplicial complex，并跟踪homology group的变化以计算persistence diagram。（详细内容参见参考文献[1]）</li>
</ul>
<p><strong>深度学习研究应用:</strong> * <strong>损失表面结构分析:</strong> 通过持久图可以了解损失表面的复杂性、局部最小值的数量及稳定性、鞍点的存在与否等。 * 示例: <a href="https://www.google.com/search?q=https://www.google.com/search%3Fq%3Dhttps://arxiv.org/abs/1803.06934">Gur-Ari et al., 2018</a> 计算了神经网络损失表面的持久图，表明宽(wide)网络比窄(narrow)网络具有更简单的拓扑结构。 * <strong>泛化性能预测:</strong> 持久图的特征（例如寿命最长的0维特征的寿命）可能与模型的泛化性能相关。 * 示例: <a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://proceedings.mlr.press/v162/perez22a.html">Perez et al., 2022</a> 提出了一种使用持久图特征来预测模型泛化性能的方法。 * <strong>模式连通性</strong>: 寻找连接不同局部最小值的路径，并分析这些路径上的能量屏障(energy barrier)。 * 示例: <a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://arxiv.org/abs/1802.10026">Garipov et al., 2018</a></p>
<p><strong>参考文献:</strong></p>
<ol type="1">
<li>Edelsbrunner, H., &amp; Harer, J. (2010). <em>计算拓扑学：导论</em>. American Mathematical Society.</li>
<li>Gur-Ari, G., Roberts, D. A., &amp; Dyer, E. (2018). <em>梯度下降发生在微小子空间中</em>. arXiv preprint arXiv:1812.04754.</li>
<li>Perez, D., Masoomi, A., DiCecco, J., &amp; Chwialkowski, K. (2022). <em>通过持久同调关系损失景观拓扑与泛化</em>. In International Conference on Machine Learning (pp.&nbsp;17953-17977). PMLR.</li>
<li>Garipov, T., Izmailov, P., Podoprikhin, D., Vetrov, D. P., &amp; Wilson, A. G. (2018). <em>损失表面、模式连通性和DNN的快速集成</em>. Advances in neural information processing systems, 31.</li>
</ol>
</section>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled" title="点击查看内容（深入探讨：多尺度损失表面分析）">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
点击查看内容（深入探讨：多尺度损失表面分析）
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<section id="多尺度损失表面分析" class="level3 callout-body-container callout-body">
<h3 class="anchored" data-anchor-id="多尺度损失表面分析">多尺度损失表面分析</h3>
<p>深度学习模型的损失表面具有不同尺度的特征。从大尺度的山谷（valley）和山脊（ridge）到小尺度的凸起（bump）和坑洞（hole），各种几何结构都会影响学习过程。多尺度分析是将这些不同尺度的特征分离并进行分析的方法。</p>
<p><strong>核心思想:</strong></p>
<ul>
<li><p><strong>小波变换 (Wavelet Transform):</strong> 小波变换是一种数学工具，可以将信号分解为不同的频率成分。将其应用于损失函数时，可以分离出不同尺度的特征。</p>
<ul>
<li><p><strong>连续小波变换 (Continuous Wavelet Transform, CWT):</strong></p>
<p><span class="math inline">\(W(a, b) = \int\_{-\infty}^{\infty} f(x) \psi\_{a,b}(x) dx\)</span></p>
<ul>
<li><span class="math inline">\(f(x)\)</span>: 分析对象函数（损失函数）</li>
<li><span class="math inline">\(\psi\_{a,b}(x) = \frac{1}{\sqrt{a}}\psi(\frac{x-b}{a})\)</span>: 小波函数（通过对母小波<span class="math inline">\(\psi\)</span>进行缩放(<span class="math inline">\(a\)</span>)和移动(<span class="math inline">\(b\)</span>)得到的函数）</li>
<li><span class="math inline">\(W(a, b)\)</span>: 在尺度<span class="math inline">\(a\)</span>，位置<span class="math inline">\(b\)</span>处的小波系数</li>
</ul></li>
<li><p><strong>母小波 (Mother Wavelet):</strong> 满足特定条件的函数（例如：墨西哥帽小波，Morlet 小波）（详细内容参见参考文献 [2]）</p></li>
</ul></li>
<li><p><strong>多分辨率分析 (Multi-resolution Analysis, MRA):</strong> 对CWT进行离散化处理，将信号分解为不同分辨率水平的方法。</p></li>
</ul>
<p><strong>数学背景（简要）:</strong></p>
<ul>
<li><strong>尺度函数 (Scaling Function):</strong> 表示低频成分的函数。</li>
<li><strong>小波函数 (Wavelet Function):</strong> 表示高频成分的函数。</li>
<li><strong>分解 (Decomposition):</strong> 将信号分解为尺度函数和小波函数的组合。</li>
<li><strong>重构 (Reconstruction):</strong> 将分解后的信号重新恢复为原始信号。 <em>(详细内容参见参考文献 [1])</em></li>
</ul>
<p><strong>深度学习研究应用:</strong></p>
<ul>
<li><p><strong>损失表面粗糙度分析:</strong> 通过小波变换量化损失表面的粗糙度，并分析其对学习速度和泛化性能的影响。</p>
<ul>
<li>例如：<a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=%5Bhttps://www.google.com/search?q=https://arxiv.org/abs/1910.00779">Li et al., 2019</a>是]([https://www.google.com/search?q=https://www.google.com/search%3Fq%3Dhttps://arxiv.org/abs/1910.00779)%E7%9A%84]) 使用基于小波的多分辨率分析来研究损失表面粗糙度对学习动力学的影响。</li>
</ul></li>
<li><p><strong>优化算法分析:</strong> 分析优化算法在每个尺度上跟踪哪些特征，以更好地理解算法的工作方式。</p></li>
</ul>
<p><strong>参考文献:</strong></p>
<p><strong>注意：</strong> 在翻译中保持了所有数学表达式和表格格式不变。 1. Mallat, S. (2008). <em>小波信号处理之旅：稀疏方法</em>. Academic press. 2. Daubechies, I. (1992). <em>小波十讲</em>. Society for industrial and applied mathematics. 3. Li, Y., Hu, W., Zhang, Y., &amp; Gu, Q. (2019). <em>深度网络损失景观的多分辨率分析</em>. arXiv preprint arXiv:1910.00779.</p>
</section>
</div>
</div>
</section>
</section>
<section id="最优化过程可视化用高斯函数窥探深度学习的学习秘密" class="level2">
<h2 class="anchored" data-anchor-id="最优化过程可视化用高斯函数窥探深度学习的学习秘密">5.4 最优化过程可视化：用高斯函数窥探深度学习的学习秘密</h2>
<p>深度学习模型的实际损失曲面（loss surface）存在于数百万到数十亿维的<em>超维度空间</em>中，具有非常复杂的几何结构。因此，直接对其进行可视化和分析实际上是<em>不可能</em>的。此外，实际损失曲面存在不可微点、不连续点、数值不稳定等众多问题，给理论分析也带来了困难。</p>
<section id="使用高斯函数进行近似分析简单中隐藏的洞察" class="level3">
<h3 class="anchored" data-anchor-id="使用高斯函数进行近似分析简单中隐藏的洞察">5.4.1 使用高斯函数进行近似分析：简单中隐藏的洞察</h3>
<p>为了克服这些限制并概念性地理解最优化过程，我们使用具有平滑（smooth）、连续（continuous）且<em>凸（convex）</em>形状的高斯函数（Gaussian function）来对损失曲面进行<em>近似（approximation）</em>。</p>
<p><strong>使用高斯函数的原因（损失曲面近似的优点）：</strong></p>
<ol type="1">
<li><strong>可微性：</strong> 高斯函数在所有点上都是无限次可微（infinitely differentiable）的。这是应用和分析基于梯度下降法（gradient descent）的最优化算法的基本条件。</li>
<li><strong>凸性（Convexity）：</strong> 单个高斯函数是凸函数（convex function）。凸函数只有一个全局最小值（global minimum），因此便于分析最优化算法的收敛性。</li>
<li><strong>对称性（Symmetry）：</strong> 高斯函数以中心点为基准具有对称形状。这意味着损失曲面在特定方向上没有偏见（bias），从而可以在分析最优化算法的行为时做出简化的假设。</li>
<li><strong>数学简洁性：</strong> 高斯函数可以用相对简单的公式表示，因此便于进行数学分析。通过这种方式可以理论理解最优化算法的工作原理，并推导出可预测的结果。</li>
<li><strong>可调的复杂度：</strong> 可以使用高斯混合模型调整复杂度。</li>
</ol>
<p><strong>高斯函数公式：</strong></p>
<p><span class="math inline">\(z = A \exp\left(-\left(\frac{(x-x_0)^2}{2\sigma_1^2} + \frac{(y-y_0)^2}{2\sigma_2^2}\right)\right)\)</span></p>
<ul>
<li><span class="math inline">\(A\)</span>: 幅值（amplitude）- 损失函数的最大高度</li>
<li><span class="math inline">\(x_0\)</span>, <span class="math inline">\(y_0\)</span>: 中心点（center）- 损失函数的最小值位置</li>
<li><span class="math inline">\(\sigma_1\)</span>, <span class="math inline">\(\sigma_2\)</span>: x轴、y轴方向的标准差（standard deviation）- 损失曲面的宽度（宽或窄）</li>
</ul>
<p>当然，实际损失曲面可能具有比高斯函数更复杂的形状。（多个局部最小值、鞍点、高原等）。但是，<em>使用单个高斯函数进行近似可以提供理解最优化算法基本行为特性（例如：收敛速度、振动模式）和比较不同算法的有用起点。</em> 为了模拟更复杂的损失曲面，可以使用由多个高斯函数组合而成的高斯混合模型（Gaussian Mixture Model, GMM）。</p>
<p><em>本节将通过使用单个高斯函数对损失曲面进行近似，并应用各种最优化算法（SGD、Adam等）来可视化学习轨迹（learning trajectory），从而直观地了解每个算法的动力学特性和优缺点。</em></p>
<div id="cell-37" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> DataLoader, Subset</span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_05.visualization.loss_surface <span class="im">import</span> hessian_eigenvectors, xy_perturb_loss, visualize_loss_surface, linear_interpolation</span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_04.utils.data <span class="im">import</span> get_dataset  </span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_04.utils.metrics <span class="im">import</span> load_model  </span>
<span id="cb41-8"><a href="#cb41-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_05.optimizers.basic <span class="im">import</span> SGD, Adam</span>
<span id="cb41-9"><a href="#cb41-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_05.visualization.gaussian_loss_surface <span class="im">import</span> (</span>
<span id="cb41-10"><a href="#cb41-10" aria-hidden="true" tabindex="-1"></a>    get_opt_params,  visualize_gaussian_fit, train_loss_surface, visualize_optimization_path</span>
<span id="cb41-11"><a href="#cb41-11" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb41-12"><a href="#cb41-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-13"><a href="#cb41-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-14"><a href="#cb41-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Device configuration</span></span>
<span id="cb41-15"><a href="#cb41-15" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> torch.device(<span class="st">"cuda"</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">"cpu"</span>)</span>
<span id="cb41-16"><a href="#cb41-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-17"><a href="#cb41-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the dataset</span></span>
<span id="cb41-18"><a href="#cb41-18" aria-hidden="true" tabindex="-1"></a>_, test_dataset <span class="op">=</span> get_dataset(dataset<span class="op">=</span><span class="st">"FashionMNIST"</span>)</span>
<span id="cb41-19"><a href="#cb41-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a small dataset</span></span>
<span id="cb41-20"><a href="#cb41-20" aria-hidden="true" tabindex="-1"></a>small_dataset <span class="op">=</span> Subset(test_dataset, torch.arange(<span class="dv">0</span>, <span class="dv">256</span>))</span>
<span id="cb41-21"><a href="#cb41-21" aria-hidden="true" tabindex="-1"></a>data_loader <span class="op">=</span> DataLoader(small_dataset, batch_size<span class="op">=</span><span class="dv">256</span>, shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb41-22"><a href="#cb41-22" aria-hidden="true" tabindex="-1"></a>loss_func <span class="op">=</span> nn.CrossEntropyLoss()</span>
<span id="cb41-23"><a href="#cb41-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-24"><a href="#cb41-24" aria-hidden="true" tabindex="-1"></a>trained_model, _ <span class="op">=</span> load_model(model_file<span class="op">=</span><span class="st">"SimpleNetwork-ReLU.pth"</span>, path<span class="op">=</span><span class="st">"tmp/models/"</span>)</span>
<span id="cb41-25"><a href="#cb41-25" aria-hidden="true" tabindex="-1"></a><span class="co"># trained_model, _ = load_model(model_file="SimpleNetwork-Tanh.pth", path="tmp/models/")</span></span>
<span id="cb41-26"><a href="#cb41-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-27"><a href="#cb41-27" aria-hidden="true" tabindex="-1"></a>trained_model <span class="op">=</span> trained_model.to(device)</span>
<span id="cb41-28"><a href="#cb41-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-29"><a href="#cb41-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Loss surface data generation</span></span>
<span id="cb41-30"><a href="#cb41-30" aria-hidden="true" tabindex="-1"></a>top_n <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb41-31"><a href="#cb41-31" aria-hidden="true" tabindex="-1"></a>top_eigenvalues, top_eignevectors <span class="op">=</span> hessian_eigenvectors(</span>
<span id="cb41-32"><a href="#cb41-32" aria-hidden="true" tabindex="-1"></a>    model<span class="op">=</span>trained_model,</span>
<span id="cb41-33"><a href="#cb41-33" aria-hidden="true" tabindex="-1"></a>    loss_func<span class="op">=</span>loss_func,</span>
<span id="cb41-34"><a href="#cb41-34" aria-hidden="true" tabindex="-1"></a>    data_loader<span class="op">=</span>data_loader,</span>
<span id="cb41-35"><a href="#cb41-35" aria-hidden="true" tabindex="-1"></a>    top_n<span class="op">=</span>top_n,</span>
<span id="cb41-36"><a href="#cb41-36" aria-hidden="true" tabindex="-1"></a>    is_cuda<span class="op">=</span><span class="va">True</span></span>
<span id="cb41-37"><a href="#cb41-37" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb41-38"><a href="#cb41-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-39"><a href="#cb41-39" aria-hidden="true" tabindex="-1"></a><span class="co"># Define lambda range</span></span>
<span id="cb41-40"><a href="#cb41-40" aria-hidden="true" tabindex="-1"></a>d_min, d_max, d_num <span class="op">=</span> <span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">30</span></span>
<span id="cb41-41"><a href="#cb41-41" aria-hidden="true" tabindex="-1"></a>lambda1 <span class="op">=</span> np.linspace(d_min, d_max, d_num).astype(np.float32)</span>
<span id="cb41-42"><a href="#cb41-42" aria-hidden="true" tabindex="-1"></a>lambda2 <span class="op">=</span> np.linspace(d_min, d_max, d_num).astype(np.float32)</span>
<span id="cb41-43"><a href="#cb41-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-44"><a href="#cb41-44" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate loss surface</span></span>
<span id="cb41-45"><a href="#cb41-45" aria-hidden="true" tabindex="-1"></a>x, y, z <span class="op">=</span> xy_perturb_loss(</span>
<span id="cb41-46"><a href="#cb41-46" aria-hidden="true" tabindex="-1"></a>    model<span class="op">=</span>trained_model,</span>
<span id="cb41-47"><a href="#cb41-47" aria-hidden="true" tabindex="-1"></a>    top_eigenvectors<span class="op">=</span>top_eignevectors,</span>
<span id="cb41-48"><a href="#cb41-48" aria-hidden="true" tabindex="-1"></a>    data_loader<span class="op">=</span>data_loader,</span>
<span id="cb41-49"><a href="#cb41-49" aria-hidden="true" tabindex="-1"></a>    loss_func<span class="op">=</span>loss_func,</span>
<span id="cb41-50"><a href="#cb41-50" aria-hidden="true" tabindex="-1"></a>    lambda1<span class="op">=</span>lambda1,</span>
<span id="cb41-51"><a href="#cb41-51" aria-hidden="true" tabindex="-1"></a>    lambda2<span class="op">=</span>lambda2,</span>
<span id="cb41-52"><a href="#cb41-52" aria-hidden="true" tabindex="-1"></a>    device<span class="op">=</span>device</span>
<span id="cb41-53"><a href="#cb41-53" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb41-54"><a href="#cb41-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-55"><a href="#cb41-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-56"><a href="#cb41-56" aria-hidden="true" tabindex="-1"></a><span class="co"># After generating loss surface data</span></span>
<span id="cb41-57"><a href="#cb41-57" aria-hidden="true" tabindex="-1"></a>popt, _, offset <span class="op">=</span> get_opt_params(x, y, z)</span>
<span id="cb41-58"><a href="#cb41-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-59"><a href="#cb41-59" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize Gaussian fitting</span></span>
<span id="cb41-60"><a href="#cb41-60" aria-hidden="true" tabindex="-1"></a>visualize_gaussian_fit(x, y, z, popt, offset, d_min, d_max, d_num)</span>
<span id="cb41-61"><a href="#cb41-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-62"><a href="#cb41-62" aria-hidden="true" tabindex="-1"></a><span class="co"># View from a different angle</span></span>
<span id="cb41-63"><a href="#cb41-63" aria-hidden="true" tabindex="-1"></a>visualize_gaussian_fit(x, y, z, popt, offset, d_min, d_max, d_num,</span>
<span id="cb41-64"><a href="#cb41-64" aria-hidden="true" tabindex="-1"></a>                      elev<span class="op">=</span><span class="dv">30</span>, azim<span class="op">=</span><span class="dv">45</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Function parameters = [29.27164346 -0.0488573  -0.06687705  0.7469189   0.94904458]</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="05_优化和可视化_files/figure-html/cell-14-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="05_优化和可视化_files/figure-html/cell-14-output-3.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>将实际损失平面数据（蓝色点）与高斯函数近似的平面（红色）进行了可视化叠加。如图所示，生成的高斯函数较好地捕捉了<em>原始损失表面数据的整体趋势</em>（特别是中心部分的凹形），并生成了类似的曲面。现在，我们将使用这个近似后的损失平面函数来分析和可视化不同的优化算法（optimizer）如何找到最小值的过程。</p>
</section>
<section id="路径可视化" class="level3">
<h3 class="anchored" data-anchor-id="路径可视化">5.4.2 路径可视化</h3>
<p>使用高斯函数近似的损失平面，我们将在2D平面上可视化优化器是如何工作的。</p>
<p>翻译后的文本：</p>
<div id="cell-41" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Gaussian fitting</span></span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a>popt, _, offset <span class="op">=</span> get_opt_params(x, y, z)</span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a>gaussian_params <span class="op">=</span> (<span class="op">*</span>popt, offset)</span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate optimization paths</span></span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a>points_sgd <span class="op">=</span> train_loss_surface(</span>
<span id="cb43-7"><a href="#cb43-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">lambda</span> params: SGD(params, lr<span class="op">=</span><span class="fl">0.1</span>),</span>
<span id="cb43-8"><a href="#cb43-8" aria-hidden="true" tabindex="-1"></a>    [d_min, d_max], <span class="dv">100</span>, gaussian_params</span>
<span id="cb43-9"><a href="#cb43-9" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb43-10"><a href="#cb43-10" aria-hidden="true" tabindex="-1"></a>points_sgd_m <span class="op">=</span> train_loss_surface(</span>
<span id="cb43-11"><a href="#cb43-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">lambda</span> params: SGD(params, lr<span class="op">=</span><span class="fl">0.05</span>, momentum<span class="op">=</span><span class="fl">0.8</span>),</span>
<span id="cb43-12"><a href="#cb43-12" aria-hidden="true" tabindex="-1"></a>    [d_min, d_max], <span class="dv">100</span>, gaussian_params</span>
<span id="cb43-13"><a href="#cb43-13" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb43-14"><a href="#cb43-14" aria-hidden="true" tabindex="-1"></a>points_adam <span class="op">=</span> train_loss_surface(</span>
<span id="cb43-15"><a href="#cb43-15" aria-hidden="true" tabindex="-1"></a>    <span class="kw">lambda</span> params: Adam(params, lr<span class="op">=</span><span class="fl">0.1</span>),</span>
<span id="cb43-16"><a href="#cb43-16" aria-hidden="true" tabindex="-1"></a>    [d_min, d_max], <span class="dv">100</span>, gaussian_params</span>
<span id="cb43-17"><a href="#cb43-17" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb43-18"><a href="#cb43-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-19"><a href="#cb43-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualization</span></span>
<span id="cb43-20"><a href="#cb43-20" aria-hidden="true" tabindex="-1"></a>visualize_optimization_path(</span>
<span id="cb43-21"><a href="#cb43-21" aria-hidden="true" tabindex="-1"></a>    x, y, z, popt, offset,</span>
<span id="cb43-22"><a href="#cb43-22" aria-hidden="true" tabindex="-1"></a>    [points_sgd, points_sgd_m, points_adam],</span>
<span id="cb43-23"><a href="#cb43-23" aria-hidden="true" tabindex="-1"></a>    act_name<span class="op">=</span><span class="st">"ReLU"</span></span>
<span id="cb43-24"><a href="#cb43-24" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>图形展示了在用高斯函数近似的损失表面上，SGD、动量SGD和Adam这三种优化算法的学习路径。在这三个算法中，无论是梯度平缓的区域还是陡峭的区域，都表现出<strong>各自不同的</strong>特性。</p>
<ul>
<li><strong>SGD（橙色）：</strong> 在梯度平缓的地方，以相对<em>较大的幅度振动</em>并朝最低点前进；在梯度陡峭的地方，则<em>振幅更大地振动</em>，并在接近最低点时<em>收敛速度变慢</em>。</li>
<li><strong>动量SGD（绿色）：</strong> 比SGD的<em>振动更小、曲线更平滑</em>地接近最低点。由于惯性（momentum）效应，在梯度陡峭的区域也能<em>相对稳定地</em>找到最低点。</li>
<li><strong>Adam（红色）：</strong> <em>振动最少，</em>对梯度变化<em>反应灵敏</em>，沿高效路径到达最低点。<em>特别是在梯度陡峭的区域，收敛速度相对较快。</em> 这归功于Adam的自适应学习率调整机制。</li>
</ul>
<p>在实践中，相比于SGD本身，应用了动量的SGD更受青睐，像Adam或AdamW这样的自适应优化算法也广泛使用。通常情况下，损失表面大部分区域是平坦的，但在最小值附近往往呈现狭窄而深邃的山谷形状。这导致较大的学习率可能会错过最小值（overshoot）甚至发散（diverge），因此通常会与逐渐减少学习率的学习率调度器（learning rate scheduler）一起使用。此外，除了优化算法的选择外，还需要综合考虑适当的学习率调度器、批量大小、正则化技术等。</p>
<p><img src="../../../assets/images/04_loss_resnet_ft_imagenet_2.png" class="img-fluid"></p>
<p><img src="../../../assets/images/04_loss_resnet_ft_imagenet_1.png" class="img-fluid"></p>
<p>上述损失面图像展示了使用ImageNet数据集新训练的ResNet-50模型的三维损失表面。（使用PyHessian计算的Hessian矩阵的前两个特征向量作为轴）。与高斯函数近似不同，实际的深度学习模型的损失表面呈现出更加复杂和不规则的形态。然而，中心区域（蓝色区域）存在最小值的大趋势仍然保持不变。这种可视化有助于直观理解深度学习模型的实际损失面具有多么复杂的地形，以及为什么优化是一个困难的问题。</p>
</section>
</section>
<section id="最优化过程的动态分析学习轨迹的探索" class="level2">
<h2 class="anchored" data-anchor-id="最优化过程的动态分析学习轨迹的探索">5.5 最优化过程的动态分析：学习轨迹的探索</h2>
<p>在深度学习模型训练中，理解最优化算法<em>如何找到损失函数最小值的路径</em>及其动态特性(dynamics)是非常重要的。<em>特别是随着大规模语言模型（LLM）的出现，数十亿参数模型的学习动力学分析和控制变得更加重要。</em></p>
<section id="训练过程的特点" class="level3">
<h3 class="anchored" data-anchor-id="训练过程的特点">5.5.1 训练过程的特点</h3>
<p>深度学习模型的训练过程可以分为初期、中期和后期三个阶段，每个阶段都有其特点。</p>
<ol type="1">
<li><p><strong>各学习阶段的特性：</strong></p>
<ul>
<li><strong>初期：</strong> 梯度范数(gradient norm)大且波动剧烈，损失函数值急剧下降。</li>
<li><strong>中期：</strong> 梯度趋于稳定，参数在最优区域(optimal region)探索(explore)。</li>
<li><strong>后期：</strong> 参数在局部最优解(local optimum)附近进行微调(fine-tuning)。（提前终止很重要）</li>
</ul></li>
<li><p><strong>层间梯度特性：</strong></p>
<ul>
<li>在深层神经网络中，接近输入层的梯度较大，而接近输出层的梯度较小的趋势。（消失梯度问题）</li>
<li>这是由于反向传播(backpropagation)时链式法则(chain rule)的作用。</li>
<li>残差连接(residual connection)可以缓解这种不平衡，帮助深层网络稳定学习。</li>
</ul></li>
<li><p><strong>参数依赖性：</strong></p>
<ul>
<li>神经网络的参数相互依赖，这使得最优化过程非线性(nonlinear)。</li>
<li>部分参数对学习的影响更大，因此参数间的平衡(balance)很重要。</li>
</ul></li>
<li><p><strong>最优化路径分析：</strong></p>
<ul>
<li>最优化过程中，参数在损失表面上移动的路径称为最优化路径(optimization path)。</li>
<li>宽广平缓的山谷(valley)形态的局部最小值比狭窄尖锐的山谷具有更好的泛化性能(generalization performance)。</li>
<li>在高维空间中，鞍点(saddle point)非常常见。（动量、Adam等算法设计用于避开这些点）</li>
<li>损失表面平缓(flat)区域会导致梯度变小，学习速度减慢。（自适应学习率算法有助于解决此问题）</li>
</ul></li>
</ol>
</section>
<section id="学习稳定性分析及控制" class="level3">
<h3 class="anchored" data-anchor-id="学习稳定性分析及控制">5.5.2 学习稳定性分析及控制</h3>
<section id="稳定性分析方法论" class="level4">
<h4 class="anchored" data-anchor-id="稳定性分析方法论">稳定性分析方法论</h4>
<p>为了分析最优化过程的稳定性(stability)，考虑以下方面。</p>
<ol type="1">
<li><p><strong>梯度诊断 (Gradient Diagnostics)：</strong></p>
<ul>
<li>检测梯度消失(vanishing gradient)或爆炸(exploding gradient)现象。</li>
<li>训练过程中定期观察梯度范数(norm)。</li>
</ul></li>
<li><p><strong>基于海森矩阵的分析 (Hessian-based Analysis)：</strong></p>
<ul>
<li>海森(Hessian)矩阵的特征值(eigenvalue)分布和条件数(condition number)表示最优化路径的稳定性。</li>
<li>（参见5.3节中的基于海森矩阵的可视化）</li>
</ul></li>
<li><p><strong>实时监控 (Real-time Monitoring)：</strong></p>
<ul>
<li>实时监控学习过程中的梯度范数、参数更新幅度、损失函数值和性能指标。</li>
</ul></li>
</ol>
<section id="稳定化技术-stabilization-techniques" class="level5">
<h5 class="anchored" data-anchor-id="稳定化技术-stabilization-techniques">稳定化技术 (Stabilization Techniques)</h5>
<ul>
<li><p><strong>梯度裁剪 (Gradient Clipping)：</strong> 将梯度大小(norm)限制在不超过某个阈值(threshold)。</p>
<p><span class="math inline">\(g \leftarrow \text{clip}(g) = \min(\max(g, -c), c)\)</span></p></li>
<li><p><span class="math inline">\(g\)</span>: 梯度, <span class="math inline">\(c\)</span>: 阈值</p></li>
<li><p><strong>自适应学习率 (Adaptive Learning Rate):</strong> Adam, RMSProp, Lion, Sophia 等根据梯度统计自动调整学习率。</p></li>
<li><p><strong>学习率调度器 (Learning Rate Scheduler):</strong> 根据学习轮次(epoch)或验证损失(validation loss)逐步降低学习率。</p></li>
<li><p><strong>超参数优化 (Hyperparameter Optimization):</strong> 自动搜索/调整与优化相关的超参数。</p></li>
</ul>
</section>
<section id="最新研究趋势" class="level5">
<h5 class="anchored" data-anchor-id="最新研究趋势">最新研究趋势</h5>
<p>最近(2024年)，学习动力学研究正朝着以下方向发展：</p>
<ul>
<li><strong>预测性稳定化 (Predictive Stabilization):</strong> 通过在学习<em>前</em>分析模型结构、初始化和数据集特性，提前消除/缓解不稳定性因素。</li>
<li><strong>综合分析 (Unified Analysis):</strong> <em>同时</em>分析曲率(curvature)信息(海森矩阵)和梯度统计，以加深对优化算法的理解。</li>
<li><strong>自动化控制 (Automated Control):</strong> 通过强化学习(reinforcement learning)等方法自动调整优化算法的超参数。</li>
</ul>
<p>这些研究使深度学习模型的学习更加稳定/高效，并有助于理解“黑盒”。</p>
<p>现在，让我们通过一个简单的例子来探讨优化过程的动态分析。</p>
<div id="cell-44" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.optim <span class="im">as</span> optim</span>
<span id="cb44-4"><a href="#cb44-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> DataLoader, Subset  <span class="co"># Import Subset</span></span>
<span id="cb44-5"><a href="#cb44-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_05.visualization.train_dynamics <span class="im">import</span> visualize_training_dynamics</span>
<span id="cb44-6"><a href="#cb44-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_04.utils.data <span class="im">import</span> get_dataset  </span>
<span id="cb44-7"><a href="#cb44-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_04.utils.metrics <span class="im">import</span> load_model  </span>
<span id="cb44-8"><a href="#cb44-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-9"><a href="#cb44-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Device configuration</span></span>
<span id="cb44-10"><a href="#cb44-10" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> torch.device(<span class="st">"cuda"</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">"cpu"</span>)</span>
<span id="cb44-11"><a href="#cb44-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-12"><a href="#cb44-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the FashionMNIST dataset (both training and testing)</span></span>
<span id="cb44-13"><a href="#cb44-13" aria-hidden="true" tabindex="-1"></a>train_dataset, test_dataset <span class="op">=</span> get_dataset(dataset<span class="op">=</span><span class="st">"FashionMNIST"</span>)</span>
<span id="cb44-14"><a href="#cb44-14" aria-hidden="true" tabindex="-1"></a>train_loader <span class="op">=</span> DataLoader(train_dataset, batch_size<span class="op">=</span><span class="dv">256</span>, shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb44-15"><a href="#cb44-15" aria-hidden="true" tabindex="-1"></a>loss_func <span class="op">=</span> nn.CrossEntropyLoss()</span>
<span id="cb44-16"><a href="#cb44-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-17"><a href="#cb44-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Load a pre-trained model (e.g., ReLU-based network)</span></span>
<span id="cb44-18"><a href="#cb44-18" aria-hidden="true" tabindex="-1"></a>trained_model, _ <span class="op">=</span> load_model(model_file<span class="op">=</span><span class="st">"SimpleNetwork-ReLU.pth"</span>, path<span class="op">=</span><span class="st">"tmp/models/"</span>)</span>
<span id="cb44-19"><a href="#cb44-19" aria-hidden="true" tabindex="-1"></a>trained_model <span class="op">=</span> trained_model.to(device)</span>
<span id="cb44-20"><a href="#cb44-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-21"><a href="#cb44-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Choose an optimizer (e.g., Adam)</span></span>
<span id="cb44-22"><a href="#cb44-22" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> optim.Adam(trained_model.parameters(), lr<span class="op">=</span><span class="fl">0.001</span>)</span>
<span id="cb44-23"><a href="#cb44-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-24"><a href="#cb44-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Call the training dynamics visualization function (e.g., train for 10 epochs with the entire training dataset)</span></span>
<span id="cb44-25"><a href="#cb44-25" aria-hidden="true" tabindex="-1"></a>metrics <span class="op">=</span> visualize_training_dynamics(</span>
<span id="cb44-26"><a href="#cb44-26" aria-hidden="true" tabindex="-1"></a>    trained_model, optimizer, train_loader, loss_func, num_epochs<span class="op">=</span><span class="dv">20</span>, device<span class="op">=</span>device</span>
<span id="cb44-27"><a href="#cb44-27" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb44-28"><a href="#cb44-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-29"><a href="#cb44-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the final results for each metric</span></span>
<span id="cb44-30"><a href="#cb44-30" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Final Loss:"</span>, metrics[<span class="st">"loss"</span>][<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb44-31"><a href="#cb44-31" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Final Grad Norm:"</span>, metrics[<span class="st">"grad_norm"</span>][<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb44-32"><a href="#cb44-32" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Final Param Change:"</span>, metrics[<span class="st">"param_change"</span>][<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb44-33"><a href="#cb44-33" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Final Weight Norm:"</span>, metrics[<span class="st">"weight_norm"</span>][<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb44-34"><a href="#cb44-34" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Final Loss Improvement:"</span>, metrics[<span class="st">"loss_improvement"</span>][<span class="op">-</span><span class="dv">1</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>上述示例展示了学习动力学(learning dynamics)的各个方面。使用针对FashionMNIST数据集预训练的SimpleNetwork-ReLU模型，在使用Adam优化算法进行额外训练时，我们可视化了每个epoch的以下五个关键指标(metric)。</p>
<ul>
<li><strong>Loss (损失):</strong> 显示了在训练过程中损失函数值如何减少。(蓝色线)</li>
<li><strong>Grad Norm (梯度范数):</strong> 表示梯度的大小(L2 norm)。(红色线)</li>
<li><strong>Param Change (参数变化量):</strong> 表示相对于前一个epoch，参数(权重)的变化量(L2 norm)。</li>
<li><strong>Weight Norm (权重范数):</strong> 表示整个模型参数(权重)的大小(L2 norm)。(紫色线)</li>
<li><strong>Loss Improvement (损失改进量):</strong> 表示与前一个epoch相比，损失函数值减少了多少。(黄色线)</li>
</ul>
<p>图表显示的内容如下。</p>
<ul>
<li><strong>Loss:</strong> 损失值在初始epoch(epoch 1)约为0.51，在训练过程中持续减少，到最终的epoch(epoch 20)时降至约0.16。</li>
<li><strong>Grad Norm:</strong> 初始epoch中的梯度范数(约4.5)较高，随着训练进行逐渐减少，在最后一个epoch中达到约2.0附近。</li>
<li><strong>Param Change:</strong> 参数变化量在训练初期较大，但随着训练的进行显示出下降的趋势。这表明模型逐渐接近最优解时参数的变化变小。</li>
<li><strong>Weight Norm:</strong> 权重范数在整个训练过程中持续增加。这意味着模型的参数通过学习变得“更大”。(但这并不一定意味着过拟合(overfitting)。)</li>
<li><strong>Loss Improvement:</strong> 损失改进量在训练初期较大，随着训练进行显示出下降的趋势。</li>
</ul>
<p><em>通过这个示例，可以直观地了解优化算法如何最小化损失函数、梯度的变化、参数的变化等，并获得对学习动力学的直观理解。</em></p>
</section>
</section>
</section>
<section id="结语" class="level3">
<h3 class="anchored" data-anchor-id="结语">结语</h3>
<p>本次第五章深入探讨了与深度学习模型训练的核心要素——优化相关的各种主题。我们探讨了权重初始化方法的重要性、不同优化算法(SGD, Momentum, Adam, Lion, Sophia, AdaFactor)的原理和特性，以及通过损失表面可视化及学习动力学分析来更好地理解深度学习模型的学习过程。</p>
<p><em>在第六章中，我们将详细介绍用于提高深度学习模型泛化性能的关键技术——正则化(regularization)。我们将探讨L1/L2正则化、dropout、批标准化(batch normalization)、数据增强(data augmentation)等不同正则化技术的原理和效果，并通过实战示例来掌握其应用方法。</em></p>
</section>
<section id="练习题" class="level3">
<h3 class="anchored" data-anchor-id="练习题">练习题</h3>
<section id="基本问题" class="level4">
<h4 class="anchored" data-anchor-id="基本问题">基本问题</h4>
<ol type="1">
<li><strong>手动计算SGD:</strong> 当学习率为0.1且动量为0.9时，手动计算以下损失函数 <span class="math inline">\(L(w) = w^2\)</span> 的SGD更新规则至少3步。初始权重设置为 <span class="math inline">\(w_0 = 2\)</span>。</li>
<li><strong>比较梯度下降的收敛速度:</strong> 对于简单的二维函数 <span class="math inline">\(f(x, y) = x^2 + 2y^2\)</span>，应用梯度下降，并通过改变学习率（0.1、0.01、0.001）来比较其收敛速度。</li>
<li><strong>初始化方法比较:</strong> 比较Kaiming初始化和Xavier初始化，并解释在使用ReLU激活函数时为什么Kaiming初始化更合适。</li>
</ol>
</section>
<section id="应用问题" class="level4">
<h4 class="anchored" data-anchor-id="应用问题">应用问题</h4>
<ol type="1">
<li><strong>Adam优化器:</strong> 解释Adam优化器的工作原理（包括公式），并说明参数 <span class="math inline">\(\beta_1\)</span> 和 <span class="math inline">\(\beta_2\)</span> 的作用。</li>
<li><strong>批归一化与初始化:</strong> 解释批归一化（Batch Normalization）如何减少对初始化方法的依赖，并解释其原因。</li>
<li><strong>高斯损失平面:</strong> 在使用高斯函数近似损失平面的例子（5.5.1节）中，说明高斯函数的参数（幅度、中心点、方差）对优化过程的影响。（通过改变各参数观察优化路径如何变化。）</li>
</ol>
</section>
<section id="深化问题" class="level4">
<h4 class="anchored">深化问题</h4>
<ol type="1">
<li><strong>Lion优化器分析:</strong> 解释Lion优化器的核心思想（包括公式），并分析与Adam相比其优缺点。</li>
<li><strong>初始化方法实验:</strong> 针给定的数据集（如：FashionMNIST）和模型（5.1节的SimpleNetwork），应用不同的初始化方法（LeCun、Xavier、Kaiming、Orthogonal）进行训练，并比较其结果（错误率、收敛速度、平均条件数、谱范数、有效秩比）。</li>
<li><strong>优化路径可视化:</strong> 参考5.5节的优化路径可视化代码，定义自己的损失函数（如：多峰形函数、非凸函数），并使用不同的优化器（SGD、Momentum、Adam、Lion等）对优化路径进行可视化和比较分析。（至少比较3种以上的优化器）</li>
</ol>
<div class="callout callout-style-default callout-note callout-titled" title="点击查看内容（基于拓扑的损失表面分析）">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-5-contents" aria-controls="callout-5" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
点击查看内容（基于拓扑的损失表面分析）
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-5" class="callout-5-contents callout-collapse collapse">
<section id="练习题解答" class="level3 callout-body-container callout-body">
<h3 class="anchored" data-anchor-id="练习题解答">练习题解答</h3>
<section id="基本问题-1" class="level4">
<h4 class="anchored" data-anchor-id="基本问题-1">基本问题</h4>
<ol type="1">
<li><p><strong>SGD 手动计算:</strong></p>
<ul>
<li><strong>第1步:</strong>
<ul>
<li><span class="math inline">\(g_0 = \frac{dL}{dw}(w_0) = 2w_0 = 4\)</span></li>
<li><span class="math inline">\(v_0 = 0\)</span> （动量初始值）</li>
<li><span class="math inline">\(w_1 = w_0 - \eta v_1 = 2 - 0.1 \cdot (0.9 \cdot 0 + 4) = 1.6\)</span></li>
</ul></li>
<li><strong>第2步:</strong>
<ul>
<li><span class="math inline">\(g_1 = 2w_1 = 3.2\)</span></li>
<li><span class="math inline">\(v_1 = 0.9 \cdot v_0 + g_0= 0.9 \cdot 0 + 4 = 4\)</span></li>
<li><span class="math inline">\(w_2 = w_1 - \eta \cdot (0.9 \cdot v_1 + g_1) = 1.6 - 0.1 \cdot (0.9 \cdot 4+ 3.2) = 0.92\)</span></li>
</ul></li>
<li><strong>第3步:</strong>
<ul>
<li><span class="math inline">\(g_2 = 2w_2 = 1.84\)</span></li>
<li><span class="math inline">\(v_2 = 0.9 \cdot 4 + 3.2 = 6.8\)</span></li>
<li><span class="math inline">\(w_3 = w_2 - \eta \cdot (0.9 * v_2 + g_2) = 0.92 - 0.1 \cdot (0.9 \cdot 6.8 + 1.84) = 0.124\)</span></li>
</ul></li>
</ul></li>
<li><p><strong>梯度下降法收敛速度比较:</strong></p>
<ul>
<li>学习率越大（0.1），初期收敛越快，但在最优解附近可能会振荡。</li>
<li>学习率越小（0.001），收敛速度越慢，但更稳定地接近最优解。</li>
<li>适当的学习率（0.01）表现出适中的收敛速度和稳定性。</li>
</ul></li>
<li><p><strong>初始化方法比较:</strong></p>
<ul>
<li><strong>Kaiming 初始化:</strong> 考虑ReLU激活函数的特性（将负输入变为0），使用标准差为 <span class="math inline">\(\sqrt{2/n_{in}}\)</span> 的分布来初始化权重。</li>
<li><strong>Xavier 初始化:</strong> 不考虑激活函数类型，保持输入和输出方差一致，使用标准差为 <span class="math inline">\(\sqrt{2/(n_{in} + n_{out})}\)</span> 的分布。</li>
<li><strong>ReLU + Kaiming:</strong> 由于ReLU在正数区域是线性激活，Kaiming初始化提供更大的方差以缓解“死神经元”问题，并帮助更快学习。</li>
</ul></li>
</ol>
</section>
<section id="应用问题-1" class="level4">
<h4 class="anchored" data-anchor-id="应用问题-1">应用问题</h4>
<ol type="1">
<li><p><strong>Adam 优化器:</strong></p>
<ul>
<li><strong>工作原理:</strong> Adam 是结合了动量（Momentum）和RMSProp思想的优化器。
<ul>
<li><strong>动量:</strong> 使用过去梯度的指数加权平均值（一阶矩，<span class="math inline">\(m_t\)</span>）来调整方向。</li>
<li><strong>RMSProp:</strong> 使用过去梯度平方的指数加权平均值（二阶矩，<span class="math inline">\(v_t\)</span>）来调整每个参数的学习率。</li>
<li><strong>偏差校正:</strong> 校正在初期阶段 <span class="math inline">\(m_t\)</span> 和 <span class="math inline">\(v_t\)</span> 可能会偏移向0的问题。</li>
</ul></li>
<li><strong>公式:</strong>
<ul>
<li><span class="math inline">\(m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t\)</span></li>
<li><span class="math inline">\(v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2\)</span></li>
<li><span class="math inline">\(\hat{m_t} = m_t / (1 - \beta_1^t)\)</span></li>
<li><span class="math inline">\(\hat{v_t} = v_t / (1 - \beta_2^t)\)</span></li>
<li><span class="math inline">\(w_{t+1} = w_t - \eta \hat{m_t} / (\sqrt{\hat{v_t}} + \epsilon)\)</span></li>
</ul></li>
<li><strong><span class="math inline">\(\beta_1\)</span>, <span class="math inline">\(\beta_2\)</span> 的作用:</strong>
<ul>
<li><span class="math inline">\(\beta_1\)</span>: 调节一阶矩（动量）的指数衰减率。 （通常为0.9）</li>
<li><span class="math inline">\(\beta_2\)</span>: 调节二阶矩（RMSProp）的指数衰减率。 （通常为0.999）</li>
</ul></li>
</ul></li>
<li><p><strong>批标准化与初始化:</strong></p></li>
</ol>
<ul>
<li><strong>批归一化:</strong> 将每个小批量的输入标准化为均值0，方差1。
<ul>
<li><strong>初始化重要性降低:</strong> 批归一化减少了网络内部协变量偏移(internal covariate shift)，降低了对初始权重分布的依赖性。</li>
<li><strong>原因:</strong> 标准化的输入将激活函数置于适当的范围内（例如：ReLU 的正区域），从而缓解梯度消失/爆炸问题，稳定学习。</li>
</ul></li>
</ul>
<ol start="3" type="1">
<li><strong>高斯损失平面:</strong>
<ul>
<li><strong>幅度 (A):</strong> 调整损失函数的总体大小。幅度大则损失值的变化范围增大，可能导致学习不稳定。</li>
<li><strong>中心点 (<span class="math inline">\(x_0\)</span>, <span class="math inline">\(y_0\)</span>):</strong> 确定损失函数最小值的位置。优化算法将向这个中心点移动。</li>
<li><strong>方差 (<span class="math inline">\(\sigma_1\)</span>, <span class="math inline">\(\sigma_2\)</span>):</strong> 表示沿各轴方向的损失函数变化程度。方差小则形状窄而尖，大则宽而平缓。如果方差不同，则需要在不同方向上调整学习速度。</li>
</ul></li>
</ol>
</section>
<section id="深化问题-1" class="level4">
<h4 class="anchored" data-anchor-id="深化问题-1">深化问题</h4>
<ol type="1">
<li><p><strong>Lion 优化器分析:</strong></p>
<ul>
<li><strong>核心思想:</strong> 使用梯度的符号(sign)进行更新。</li>
<li><strong>公式:</strong> <code>c_t = β_1 * m_{t-1} + (1 - β_1) * g_t     w_{t+1} = w_t - η * sign(c_t)     m_t = c_t</code>
<ul>
<li>由于只使用更新的符号，因此不需要像 Adam 那样计算和存储二阶矩。</li>
</ul></li>
<li><strong>优点:</strong>
<ul>
<li>比 Adam 使用更少的内存。（不存储二阶矩）</li>
<li>更新大小对所有参数都相同，因此对稀疏梯度具有鲁棒性。</li>
</ul></li>
<li><strong>缺点:</strong>
<ul>
<li>忽略了梯度的大小信息，因此在某些情况下可能比 Adam 收敛得慢。</li>
<li>学习率调整可能比 Adam 更敏感。</li>
</ul></li>
</ul></li>
<li><p><strong>初始化方法实验:</strong></p>
<ul>
<li><strong>实验设计:</strong>
<ul>
<li>使用相同的模型(SimpleNetwork)和数据集(FashionMNIST)。</li>
<li>分别应用 LeCun, Xavier, Kaiming, Orthogonal 初始化。</li>
<li>使用相同的优化算法（例如：Adam）和学习率。</li>
<li>在足够多的轮次（例如：20 轮）中进行训练，并在每轮后记录评估指标（错误率、收敛速度、平均条件数、谱范数、有效秩比）。</li>
</ul></li>
<li><strong>结果分析:</strong>
<ul>
<li>使用 ReLU 激活函数时，Kaiming 初始化可能表现出最佳性能。</li>
<li>Orthogonal 初始化在 RNN/LSTM 中可能表现良好。</li>
<li>Xavier 初始化在 tanh, sigmoid 激活函数中可能表现良好。</li>
<li>LeCun 初始化在现代网络中的性能可能会下降。</li>
</ul></li>
</ul></li>
<li><p><strong>优化路径可视化:</strong></p></li>
</ol>
<ul>
<li><strong>定义自己的损失函数:</strong></li>
<li>例: <span class="math inline">\(f(x, y) = (x^2 + y - 11)^2 + (x + y^2 - 7)^2\)</span> (Himmelblau 函数，多峰形函数)</li>
<li>例: <span class="math inline">\(f(x, y) = 0.5x^2 - 0.25y^2 + 3\)</span> (有鞍点(saddle point)的非凸函数)</li>
<li><strong>选择优化算法:</strong> SGD, Momentum(SGD with momentum), Adam, Lion</li>
<li><strong>可视化:</strong> 参考第5.5节的代码，将每个优化器的优化路径在二维平面上进行可视化。</li>
<li><strong>结果分析:</strong>
<ul>
<li>SGD 容易陷入局部最小值/鞍点。</li>
<li>Momentum 通过惯性有可能逃脱局部最小值，但可能会发生振荡。</li>
<li>Adam 因为自适应学习率可以更高效地达到最优解。</li>
<li>Lion 可能表现出与 Adam 相似或更快的收敛速度，但在学习率调整上可能更为敏感。</li>
<li>根据损失函数的形状（如多峰、鞍点等）比较分析优化结果。</li>
</ul></li>
</ul>
</section>
</section>
</div>
</div>
</section>
</section>
<section id="参考资料" class="level3">
<h3 class="anchored" data-anchor-id="参考资料">参考资料</h3>
<ol type="1">
<li><strong><a href="https://arxiv.org/abs/1609.04747">An overview of gradient descent optimization algorithms</a></strong> (Sebastian Ruder, 2016) - 关于深度学习优化算法的优秀概述论文。比较分析了SGD、Momentum、AdaGrad、RMSProp、Adam等不同算法。</li>
<li><strong><a href="https://arxiv.org/abs/1712.09913">Visualizing the Loss Landscape of Neural Nets</a></strong> (Li et al., 2018) - 关于损失面可视化的开创性论文。展示了残差连接(residual connection)如何平滑损失面。</li>
<li><strong><a href="https://www.google.com/search?q=https://ruder.io/deep-learning-optimization-2023/">Optimization for Deep Learning Highlights in 2023</a></strong> (Sebastian Ruder, 2023) - 概述了2023年深度学习优化的主要内容的博客文章。对于了解最新研究趋势非常有用。</li>
<li><strong><a href="https://arxiv.org/abs/2110.08536">Improving Deep Learning with Better Initialization</a></strong> (Mishkin &amp; Matas, 2021) - 包含现代初始化研究趋势的论文。比较了不同的初始化方法，并提供了实用指南。</li>
<li><strong><a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://arxiv.org/abs/2302.06675">Symbolic Discovery of Optimization Algorithms</a></strong> (Chen et al.&nbsp;2023) - 关于Google Brain发现的Lion算法的论文。</li>
<li><strong><a href="https://arxiv.org/abs/1912.07145">PyHessian: Neural Networks Through the Lens of the Hessian</a></strong> (Yao et al., 2020) - 论文介绍了使用Hessian矩阵进行损失面分析的工具PyHessian。</li>
<li><strong><a href="https://arxiv.org/abs/1705.08292">The Marginal Value of Adaptive Gradient Methods in Machine Learning</a></strong> (Wilson et al., 2017) - 论文展示了自适应学习率方法（如Adam）并不总是优于SGD。</li>
<li><strong><a href="https://www.offconvex.org/2016/03/22/saddle-efficiency/">How to escape saddle points efficiently</a></strong> (Ge et al., 2015) - 提出了使用随机梯度下降高效逃离鞍点的方法的论文。</li>
<li><strong><a href="https://arxiv.org/abs/2411.01593">A Closer Look at Smoothness in Deep Learning: A Tensor Decomposition Approach</a></strong> (Li et al., 2024) - 论文使用张量分解方法分析深度学习模型的平滑性(smoothness)。</li>
<li><strong><a href="https://arxiv.org/abs/2502.00894">Understanding Measures of Efficiency for Stochastic Optimization</a></strong> (Defazio &amp; Bottou, 2025) - 提出衡量随机优化算法效率的方法的论文。</li>
<li><strong><a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://www.deeplearningbook.org/">Deep Learning</a></strong> (Goodfellow, Bengio, Courville, 2016) - 深度学习教科书。第6章（深度前馈网络）、第8章（训练深度模型的优化）讨论了初始化和优化相关的内容。</li>
<li><strong><a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://cs231n.github.io/optimization-1/">Stanford CS231n: Convolutional Neural Networks for Visual Recognition</a></strong> - 斯坦福大学深度学习课程。在优化部分讨论了相关的内容。</li>
<li><strong><a href="https://www.google.com/search?q=https://paperswithcode.com/methods/category/optimization-methods">Papers with Code - Optimization Methods</a></strong> - 收集了最新优化相关论文的网站。</li>
<li><strong><a href="https://arxiv.org/abs/1609.04747">梯度下降优化算法综述</a></strong> (Sebastian Ruder, 2016) - 这是一篇关于深度学习优化算法的优秀概述论文，比较分析了SGD、Momentum、AdaGrad、RMSProp、Adam等不同算法。2. <strong><a href="https://arxiv.org/abs/1712.09913">神经网络损失景观可视化</a></strong> (Li et al., 2018) - 这是一篇关于损失表面可视化的开创性论文，展示了残差连接（residual connection）如何平滑损失表面。3. <strong><a href="https://www.google.com/search?q=https://ruder.io/deep-learning-optimization-2023/">2023年深度学习优化亮点</a></strong> (Sebastian Ruder, 2023) - 这是一篇总结了2023年深度学习优化主要内容的博客文章，对了解最新研究趋势非常有用。4. <strong><a href="https://arxiv.org/abs/2110.08536">通过更好的初始化改进深度学习</a></strong> (Mishkin &amp; Matas, 2021) - 这是一篇包含现代初始化研究趋势的论文，比较了各种初始化方法，并提供了实用指南。5. <strong><a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://arxiv.org/abs/2302.06675">优化算法的符号发现</a></strong> (Chen et al.&nbsp;2023) - 这是一篇关于Google Brain发现的Lion算法的论文。6. <strong><a href="https://arxiv.org/abs/1912.07145">PyHessian：通过海森矩阵视角看神经网络</a></strong> (Yao et al., 2020) - 这是一篇关于使用海森矩阵进行损失表面分析的工具PyHessian的论文。7. <strong><a href="https://arxiv.org/abs/1705.08292">机器学习中自适应梯度方法的边际价值</a></strong> (Wilson et al., 2017) - 这篇论文展示了自适应学习率方法（如Adam等）并不总是优于SGD。8. <strong><a href="https://www.offconvex.org/2016/03/22/saddlepoints/">如何高效地逃离鞍点</a></strong> (Ge et al., 2015) - 这是一篇博客文章，解释了如何使用扰动梯度下降法（perturbed gradient descent）高效地逃离鞍点。9. <strong><a href="https://www.google.com/search?q=https://openreview.net/forum%3Fid%3DFpgg9h-xO_a">现代初始化方法的深入理解：使用块对角矩阵</a></strong> (Huang et al., 2021) - 这是一篇使用块对角矩阵分析初始化方法的论文。10. <strong><a href="https://www.google.com/search?q=https://proceedings.neurips.cc/paper/2020/hash/9c838d2e45b2ad1094d42f4ef36764f6-Abstract.html">AdaHessian: 一种自适应的二阶优化器</a></strong> (Yao et al., 2020) - 使用 Hessian 矩阵的对角成分来利用二阶信息的 AdaHessian 优化器的论文。11. <strong><a href="https://arxiv.org/abs/2411.01593">深入探讨深度学习中的平滑性：张量分解方法</a></strong> (Li et al., 2024) - 使用张量分解来分析深度学习模型的平滑性的论文。12. <strong><a href="https://arxiv.org/abs/2502.00894">理解随机优化效率度量</a></strong> (Defazio &amp; Bottou, 2025) - 提出衡量随机优化算法效率的方法的论文。13. <strong><a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://www.deeplearningbook.org/">深度学习</a></strong> (Goodfellow, Bengio, Courville, 2016) - 深度学习教科书。第6章（深度前馈网络），第8章（训练深度模型的优化）讨论了初始化和优化相关内容。14. <strong><a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://cs231n.github.io/optimization-1/">斯坦福 CS231n: 卷积神经网络用于视觉识别</a></strong> - 斯坦福大学深度学习课程。优化部分讨论了相关优化内容。15. <strong><a href="https://www.google.com/search?q=https://paperswithcode.com/methods/category/optimization-methods">Papers with Code - 优化方法</a></strong> - 汇集了最新优化相关论文的网站。</li>
</ol>


</section>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>