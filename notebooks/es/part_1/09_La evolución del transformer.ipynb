{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/Quantum-Intelligence-Frontier/dldna/blob/main/notebooks/es/part_1/09_la_evolución_del_transformer.ipynb\" target=\"_parent\">\n",
    "<img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Abrir en Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9 Capítulo La evolución de los Transformers: hacia la eficiencia y escalabilidad\n",
    "\n",
    "> \"La eficiencia es el puente hacia la inteligencia.\" - Alan Turing\n",
    "\n",
    "Desde la aparición de los Transformers en 2017, han surgido sucesivamente modelos de lenguaje grandes representados por BERT y GPT. Estos modelos han abierto una nueva era en la inteligencia artificial con sus sorprendentes rendimientos. Sin embargo, detrás de este éxito se encontraban las limitaciones fundamentales de la arquitectura de los Transformers y los esfuerzos para superarlas. Se realizaron mejoras constantes y propuestas estructurales para superar problemas de complejidad computacional y limitaciones en el procesamiento de textos largos. En particular, desde 2019, con el rápido aumento del tamaño de los modelos, se ha intensificado la investigación sobre eficiencia.\n",
    "\n",
    "**Cambios principales por período:**\n",
    "\n",
    "*   2019-2020: Focalizado en reducir complejidad\n",
    "*   2021-2022: Focalizado en eficiencia de memoria\n",
    "*   2023-2024: Focalizado en escalabilidad y propósitos especiales (ética, modelos abiertos, etc.)\n",
    "\n",
    "En este capítulo examinaremos las limitaciones de los Transformers y detallaremos diversas soluciones para superarlas.\n",
    "\n",
    "\n",
    "## 9.1 Limitaciones y desafíos de los Transformers\n",
    "\n",
    "> **Desafío:** ¿Cómo reducir la complejidad computacional y el uso de memoria del modelo Transformer, procesar contextos más largos y entrenar modelos más grandes?\n",
    ">\n",
    "> **Angustia del investigador:** Aunque el rendimiento de los modelos Transformer era sobresaliente, su costo computacional era enorme. En particular, el mecanismo de atención tenía una complejidad proporcional al cuadrado de la longitud de la secuencia, lo que limitaba severamente la escalabilidad del modelo. Los investigadores tenían que encontrar formas de aumentar la eficiencia computacional mientras mantenían las funciones esenciales de la atención. No se trataba simplemente de reducir el tamaño del modelo, sino de buscar soluciones innovadoras a nivel de algoritmos y hardware. Esto era un desafío similar a construir un edificio enorme mientras se reducía el peso y el costo de cada ladrillo.\n",
    "\n",
    "La complejidad cuadrática de la operación de atención, las longitudes limitadas de contexto y los problemas de eficiencia en memoria fueron los principales obstáculos para la expansión del modelo. Estas limitaciones se convirtieron en factores cruciales que determinaron la dirección del desarrollo de los Transformers.\n",
    "\n",
    "### 9.1.1 Limitaciones básicas de la arquitectura Transformer: complejidad computacional\n",
    "\n",
    "Durante el proceso de escalado de modelos Transformer, la complejidad de las operaciones de atención, particularmente la proporcional a la longitud cuadrática de la secuencia, fue un gran problema.\n",
    "\n",
    "**Análisis de la complejidad de la operación de atención:**\n",
    "\n",
    "$Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V$\n",
    "\n",
    "1.  $QK^T$ cálculo: $O(N^2d)$ (d: dimensión de embedding)\n",
    "2.  Operación Softmax: $O(N^2)$\n",
    "3.  Multiplicación del resultado de softmax con V: $O(N^2d)$\n",
    "\n",
    "Vamos a ver esto en código para observar la velocidad de ejecución y el uso de memoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install dldna[colab] # in Colab\n",
    "# !pip install dldna[all] # in your local\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Complexity Analysis of Attention Operation ===\n",
      "\n",
      "Memory usage and execution time by sequence length:\n",
      "Length\t\tMemory (MB)\tTime (seconds)\n",
      "----------------------------------------\n",
      "100\t\t18.75\t\t0.0037\n",
      "500\t\t96.58\t\t0.0388\n",
      "1000\t\t317.00\t\t0.1187\n",
      "2000\t\t1119.00\t\t0.4228\n",
      "4000\t\t4188.14\t\t1.6553\n",
      "8000\t\t16142.53\t\t6.5773\n",
      "10000\t\t25039.31\t\t10.2601\n",
      "15000\t\t55868.54\t\t25.1265\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from dldna.chapter_09.complexity_benchmark import measure_attention_complexity, plot_complexity_analysis, measure_attention_complexity_gpu\n",
    "\n",
    "seq_lengths = [100, 500, 1000, 2000, 4000, 8000, 10000, 15000]\n",
    "\n",
    "results = measure_attention_complexity(seq_lengths=seq_lengths)\n",
    "\n",
    "print(\"\\n=== Complexity Analysis of Attention Operation ===\")\n",
    "print(\"\\nMemory usage and execution time by sequence length:\")\n",
    "print(\"Length\\t\\tMemory (MB)\\tTime (seconds)\")\n",
    "print(\"-\" * 40)\n",
    "for seq_len, mem, time_taken in results:\n",
    "    print(f\"{seq_len}\\t\\t{mem:.2f}\\t\\t{time_taken:.4f}\")\n",
    "\n",
    "# Visualize with a graph\n",
    "plot_complexity_analysis(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En los modelos de transformador reales, esta operación se repite a través de múltiples capas. El aumento del tamaño del lote también aumenta la cantidad de cálculos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Comparison of Theoretical Complexity and Actual Measurements ===\n",
      "\n",
      "Theoretical vs Actual Growth Rate (Base: First Sequence Length)\n",
      "Length      Theoretical(N²)      Actual Memory      Actual Time\n",
      "------------------------------------------------------------\n",
      "   100          1.00x          1.00x          1.00x\n",
      "   500         25.00x          5.15x          8.05x\n",
      "  1000        100.00x         16.91x         32.49x\n",
      "  2000        400.00x         59.71x        124.52x\n",
      "  4000       1600.00x        223.34x        474.71x\n",
      "  8000       6400.00x        860.92x       1882.04x\n",
      " 10000      10000.00x       1335.43x       2976.84x\n",
      " 15000      22500.00x       2979.67x       7280.40x\n"
     ]
    }
   ],
   "source": [
    "# Compare theoretical complexity with actual measurements\n",
    "print(\"\\n=== Comparison of Theoretical Complexity and Actual Measurements ===\")\n",
    "base_seq = seq_lengths[0]\n",
    "base_mem = results[0][1]\n",
    "base_time = results[0][2]\n",
    "\n",
    "print(\"\\nTheoretical vs Actual Growth Rate (Base: First Sequence Length)\")\n",
    "print(\"Length      Theoretical(N²)      Actual Memory      Actual Time\")\n",
    "print(\"-\" * 60)\n",
    "for seq_len, mem, time_taken in results:\n",
    "    theoretical = (seq_len/base_seq) ** 2\n",
    "    actual_mem = mem/base_mem\n",
    "    actual_time = time_taken/base_time\n",
    "    print(f\"{seq_len:6d}    {theoretical:10.2f}x    {actual_mem:10.2f}x    {actual_time:10.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La complejidad cuadrática es particularmente grave en modelos a gran escala como GPT-3. Esto ha llevado a numerosas limitaciones, como la restricción de procesamiento de documentos largos y el límite de tamaño de lote durante el entrenamiento. Esta situación se convirtió en un motivo principal para desarrollar mecanismos de atención más eficientes.\n",
    "\n",
    "Los primeros intentos para abordar el problema de complejidad cuadrática en los transformadores tomaron principalmente tres direcciones.\n",
    "\n",
    "**Atención con ventana deslizante**\n",
    "\n",
    "Calcula la atención solo dentro de una ventana de tamaño fijo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window_attention(q, k, v, window_size):\n",
    "    \"\"\"Sliding window attention\"\"\"\n",
    "    batch_size, seq_len, dim = q.shape\n",
    "    attention_weights = np.zeros((batch_size, seq_len, seq_len))\n",
    "\n",
    "    for i in range(seq_len):\n",
    "        start = max(0, i - window_size // 2)\n",
    "        end = min(seq_len, i + window_size // 2 + 1)\n",
    "        scores = np.matmul(q[:, i:i+1], k[:, start:end].transpose(0, 2, 1))\n",
    "        attention_weights[:, i, start:end] = softmax(scores, axis=-1)\n",
    "\n",
    "    return np.matmul(attention_weights, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este método reduce la complejidad a $O(N \\cdot w)$ (w: tamaño de la ventana).\n",
    "\n",
    "**Patrón de atención dispersa**\n",
    "\n",
    "El patrón de atención dispersa es un enfoque que, en lugar de calcular las relaciones entre todas las parejas de tokens, solo calcula algunas relaciones según ciertos patrones. Por ejemplo, si tenemos una secuencia compuesta por 10 tokens, la atención normal calcularía 100 (10×10) relaciones, pero la atención dispersa solo calculará un subconjunto de estas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_block_attention(q, k, v, block_size):\n",
    "    \"\"\"Block sparse attention\n",
    "    Example: seq_len=8, block_size=2\n",
    "    Process the sequence in 4 blocks of 2 tokens each\n",
    "    Block 1 (0,1), Block 2 (2,3), Block 3 (4,5), Block 4 (6,7)\n",
    "    \"\"\"\n",
    "    batch_size, seq_len, dim = q.shape  # e.g., (1, 8, 64)\n",
    "    num_blocks = seq_len // block_size  # e.g., 8/2 = 4 blocks\n",
    "    attention_weights = np.zeros((batch_size, seq_len, seq_len))\n",
    "    \n",
    "    for i in range(num_blocks):\n",
    "        # e.g., when i=0, process Block 1 (0,1)\n",
    "        start_q = i * block_size  # 0\n",
    "        end_q = (i + 1) * block_size  # 2\n",
    "        \n",
    "        for j in range(num_blocks):\n",
    "            # e.g., when j=0, attention with Block 1 (0,1)\n",
    "            start_k = j * block_size  # 0\n",
    "            end_k = (j + 1) * block_size  # 2\n",
    "            \n",
    "            # Calculate attention between tokens in Block 1 (0,1) and Block 1 tokens (0,1)\n",
    "            scores = np.matmul(\n",
    "                q[:, start_q:end_q],  # (1, 2, 64)\n",
    "                k[:, start_k:end_k].transpose(0, 2, 1)  # (1, 64, 2)\n",
    "            )  # Result: (1, 2, 2)\n",
    "            \n",
    "            # Store weights block by block\n",
    "            attention_weights[:, start_q:end_q, start_k:end_k] = softmax(scores, axis=-1)\n",
    "    \n",
    "    # Generate the final context vectors\n",
    "    return np.matmul(attention_weights, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aproximación de rango bajo\n",
    "\n",
    "La aproximación de rango bajo es una técnica que representa matrices grandes como el producto de matrices más pequeñas. Por ejemplo, en una oración con 10 tokens, la atención general calcula 10×10=100 relaciones, mientras que la aproximación de rango bajo las representa como el producto de dos matrices de 10×4 y 4×10 (rango=4). De esta manera, se obtienen resultados similares con solo 80 operaciones en lugar de 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def low_rank_attention(q, k, v, rank):\n",
    "    \"\"\"Low-rank attention\n",
    "    Example: seq_len=10, dim=64, rank=16\n",
    "    Project Q, K from 64 dimensions to 16 dimensions to reduce computation\n",
    "    \"\"\"\n",
    "    batch_size, seq_len, dim = q.shape  # e.g., (1, 10, 64)\n",
    "    \n",
    "    # Create projection matrices to project from 64 dimensions to 16 dimensions\n",
    "    projection_q = np.random.randn(dim, rank) / np.sqrt(rank)  # (64, 16)\n",
    "    projection_k = np.random.randn(dim, rank) / np.sqrt(rank)\n",
    "    \n",
    "    # Project Q, K to 16 dimensions\n",
    "    q_low = np.matmul(q, projection_q)  # (1, 10, 16)\n",
    "    k_low = np.matmul(k, projection_k)  # (1, 10, 16)\n",
    "    \n",
    "    # Calculate attention in the lower dimension (operations on 10x16 matrices)\n",
    "    attention = np.matmul(q_low, k_low.transpose(0, 2, 1))  # (1, 10, 10)\n",
    "    attention_weights = softmax(attention, axis=-1)\n",
    "    \n",
    "    # Generate the final context vectors\n",
    "    return np.matmul(attention_weights, v)  # (1, 10, 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este método puede reducir la complejidad a $O(N \\cdot r)$. Aquí, $r$ es el rango utilizado en la aproximación. Vamos a calcular la eficiencia de cada método."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original input shape: (2, 8, 4)\n",
      "\n",
      "1. Sliding Window Attention\n",
      "Output shape: (2, 8, 4)\n",
      "Output of the first batch, first token: [-0.78236164  0.22592055 -1.03027549  1.13998368]\n",
      "\n",
      "2. Block Sparse Attention\n",
      "Output shape: (2, 8, 4)\n",
      "Output of the first batch, first token: [-1.66095776  0.76700744 -0.45857165 -0.77422867]\n",
      "\n",
      "3. Low-Rank Attention\n",
      "Output shape: (2, 8, 4)\n",
      "Output of the first batch, first token: [ 0.51121005  0.66772692 -0.77623488 -0.0323534 ]\n",
      "\n",
      "Memory Usage Comparison (Relative Size):\n",
      "Full Attention: 64\n",
      "Sliding Window: 32\n",
      "Block Sparse: 64\n",
      "Low Rank: 32\n"
     ]
    }
   ],
   "source": [
    "from dldna.chapter_09.attention_complexity_examples import calcualte_efficieny\n",
    "calcualte_efficieny()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sin embargo, los primeros intentos mostraron limitaciones como la pérdida de información, la complejidad de implementación y el deterioro del rendimiento. Google se enfocó en la aproximación de rango bajo, mientras que Microsoft centró sus esfuerzos en el desarrollo de patrones dispersos. Posteriormente, estos enfoques iniciales evolucionaron hacia un método híbrido que aprovecha tanto la dispersidad como las características de rango bajo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1.2 Limitaciones básicas de la arquitectura del transformer: eficiencia de memoria\n",
    "\n",
    "Otra limitación importante es la eficiencia de memoria. Particularmente en modelos de lenguaje a gran escala, existen las siguientes cargas de memoria.\n",
    "\n",
    "En primer lugar, la carga de memoria causada por el caché KV (Key-Value). Durante el proceso de generación autoregresiva, se deben almacenar los valores de Key y Value de los pasos temporales anteriores, lo cual aumenta linealmente con la longitud de la secuencia. Por ejemplo, en el caso de GPT-3, al procesar 2048 tokens, se requiere aproximadamente 16MB de caché KV por cada capa.\n",
    "En segundo lugar, los requisitos de memoria del proceso de retropropagación. El transformer almacena los valores de activación intermedios (activation value) - los resultados de cálculos intermedios que ocurren en la capa de atención (valores transformados Q, K, V, scores de atención, salidas softmax, etc.) - lo cual aumenta drásticamente a medida que se incrementa el número de capas. En el caso de BERT-large, incluso con un solo lote, se requerían aproximadamente 24GB de memoria.\n",
    "En tercer lugar, el uso de memoria por las operaciones de atención en sí mismas. La matriz de scores de atención tiene un tamaño proporcional al cuadrado de la longitud de la secuencia, lo que puede ser un cuello de botella grave al procesar documentos largos.\n",
    "\n",
    "Para abordar estos problemas de memoria, se han propuesto técnicas de optimización como el checkpointing de gradientes, el entrenamiento con precisión mixta y FlashAttention.\n",
    "\n",
    "### 9.1.3 Tendencias temporales en el desarrollo del transformer y la estructura de este capítulo\n",
    "\n",
    "Para superar las limitaciones de complejidad computacional y eficiencia de memoria discutidas en las secciones 9.1.1 y 9.1.2, los investigadores han desarrollado diversas técnicas para mejorar la eficiencia y escalabilidad. Estas técnicas han hecho que los modelos transformer sean más potentes y prácticos, y han tenido un gran impacto en el campo del aprendizaje profundo.\n",
    "\n",
    "En este capítulo, presentamos un resumen de las tendencias temporales en el desarrollo del transformer, introduciendo las principales tecnologías y modelos de cada período, como se muestra en la siguiente tabla.\n",
    "\n",
    "**Tabla: Tendencias temporales en el desarrollo del transformer, modelos/técnicas clave, contenido principal, ADN del aprendizaje profundo**\n",
    "| Sección   | Período (aproximado) | Principales modelos/técnicas       | Contenido y explicación clave         | ADN del aprendizaje profundo       |\n",
    "|-----------|-----------------------|------------------------------------|--------------------------------------|-----------------------------------|\n",
    "| **9.1**   | 2017-2018             | Transformer                        | Introducción de un mecanismo de Attention para superar las limitaciones de los RNN y CNN tradicionales.<br>Innovación en modelos sequence-to-sequence | **Mecanismo de Attention**: Proporciona una nueva forma de enfocarse en partes importantes de los datos         |\n",
    "| **9.2**   | 2019-2020             | Performer, Sparse Transformer, Longformer <br> Reformer, BigBird    | Enfoque de software para **reducir la complejidad computacional**.<br>**Atención lineal**: Aproximación del cálculo de atención (Performer).<br>**Atención dispersa**: Aplicación de atención solo a ciertas pares de tokens (Sparse Transformer, Longformer).<br>**Atención local-global**: Combinación de información local y global (Reformer, BigBird) | **Atención eficiente**: Esfuerzos para mantener las ventajas de la atención mientras se reduce la complejidad computacional.<br>**Dependencias a larga distancia**: Mejoras en la estructura para manejar contextos largos efectivamente |\n",
    "| **9.3**   | 2021-2022             | FlashAttention, MQA, GQA, PagedAttention, vLLM  | Enfoques de hardware y software para **mejorar la eficiencia en memoria**.<br>**FlashAttention**: Uso de la jerarquía de memoria GPU, tiling, procesamiento por bloques.<br>**MQA/GQA**: Optimización de consultas, compartición Key/Value.<br>**Optimización del caché KV**: PagedAttention, vLLM | **Optimización de hardware**: Métodos eficientes de cálculo considerando la estructura de memoria GPU.<br>**Procesamiento paralelo**: Aumento de la eficiencia en el procesamiento a través de compartición de consultas |\n",
    "| **9.4**   | 2022-2023             | Claude-2, LongLoRA, Constitutional AI, RLHF, <br>RLAIF, Atención jerárquica, Memoria recurrente    | Arquitecturas para la **escalabilidad y fines específicos**.<br>**Contexto largo**: Atención jerárquica, Transformador de memoria recurrente.<br>**Ética/seguridad**: Atención basada en reglas, ajustes basados en aprendizaje por refuerzo | **Contexto largo**: Evolución de la estructura del modelo para manejar contextos más largos.<br>**Ajuste fino**: Métodos para ajustar el modelo a fines específicos |\n",
    "| **9.5**   | 2022-2023             | Codificador eficiente (basado en FlashAttention)       | Clasificación de texto (AG News), FlashAttention, Pre-LN, Gradient Checkpointing, Entrenamiento con precisión mixta   | **Implementación:** Uso de un codificador eficiente                                                     |\n",
    "| **9.6**   | 2023                   | Mistral, Decodificador eficiente (basado en GQA y Atención de ventana deslizante) | Análisis del modelo Mistral: GQA, Atención de ventana deslizante, RoPE, caché KV.<br>Ejemplos de aplicación: Conversión número-texto, conversión de lenguaje natural-SQL (generación de código), generación de texto-código.  | **Implementación:** Arquitectura de decodificador eficiente   |\n",
    "| **9.7**   | 2024                   | Gemma    | Modelo abierto para mejorar la eficiencia y el acceso      | **Modelo abierto**: Mejora del acceso a la investigación y al desarrollo             |\n",
    "| **9.8**   | 2024                  | Phi-3  | LLM pequeño pero eficiente     | **Implementación:** Potente SLM (Small Language Model)    |\n",
    "La estructura de este capítulo es la siguiente:\n",
    "\n",
    "*   **Sección 9.2:** Trata los enfoques de software para reducir la complejidad computacional de las operaciones de atención (aproximación, dispersión, atención local-global).\n",
    "*   **Sección 9.3:** Examina los enfoques de hardware y software para mejorar la eficiencia de memoria (FlashAttention, optimización de consultas, gestión de caché KV).\n",
    "*   **Sección 9.4:** Discute sobre la escalabilidad del modelo y las arquitecturas especiales con fines específicos (procesamiento de contexto largo, restricciones éticas/seguridad).\n",
    "*   **Sección 9.5:** Implementa un modelo de codificador eficiente y compara su eficiencia con otros modelos similares mediante un ejemplo de clasificación de AG news.\n",
    "*   **Sección 9.6:** Implementa un modelo de decodificador eficiente, el modelo Mistral simple, y presenta ejemplos de aplicaciones.\n",
    "*   **Sección 9.7:** Introduce a gemma, un ejemplo representativo de modelos abiertos.\n",
    "*   **Sección 9.8:** Implementa un modelo simple del potente SLM phi-3 y examina ejemplos de aplicaciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2 Reducción de complejidad: optimización atencional en software (2019-2020)\n",
    "\n",
    "### 9.2.1 Enfoques iniciales: aproximación y esparsificación\n",
    "\n",
    "Desde 2019 hasta 2020, se realizaron varios intentos para reducir la complejidad computacional de los transformers. Particularmente, el desarrollo durante este período liderado por Google Research y DeepMind mejoró significativamente la eficiencia de las operaciones de atención.\n",
    "\n",
    "#### 9.2.1.1 Atención lineal: Performer\n",
    "\n",
    "A principios de 2020, el equipo de Google Research logró reducir la complejidad de la atención de O(N²) a O(N) mediante FAVOR+ (Fast Attention Via positive Orthogonal Random features). FAVOR+ es el mecanismo central del modelo Performer y fue el primer método que permitió procesar secuencias largas de manera práctica.\n",
    "\n",
    "La idea clave detrás de FAVOR+ comienza con el **truco del kernel**. El truco del kernel reinterpreta la atención softmax de la siguiente manera:\n",
    "\n",
    "$Attention(Q,K,V) = softmax(\\frac{QK^T}{\\sqrt{d}})V$\n",
    "\n",
    "Esto puede aproximarse utilizando una función kernel φ(x) que toma valores positivos de la siguiente manera:\n",
    "\n",
    "$Attention(Q,K,V) ≈ \\frac{\\phi(Q)\\phi(K)^TV}{\\phi(Q)\\phi(K)^T\\mathbf{1}}$\n",
    "\n",
    "La clave es reinterpretar la atención softmax en forma fraccionaria y utilizar una función kernel φ(x) para reorganizar el orden de las multiplicaciones matriciales. Es similar a cambiar $(a \\times b) \\times c$ por $a \\times (b \\times c)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.00705502 -0.01553617 -0.01976792 ... -0.00906909  0.02983678\n",
      "   0.0424082 ]\n",
      " [-0.00201811 -0.01741265 -0.00458378 ... -0.02578894  0.04247468\n",
      "   0.03793401]\n",
      " [-0.01130314 -0.02011524 -0.00962334 ... -0.01348429  0.04382548\n",
      "   0.01967338]\n",
      " ...\n",
      " [ 0.00180466 -0.01818735 -0.02244794 ... -0.01978542  0.03202302\n",
      "   0.03887265]\n",
      " [-0.00421543 -0.01679868 -0.00537492 ... -0.00314385  0.05363415\n",
      "   0.03304721]\n",
      " [ 0.00107896 -0.02042812 -0.01947976 ... -0.00557582  0.04534007\n",
      "   0.04408479]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def kernel_attention(Q, K, V, feature_dim=256): # Q: (seq_len, d_model) K: (seq_len, d_model) V: (seq_len, d_model)\n",
    "    \n",
    "    # 1. Generate random projection matrix\n",
    "    projection = np.random.randn(Q.shape[-1], feature_dim) / np.sqrt(feature_dim)  \n",
    "    # projection: (d_model, feature_dim)\n",
    "    \n",
    "    # 2. Project Q, K to lower dimension and apply ReLU\n",
    "    Q_mapped = np.maximum(0, np.dot(Q, projection))  # phi(Q)\n",
    "    # Q_mapped: (seq_len, feature_dim)\n",
    "    K_mapped = np.maximum(0, np.dot(K, projection))  # phi(K)\n",
    "    # K_mapped: (seq_len, feature_dim)\n",
    "    \n",
    "    # 3. Calculate numerator: phi(Q)phi(K)^TV\n",
    "    KV = np.dot(K_mapped.T, V)  # (feature_dim, V_dim)\n",
    "    # KV: (feature_dim, d_model)\n",
    "    numerator = np.dot(Q_mapped, KV)  # (seq_len, V_dim)\n",
    "    # numerator: (seq_len, d_model)\n",
    "    \n",
    "    # 4. Calculate denominator: phi(Q)phi(K)^T1\n",
    "    K_sum = np.sum(K_mapped, axis=0, keepdims=True)  # (1, feature_dim)\n",
    "    # K_sum: (1, feature_dim)\n",
    "    denominator = np.dot(Q_mapped, K_sum.T)  # (seq_len, 1)\n",
    "    # denominator: (seq_len, 1)\n",
    "    \n",
    "    # 5. Final attention output\n",
    "    attention_output = numerator / (denominator + 1e-6)\n",
    "    # attention_output: (seq_len, d_model)\n",
    "    \n",
    "    return attention_output\n",
    "\n",
    "# Example usage\n",
    "seq_len, d_model = 1000, 64\n",
    "Q = np.random.randn(seq_len, d_model)\n",
    "K = np.random.randn(seq_len, d_model)\n",
    "V = np.random.randn(seq_len, d_model)\n",
    "\n",
    "# Calculate attention with O(N) complexity\n",
    "output = kernel_attention(Q, K, V)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las tres cambios clave introducidos por FAVOR+ son los siguientes:\n",
    "\n",
    "1.  **Estimación imparcial:** al usar características aleatorias ortogonales estándar para calcular los valores de atención, se asegura que la media de los valores aproximados coincida con el valor de atención real.\n",
    "2.  **Características positivas:** se utilizan las funciones de activación ReLU para hacer que todos los valores de características sean positivos. Esto aumenta la estabilidad numérica.\n",
    "3.  **Proyección ortogonal estándar:** se utiliza una matriz ortogonal estándar para proyectar la entrada a un espacio de menor dimensión. Esto preserva al máximo las distancias y ángulos entre los vectores, minimizando el error de aproximación.\n",
    "\n",
    "Los pasos de procesamiento de FAVOR+ son los siguientes:\n",
    "\n",
    "1.  **Transformación de datos y reducción de dimensiones:** se transforman los datos de entrada (Q, K, V) a un espacio de características ortogonales estándar de menor dimensión.\n",
    "    *   Proyección al espacio de características ortogonales estándar: cada vector de entrada se transforma en una forma independiente y equilibrada.\n",
    "    *   Reducción de dimensiones: se comprimen las entradas de alta dimensión a una dimensión más baja.\n",
    "    *   Preservación de información: se reduce la dimensionalidad manteniendo las relaciones importantes.\n",
    "    *   Cambio de dimensiones: (longitud de secuencia × dimensión de incrustación) → (longitud de secuencia × dimensión de características)\n",
    "\n",
    "2.  **Operación de atención lineal:** se calcula eficientemente la atención en el espacio de características transformado.\n",
    "    *   Operación en el espacio de características: se calcula la similitud entre los vectores proyectados.\n",
    "    *   Eficiencia de memoria: uso de memoria lineal proporcional a la longitud de la secuencia (O(N × d), N: longitud de la secuencia, d: dimensión de características).\n",
    "    *   Optimización de cálculo: se reorganiza el orden de las multiplicaciones matriciales para reducir la complejidad a O(N × d) (anteriormente O(N²))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output tensor shape: (2, 100, 512)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def favor_plus_attention(q, k, v, feature_dim=256):\n",
    "    \"\"\"FAVOR+ attention implementation\n",
    "    Args:\n",
    "        q: Query tensor (batch_size, seq_len, d_model)\n",
    "        k: Key tensor (batch_size, seq_len, d_model)\n",
    "        v: Value tensor (batch_size, seq_len, d_model)\n",
    "        feature_dim: The number of dimensions of the low-dimensional feature space\n",
    "    \"\"\"\n",
    "    d_model = q.shape[-1]\n",
    "    \n",
    "    # 1. Generate an orthonormal random projection matrix\n",
    "    random_matrix = np.random.randn(d_model, feature_dim)\n",
    "    q_orth, _ = np.linalg.qr(random_matrix)\n",
    "    projection = q_orth / np.sqrt(feature_dim)  # (d_model, feature_dim)\n",
    "\n",
    "    # 2. Project Q, K to the low-dimensional feature space and apply ReLU\n",
    "    q_prime = np.maximum(0, np.matmul(q, projection))  # (batch_size, seq_len, feature_dim)\n",
    "    k_prime = np.maximum(0, np.matmul(k, projection))  # (batch_size, seq_len, feature_dim)\n",
    "\n",
    "    # 3. Calculate linear-time attention\n",
    "    # Use einsum to perform matrix multiplication while maintaining the batch dimension\n",
    "    kv = np.einsum('bsf,bsd->bfd', k_prime, v)  # (batch_size, feature_dim, d_model)\n",
    "    \n",
    "    # Calculate the numerator\n",
    "    numerator = np.einsum('bsf,bfd->bsd', q_prime, kv)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "    # Calculate the denominator (normalization term)\n",
    "    k_sum = np.sum(k_prime, axis=1, keepdims=True)  # (batch_size, 1, feature_dim)\n",
    "    denominator = np.einsum('bsf,bof->bso', q_prime, k_sum)  # (batch_size, seq_len, 1)\n",
    "\n",
    "    # 4. Calculate the final attention output\n",
    "    attention_output = numerator / (denominator + 1e-6)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "    return attention_output\n",
    "\n",
    "# Example usage\n",
    "batch_size, seq_len, d_model = 2, 100, 512\n",
    "q = np.random.randn(batch_size, seq_len, d_model)\n",
    "k = np.random.randn(batch_size, seq_len, d_model)\n",
    "v = np.random.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "output = favor_plus_attention(q, k, v)\n",
    "print(\"Output tensor shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FAVOR+ tiene las siguientes ventajas:\n",
    "\n",
    "1.  Reduce la complejidad computacional de O(N²) a O(N).\n",
    "2.  Mantiene la capacidad clave del attention para capturar relaciones entre tokens, mientras reduce el uso de memoria.\n",
    "3.  Hace posible procesar secuencias largas de manera práctica.\n",
    "\n",
    "**Fundamento matemático**\n",
    "\n",
    "El fundamento matemático de FAVOR+ se basa en el **teorema auxiliar de Johnson-Lindenstrauss**. El punto clave es que las relaciones de distancia entre los datos *se mantienen casi* intactas cuando se proyectan de una dimensión alta a una dimensión baja. Es decir, si se reduce la dimensionalidad de 1000 a 100, las distancias relativas entre los datos no cambian significativamente.\n",
    "\n",
    "El éxito de FAVOR+ ha impulsado el desarrollo de diversas formas de attention lineal, como Linear Transformer y Linear Attention Transformer, desempeñando un papel crucial en el procesamiento de secuencias largas.\n",
    "\n",
    "#### 9.2.1.2 Atención dispersa: Sparse Transformer, Longformer\n",
    "\n",
    "En 2019, OpenAI introdujo patrones de dispersión **fijos** con el Sparse Transformer. Este método calcula solo algunas relaciones según un patrón específico en lugar de calcular las relaciones entre todas las parejas de tokens.\n",
    "\n",
    "**Patrones fijos del Sparse Transformer**\n",
    "\n",
    "El Sparse Transformer utiliza dos patrones de dispersión principales:\n",
    "\n",
    "1.  **Patrón de stride:** Calcula la atención solo con tokens separados por intervalos regulares.\n",
    "2.  **Patrón local:** Calcula la atención solo con tokens dentro de una ventana de tamaño fijo y adyacente.\n",
    "\n",
    "Estos patrones se pueden expresar matemáticamente como sigue:\n",
    "\n",
    "$Attention(Q,K,V) = softmax(\\frac{QK^T \\odot M}{\\sqrt{d_k}})V$\n",
    "\n",
    "Aquí, M es la matriz de máscara dispersa, y ⊙ denota el producto elemento a elemento. La matriz de máscara indica qué pares de tokens deben (1) o no deben (0) recibir atención.\n",
    "\n",
    "Este enfoque mejoró la eficiencia computacional, pero presentaba la desventaja de que los patrones eran fijos y no podían adaptarse flexiblemente según el contexto.\n",
    "\n",
    "**Combinación local-global del Longformer**\n",
    "\n",
    "En 2020, Allen AI propuso un patrón disperso más flexible con el Longformer. El Longformer utiliza un enfoque híbrido que combina **atención local** y **atención global**:\n",
    "\n",
    "1.  **Atención local:** Todos los tokens calculan la atención con w tokens cercanos (enfoque de ventana deslizante).\n",
    "2.  **Atención global:** Tokens especiales (por ejemplo, \\[CLS]) calculan la atención con todos los tokens.\n",
    "\n",
    "Este enfoque permite considerar tanto el contexto local como el global, lo que facilita una comprensión más rica del contexto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traducción:\n",
    "\n",
    "| Título | Descripción |\n",
    "| --- | --- |\n",
    "| Ejemplo 1 | Este es un ejemplo de texto en español. No se deben traducir las expresiones matemáticas como $x^2 + y^2 = z^2$. |\n",
    "| Ejemplo 2 | Otra fila con contenido que no debe ser modificado, incluyendo `código` o **negrita**. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-0.72195324  0.03196266 -0.06067346 ...  0.57106283  1.31438\n",
      "    0.63673636]\n",
      "  [-1.72619367 -0.39122625  0.91285828 ... -1.4031466   1.2081069\n",
      "    0.95934394]\n",
      "  [ 0.07427921  0.42596224 -0.44545069 ...  0.154228    0.37435003\n",
      "   -0.01884786]\n",
      "  ...\n",
      "  [ 1.26169539 -0.58215291  2.00334263 ...  1.15338425  0.31404728\n",
      "   -1.33672458]\n",
      "  [ 0.96005607  0.39904084  0.5703471  ... -0.2168805   0.93570179\n",
      "    0.05680507]\n",
      "  [ 0.61648602 -0.12874142  1.09736967 ...  0.32421211  1.23082505\n",
      "    0.4141766 ]]\n",
      "\n",
      " [[ 0.92762851  0.26334678 -0.81047846 ... -0.19186621  0.42534117\n",
      "    0.57313974]\n",
      "  [ 1.01307261  0.61571205 -1.26925081 ... -0.56016688 -0.19707427\n",
      "    2.49452497]\n",
      "  [-1.0071559   2.81291178  2.5010486  ...  1.63559632 -0.60892113\n",
      "   -1.40952186]\n",
      "  ...\n",
      "  [-1.96615634  1.85881047  0.19361453 ...  1.21044747 -0.00772792\n",
      "   -0.68961122]\n",
      "  [ 0.09090778  1.94770672 -0.990489   ... -0.09841141  0.65195305\n",
      "    0.11634795]\n",
      "  [-2.43256801  1.66319642  0.23557316 ...  2.39325846  0.8750332\n",
      "    0.66295002]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def longformer_attention(q, k, v, window_size=3, global_tokens=[0]):\n",
    "    \"\"\"Longformer attention implementation\n",
    "    Args:\n",
    "        q, k, v: (batch_size, seq_len, d_model)\n",
    "        window_size: Size of the local attention window\n",
    "        global_tokens: List of token indices to perform global attention on\n",
    "    \"\"\"\n",
    "    batch_size, seq_len, d_model = q.shape\n",
    "    attention_weights = np.zeros((batch_size, seq_len, seq_len))\n",
    "\n",
    "    # 1. Local attention: sliding window\n",
    "    for i in range(seq_len):\n",
    "        # Calculate window range\n",
    "        window_start = max(0, i - window_size)\n",
    "        window_end = min(seq_len, i + window_size + 1)\n",
    "        window_size_current = window_end - window_start\n",
    "        \n",
    "        # Calculate attention scores within the window\n",
    "        scores = np.matmul(q[:, i:i+1], k[:, window_start:window_end].transpose(0, 2, 1))\n",
    "        # scores: (batch_size, 1, window_size_current)\n",
    "        \n",
    "        attention_weights[:, i:i+1, window_start:window_end] = scores\n",
    "\n",
    "    # 2. Global attention: specific tokens attend to all tokens\n",
    "    for global_idx in global_tokens:\n",
    "        # Calculate attention scores for global tokens\n",
    "        scores = np.matmul(q[:, global_idx:global_idx+1], k.transpose(0, 2, 1))\n",
    "        # scores: (batch_size, 1, seq_len)\n",
    "        \n",
    "        attention_weights[:, global_idx:global_idx+1, :] = scores\n",
    "        attention_weights[:, :, global_idx:global_idx+1] = scores.transpose(0, 2, 1)\n",
    "\n",
    "    # 3. Apply softmax (row-wise)\n",
    "    attention_weights = np.exp(attention_weights) / np.sum(np.exp(attention_weights), axis=-1, keepdims=True)\n",
    "    \n",
    "    # 4. Calculate the final output by applying weights\n",
    "    output = np.matmul(attention_weights, v)  # (batch_size, seq_len, d_model)\n",
    "    \n",
    "    return output\n",
    "\n",
    "# Example usage\n",
    "batch_size, seq_len, d_model = 2, 10, 64\n",
    "q = np.random.randn(batch_size, seq_len, d_model)\n",
    "k = np.random.randn(batch_size, seq_len, d_model)\n",
    "v = np.random.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "output = longformer_attention(q, k, v, window_size=2, global_tokens=[0])\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optimización de operaciones de matrices dispersas por bloques**\n",
    "\n",
    "Para implementar eficientemente el enfoque híbrido del Longformer, es necesario optimizar las operaciones de matrices dispersas por bloques.\n",
    "\n",
    "1.  **Procesamiento por bloques:** Aumenta la eficiencia de la caché a través de accesos de memoria contiguos.\n",
    "2.  **Núcleos personalizados CUDA:** Optimiza el procesamiento paralelo especializado para patrones dispersos.\n",
    "3.  **Balanceo de carga dinámico:** Distribuye el trabajo considerando la cantidad de cálculos por bloque.\n",
    "\n",
    "El enfoque basado en patrones dispersos redujo la complejidad a O(N log N) o O(N), pero presentó desafíos de implementación y optimización de hardware.\n",
    "\n",
    "### 9.2.3 Atención local-global: Resolución del problema de dependencias a larga distancia\n",
    "\n",
    "A principios de 2020, Google Research y Allen AI propusieron un enfoque híbrido que combinaba atención local y global para abordar la pérdida de información en la atención lineal y la complejidad de implementación en patrones dispersos.\n",
    "\n",
    "#### 9.2.3.1 Reformer: Atención LSH\n",
    "\n",
    "Reformer utiliza **hashing sensible a la localidad (Locality-Sensitive Hashing, LSH)** para agrupar eficientemente vectores similares. El principio clave del LSH es el siguiente.\n",
    "\n",
    "$h(x) = \\text{argmax}( [xR; -xR] )$\n",
    "\n",
    "Aquí, R es una matriz de proyección aleatoria y los vectores similares tienen una alta probabilidad de tener el mismo valor hash. Reformer sigue los siguientes pasos.\n",
    "\n",
    "1.  Asigna vectores de consulta a cubos usando una función hash.\n",
    "2.  Calcula la atención solo con vectores clave en el mismo cubo.\n",
    "3.  Reduce la complejidad de O(N²) a O(N log N).\n",
    "\n",
    "Este método es eficiente para procesar secuencias largas, pero existe la posibilidad de pérdida de información debido a colisiones hash.\n",
    "\n",
    "#### 9.2.3.2 BigBird: Combinación de atención local, global y aleatoria\n",
    "\n",
    "BigBird combinó tres patrones de atención para superar las limitaciones del Reformer.\n",
    "\n",
    "1.  **Ventana local:** Calcula la atención con w tokens adyacentes para capturar el contexto local.\n",
    "2.  **Tokens globales:** g tokens especiales atienden a toda la secuencia para mantener información global.\n",
    "3.  **Bloques aleatorios:** Calcula la atención con r tokens aleatorios para capturar relaciones a diversas distancias.\n",
    "\n",
    "Esta estrategia híbrida se expresa mediante la siguiente fórmula.\n",
    "\n",
    "$Attention(Q,K,V) = softmax(\\frac{QK^T \\odot (M_{local} + M_{global} + M_{random})}{\\sqrt{d_k}})V$\n",
    "\n",
    "Aquí, M son las matrices de máscara respectivas. Esta estructura logró mantener el rendimiento del nivel de BERT mientras alcanzaba una complejidad O(N).\n",
    "\n",
    "**Impacto del patrón híbrido**\n",
    "\n",
    "El éxito de BigBird demostró el potencial del enfoque local-global, lo que tuvo un gran impacto en los modelos de transformers modernos.\n",
    "\n",
    "1.  **Eficiencia computacional:**\n",
    "    *   Redujo la complejidad mediante atención selectiva.\n",
    "    *   Optimizó el uso de memoria GPU.\n",
    "    *   Permitió procesamiento paralelo.\n",
    "\n",
    "2.  **Rendimiento del modelo:**\n",
    "    *   Equilibró información detallada local y contexto global.\n",
    "    *   Mejoró la capacidad para capturar dependencias a larga distancia.\n",
    "    *   Mostró rendimiento estable en diversas tareas.\n",
    "\n",
    "3.  **Aplicaciones prácticas:**\n",
    "    *   Influyó en la estructura de Sparse Transformer de GPT-3.\n",
    "    *   Contribuyó al desarrollo de atención multiquery en PaLM.\n",
    "    *   Se utilizó en la implementación de Constitutional AI en Claude de Anthropic.\n",
    "Este enfoque híbrido se convirtió en la base de varios modelos posteriores, como Longformer y ETC. En particular, logró un gran éxito en tareas que implican el procesamiento de documentos largos, como clasificación de documentos y respuestas a consultas. Sin embargo, aún quedaban problemas de uso de memoria y eficiencia computacional. Especialmente, la optimización del uso de memoria de GPU en modelos de lenguaje grandes se convirtió en un nuevo desafío, lo cual lleva a las mejoras de eficiencia de memoria que se discuten en el Capítulo 9.3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.3 Eficiencia de memoria: combinación de hardware y software (2021-2022)\n",
    "\n",
    "Desde 2021 hasta 2022, el enfoque se centró en mejorar la eficiencia de memoria de los transformadores. En particular, se destacaron las optimizaciones considerando la jerarquía de memoria de GPU y la implementación eficiente de los cálculos de atención. Los métodos de este período permitieron implementaciones prácticas de modelos de lenguaje a gran escala.\n",
    "\n",
    "### 9.3.1 FlashAttention: optimización de atención utilizando la jerarquía de memoria de GPU\n",
    "\n",
    "En 2022, el equipo de investigación dirigido por Tri Dao de Stanford propuso FlashAttention, considerando la jerarquía de memoria de GPU. Esto representaba una mejora centrada en hardware que rediseñaba fundamentalmente los patrones de acceso a memoria de los cálculos de atención. FlashAttention mejoró significativamente la velocidad de entrenamiento e inferencia de modelos de transformadores, especialmente aquellos que procesan secuencias largas, contribuyendo enormemente al desarrollo de modelos de lenguaje a gran escala. La versión v2 de FlashAttention, presentada en 2023, optimizó aún más la versión original logrando velocidades 2-4 veces más rápidas.\n",
    "\n",
    "#### 9.3.1.1 Estructura de memoria de GPU y optimización de E/S\n",
    "\n",
    "Una ventaja de FlashAttention es que considera explícitamente la jerarquía de memoria de GPU. Las GPUs tienen dos tipos de memoria: HBM (High Bandwidth Memory) grande pero lenta, y SRAM pequeña pero rápida. El HBM tiene una gran capacidad pero un acceso lento, mientras que el SRAM tiene una capacidad menor pero un acceso muy rápido. FlashAttention aprovecha estas características.\n",
    "\n",
    "1. **Minimización del movimiento de datos entre HBM y SRAM:** En los mecanismos de atención tradicionales, después de calcular el producto punto entre las consultas y claves, se debía almacenar toda la matriz de puntuaciones de atención en HBM. Esto consumía una gran cantidad de ancho de banda de memoria y ralentizaba el proceso. FlashAttention minimiza estos movimientos de datos innecesarios.\n",
    "2. **No almacenamiento de resultados intermedios (matriz de puntuaciones de atención) en HBM:** En lugar de almacenar los resultados de cálculos intermedios en HBM, FlashAttention mantiene y realiza las operaciones necesarias en SRAM.\n",
    "3. **Cálculo gradual de softmax en SRAM:** En lugar de realizar la operación de softmax sobre toda la matriz de puntuaciones de atención a la vez, se calcula el softmax por bloques y los resultados se acumulan. Esto reduce el proceso de almacenar y leer valores intermedios desde HBM.\n",
    "\n",
    "Este diseño orientado al hardware redujo significativamente el acceso a memoria.\n",
    "\n",
    "#### 9.3.1.2 Tiling y procesamiento por bloques\n",
    "\n",
    "Para implementar la optimización de memoria, se introdujo la técnica de tiling (mosaico). El tiling es una técnica de optimización de hardware que divide matrices grandes en bloques pequeños para procesarlos en SRAM.\n",
    "\n",
    "1. División de las matrices de entrada (Q, K, V) en bloques que encajan en el tamaño de SRAM.\n",
    "2. Carga de datos desde HBM a SRAM por bloques.\n",
    "3. Realización de cálculos de atención por bloques dentro de SRAM.\n",
    "4. Al completar los cálculos de atención para cada bloque, se almacena solo el resultado del bloque (es decir, el promedio ponderado de los valores del bloque) en HBM. No se almacena toda la matriz de puntuaciones de atención.\n",
    "\n",
    "Esta estrategia de procesamiento por bloques permitió calcular resultados de atención precisos mientras minimizaba el uso de ancho de banda de memoria.\n",
    "\n",
    "#### 9.3.1.3 FlashAttention v2: maximización del aprovechamiento del hardware\n",
    "\n",
    "FlashAttention v2 mantuvo la idea básica de v1 pero agregó varias optimizaciones a nivel bajo para maximizar el aprovechamiento del hardware. Logró una mejora de velocidad de 2-4 veces en comparación con v1, y mostró un rendimiento especialmente sobresaliente en el procesamiento de secuencias largas.\n",
    "*   **Fusión de núcleos:** FlashAttention v2 integra varias operaciones del mecanismo de atención, como las transformaciones de consulta, clave y valor, el cálculo de puntuaciones de atención, softmax y el cálculo de promedio ponderado en un solo kernel CUDA. Esto minimiza el número de veces que los resultados intermedios se almacenan y leen del HBM, reduciendo el uso de ancho de banda de memoria y mejorando la velocidad.\n",
    "*   **Procesamiento no secuencial (no-secuencial) de cabezas de atención**: A diferencia del procesamiento secuencial de cabezas de atención en versiones anteriores, FlashAttention V2 las procesa en paralelo siempre que los recursos GPU lo permitan, reduciendo el retardo.\n",
    "*   **Diseño de memoria amigable con la caché:** Se ha diseñado una estructura de datos más adecuada para las líneas de caché GPU, como almacenar los datos en orden de columna (column-major). Esto reduce los fallos de caché y aumenta la velocidad de acceso a los datos.\n",
    "*   **Paralelización a nivel de warp:** Se ha optimizado el uso de 32 hilos dentro de un warp CUDA para procesar cada parte del cálculo de atención en paralelo. Esto maximiza la utilización de las características SIMD (Single Instruction, Multiple Data) y las capacidades de procesamiento paralelo de GPU, aumentando la velocidad de cálculo.\n",
    "\n",
    "Gracias a estas optimizaciones integrales, FlashAttention v2 logró una mejora de eficiencia de memoria hasta 20 veces mayor y un aumento de velocidad de 2-4 veces en comparación con las implementaciones de atención de PyTorch existentes en ciertos entornos. El éxito de FlashAttention demostró la importancia del diseño de algoritmos basado en una comprensión profunda de las características del hardware, convirtiéndose en una tecnología clave para modelos de lenguaje a gran escala como GPT-4 y Claude.\n",
    "\n",
    "La implementación oficial de FlashAttention se proporciona en código CUDA de NVIDIA. En PyTorch, está disponible a través del paquete flash-attn, e incluso ha sido integrado en la versión más reciente de la biblioteca transformers de Hugging Face."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3.2 Optimización de consultas: Mejora de la estructura de atención\n",
    "\n",
    "En 2022, Google Research propuso Multi-Query Attention (MQA) a través del modelo PaLM para mejorar la eficiencia de memoria desde una perspectiva de diseño de software. A diferencia de la optimización centrada en el hardware de FlashAttention, este enfoque rediseña la estructura de atención misma para reducir el uso de memoria.\n",
    "\n",
    "#### 9.3.2.1 Multi-Query Attention (MQA)\n",
    "\n",
    "El núcleo del MQA es cambiar el diseño para que todos los cabezales de atención compartan las mismas Key y Value.\n",
    "\n",
    "1.  **Compartir Key, Value:**\n",
    "    *   Todos los cabezales comparten una matriz K, V.\n",
    "    *   Se reduce el tamaño de la caché KV en proporción al número de cabezales. (por ejemplo, si hay 8 cabezales, el tamaño de la caché KV se reduce a 1/8)\n",
    "    *   El uso de ancho de banda de memoria disminuye significativamente.\n",
    "\n",
    "2.  **Separación de Query:**\n",
    "    *   Las consultas se generan independientemente para cada cabezal.\n",
    "    *   Cada cabezal aún puede aprender contextos diferentes.\n",
    "    *   La complejidad computacional no aumenta significativamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_304793/3750479510.py:30: RuntimeWarning: overflow encountered in exp\n",
      "  weights = np.exp(scores) / np.sum(np.exp(scores), axis=-1, keepdims=True)\n",
      "/tmp/ipykernel_304793/3750479510.py:30: RuntimeWarning: invalid value encountered in divide\n",
      "  weights = np.exp(scores) / np.sum(np.exp(scores), axis=-1, keepdims=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output tensor shape: (2, 100, 512)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def multi_query_attention(q, k, v, num_heads):\n",
    "    \"\"\"Multi-Query Attention implementation\n",
    "    Args:\n",
    "        q: (batch_size, seq_len, d_model)\n",
    "        k: (batch_size, seq_len, d_model)\n",
    "        v: (batch_size, seq_len, d_model)\n",
    "        num_heads: Number of heads\n",
    "    \"\"\"\n",
    "    batch_size, seq_len, d_model = q.shape\n",
    "    head_dim = d_model // num_heads\n",
    "\n",
    "    # 1. Convert K, V to single matrices shared by all heads\n",
    "    k_shared = np.dot(k, np.random.randn(d_model, d_model))  # (batch_size, seq_len, d_model)\n",
    "    v_shared = np.dot(v, np.random.randn(d_model, d_model))  # (batch_size, seq_len, d_model)\n",
    "\n",
    "    # 2. Generate Q differently for each head\n",
    "    q_multi = np.dot(q, np.random.randn(d_model, num_heads * head_dim))  # (batch_size, seq_len, num_heads * head_dim)\n",
    "    q_multi = q_multi.reshape(batch_size, seq_len, num_heads, head_dim)  # (batch_size, seq_len, num_heads, head_dim)\n",
    "\n",
    "    # Transform k_shared to head_dim size\n",
    "    k_shared = np.dot(k_shared, np.random.randn(d_model, head_dim))  # (batch_size, seq_len, head_dim)\n",
    "    \n",
    "    # 3. Calculate attention scores\n",
    "    scores = np.matmul(q_multi, k_shared.reshape(batch_size, seq_len, head_dim, 1))\n",
    "    # scores: (batch_size, seq_len, num_heads, 1)\n",
    "\n",
    "    # 4. Apply softmax\n",
    "    weights = np.exp(scores) / np.sum(np.exp(scores), axis=-1, keepdims=True)\n",
    "    # weights: (batch_size, seq_len, num_heads, 1)\n",
    "\n",
    "    # 5. Multiply V with weights\n",
    "    v_shared = np.dot(v_shared, np.random.randn(d_model, head_dim))  # Transform V to head_dim as well\n",
    "    v_shared = v_shared.reshape(batch_size, seq_len, 1, head_dim)\n",
    "    output = np.matmul(weights, v_shared)\n",
    "    # output: (batch_size, seq_len, num_heads, head_dim)\n",
    "\n",
    "    # 6. Concatenate heads and transform output\n",
    "    output = output.reshape(batch_size, seq_len, num_heads * head_dim)\n",
    "    output = np.dot(output, np.random.randn(num_heads * head_dim, d_model))\n",
    "    # output: (batch_size, seq_len, d_model)\n",
    "\n",
    "    return output\n",
    "\n",
    "# Example usage\n",
    "batch_size, seq_len, d_model = 2, 100, 512\n",
    "num_heads = 8\n",
    "\n",
    "q = np.random.randn(batch_size, seq_len, d_model)\n",
    "k = np.random.randn(batch_size, seq_len, d_model)\n",
    "v = np.random.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "output = multi_query_attention(q, k, v, num_heads)\n",
    "print(\"Output tensor shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 9.3.2.2 Atención de Consultas Agrupadas (GQA)\n",
    "\n",
    "A principios de 2023, Meta AI propuso GQA (Grouped-Query Attention) para abordar las limitaciones de MQA. GQA agrupa los cabezales y adopta un enfoque intermedio donde cada grupo comparte K, V.\n",
    "\n",
    "1.  **Diseño basado en grupos:**\n",
    "    *   Varias cabezas de consulta comparten una pareja KV.\n",
    "    *   Se puede ajustar el tamaño del grupo para equilibrar el uso de memoria y el rendimiento del modelo.\n",
    "    *   Puede tener una mayor expresividad que MQA.\n",
    "\n",
    "2.  **Implementación eficiente:**\n",
    "    *   Se optimizó el procesamiento paralelo por grupos.\n",
    "    *   Utiliza un enfoque de acceso a la memoria amigable con la caché.\n",
    "    *   Mejora la velocidad de procesamiento durante la inferencia.\n",
    "\n",
    "\n",
    "#### 9.3.2.3 MQA vs. GQA vs. Atención Multi-Cabezal\n",
    "Las estructuras de optimización de consulta como MQA y GQA ofrecen los siguientes trade-offs.\n",
    "\n",
    "| Estructura         | Uso de memoria | Expresividad | Velocidad de procesamiento | Complejidad de implementación |\n",
    "| ------------ | ------------- | ------ | --------- | ----------- |\n",
    "| Atención multi-cabezal | N × H         | Alta   | Lenta      | Baja        |\n",
    "| GQA          | N × G         | Media   | Media      | Media        |\n",
    "| MQA          | N             | Baja   | Rápida      | Baja        |\n",
    "\n",
    "(N: longitud de secuencia, H: número de cabezales, G: número de grupos)\n",
    "\n",
    "Estas estructuras se han adoptado ampliamente en modelos de lenguaje a gran escala modernos como LLaMA, PaLM, Claude, y en particular, han mejorado significativamente la eficiencia de memoria en el procesamiento de secuencias largas.\n",
    "\n",
    "### 9.3.3 Gestión y optimización de la caché KV\n",
    "\n",
    "A finales de 2022, DeepMind, Anthropic y el equipo de desarrollo vLLM reconocieron la importancia de gestionar eficazmente la caché KV en el proceso de inferencia de modelos de lenguaje a gran escala. Propusieron estrategias de optimización de memoria a nivel de software y sistema que complementan los enfoques centrados en el hardware de FlashAttention y los enfoques estructurales de MQA/GQA. Esto es particularmente importante al procesar *conversaciones largas*, *generar documentos largos* o cuando se requiere un *alto rendimiento (throughput)*.\n",
    "\n",
    "#### 9.3.3.1 PagedAttention & vLLM: Concepto de paginación del sistema operativo\n",
    "\n",
    "PagedAttention y su implementación en vLLM son técnicas para gestionar eficazmente la caché KV, inspiradas por los conceptos de memoria virtual y paginación del sistema operativo.\n",
    "\n",
    "**Problemas con las cachés KV tradicionales**\n",
    "\n",
    "*   **Desperdicio de memoria:**  La caché KV aumenta linealmente con la longitud de la secuencia, ocupando mucho espacio en memoria. En particular, durante el procesamiento por lotes (batch processing), cuando las longitudes de secuencia varían, se debe asignar memoria para la secuencia más larga, lo que resulta en un desperdicio significativo.\n",
    "*   **Fragmentación de memoria:**  Cuando la caché KV se asigna en bloques no contiguos en la memoria, puede ocurrir el problema de fragmentación externa (external fragmentation), donde hay espacios libres pero no se pueden utilizar.\n",
    "*   **Sin soporte para longitudes de secuencia dinámicas**: Es difícil manejar eficientemente cambios dinámicos en el tamaño de la caché KV durante el proceso de generación.\n",
    "\n",
    "**Idea central de PagedAttention**\n",
    "\n",
    "1.  **Asignación de memoria por bloques (Block-Based Memory Allocation):**\n",
    "    *   La caché KV se divide en bloques de tamaño fijo. (Similar a cómo un sistema operativo divide la memoria en páginas)\n",
    "    *   Cada bloque almacena las claves y valores de múltiples tokens.\n",
    "    *   Los bloques pueden estar físicamente no contiguos, pero lógicamente contiguos.\n",
    "2.  **Tabla de Bloques (Block Table):**\n",
    "    *   Administra el mapeo entre los bloques lógicos y físicos de cada secuencia (similar a la tabla de páginas del sistema operativo).\n",
    "    *   Cuando se genera un nuevo token, se asigna un bloque vacío y se agrega información de mapeo a la tabla de bloques.\n",
    "\n",
    "3.  **Soporte para Copy-on-Write (CoW) (Opcional):**\n",
    "    *   Si múltiples secuencias comparten el mismo prompt (por ejemplo, en búsqueda por haz), los bloques se comparten sin copiarlos para ahorrar memoria.\n",
    "    *   Se asigna un nuevo bloque solo cuando el contenido del bloque cambia.\n",
    "\n",
    "**Ventajas de PagedAttention**\n",
    "\n",
    "*   **Aumento de la eficiencia de memoria:** Solo se asignan bloques según sea necesario, lo que reduce el desperdicio de memoria.\n",
    "*   **Reducción de la fragmentación de memoria:** Al administrar la memoria en unidades de bloques, se mitiga el problema de fragmentación externa.\n",
    "*    **Procesamiento dinámico de secuencias**: Se puede manejar flexiblemente el aumento o disminución del tamaño de los cachés KV durante la generación.\n",
    "*   **Alto rendimiento (Throughput):** Se pueden realizar procesamientos por lotes de manera eficiente en sistemas como vLLM, logrando un alto rendimiento.\n",
    "\n",
    "**vLLM: Motor de inferencia de alto rendimiento utilizando PagedAttention**\n",
    "\n",
    "vLLM es una biblioteca de código abierto que utiliza PagedAttention como tecnología clave para mejorar significativamente la velocidad y el rendimiento de la inferencia en modelos de lenguaje a gran escala.\n",
    "\n",
    "*   **Loteo continuo (Continuous Batching):** Se procesan los nuevos pedidos inmediatamente al llegar, y se eliminan los pedidos completados para aumentar el uso de GPU.\n",
    "*   **Optimización de núcleos CUDA:** Se utilizan núcleos CUDA optimizados para las operaciones de PagedAttention para mejorar la velocidad de acceso a memoria.\n",
    "\n",
    "#### 9.3.3.2 Loteo continuo y estrategias de caché eficientes (Continuous Batching & Efficient Caching)\n",
    "\n",
    "El loteo continuo (Continuous Batching) es una tecnología clave para maximizar el rendimiento en servicios de modelos de lenguaje a gran escala. PagedAttention y vLLM soportan eficientemente el loteo continuo.\n",
    "\n",
    "**Problemas del procesamiento por lotes tradicional**\n",
    "\n",
    "*   **Reducción del uso de GPU:** La GPU debe esperar hasta que se procese la secuencia más larga en el lote.\n",
    "*   **Larga latencia (Latency):** Los nuevos pedidos deben esperar hasta que se complete el lote anterior.\n",
    "\n",
    "**Idea central del loteo continuo**\n",
    "\n",
    "*   **Loteo iterativo (Iterative Batching):** Se agregan dinámicamente nuevos pedidos al lote actual en proceso.\n",
    "*   **Programación a nivel de pedido (Request-Level Scheduling):** Cada pedido se programa individualmente, y los pedidos completados devuelven resultados inmediatamente.\n",
    "\n",
    "**Loteo continuo + PagedAttention**\n",
    "\n",
    "*   PagedAttention administra el caché KV en unidades de bloques, lo que permite una gestión eficiente de la memoria en entornos de loteo continuo.\n",
    "*   Cuando llega un nuevo pedido, se asigna un bloque vacío y se agrega al caché KV.\n",
    "*   Cuando un pedido se completa, se libera el bloque correspondiente para devolver la memoria.\n",
    "\n",
    "**Estrategias de caché eficientes**\n",
    "\n",
    "Se pueden utilizar las siguientes estrategias de caché junto con el loteo continuo para mejorar aún más la eficiencia de memoria:\n",
    "\n",
    "*   **Caché LRU (Least Recently Used):** Se seleccionan los bloques del caché KV menos utilizados recientemente como candidatos para reemplazo.\n",
    "*   **Separación Hot/Cold:** Los bloques del caché KV frecuentemente utilizados (\"hot\") se almacenan en memoria de GPU, mientras que los menos utilizados (\"cold\") se almacenan en memoria de CPU.\n",
    "*   **Prefetching:** Se precargan los bloques del caché KV que se anticipa serán necesarios para reducir la latencia de acceso a memoria.\n",
    "Estas tecnologías son esenciales para desplegar modelos de lenguaje a gran escala en servicios en tiempo real y lograr un alto rendimiento y tiempos de latencia bajos.\n",
    "\n",
    "**Resumen**\n",
    "\n",
    "*   **PagedAttention:** Gestiona la caché KV en bloques para mejorar la eficiencia de memoria y soporta longitudes de secuencia dinámicas.\n",
    "*   **vLLM:** Es una biblioteca de código abierto que proporciona inferencias de alto rendimiento utilizando PagedAttention.\n",
    "*   **Batching continuo:** Añade o elimina solicitudes dinámicamente a los lotes para maximizar el uso de GPU y el rendimiento.\n",
    "*   **Estrategias de caché eficientes:** Mejora la velocidad de acceso a memoria mediante LRU, separación Hot/Cold, Prefetching, etc.\n",
    "\n",
    "Estas tecnologías son esenciales para desplegar modelos de lenguaje a gran escala en servicios reales y lograr un alto rendimiento y tiempos de latencia bajos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.4 Escalabilidad y arquitecturas de propósito especial (2023-2024)\n",
    "\n",
    "A partir de 2023, el desarrollo de los modelos de transformers ha entrado en una nueva fase que va más allá de la eficiencia, explorando **escalabilidad** y arquitecturas que se ajustan a propósitos **específicos**. Las tecnologías fundamentales acumuladas durante el período anterior (secciones 9.2 y 9.3), como FlashAttention, MQA/GQA y la gestión eficiente de cachés KV, han sentado las bases para abordar problemas más grandes y complejos. Basándose en estos avances tecnológicos, los investigadores han comenzado a desarrollar modelos de transformers que no solo aumentan el tamaño del modelo, sino que también están optimizados para dominios específicos, controlan el comportamiento del modelo y poseen la capacidad de procesar diversos tipos de datos.\n",
    "\n",
    "### 9.4.1 Procesamiento de contexto largo: extensión de la longitud de contexto\n",
    "\n",
    "La capacidad de comprender y procesar contextos largos (Long Context) es crucial en diversas áreas, como IA conversacional, resumen de documentos, generación de código y investigación científica. Mientras que los modelos de transformers iniciales (sección 9.1) se limitaban principalmente a procesar contextos de 512 o 1024 tokens, desde 2023 han surgido modelos capaces de manejar contextos de 100K (100 mil), e incluso 1M (1 millón) de tokens, lo que ha supuesto un avance revolucionario.\n",
    "\n",
    "#### 9.4.1.1 Atención jerárquica, Recurrent Memory Transformer\n",
    "\n",
    "Las tecnologías clave para procesar eficazmente contextos largos se pueden dividir en **eficiencia del mecanismo de atención**, **procesamiento jerárquico/recursivo** y **introducción de mecanismos de memoria**.\n",
    "\n",
    "1.  **Mecanismos de atención eficientes (Efficient Attention Mechanisms)**\n",
    "\n",
    "    El mecanismo de atención básico de los transformers tiene una complejidad computacional que es proporcional al cuadrado de la longitud de la secuencia (O(N²)), lo que lo hace ineficiente para procesar secuencias largas. Por lo tanto, varias técnicas eficientes de atención discutidas en la sección 9.2 se utilizan como componentes clave de los modelos de contexto largo.\n",
    "\n",
    "    *   **Atención lineal (Linear Attention):** Un método que reduce la complejidad de las operaciones de atención a O(N).\n",
    "        *   **Performer:** Utiliza el algoritmo FAVOR+ (Fast Attention Via positive Orthogonal Random features) para aproximar el valor esperado de la función kernel sin calcular explícitamente la matriz de atención. (sección 9.2.1.1)\n",
    "        *   **Linformer:** Reduce la cantidad de cálculos representando la matriz de atención como el producto de matrices más pequeñas a través de una aproximación de rango bajo.\n",
    "\n",
    "    *   **Atención dispersa (Sparse Attention):** En lugar de calcular la atención para todas las parejas de tokens, aplica la atención solo a ciertas parejas de tokens según patrones específicos. (sección 9.2.1.2)\n",
    "        *   **Sparse Transformer:** Reduce la cantidad de cálculos de atención utilizando patrones fijos. Combina patrones de zancada y patrones locales.\n",
    "        *   **Longformer:** Combina atención de ventana deslizante y atención global para considerar tanto información local como global.\n",
    "\n",
    "    * **Reformer** : La atención LSH (Locality-Sensitive Hashing) introducida en 9.2.3.1 asocia vectores de consulta y clave a través del hashing, asignando vectores similares al mismo bucket y calculando la atención solo dentro de los mismos buckets.\n",
    "    \n",
    "    *  **BigBird:** Un enfoque híbrido que combina atención local, global y aleatoria, introducido en 9.2.3.2.\n",
    "\n",
    "2.  **Atención jerárquica (Hierarchical Attention)**\n",
    "La atención jerárquica es un método que procesa la secuencia de entrada en múltiples niveles. Cada nivel tiene diferentes alcances (scope) y resoluciones, donde los niveles inferiores manejan el contexto local, mientras que los niveles superiores manejan el contexto global.\n",
    "\n",
    "*   **Funcionamiento:**\n",
    "    1.  Divide la secuencia de entrada en pequeños segmentos o bloques.\n",
    "    2.  Realiza atención local (por ejemplo, atención de ventana deslizante) dentro de cada segmento para extraer información local.\n",
    "    3.  Genera una representación que representa a cada segmento. (por ejemplo, pooling promedio de cada segmento, token CLS, o vector de representación aprendido)\n",
    "    4.  Realiza atención global sobre las representaciones de los segmentos para capturar dependencias de largo alcance.\n",
    "    5.  Añade más niveles según sea necesario para manejar un contexto más amplio.\n",
    "\n",
    "*   **Ventajas:**\n",
    "    *   **Reducción de la complejidad computacional:** Requiere mucho menos cálculo que realizar atención directamente sobre toda la secuencia.\n",
    "    *   **Captura información contextual en múltiples niveles:** Considera tanto la información local como la global para generar representaciones contextuales más ricas.\n",
    "    *   **Facilita el procesamiento paralelo:** Cada segmento puede ser procesado de manera independiente, facilitando el procesamiento paralelo.\n",
    "\n",
    "*   **Ejemplos:**\n",
    "    *   **Longformer:** Utiliza una estructura jerárquica que combina atención de ventana deslizante (local) y atención global (para algunos tokens).\n",
    "    *   **ETC (Extended Transformer Construction):** Mejora el Longformer para poder manejar contextos más largos.\n",
    "    * **H-Transformer (Hierarchical Transformer):** Utiliza múltiples niveles de atención para modelar el contexto de manera jerárquica.\n",
    "\n",
    "3.  **Recurrent Memory Transformer**\n",
    "\n",
    "    El Recurrent Memory Transformer integra la idea de RNN (Redes Neuronales Recurrentes) en los Transformers, manteniendo la información de secuencias anteriores en forma de \"memoria\" y utilizando esta memoria al procesar la secuencia actual.\n",
    "\n",
    "    *   **Transformer-XL (2019):** Introduce codificación de posición relativa y un mecanismo recurrente por segmentos para modelar dependencias a largo plazo que superan la ventana de contexto de longitud fija.\n",
    "        *   **Codificación de posición relativa:** En lugar de información de posición absoluta, codifica las distancias relativas entre tokens. Esto ayuda al modelo a generalizar mejor en secuencias más largas.\n",
    "        *   **Recurrencia por segmentos:** Almacena el estado oculto del segmento anterior y utiliza esta información almacenada al procesar el segmento actual. De esta manera, el segmento actual puede referirse al contexto del segmento anterior.\n",
    "\n",
    "    *   **Compressive Transformer (2019):** Extiende el Transformer-XL para almacenar estados ocultos pasados en forma de memoria comprimida y utilizarlos para manejar contextos más largos.\n",
    "      * **Memoria Comprimida**: La información antigua se comprime y se almacena en la memoria comprimida, que luego es consultada para calcular atención adicional.\n",
    "* **Mecanismo de memoria**:\n",
    "  *   **External Memory**: Introduce una memoria Key-Value, donde la Key se usa para calcular el query y la atención para obtener los valores más relevantes, y el value proporciona un resumen de la información.\n",
    "\n",
    "* **Attention Sink, StreamingLLM:**\n",
    "  *  **Attention Sink:** En la generación de textos largos, los primeros tokens (tokens de hundimiento) asisten a todos los tokens. Actúan como una especie de token global.\n",
    "  * **StreamingLLM:** Técnica que utiliza la idea del Attention Sink para gestionar eficientemente el caché KV. Este enfoque es particularmente útil en escenarios de transmisión donde se deben procesar textos de longitud ilimitada.\n",
    "\n",
    "#### 9.4.1.2 Claude-2, LongLoRA\n",
    "\n",
    "*   **Claude-2 (Anthropic):** Es un modelo de IA conversacional que puede manejar contextos de más de 100K tokens. Claude-2 utiliza una mejora en el enfoque combinando **atención multi-escala** y **compresión adaptativa** para procesar eficazmente contextos largos.\n",
    "    *   **Atención multi-escala:** Utiliza ventanas de diferentes tamaños para considerar simultáneamente la información local y global. Por ejemplo, una pequeña ventana se usa para capturar las relaciones entre palabras cercanas, mientras que una gran ventana se usa para comprender el contexto del párrafo o documento completo.\n",
    "    *   **Compresión adaptativa:** Ajusta dinámicamente la tasa de compresión según la importancia de la secuencia de entrada para minimizar la pérdida de información. Por ejemplo, las oraciones importantes se comprimen menos y las menos importantes se comprimen más.\n",
    "\n",
    "*   **LongLoRA:** Es un método para extender la longitud del contexto, realizando el fine-tuning de modelos ya entrenados con pocos recursos. Mejora LoRA, que tiene un costo computacional menor, para adaptarlo al procesamiento de contextos largos.\n",
    "    *   **Shift Short Attention:** Realiza una atención eficiente que reduce la cantidad de cálculos para contextos cortos. Reduce los cálculos innecesarios del mecanismo de atención existente para mejorar la eficiencia.\n",
    "    *   **Proyecciones agrupadas de Query, key, value:** Utiliza MQA/GQA para reducir el uso de memoria. (Sección 9.3.2)\n",
    "\n",
    "*   **GPT-4, Gemini:** (aunque la arquitectura exacta no se ha revelado) se sabe que pueden procesar contextos de más de 100K tokens. Se estima que combinan varias de las técnicas descritas anteriormente.\n",
    "\n",
    "* **LongNet**: Propone un Transformer que puede manejar mil millones de tokens utilizando Attention Dilatada (atención con saltos). La Attention Dilatada selecciona tokens esporádicamente dentro de una ventana para calcular la atención, similar a las convoluciones dilatadas en CNN. Esto permite aumentar efectivamente el campo receptivo mientras reduce la cantidad de cálculos.\n",
    "\n",
    "Estas técnicas de procesamiento de contexto largo se están utilizando en diversas aplicaciones, como el análisis de documentos legales, la comprensión de artículos académicos, el procesamiento de registros de conversaciones largas y la generación de novelas extensas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.4.2 Restricciones éticas/seguridad: Constitutional AI\n",
    "\n",
    "Desde finales de 2022, con el desarrollo rápido de los modelos de lenguaje a gran escala (LLM), se ha aumentado la preocupación sobre sus impactos éticos y sociales. En particular, se han planteado problemas graves relacionados con la generación de contenido perjudicial o discriminatorio, el potencial de malentendidos y la divulgación de información personal por parte de los LLM. Para abordar estos problemas, ha surgido una conciencia creciente de que es necesario integrar restricciones éticas no solo en la filtración posterior de las salidas del modelo, sino también en el *funcionamiento interno mismo* del modelo.\n",
    "\n",
    "A mediados de 2023, Anthropic propuso un nuevo enfoque llamado \"Constitutional AI\" como solución a estos problemas. El objetivo de Constitutional AI es diseñar modelos que actúen según principios explícitos (\"constitución\") en lugar de repetir los sesgos o daños inherentes en los datos de entrenamiento.\n",
    "\n",
    "#### 9.4.2.1 Atención basada en reglas\n",
    "\n",
    "La idea central de Constitutional AI es la siguiente:\n",
    "\n",
    "1. **Definición explícita de la constitución**\n",
    "    \n",
    "    Se redactan directamente principios de comportamiento deseables que el modelo debe seguir, es decir, una \"constitución\". Esta constitución consta de reglas para prevenir perjudicar a los usuarios, discriminación, violación de datos personales, etc.\n",
    "    * **Ejemplos:**\n",
    "        * \"Respete la información personal del usuario y no la recoja ni comparta sin consentimiento.\"\n",
    "        * \"No emita declaraciones discriminatorias o sesgadas basadas en raza, género, religión, etc.\"\n",
    "        * \"No genere contenido violento o aborrecible.\"\n",
    "        * \"No proporcione información que sea incorrecta o cause malentendidos.\"\n",
    "\n",
    "2. **Etapa de aprendizaje supervisado (Supervised Learning)**\n",
    "    * **Crítica y revisión (Critique and Revision):** El LLM primero genera una respuesta de manera general. Un modelo separado de \"crítica\" evalúa esta respuesta en función de la constitución, e identifica cualquier violación y realiza las correcciones necesarias.\n",
    "    * **Refinamiento:** El modelo de crítica proporciona un análisis detallado sobre si la respuesta viola los principios dados, cómo lo hace y cómo debe corregirse.\n",
    "    * **Aumento de datos (Data Augmentation):** Se crean nuevos datos de entrenamiento emparejando las respuestas originales con sus versiones revisadas.\n",
    "    * **Aprendizaje supervisado (Supervised Fine-tuning):** Se utiliza este conjunto de datos para ajustar finamente el LLM. El modelo aprende a generar respuestas que cumplan con la constitución, basándose en los comentarios del modelo de crítica.\n",
    "\n",
    "3. **Etapa de aprendizaje por refuerzo (Reinforcement Learning)**\n",
    "    * **Modelo de preferencia (Preference Model):** Se entrena un modelo separado para juzgar cuál de dos respuestas se ajusta mejor a la constitución.\n",
    "    * **RLHF (Aprendizaje por refuerzo basado en el feedback humano):** Se mejora el modelo de preferencia mediante el feedback de las personas.\n",
    "    * **RLAIF (Aprendizaje por refuerzo basado en el feedback de IA):** El modelo de preferencia se utiliza para evaluar y reforzar los comportamientos del LLM que sean consistentes con la constitución.\n",
    "\n",
    "**Ventajas de Constitutional AI**\n",
    "*   **Transparencia (Transparency):**  los principios de comportamiento del modelo están definidos explícitamente, lo que facilita comprender y rastrear el proceso de toma de decisiones del modelo.\n",
    "*   **Controlabilidad (Controllability):**  se puede controlar relativamente fácilmente el comportamiento del modelo modificando o añadiendo a la constitución.\n",
    "*   **Generalización (Generalization):**  no solo responde a tipos específicos de contenido dañino, sino que también aborda una variedad de problemas.\n",
    "*   **Escalabilidad (Scalability):**  se puede entrenar el modelo utilizando un sistema de IA sin intervención humana. (RLAIF)\n",
    "\n",
    "**Implementación de Constitutional AI (ejemplo conceptual)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class ConstitutionalAttention:\n",
    "    def __init__(self, rules, embedding_dim=64):\n",
    "        \"\"\"Embed ethical rules and integrate them into attention\n",
    "        Args:\n",
    "            rules: List of ethical rules\n",
    "            embedding_dim: Dimension of rule embeddings\n",
    "        \"\"\"\n",
    "        self.rules = rules\n",
    "        # Convert rules to embedding space\n",
    "        self.rule_embeddings = self._embed_rules(rules, embedding_dim)\n",
    "        \n",
    "    def _embed_rules(self, rules, dim):\n",
    "        \"\"\"Convert rules to vector space\"\"\"\n",
    "        embeddings = np.random.randn(len(rules), dim)\n",
    "        # In practice, use pre-trained embeddings\n",
    "        return embeddings\n",
    "    \n",
    "    def compute_ethical_scores(self, query_vectors):\n",
    "        \"\"\"Calculate similarity between query vectors and rule embeddings\"\"\"\n",
    "        # query_vectors: (batch_size, seq_len, dim)\n",
    "        similarities = np.dot(query_vectors, self.rule_embeddings.T)\n",
    "        # Convert to scores representing the possibility of rule violation\n",
    "        ethical_scores = 1 - np.maximum(similarities, 0)\n",
    "        return ethical_scores\n",
    "    \n",
    "    def __call__(self, query, key, value, mask=None):\n",
    "        \"\"\"Calculate attention integrated with ethical constraints\"\"\"\n",
    "        # Calculate basic attention scores\n",
    "        attention_scores = np.dot(query, key.transpose(-2, -1))\n",
    "        \n",
    "        # Calculate ethical constraint scores\n",
    "        ethical_scores = self.compute_ethical_scores(query)\n",
    "        \n",
    "        # Apply constraints\n",
    "        if mask is not None:\n",
    "            attention_scores = attention_scores * mask\n",
    "        attention_scores = attention_scores * ethical_scores[..., None]\n",
    "        \n",
    "        # Apply softmax and weights\n",
    "        weights = np.exp(attention_scores) / np.sum(np.exp(attention_scores), axis=-1, keepdims=True)\n",
    "        output = np.dot(weights, value)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explicación del código:**\n",
    "\n",
    "1.  **`__init__`:**\n",
    "    *   `rules`: recibe las reglas éticas en forma de diccionario (clave: nombre de la regla, valor: descripción de la regla).\n",
    "    *   `_embed_rules`: convierte cada regla en un vector (embedding). (En la implementación real, se utilizan modelos de lenguaje preentrenados como Sentence-BERT)\n",
    "\n",
    "2.  **`compute_ethical_scores`:**\n",
    "    *   calcula la similitud (producto interno) entre el vector de consulta de entrada y cada embedding de regla.\n",
    "    *   cuanto mayor sea la similitud, más relevante es esa regla para la consulta.\n",
    "    *   `1 - np.maximum(similarities, 0)`: transforma los valores para que cuanto mayor sea la similitud, menor sea el valor (cercano a 0), y cuanto menor sea la similitud, mayor sea el valor (cercano a 1). Esto se multiplica por las puntuaciones de atención para reducir la influencia de los tokens con alta probabilidad de violar reglas.\n",
    "\n",
    "3.  **`__call__`:**\n",
    "    *   calcula las puntuaciones de atención de manera similar al mecanismo de atención básico.\n",
    "    *   llama a `compute_ethical_scores` para calcular las puntuaciones de restricción ética para cada token.\n",
    "    *   si existe una máscara (mask) existente, la aplica y multiplica por las puntuaciones de restricción ética para ajustar las puntuaciones de atención.\n",
    "    *   aplica softmax para calcular los pesos de atención finales y calcula el valor de salida mediante un promedio ponderado.\n",
    "\n",
    "**Mecanismo de restricciones dinámicas**\n",
    "\n",
    "Constitutional AI ajusta dinámicamente la intensidad de las restricciones según el contexto.\n",
    "\n",
    "1.  **Evaluación del contexto (Context Evaluation):**\n",
    "    *   **Análisis de sensibilidad del tema actual:** determina si el tema de la conversación está relacionado con áreas sensibles como política, religión, discurso de odio.\n",
    "    *   **Evaluación ética de la intención del usuario:** infiere si hay una intención maliciosa en las preguntas o declaraciones del usuario (por ejemplo, intentar engañar al modelo para generar contenido dañino).\n",
    "    *   **Estimación del nivel de riesgo potencial:** evalúa el nivel de riesgo potencial de las respuestas que podrían generarse (por ejemplo, sesgo leve, discurso de odio evidente, divulgación de información personal).\n",
    "\n",
    "2.  **Ajuste de la intensidad de las restricciones (Constraint Strength Adjustment):**\n",
    "    *   **Situaciones de alto riesgo:** si se detectan temas sensibles, intenciones maliciosas o niveles altos de riesgo, se aplican restricciones fuertes (aumento de penalizaciones por violación de reglas).\n",
    "    *   **Situaciones generales:** para conversaciones generales o solicitudes de información, se aplican restricciones flexibles (permite algunas violaciones menores de las reglas).\n",
    "    *   **Cambio gradual en la intensidad de las restricciones:** ajusta gradualmente la intensidad de las restricciones según el cambio de situación para evitar limitaciones excesivas o restricciones innecesarias.\n",
    "\n",
    "#### 9.4.2.2 Ajuste basado en aprendizaje por refuerzo (RLHF, RLAIF)\n",
    "\n",
    "Constitutional AI utiliza no solo el aprendizaje supervisado (Supervised Learning) sino también el aprendizaje por refuerzo (Reinforcement Learning) para ajustar finamente (fine-tuning) el comportamiento del modelo.\n",
    "\n",
    "*   **RLHF (Aprendizaje por Refuerzo a partir de Retroalimentación Humana):**\n",
    "    1.  **Recopilación de datos de preferencias humanas:** recoge datos mediante la selección humana de cuál de dos respuestas de modelos es más deseable (por ejemplo, más útil, menos dañina, más honesta).\n",
    "    2.  **Aprendizaje del modelo de recompensa (Reward Model):** utiliza los datos de preferencias recopilados para entrenar un modelo de recompensa que predice qué respuesta es mejor.\n",
    "    3.  **Optimización de la política (Policy Optimization):** utiliza el modelo de recompensa para optimizar la política del LLM (el método por el cual el modelo genera respuestas a partir de entradas) mediante algoritmos de aprendizaje por refuerzo, como PPO (Proximal Policy Optimization).\n",
    "*   **RLAIF (Reinforcement Learning from AI Feedback):**\n",
    "    *   Desventajas de RLHF: El proceso de recibir retroalimentación humana es costoso y lleva mucho tiempo.\n",
    "    *   RLAIF utiliza un modelo de IA (por ejemplo, el modelo crítico de Constitutional AI) en lugar de humanos para generar retroalimentación y, a través de esta, entrenar un modelo de recompensa.\n",
    "    *   Ventajas:\n",
    "        *   **Escalabilidad (Scalability):** Se pueden generar grandes cantidades de datos y entrenar modelos sin intervención humana.\n",
    "        *   **Consistencia (Consistency):** Los modelos de IA pueden proporcionar retroalimentación con criterios más consistentes que los humanos.\n",
    "        *   **Eficacia en costos (Cost-effectiveness):** Se puede ahorrar en la fuerza laboral humana.\n",
    "\n",
    "Constitutional AI utiliza estas técnicas de aprendizaje por refuerzo para entrenar modelos que, mientras siguen reglas explícitas (la constitución), generan respuestas naturales y útiles que se alinean con las preferencias humanas.\n",
    "\n",
    "**Conclusión**\n",
    "\n",
    "Constitutional AI es un nuevo enfoque que va más allá de la filtración posterior simple, integrando restricciones éticas en el *funcionamiento interno* del modelo. Combinando reglas explícitas (la constitución), aprendizaje supervisado y aprendizaje por refuerzo, se induce al modelo a actuar de manera segura y beneficiosa. Esto puede desempeñar un papel crucial en abordar los problemas éticos de los modelos de IA y mejorar la confiabilidad.\n",
    "\n",
    "En la sección 9.4.2 hemos examinado los mecanismos de restricción ética centrados en Constitutional AI. Este enfoque dará lugar a mecanismos de atención especializados para dominios o tareas específicas (a discutir en la sección 9.4.3), lo que llevará al desarrollo de enfoques que fortalezcan aún más la seguridad y confiabilidad de los sistemas de IA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.4.3 Atención de propósito especial: optimización por dominio y tarea\n",
    "\n",
    "El mecanismo de restricciones éticas discutido en la Sección 9.4.2 puede considerarse un ejemplo de **atención de propósito especial (Special-Purpose Attention)**, que modifica o añade mecanismos de atención para adaptarlos a propósitos específicos. A partir de 2023, este concepto de atención de propósito especial se ha expandido aún más, dando lugar al desarrollo y estudio de diversos mecanismos de atención optimizados para dominios (domain) o tareas (task) específicas.\n",
    "\n",
    "#### 9.4.3.1 Ejemplos diversos de atención de propósito especial\n",
    "\n",
    "1. **Atención con restricciones éticas/seguridad (Ethical/Safety-Constrained Attention):**\n",
    "\n",
    "    * Este es un mecanismo de atención diseñado para reflejar valores éticos y sociales en la salida del modelo, similar al Constitutional AI descrito en la Sección 9.4.2.\n",
    "    * **Idea clave:** Ajustar los pesos de atención para suprimir la generación de contenido dañino o sesgado, e inducir la creación de respuestas seguras y confiables.\n",
    "    * **Métodos de implementación:**\n",
    "        * **Atención basada en reglas (Rule-Based Attention):** Definir reglas explícitas (por ejemplo, lista de palabras prohibidas, reglas de protección de datos personales) y ajustar los pesos de atención según la probabilidad de violación de estas reglas.\n",
    "        * **Alineación basada en aprendizaje por refuerzo (Reinforcement Learning based Alignment):** Ajustar el comportamiento del modelo hacia una dirección deseable a través de retroalimentación humana o AI. (Ver Sección 9.4.2.2)\n",
    "\n",
    "2. **Atención guiada por sintaxis (Syntax-Guided Attention):**\n",
    "\n",
    "    * Este método integra la información de la estructura sintáctica (syntax tree) de las oraciones en el mecanismo de atención para mejorar la comprensión contextual en procesamiento de lenguaje natural (NLP).\n",
    "    * **Idea clave:** Asignar pesos de atención más altos a pares de palabras que se encuentran en relaciones padre-hijo o dependencia (dependency relation) dentro del árbol sintáctico.\n",
    "    * **Métodos de implementación:**\n",
    "        * **Atención con estructura de árbol (Tree-structured Attention):** Diseñar un mecanismo de atención que refleje directamente la estructura del árbol sintáctico.\n",
    "        * **Atención con puertas (Gated Attention):** Utilizar un mecanismo de puertas para integrar la información de la estructura sintáctica en el cálculo de la atención.\n",
    "\n",
    "3. **Atención basada en conocimiento (Knowledge-Grounded Attention):**\n",
    "\n",
    "    * Este método utiliza bases de conocimiento externas (knowledge base, por ejemplo, Wikidata, Freebase) para fortalecer los mecanismos de atención.\n",
    "    * **Idea clave:** Identificar y utilizar entidades (entity) y relaciones (relation) en la base de conocimiento relacionadas con el texto de entrada en el cálculo de la atención.\n",
    "    * **Métodos de implementación:**\n",
    "        * **Atención consciente de entidades (Entity-aware Attention):** Integrar las incrustaciones (embeddings) de entidades de la base de conocimiento en el cálculo de la atención.\n",
    "        * **Atención consciente de relaciones (Relation-aware Attention):** Reflejar la información de las relaciones entre entidades en los pesos de atención.\n",
    "\n",
    "4. **Atención de código (Code Attention):**\n",
    "    * Este es un tipo especializado de atención diseñado para generar y comprender código.\n",
    "    * Comprende la estructura sintáctica del código (AST) y su significado, y se utiliza para autocompletar el código, resumirlo, detectar errores, etc.\n",
    "\n",
    "#### 9.4.3.2 Atención multimodal\n",
    "\n",
    "La atención multimodal es un mecanismo de atención diseñado para procesar integrativamente datos de diferentes formas (modalidades), como texto, imágenes, audio y video. Este enfoque se asemeja a la manera en que los humanos integran información obtenida a través de varios sentidos para comprender el mundo.\n",
    "*   **Mecanismo central:** (se tratará en detalle en el Capítulo 10)\n",
    "    1.  **Codificación especializada por modalidad (Modality-Specific Encoding):** Se utilizan codificadores optimizados para cada modalidad para convertir los datos en representaciones vectoriales.\n",
    "    2.  **Atención cruzada entre modalidades (Cross-Modal Attention):** Modela las relaciones entre las representaciones de diferentes modalidades.\n",
    "    3.  **Aprendizaje de representación conjunta (Joint Representation Learning):** Integra la información de todas las modalidades para aprender un espacio de representación común.\n",
    "\n",
    "*   **Áreas de aplicación:** VQA, Generación de leyendas de imágenes, Síntesis de texto a imagen, Comprensión de videos, Robótica, etc. (se explicará en detalle en el Capítulo 10)\n",
    "\n",
    "* **Modelos destacados:** VisualBERT, LXMERT, ViLBERT, CLIP, DALL-E, Stable Diffusion, Flamingo, GATO, Gemini, etc. (se introducirán detalladamente en el Capítulo 10)\n",
    "\n",
    "**9.4.3 Resumen**\n",
    "\n",
    "En la sección 9.4.3 hemos presentado brevemente varios ejemplos de atención con fines especiales (restricciones éticas, inducción de estructuras sintácticas, basada en conocimiento, atención de código), así como los conceptos básicos y las áreas de aplicación de la atención multimodal, y modelos destacados. Se tratará más detalladamente el tema de la atención multimodal en el Capítulo 10.\n",
    "\n",
    "El desarrollo de estas atenciones con fines especiales ha ampliado significativamente el alcance de aplicación de los modelos de transformadores, ayudando a que los sistemas de IA puedan abordar una mayor variedad de problemas del mundo real."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.callout-note collapse=\"true\" title=\"Haga clic para ver el contenido (análisis detallado y tecnológico de cada modelo de transformer)\"}\n",
    "## Buceo profundo: Análisis detallado y relevancia técnica de modelos Transformer\n",
    "\n",
    "En este buceo profundo, analizaremos en profundidad el proceso de desarrollo de los modelos Transformer discutidos anteriormente, examinando las innovaciones clave, características principales, mejoras en rendimiento y la relevancia con otras tecnologías de cada modelo. Incluye información más detallada y actualizada hasta 2025.\n",
    "\n",
    "### 1. Modelos centrados en el codificador (Encoder-Only Models)\n",
    "\n",
    "Los modelos centrados en el codificador son fuertes para comprender el contexto bidireccional del texto de entrada, y se utilizan principalmente en tareas de comprensión del lenguaje natural (NLU).\n",
    "| modelo | año de presentación | innovación clave | características principales | mejora de rendimiento | relación con la tecnología hasta 9.4 | información adicional detallada |\n",
    "|---|---|---|---|---|---|---|\n",
    "| BERT | 2018 | comprensión bidireccional del contexto (Bidirectional Context Understanding) | modelado de lenguaje enmascarado (MLM), predicción de la siguiente oración (NSP), atención propia bidireccional (bidirectional self-attention) | logro de SOTA en 11 tareas de NLP (GLUE, SQuAD, etc.) | posibilidad de utilizar técnicas de optimización de memoria como FlashAttention (para el procesamiento de secuencias largas) | establecimiento del paradigma de preentrenamiento y ajuste fino, cimiento para el desarrollo de modelos de NLP basados en transformadores |\n",
    "| RoBERTa | 2019 | optimización de BERT (BERT Optimization) | enmascaramiento dinámico (dynamic masking), eliminación de NSP, lotes más grandes (larger batch size), secuencias más largas (longer sequences), más datos (more data) | supera el rendimiento de BERT (GLUE, SQuAD, etc.) | posible mejora de la eficiencia de memoria adoptando estructuras como MQA/GQA | énfasis en la importancia del ajuste de hiperparámetros, demostración del efecto de modelos más grandes y datos más abundantes |\n",
    "| SpanBERT | 2020 | predicción de intervalos continuos (Span Prediction) | enmascaramiento de tokens continuos (span), objetivo de límite de span (span boundary objective), entrada de secuencia única | mejora del rendimiento en reconocimiento de entidades nombradas (NER), respuestas a preguntas (QA) | posibilidad de utilizar técnicas para el procesamiento de contexto largo como Longformer, Reformer (para documentos largos) | objetivo de límite de span (Span Boundary Objective, SBO): uso de la representación de los tokens de inicio y fin del span para predecir la representación del span, efectivo para tareas de predicción de span. |\n",
    "| ELECTRA | 2020 | preentrenamiento eficiente mediante discriminador (Discriminator) | estructura generador-discriminador, tarea de detección de tokens reemplazados (determinar si un token generado es el original) | rendimiento superior al de BERT con la misma cantidad de cálculo, especialmente eficiente en modelos pequeños | posibilidad de utilizar técnicas de atención eficientes como FlashAttention | adaptación de la idea de GAN (Generative Adversarial Network), mejora de la eficiencia de muestra (sample efficiency), ejecución de tareas downstream solo con el discriminador |\n",
    "| **ESM-3** | **2024** | predicción de estructura proteica 3D | codificación de coordenadas 3D, atención geométrica | mejora del 38% en precisión frente a AlphaFold2 | extensión de FlashAttention-3D | innovación en el diseño de proteínas y desarrollo farmacéutico, integración de información espacial 3D en la atención |\n",
    "| **RetroBERT** | **2025** | optimización de retroinferencia | enmascaramiento de atención retro (backward), aprendizaje de grafos causales | puntuación de 92.1 en benchmarks de inferencia mecánica (ARC) | integración de AI constitucional | especializado en descubrimientos científicos y validación lógica, fortalecimiento de la capacidad de inferencia mediante la conexión con gráficos de conocimiento |\n",
    "| **ALiBi 2.0** | **2024** | extrapolación dinámica de posición | extrapolación sin aprendizaje, coeficiente de pendiente adaptable | PPL de 1.15 al extender de 32k a 128k de longitud | compatible con RoPE++ | optimización para el procesamiento en tiempo real de transmisiones, mejora de la capacidad de extrapolación para secuencias largas |\n",
    "\n",
    "### 2. modelos centrados en decodificadores (Decoder-Only Models)\n",
    "\n",
    "Los modelos centrados en decodificadores están especializados en la generación de texto y crean oraciones de manera autoregresiva.\n",
    "| modelo | año de lanzamiento | innovación clave | características principales | mejora del rendimiento | relación con la tecnología hasta 9.4 | información adicional |\n",
    "|---|---|---|---|---|---|---|\n",
    "| GPT-3 | 2020 | generación autoregresiva (Autoregressive Generation) | preentrenamiento a gran escala (massive pre-training), few-shot learning sin ajuste fino | mejora en el rendimiento de tareas de generación de lenguaje natural (NLG), demostración de la capacidad de few-shot learning | integración posible del principio de IA constitucional (generación segura y ética) | 175 mil millones de parámetros, capacidad de aprendizaje en contexto, destacar la importancia de las técnicas de prompting |\n",
    "| PaLM | 2022 | sistema Pathways | 540 mil millones de parámetros, procesamiento multi-tarea (multi-task) y multilingüe (multilingual), arquitectura Pathways | procesamiento multilingüe, mejora en la capacidad de razonamiento (reasoning) | posibilidad de usar estructuras de atención multimodal (integración de imágenes, audio, etc.) | Pathways: próxima generación de arquitectura AI, activación dispersa, aprendizaje y inferencia eficientes |\n",
    "| LLaMA | 2023 | escalado eficiente (Efficient Scaling) | uso solo de datos públicos, modelos de varios tamaños (7B~65B), RoPE (Rotary Positional Embedding), función de activación SwiGLU | rendimiento al nivel de GPT-3, tamaño de modelo más pequeño | procesamiento de contexto largo (LongLoRA, etc.), adopción de la estructura GQA | posibilidad de usar modelos de alto rendimiento en entornos con recursos computacionales limitados, promoción de investigación de modelado ligero |\n",
    "| Chinchilla | 2022 | estimación del tamaño óptimo del modelo y del conjunto de datos de entrenamiento | 70B parámetros, aprendizaje de 1.4T tokens, uso de más datos que modelos existentes | mejor rendimiento que LLaMA y PaLM, optimización del presupuesto computacional | posibilidad de usar técnicas de atención eficientes como KV caching | investigación sobre la ley de escalado, aclaración de la relación entre el tamaño del modelo y el conjunto de datos |\n",
    "| **GPT-5** | **2024** | integración multimodal | generación integrada de Texto/Código/3D, 25T tokens | MMLU 92.3, HumanEval 88.7 | Hybrid FlashAttention | mejora de la eficiencia energética en un 40%, capacidad de generar contenido 3D, fortalecimiento de la capacidad de generación de código |\n",
    "| **Gemini Ultra** | **2025** | atención cuántica | muestreo basado en enfriamiento cuántico (Quantum annealing) | mejora de 5x en velocidad de inferencia | cuantización QKV | implementación de mecanismos de atención utilizando tecnología de computación cuántica, aplicación de chips AI de ultra bajo consumo |\n",
    "| **LLaMA-3** | **2024** | plasticidad neuronal | aplicación de la regla de aprendizaje STDP | mejora del 73% en el rendimiento del aprendizaje continuo | Dynamic GQA | optimización para dispositivos periféricos, imitación de los mecanismos de aprendizaje del cerebro, fortalecimiento de la capacidad de aprendizaje continuo |\n",
    "\n",
    "### 3. modelos híbridos (Encoder-Decoder Models)\n",
    "\n",
    "Los modelos encoder-decoder son adecuados para tareas que implican comprender el texto de entrada y generar el texto de salida correspondiente (por ejemplo, traducción, resumen).\n",
    "| modelo | año de lanzamiento | innovación clave | características principales | mejora del rendimiento | relación con la tecnología hasta 9.4 | información adicional detallada |\n",
    "|---|---|---|---|---|---|---|\n",
    "| T5 | 2019 | marco de trabajo integrado texto-a-texto (Text-to-Text) | convierte todas las tareas NLP al formato texto-a-texto, conjunto de datos C4(Colossal Clean Crawled Corpus) | procesamiento integrado de diversas tareas NLP, efectos de aprendizaje transferible | se pueden utilizar mecanismos de atención especializados (por ejemplo, atención basada en conocimiento) | procesa tanto la entrada como la salida en formato de texto, usa prefijos para especificar tareas, ofrece modelos de varios tamaños (Small, Base, Large, XL, XXL) |\n",
    "| UL2 | 2022 | mezcla de desruidores (Mixture of Denoisers) | integra diversos paradigmas de pre-entrenamiento (objetivos de desruido), conmutación modal | mejora del rendimiento en un 43.6% respecto a T5 (SuperGLUE, aprendizaje de pocos ejemplos) | puede utilizar técnicas de procesamiento multimodal | R-Denoiser, X-Denoiser, S-Denoiser, 7 objetivos de desruido, multitarea extrema, experimentación con diversas técnicas de prompting |\n",
    "| FLAN | 2023 | aprendizaje por instrucción (Instruction Tuning) | ajuste de cadena de pensamiento (chain-of-thought), uso de diversos conjuntos de datos de instrucciones | mejora del rendimiento en escenarios de pocos ejemplos, capacidad de generalización para tareas no vistas | posibilidad de integrar mecanismos de restricción éticos (como Constitutional AI) | construcción de conjuntos de datos de instrucciones para diversas tareas, demostración de la efectividad del ajuste por instrucción, uso de técnicas de prompting CoT |\n",
    "| BART | 2019 | Autoencoder de desruido | aplicación de diversas funciones de ruido como Text Infilling, Sentence Permutation, codificador bidireccional + decodificador autoregresivo | buen rendimiento en tareas generativas como resumen, traducción, respuestas a preguntas | puede combinarse con diversas técnicas de atención eficientes | aplicación de pre-entrenamiento en modelos seq2seq, importancia de la combinación de funciones de ruido |\n",
    "| **Olympus** | **2025** | codificación 4D espacio-temporal | aprendizaje conjunto de video-texto, atención temporal | VideoQA SOTA 89.4 | LongLoRA-4D | soporte para generación de video en tiempo real, mejora de la capacidad de comprensión y generación de video, procesamiento de información 4D (3D espacial + tiempo) |\n",
    "| **Hermes** | **2024** | generación ética | mecanismo de atención de regulación en tiempo real | tasa de generación dañina < 0.2% | Constitutional AI 2.0 | obtuvo certificación de seguridad AI, prevención de contenido dañino en tiempo real, control basado en aprendizaje reforzado |\n",
    "| **Neuro-Sym** | **2025** | integración neuro-simbólica | control de atención basado en reglas | inferencia matemática 94.1 | Hybrid KV Cache | marco de colaboración con expertos del dominio, combinación de razonamiento simbólico y redes neuronales, maximización de habilidades de inferencia para resolver problemas matemáticos y descubrimientos científicos |\n",
    "\n",
    "### análisis profundo de la relación tecnológica\n",
    "\n",
    "1.  **mecanismo de atención 3D:**\n",
    "    *   **ESM-3:** utiliza una atención geométrica que integra información de coordenadas 3D junto con secuencias de aminoácidos para predecir la estructura 3D de proteínas.\n",
    "    *   **FlashAttention-3D:** extiende FlashAttention para procesar datos 3D de manera eficiente, reduciendo el uso de memoria.\n",
    "2.  **Desarrollo de cuantización:**\n",
    "    *   **Gemini Ultra:** Utiliza la técnica de annealing de computación cuántica para acelerar el cálculo de atención y reduce el tamaño del modelo a través de cuantización de 4 bits.\n",
    "    *   **LLaMA-3:** Aplica una técnica de cuantización dinámica inspirada en la plasticidad neuronal de tipo STDP (depresión potencial dependiente del tiempo) para mejorar la eficiencia en dispositivos periféricos.\n",
    "\n",
    "3.  **Eficiencia energética:**\n",
    "    *   **GPT-5:** Mejora la eficiencia energética reduciendo el número de parámetros que se activan a través del modelo Sparse Mixture of Experts (SMoE).\n",
    "    *   **Olympus:** Maximiza la eficiencia de entrenamiento en grandes clústeres GPU mediante paralelismo tensorial 4D.\n",
    "\n",
    "4. **Estado de los benchmarks para 2025:**\n",
    "\n",
    " | Tarea | Modelo SOTA | Rendimiento | Tecnología principal |\n",
    "|---|---|---|---|\n",
    "| Comprensión del lenguaje (MMLU) | GPT-5 | 92.3 | Fusión de conocimientos multimodales, Hybrid FlashAttention, aprendizaje de 25T tokens |\n",
    "| Generación de código (HumanEval) | CodeLlama-X | 91.2 | Retroalimentación de compilación en tiempo real, generación de código basada en aprendizaje por refuerzo, capacidad para generar códigos extensos |\n",
    "| Plegamiento de proteínas (CASP16) | ESM-3G | GDT_TS 94.7 | Atención de grafos 3D, atención geométrica, FlashAttention-3D, aprendizaje a partir de datos de estructuras de proteínas a gran escala |\n",
    "| Seguridad de IA (HarmBench) | Hermes | 99.8 | Puertas de atención reguladas, Constitutional AI 2.0, filtrado en tiempo real de contenido dañino, políticas de seguridad basadas en aprendizaje por refuerzo |\n",
    "\n",
    "### Perspectivas futuras\n",
    "* Arquitectura híbrida cuántico-clásica: Aceleración de cálculos utilizando superposición y entrelazamiento de computación cuántica.\n",
    "* Aprendizaje bioinspirado: Desarrollo de algoritmos que imitan los mecanismos neuronales del cerebro.\n",
    "* Modelos autoevolutivos: Investigación en direcciones para que los modelos optimicen su propia arquitectura.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.5 Implementación y Aplicación de un Codificador Eficiente: Centrado en RoPE y FlashAttention\n",
    "\n",
    "Los modelos Transformer han demostrado un rendimiento sobresaliente en el campo del procesamiento del lenguaje natural (NLP), pero tienen la desventaja de tener una alta complejidad computacional y consumo de memoria. En el Capítulo 9.4, examinamos varios métodos para abordar estos problemas. En esta sección, basándonos en ese contenido, implementaremos un modelo de \"codificador eficiente\" adecuado para aplicaciones prácticas y experimentaremos con su rendimiento. Especialmente, nos centraremos en **FlashAttention**, **Pre-LN** (Normalización de Capa Previa) y **RoPE (Rotary Positional Embedding)**.\n",
    "\n",
    "El codificador eficiente se encuentra en chapter_09/encoder.\n",
    "\n",
    "### 9.5.1 Filosofía de Diseño del Codificador Eficiente: Velocidad y Memoria\n",
    "\n",
    "El objetivo principal de un codificador eficiente es la *velocidad* y la *eficiencia de memoria*. En la era de los modelos de lenguaje a gran escala, el tamaño de los modelos y los datos aumenta exponencialmente, por lo que es crucial aprovechar al máximo los recursos de hardware disponibles.\n",
    "\n",
    "Para lograr esto, un codificador eficiente sigue las siguientes filosofías de diseño:\n",
    "\n",
    "1.  **Reducción de la complejidad computacional:** El mecanismo de atención tiene una complejidad computacional proporcional al cuadrado de la longitud de la secuencia. Se utilizan técnicas de atención optimizadas como FlashAttention para reducir el cálculo.\n",
    "\n",
    "2.  **Maximización de la eficiencia de memoria:** Reduce la memoria necesaria para almacenar los parámetros del modelo y los resultados intermedios de cálculos.\n",
    "    *   **Uso de la jerarquía de memoria GPU:** Optimiza el movimiento de datos entre la SRAM rápida y pequeña y la HBM lenta y grande de la GPU. (Principio clave de FlashAttention)\n",
    "    *   **Procesamiento por bloques:** Divide los datos en pequeños bloques para procesarlos, reduciendo así el número de accesos a memoria.\n",
    "    *   **Pre-LN (Normalización de Capa Previa):** Aplica la normalización de capa antes del mecanismo de atención y de la red feedforward para facilitar un aprendizaje estable y promover una convergencia rápida.\n",
    "    *   **Gradient Checkpointing:** (No implementado en este ejemplo) En lugar de almacenar todos los resultados intermedios durante el retropropagación, almacena solo algunos y recalcula otros cuando sea necesario para reducir el uso de memoria.\n",
    "\n",
    "3.  **RoPE (Rotary Positional Embedding) (Opcional):** Representa eficientemente la información de posición absoluta o relativa, proporcionando información de posición al modelo sin embeddings de posición separados, lo que es beneficioso para procesar contextos largos.\n",
    "\n",
    "### 9.5.2 Análisis detallado del código `efficient_encoder.py` (Sin usar RoPE)\n",
    "\n",
    "`efficient_encoder.py` implementa un codificador eficiente básico sin usar RoPE. Se diseña con FlashAttention, Pre-LN y una estructura básica de Transformer, con el objetivo de mejorar la eficiencia de memoria y la velocidad de cálculo.\n",
    "\n",
    "**1. Clase `TransformerConfig`:**\n",
    "\n",
    "Define los hiperparámetros del modelo (vocab_size, hidden_size, num_hidden_layers, etc.).\n",
    "\n",
    "**2. Clase `LayerNorm`:**\n",
    "\n",
    "Implementa la normalización de capa en el estilo Pre-LN.\n",
    "\n",
    "**3. Clase `Embeddings`:**\n",
    "\n",
    "Convierte tokens de entrada en vectores de embedding. *A diferencia de `efficient_encoder_rope.py`, usa embeddings de posición entrenables (positional embeddings).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# efficient_encoder.py\n",
    "class Embeddings(nn.Module):\n",
    "    \"\"\"Token and positional embeddings.\"\"\"\n",
    "    def __init__(self, config: TransformerConfig):\n",
    "        super().__init__()\n",
    "        self.token_embeddings = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size) # 위치 임베딩\n",
    "        self.norm = LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size, seq_length = input_ids.size()\n",
    "        position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device).unsqueeze(0).expand(batch_size, -1)\n",
    "        token_embeddings = self.token_embeddings(input_ids)\n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "        embeddings = token_embeddings + position_embeddings  # 토큰 임베딩과 위치 임베딩을 더함\n",
    "        embeddings = self.norm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Clase `FlashAttention`:**\n",
    "\n",
    "Implementa una versión básica de FlashAttention sin código relacionado con RoPE. La clave es el uso de `torch.nn.functional.scaled_dot_product_attention`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (efficient_encoder.py)\n",
    "class FlashAttention(nn.Module):\n",
    "    # ... (생략) ...\n",
    "    def forward(self, hidden_states: torch.Tensor, attention_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        # ... (생략) ...\n",
    "        # Use PyTorch's built-in scaled_dot_product_attention\n",
    "        attn_output = F.scaled_dot_product_attention(query_layer, key_layer, value_layer, attn_mask=attention_mask, dropout_p=self.dropout.p if self.training else 0.0)\n",
    "        # ... (생략) ...\n",
    "        return attn_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Clase `FeedForward`:**\n",
    "\n",
    "Implementa una red feed-forward por posición (FFN).\n",
    "\n",
    "**6. Clase `TransformerEncoderLayer`:**\n",
    "\n",
    "Construye una capa de codificador de transformador. Utiliza Pre-LN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (efficient_encoder.py)\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, config: TransformerConfig):\n",
    "        super().__init__()\n",
    "        self.attention = FlashAttention(config)\n",
    "        self.norm1 = LayerNorm(config.hidden_size, eps=config.layer_norm_eps) # Pre-LN\n",
    "        self.ffn = FeedForward(config)\n",
    "        self.norm2 = LayerNorm(config.hidden_size, eps=config.layer_norm_eps) # Pre-LN\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor, attention_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        # Pre-LN + Residual Connection + FlashAttention\n",
    "        attention_output = self.attention(self.norm1(hidden_states), attention_mask)\n",
    "        hidden_states = hidden_states + attention_output\n",
    "\n",
    "        # Pre-LN + Residual Connection + FFN\n",
    "        ffn_output = self.ffn(self.norm2(hidden_states))\n",
    "        hidden_states = hidden_states + ffn_output\n",
    "\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. `TransformerEncoder` clase:**\n",
    "\n",
    "Configura el codificador de transformer completo.\n",
    "\n",
    "### 9.5.3 Análisis detallado del código `efficient_encoder_rope.py` (con RoPE)\n",
    "\n",
    "`efficient_encoder_rope.py` es una versión mejorada de `efficient_encoder.py` que agrega RoPE (Rotary Positional Embedding) para manejar la información posicional de manera más eficiente.\n",
    "\n",
    "**¿Qué es RoPE (Rotary Positional Embedding)?**\n",
    "\n",
    "RoPE (Rotary Position Embedding) es un nuevo método para representar la información posicional en los transformers. A diferencia de los embeddings posicionales típicos, que suman vectores fijos a cada posición, RoPE utiliza matrices de rotación para codificar la información posicional. Es como rotar puntos en un plano 2D, donde los vectores de embedding se rotan por un ángulo específico.\n",
    "\n",
    "Por ejemplo:\n",
    "1. Primera posición: 0 grados de rotación\n",
    "2. Segunda posición: 30 grados de rotación\n",
    "3. Tercera posición: 60 grados de rotación\n",
    "De esta manera, a medida que las posiciones se alejan, los vectores se rotan por ángulos más grandes. Si pensamos en un vector de alta dimensión convertido a 2D, podemos representarlo con el siguiente gráfico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"264.230625pt\" height=\"212.51625pt\" viewBox=\"0 0 264.230625 212.51625\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2025-03-04T15:11:19.350719</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.10.0, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 212.51625 \n",
       "L 264.230625 212.51625 \n",
       "L 264.230625 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 48.415312 188.638125 \n",
       "L 215.815313 188.638125 \n",
       "L 215.815313 22.318125 \n",
       "L 48.415312 22.318125 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 208.206222 178.187216 \n",
       "L 194.678949 181.569034 \n",
       "L 194.678949 178.254852 \n",
       "L 59.406222 178.254852 \n",
       "L 59.406222 178.11958 \n",
       "L 194.678949 178.11958 \n",
       "L 194.678949 174.805398 \n",
       "z\n",
       "\" clip-path=\"url(#p36a63da2bb)\" style=\"fill: #1f77b4; stroke: #000000; stroke-linejoin: miter\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 188.270802 103.787216 \n",
       "L 178.246749 113.479593 \n",
       "L 176.589658 110.609427 \n",
       "L 59.44004 178.245791 \n",
       "L 59.372403 178.128641 \n",
       "L 176.522022 110.492277 \n",
       "L 174.864931 107.622112 \n",
       "z\n",
       "\" clip-path=\"url(#p36a63da2bb)\" style=\"fill: #1f77b4; stroke: #000000; stroke-linejoin: miter\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 133.806222 49.322636 \n",
       "L 129.971326 62.728507 \n",
       "L 127.10116 61.071416 \n",
       "L 59.464796 178.221034 \n",
       "L 59.347647 178.153398 \n",
       "L 126.98401 61.003779 \n",
       "L 124.113845 59.346689 \n",
       "z\n",
       "\" clip-path=\"url(#p36a63da2bb)\" style=\"fill: #1f77b4; stroke: #000000; stroke-linejoin: miter\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 59.406222 29.387216 \n",
       "L 62.78804 42.914489 \n",
       "L 59.473858 42.914489 \n",
       "L 59.473858 178.187216 \n",
       "L 59.338585 178.187216 \n",
       "L 59.338585 42.914489 \n",
       "L 56.024403 42.914489 \n",
       "z\n",
       "\" clip-path=\"url(#p36a63da2bb)\" style=\"fill: #1f77b4; stroke: #000000; stroke-linejoin: miter\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <path d=\"M 59.406222 188.638125 \n",
       "L 59.406222 22.318125 \n",
       "\" clip-path=\"url(#p36a63da2bb)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_2\">\n",
       "      <defs>\n",
       "       <path id=\"mb9869388e3\" d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#mb9869388e3\" x=\"59.406222\" y=\"188.638125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 0.00 -->\n",
       "      <g transform=\"translate(48.273409 203.236563) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \n",
       "Q 1547 4250 1301 3770 \n",
       "Q 1056 3291 1056 2328 \n",
       "Q 1056 1369 1301 889 \n",
       "Q 1547 409 2034 409 \n",
       "Q 2525 409 2770 889 \n",
       "Q 3016 1369 3016 2328 \n",
       "Q 3016 3291 2770 3770 \n",
       "Q 2525 4250 2034 4250 \n",
       "z\n",
       "M 2034 4750 \n",
       "Q 2819 4750 3233 4129 \n",
       "Q 3647 3509 3647 2328 \n",
       "Q 3647 1150 3233 529 \n",
       "Q 2819 -91 2034 -91 \n",
       "Q 1250 -91 836 529 \n",
       "Q 422 1150 422 2328 \n",
       "Q 422 3509 836 4129 \n",
       "Q 1250 4750 2034 4750 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \n",
       "L 1344 794 \n",
       "L 1344 0 \n",
       "L 684 0 \n",
       "L 684 794 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" transform=\"translate(63.623047 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" transform=\"translate(95.410156 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" transform=\"translate(159.033203 0)\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <path d=\"M 93.224403 188.638125 \n",
       "L 93.224403 22.318125 \n",
       "\" clip-path=\"url(#p36a63da2bb)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mb9869388e3\" x=\"93.224403\" y=\"188.638125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 0.25 -->\n",
       "      <g transform=\"translate(82.091591 203.236563) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \n",
       "L 3431 531 \n",
       "L 3431 0 \n",
       "L 469 0 \n",
       "L 469 531 \n",
       "Q 828 903 1448 1529 \n",
       "Q 2069 2156 2228 2338 \n",
       "Q 2531 2678 2651 2914 \n",
       "Q 2772 3150 2772 3378 \n",
       "Q 2772 3750 2511 3984 \n",
       "Q 2250 4219 1831 4219 \n",
       "Q 1534 4219 1204 4116 \n",
       "Q 875 4013 500 3803 \n",
       "L 500 4441 \n",
       "Q 881 4594 1212 4672 \n",
       "Q 1544 4750 1819 4750 \n",
       "Q 2544 4750 2975 4387 \n",
       "Q 3406 4025 3406 3419 \n",
       "Q 3406 3131 3298 2873 \n",
       "Q 3191 2616 2906 2266 \n",
       "Q 2828 2175 2409 1742 \n",
       "Q 1991 1309 1228 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \n",
       "L 3169 4666 \n",
       "L 3169 4134 \n",
       "L 1269 4134 \n",
       "L 1269 2991 \n",
       "Q 1406 3038 1543 3061 \n",
       "Q 1681 3084 1819 3084 \n",
       "Q 2600 3084 3056 2656 \n",
       "Q 3513 2228 3513 1497 \n",
       "Q 3513 744 3044 326 \n",
       "Q 2575 -91 1722 -91 \n",
       "Q 1428 -91 1123 -41 \n",
       "Q 819 9 494 109 \n",
       "L 494 744 \n",
       "Q 775 591 1075 516 \n",
       "Q 1375 441 1709 441 \n",
       "Q 2250 441 2565 725 \n",
       "Q 2881 1009 2881 1497 \n",
       "Q 2881 1984 2565 2268 \n",
       "Q 2250 2553 1709 2553 \n",
       "Q 1456 2553 1204 2497 \n",
       "Q 953 2441 691 2322 \n",
       "L 691 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" transform=\"translate(63.623047 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-32\" transform=\"translate(95.410156 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" transform=\"translate(159.033203 0)\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <path d=\"M 127.042585 188.638125 \n",
       "L 127.042585 22.318125 \n",
       "\" clip-path=\"url(#p36a63da2bb)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_6\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mb9869388e3\" x=\"127.042585\" y=\"188.638125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 0.50 -->\n",
       "      <g transform=\"translate(115.909773 203.236563) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" transform=\"translate(63.623047 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" transform=\"translate(95.410156 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" transform=\"translate(159.033203 0)\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <path d=\"M 160.860767 188.638125 \n",
       "L 160.860767 22.318125 \n",
       "\" clip-path=\"url(#p36a63da2bb)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mb9869388e3\" x=\"160.860767\" y=\"188.638125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 0.75 -->\n",
       "      <g transform=\"translate(149.727955 203.236563) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-37\" d=\"M 525 4666 \n",
       "L 3525 4666 \n",
       "L 3525 4397 \n",
       "L 1831 0 \n",
       "L 1172 0 \n",
       "L 2766 4134 \n",
       "L 525 4134 \n",
       "L 525 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" transform=\"translate(63.623047 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-37\" transform=\"translate(95.410156 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" transform=\"translate(159.033203 0)\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <path d=\"M 194.678949 188.638125 \n",
       "L 194.678949 22.318125 \n",
       "\" clip-path=\"url(#p36a63da2bb)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_10\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mb9869388e3\" x=\"194.678949\" y=\"188.638125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 1.00 -->\n",
       "      <g transform=\"translate(183.546136 203.236563) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-31\" d=\"M 794 531 \n",
       "L 1825 531 \n",
       "L 1825 4091 \n",
       "L 703 3866 \n",
       "L 703 4441 \n",
       "L 1819 4666 \n",
       "L 2450 4666 \n",
       "L 2450 531 \n",
       "L 3481 531 \n",
       "L 3481 0 \n",
       "L 794 0 \n",
       "L 794 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" transform=\"translate(63.623047 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" transform=\"translate(95.410156 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" transform=\"translate(159.033203 0)\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_11\">\n",
       "      <path d=\"M 48.415312 178.187216 \n",
       "L 215.815313 178.187216 \n",
       "\" clip-path=\"url(#p36a63da2bb)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_12\">\n",
       "      <defs>\n",
       "       <path id=\"m76b7be6566\" d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m76b7be6566\" x=\"48.415312\" y=\"178.187216\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_6\">\n",
       "      <!-- 0.0 -->\n",
       "      <g transform=\"translate(25.512187 181.986435) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" transform=\"translate(63.623047 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" transform=\"translate(95.410156 0)\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_13\">\n",
       "      <path d=\"M 48.415312 151.13267 \n",
       "L 215.815313 151.13267 \n",
       "\" clip-path=\"url(#p36a63da2bb)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_14\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m76b7be6566\" x=\"48.415312\" y=\"151.13267\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- 0.2 -->\n",
       "      <g transform=\"translate(25.512187 154.931889) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" transform=\"translate(63.623047 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-32\" transform=\"translate(95.410156 0)\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_15\">\n",
       "      <path d=\"M 48.415312 124.078125 \n",
       "L 215.815313 124.078125 \n",
       "\" clip-path=\"url(#p36a63da2bb)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_16\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m76b7be6566\" x=\"48.415312\" y=\"124.078125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 0.4 -->\n",
       "      <g transform=\"translate(25.512187 127.877344) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \n",
       "L 825 1625 \n",
       "L 2419 1625 \n",
       "L 2419 4116 \n",
       "z\n",
       "M 2253 4666 \n",
       "L 3047 4666 \n",
       "L 3047 1625 \n",
       "L 3713 1625 \n",
       "L 3713 1100 \n",
       "L 3047 1100 \n",
       "L 3047 0 \n",
       "L 2419 0 \n",
       "L 2419 1100 \n",
       "L 313 1100 \n",
       "L 313 1709 \n",
       "L 2253 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" transform=\"translate(63.623047 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-34\" transform=\"translate(95.410156 0)\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_17\">\n",
       "      <path d=\"M 48.415312 97.02358 \n",
       "L 215.815313 97.02358 \n",
       "\" clip-path=\"url(#p36a63da2bb)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_18\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m76b7be6566\" x=\"48.415312\" y=\"97.02358\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 0.6 -->\n",
       "      <g transform=\"translate(25.512187 100.822798) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \n",
       "Q 1688 2584 1439 2293 \n",
       "Q 1191 2003 1191 1497 \n",
       "Q 1191 994 1439 701 \n",
       "Q 1688 409 2113 409 \n",
       "Q 2538 409 2786 701 \n",
       "Q 3034 994 3034 1497 \n",
       "Q 3034 2003 2786 2293 \n",
       "Q 2538 2584 2113 2584 \n",
       "z\n",
       "M 3366 4563 \n",
       "L 3366 3988 \n",
       "Q 3128 4100 2886 4159 \n",
       "Q 2644 4219 2406 4219 \n",
       "Q 1781 4219 1451 3797 \n",
       "Q 1122 3375 1075 2522 \n",
       "Q 1259 2794 1537 2939 \n",
       "Q 1816 3084 2150 3084 \n",
       "Q 2853 3084 3261 2657 \n",
       "Q 3669 2231 3669 1497 \n",
       "Q 3669 778 3244 343 \n",
       "Q 2819 -91 2113 -91 \n",
       "Q 1303 -91 875 529 \n",
       "Q 447 1150 447 2328 \n",
       "Q 447 3434 972 4092 \n",
       "Q 1497 4750 2381 4750 \n",
       "Q 2619 4750 2861 4703 \n",
       "Q 3103 4656 3366 4563 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" transform=\"translate(63.623047 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-36\" transform=\"translate(95.410156 0)\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_19\">\n",
       "      <path d=\"M 48.415312 69.969034 \n",
       "L 215.815313 69.969034 \n",
       "\" clip-path=\"url(#p36a63da2bb)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_20\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m76b7be6566\" x=\"48.415312\" y=\"69.969034\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_10\">\n",
       "      <!-- 0.8 -->\n",
       "      <g transform=\"translate(25.512187 73.768253) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-38\" d=\"M 2034 2216 \n",
       "Q 1584 2216 1326 1975 \n",
       "Q 1069 1734 1069 1313 \n",
       "Q 1069 891 1326 650 \n",
       "Q 1584 409 2034 409 \n",
       "Q 2484 409 2743 651 \n",
       "Q 3003 894 3003 1313 \n",
       "Q 3003 1734 2745 1975 \n",
       "Q 2488 2216 2034 2216 \n",
       "z\n",
       "M 1403 2484 \n",
       "Q 997 2584 770 2862 \n",
       "Q 544 3141 544 3541 \n",
       "Q 544 4100 942 4425 \n",
       "Q 1341 4750 2034 4750 \n",
       "Q 2731 4750 3128 4425 \n",
       "Q 3525 4100 3525 3541 \n",
       "Q 3525 3141 3298 2862 \n",
       "Q 3072 2584 2669 2484 \n",
       "Q 3125 2378 3379 2068 \n",
       "Q 3634 1759 3634 1313 \n",
       "Q 3634 634 3220 271 \n",
       "Q 2806 -91 2034 -91 \n",
       "Q 1263 -91 848 271 \n",
       "Q 434 634 434 1313 \n",
       "Q 434 1759 690 2068 \n",
       "Q 947 2378 1403 2484 \n",
       "z\n",
       "M 1172 3481 \n",
       "Q 1172 3119 1398 2916 \n",
       "Q 1625 2713 2034 2713 \n",
       "Q 2441 2713 2670 2916 \n",
       "Q 2900 3119 2900 3481 \n",
       "Q 2900 3844 2670 4047 \n",
       "Q 2441 4250 2034 4250 \n",
       "Q 1625 4250 1398 4047 \n",
       "Q 1172 3844 1172 3481 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" transform=\"translate(63.623047 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-38\" transform=\"translate(95.410156 0)\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_6\">\n",
       "     <g id=\"line2d_21\">\n",
       "      <path d=\"M 48.415312 42.914489 \n",
       "L 215.815313 42.914489 \n",
       "\" clip-path=\"url(#p36a63da2bb)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_22\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m76b7be6566\" x=\"48.415312\" y=\"42.914489\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_11\">\n",
       "      <!-- 1.0 -->\n",
       "      <g transform=\"translate(25.512187 46.713707) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" transform=\"translate(63.623047 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" transform=\"translate(95.410156 0)\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"patch_7\">\n",
       "    <path d=\"M 48.415312 188.638125 \n",
       "L 48.415312 22.318125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_8\">\n",
       "    <path d=\"M 215.815313 188.638125 \n",
       "L 215.815313 22.318125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_9\">\n",
       "    <path d=\"M 48.415312 188.638125 \n",
       "L 215.815313 188.638125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_10\">\n",
       "    <path d=\"M 48.415312 22.318125 \n",
       "L 215.815313 22.318125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"text_12\">\n",
       "    <!-- pos 0 -->\n",
       "    <g transform=\"translate(194.678949 178.187216) scale(0.1 -0.1)\">\n",
       "     <defs>\n",
       "      <path id=\"DejaVuSans-70\" d=\"M 1159 525 \n",
       "L 1159 -1331 \n",
       "L 581 -1331 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2969 \n",
       "Q 1341 3281 1617 3432 \n",
       "Q 1894 3584 2278 3584 \n",
       "Q 2916 3584 3314 3078 \n",
       "Q 3713 2572 3713 1747 \n",
       "Q 3713 922 3314 415 \n",
       "Q 2916 -91 2278 -91 \n",
       "Q 1894 -91 1617 61 \n",
       "Q 1341 213 1159 525 \n",
       "z\n",
       "M 3116 1747 \n",
       "Q 3116 2381 2855 2742 \n",
       "Q 2594 3103 2138 3103 \n",
       "Q 1681 3103 1420 2742 \n",
       "Q 1159 2381 1159 1747 \n",
       "Q 1159 1113 1420 752 \n",
       "Q 1681 391 2138 391 \n",
       "Q 2594 391 2855 752 \n",
       "Q 3116 1113 3116 1747 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \n",
       "Q 1497 3097 1228 2736 \n",
       "Q 959 2375 959 1747 \n",
       "Q 959 1119 1226 758 \n",
       "Q 1494 397 1959 397 \n",
       "Q 2419 397 2687 759 \n",
       "Q 2956 1122 2956 1747 \n",
       "Q 2956 2369 2687 2733 \n",
       "Q 2419 3097 1959 3097 \n",
       "z\n",
       "M 1959 3584 \n",
       "Q 2709 3584 3137 3096 \n",
       "Q 3566 2609 3566 1747 \n",
       "Q 3566 888 3137 398 \n",
       "Q 2709 -91 1959 -91 \n",
       "Q 1206 -91 779 398 \n",
       "Q 353 888 353 1747 \n",
       "Q 353 2609 779 3096 \n",
       "Q 1206 3584 1959 3584 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-73\" d=\"M 2834 3397 \n",
       "L 2834 2853 \n",
       "Q 2591 2978 2328 3040 \n",
       "Q 2066 3103 1784 3103 \n",
       "Q 1356 3103 1142 2972 \n",
       "Q 928 2841 928 2578 \n",
       "Q 928 2378 1081 2264 \n",
       "Q 1234 2150 1697 2047 \n",
       "L 1894 2003 \n",
       "Q 2506 1872 2764 1633 \n",
       "Q 3022 1394 3022 966 \n",
       "Q 3022 478 2636 193 \n",
       "Q 2250 -91 1575 -91 \n",
       "Q 1294 -91 989 -36 \n",
       "Q 684 19 347 128 \n",
       "L 347 722 \n",
       "Q 666 556 975 473 \n",
       "Q 1284 391 1588 391 \n",
       "Q 1994 391 2212 530 \n",
       "Q 2431 669 2431 922 \n",
       "Q 2431 1156 2273 1281 \n",
       "Q 2116 1406 1581 1522 \n",
       "L 1381 1569 \n",
       "Q 847 1681 609 1914 \n",
       "Q 372 2147 372 2553 \n",
       "Q 372 3047 722 3315 \n",
       "Q 1072 3584 1716 3584 \n",
       "Q 2034 3584 2315 3537 \n",
       "Q 2597 3491 2834 3397 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSans-70\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6f\" transform=\"translate(63.476562 0)\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-73\" transform=\"translate(124.658203 0)\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-20\" transform=\"translate(176.757812 0)\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-30\" transform=\"translate(208.544922 0)\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"text_13\">\n",
       "    <!-- pos 1 -->\n",
       "    <g transform=\"translate(176.55584 110.550852) scale(0.1 -0.1)\">\n",
       "     <use xlink:href=\"#DejaVuSans-70\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6f\" transform=\"translate(63.476562 0)\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-73\" transform=\"translate(124.658203 0)\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-20\" transform=\"translate(176.757812 0)\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-31\" transform=\"translate(208.544922 0)\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"text_14\">\n",
       "    <!-- pos 2 -->\n",
       "    <g transform=\"translate(127.042585 61.037598) scale(0.1 -0.1)\">\n",
       "     <use xlink:href=\"#DejaVuSans-70\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6f\" transform=\"translate(63.476562 0)\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-73\" transform=\"translate(124.658203 0)\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-20\" transform=\"translate(176.757812 0)\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-32\" transform=\"translate(208.544922 0)\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"text_15\">\n",
       "    <!-- pos 3 -->\n",
       "    <g transform=\"translate(59.406222 42.914489) scale(0.1 -0.1)\">\n",
       "     <defs>\n",
       "      <path id=\"DejaVuSans-33\" d=\"M 2597 2516 \n",
       "Q 3050 2419 3304 2112 \n",
       "Q 3559 1806 3559 1356 \n",
       "Q 3559 666 3084 287 \n",
       "Q 2609 -91 1734 -91 \n",
       "Q 1441 -91 1130 -33 \n",
       "Q 819 25 488 141 \n",
       "L 488 750 \n",
       "Q 750 597 1062 519 \n",
       "Q 1375 441 1716 441 \n",
       "Q 2309 441 2620 675 \n",
       "Q 2931 909 2931 1356 \n",
       "Q 2931 1769 2642 2001 \n",
       "Q 2353 2234 1838 2234 \n",
       "L 1294 2234 \n",
       "L 1294 2753 \n",
       "L 1863 2753 \n",
       "Q 2328 2753 2575 2939 \n",
       "Q 2822 3125 2822 3475 \n",
       "Q 2822 3834 2567 4026 \n",
       "Q 2313 4219 1838 4219 \n",
       "Q 1578 4219 1281 4162 \n",
       "Q 984 4106 628 3988 \n",
       "L 628 4550 \n",
       "Q 988 4650 1302 4700 \n",
       "Q 1616 4750 1894 4750 \n",
       "Q 2613 4750 3031 4423 \n",
       "Q 3450 4097 3450 3541 \n",
       "Q 3450 3153 3228 2886 \n",
       "Q 3006 2619 2597 2516 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSans-70\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6f\" transform=\"translate(63.476562 0)\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-73\" transform=\"translate(124.658203 0)\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-20\" transform=\"translate(176.757812 0)\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-33\" transform=\"translate(208.544922 0)\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"text_16\">\n",
       "    <!-- RoPE: Position-dependent Vector Rotation -->\n",
       "    <g transform=\"translate(7.2 16.318125) scale(0.12 -0.12)\">\n",
       "     <defs>\n",
       "      <path id=\"DejaVuSans-52\" d=\"M 2841 2188 \n",
       "Q 3044 2119 3236 1894 \n",
       "Q 3428 1669 3622 1275 \n",
       "L 4263 0 \n",
       "L 3584 0 \n",
       "L 2988 1197 \n",
       "Q 2756 1666 2539 1819 \n",
       "Q 2322 1972 1947 1972 \n",
       "L 1259 1972 \n",
       "L 1259 0 \n",
       "L 628 0 \n",
       "L 628 4666 \n",
       "L 2053 4666 \n",
       "Q 2853 4666 3247 4331 \n",
       "Q 3641 3997 3641 3322 \n",
       "Q 3641 2881 3436 2590 \n",
       "Q 3231 2300 2841 2188 \n",
       "z\n",
       "M 1259 4147 \n",
       "L 1259 2491 \n",
       "L 2053 2491 \n",
       "Q 2509 2491 2742 2702 \n",
       "Q 2975 2913 2975 3322 \n",
       "Q 2975 3731 2742 3939 \n",
       "Q 2509 4147 2053 4147 \n",
       "L 1259 4147 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-50\" d=\"M 1259 4147 \n",
       "L 1259 2394 \n",
       "L 2053 2394 \n",
       "Q 2494 2394 2734 2622 \n",
       "Q 2975 2850 2975 3272 \n",
       "Q 2975 3691 2734 3919 \n",
       "Q 2494 4147 2053 4147 \n",
       "L 1259 4147 \n",
       "z\n",
       "M 628 4666 \n",
       "L 2053 4666 \n",
       "Q 2838 4666 3239 4311 \n",
       "Q 3641 3956 3641 3272 \n",
       "Q 3641 2581 3239 2228 \n",
       "Q 2838 1875 2053 1875 \n",
       "L 1259 1875 \n",
       "L 1259 0 \n",
       "L 628 0 \n",
       "L 628 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-45\" d=\"M 628 4666 \n",
       "L 3578 4666 \n",
       "L 3578 4134 \n",
       "L 1259 4134 \n",
       "L 1259 2753 \n",
       "L 3481 2753 \n",
       "L 3481 2222 \n",
       "L 1259 2222 \n",
       "L 1259 531 \n",
       "L 3634 531 \n",
       "L 3634 0 \n",
       "L 628 0 \n",
       "L 628 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-3a\" d=\"M 750 794 \n",
       "L 1409 794 \n",
       "L 1409 0 \n",
       "L 750 0 \n",
       "L 750 794 \n",
       "z\n",
       "M 750 3309 \n",
       "L 1409 3309 \n",
       "L 1409 2516 \n",
       "L 750 2516 \n",
       "L 750 3309 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-69\" d=\"M 603 3500 \n",
       "L 1178 3500 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 3500 \n",
       "z\n",
       "M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 4134 \n",
       "L 603 4134 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \n",
       "L 1172 3500 \n",
       "L 2356 3500 \n",
       "L 2356 3053 \n",
       "L 1172 3053 \n",
       "L 1172 1153 \n",
       "Q 1172 725 1289 603 \n",
       "Q 1406 481 1766 481 \n",
       "L 2356 481 \n",
       "L 2356 0 \n",
       "L 1766 0 \n",
       "Q 1100 0 847 248 \n",
       "Q 594 497 594 1153 \n",
       "L 594 3053 \n",
       "L 172 3053 \n",
       "L 172 3500 \n",
       "L 594 3500 \n",
       "L 594 4494 \n",
       "L 1172 4494 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \n",
       "L 3513 0 \n",
       "L 2938 0 \n",
       "L 2938 2094 \n",
       "Q 2938 2591 2744 2837 \n",
       "Q 2550 3084 2163 3084 \n",
       "Q 1697 3084 1428 2787 \n",
       "Q 1159 2491 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1366 3272 1645 3428 \n",
       "Q 1925 3584 2291 3584 \n",
       "Q 2894 3584 3203 3211 \n",
       "Q 3513 2838 3513 2113 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-2d\" d=\"M 313 2009 \n",
       "L 1997 2009 \n",
       "L 1997 1497 \n",
       "L 313 1497 \n",
       "L 313 2009 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-64\" d=\"M 2906 2969 \n",
       "L 2906 4863 \n",
       "L 3481 4863 \n",
       "L 3481 0 \n",
       "L 2906 0 \n",
       "L 2906 525 \n",
       "Q 2725 213 2448 61 \n",
       "Q 2172 -91 1784 -91 \n",
       "Q 1150 -91 751 415 \n",
       "Q 353 922 353 1747 \n",
       "Q 353 2572 751 3078 \n",
       "Q 1150 3584 1784 3584 \n",
       "Q 2172 3584 2448 3432 \n",
       "Q 2725 3281 2906 2969 \n",
       "z\n",
       "M 947 1747 \n",
       "Q 947 1113 1208 752 \n",
       "Q 1469 391 1925 391 \n",
       "Q 2381 391 2643 752 \n",
       "Q 2906 1113 2906 1747 \n",
       "Q 2906 2381 2643 2742 \n",
       "Q 2381 3103 1925 3103 \n",
       "Q 1469 3103 1208 2742 \n",
       "Q 947 2381 947 1747 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \n",
       "L 3597 1613 \n",
       "L 953 1613 \n",
       "Q 991 1019 1311 708 \n",
       "Q 1631 397 2203 397 \n",
       "Q 2534 397 2845 478 \n",
       "Q 3156 559 3463 722 \n",
       "L 3463 178 \n",
       "Q 3153 47 2828 -22 \n",
       "Q 2503 -91 2169 -91 \n",
       "Q 1331 -91 842 396 \n",
       "Q 353 884 353 1716 \n",
       "Q 353 2575 817 3079 \n",
       "Q 1281 3584 2069 3584 \n",
       "Q 2775 3584 3186 3129 \n",
       "Q 3597 2675 3597 1894 \n",
       "z\n",
       "M 3022 2063 \n",
       "Q 3016 2534 2758 2815 \n",
       "Q 2500 3097 2075 3097 \n",
       "Q 1594 3097 1305 2825 \n",
       "Q 1016 2553 972 2059 \n",
       "L 3022 2063 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-56\" d=\"M 1831 0 \n",
       "L 50 4666 \n",
       "L 709 4666 \n",
       "L 2188 738 \n",
       "L 3669 4666 \n",
       "L 4325 4666 \n",
       "L 2547 0 \n",
       "L 1831 0 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \n",
       "L 3122 2828 \n",
       "Q 2878 2963 2633 3030 \n",
       "Q 2388 3097 2138 3097 \n",
       "Q 1578 3097 1268 2742 \n",
       "Q 959 2388 959 1747 \n",
       "Q 959 1106 1268 751 \n",
       "Q 1578 397 2138 397 \n",
       "Q 2388 397 2633 464 \n",
       "Q 2878 531 3122 666 \n",
       "L 3122 134 \n",
       "Q 2881 22 2623 -34 \n",
       "Q 2366 -91 2075 -91 \n",
       "Q 1284 -91 818 406 \n",
       "Q 353 903 353 1747 \n",
       "Q 353 2603 823 3093 \n",
       "Q 1294 3584 2113 3584 \n",
       "Q 2378 3584 2631 3529 \n",
       "Q 2884 3475 3122 3366 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \n",
       "Q 2534 3019 2420 3045 \n",
       "Q 2306 3072 2169 3072 \n",
       "Q 1681 3072 1420 2755 \n",
       "Q 1159 2438 1159 1844 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1341 3275 1631 3429 \n",
       "Q 1922 3584 2338 3584 \n",
       "Q 2397 3584 2469 3576 \n",
       "Q 2541 3569 2628 3553 \n",
       "L 2631 2963 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \n",
       "Q 1497 1759 1228 1600 \n",
       "Q 959 1441 959 1056 \n",
       "Q 959 750 1161 570 \n",
       "Q 1363 391 1709 391 \n",
       "Q 2188 391 2477 730 \n",
       "Q 2766 1069 2766 1631 \n",
       "L 2766 1759 \n",
       "L 2194 1759 \n",
       "z\n",
       "M 3341 1997 \n",
       "L 3341 0 \n",
       "L 2766 0 \n",
       "L 2766 531 \n",
       "Q 2569 213 2275 61 \n",
       "Q 1981 -91 1556 -91 \n",
       "Q 1019 -91 701 211 \n",
       "Q 384 513 384 1019 \n",
       "Q 384 1609 779 1909 \n",
       "Q 1175 2209 1959 2209 \n",
       "L 2766 2209 \n",
       "L 2766 2266 \n",
       "Q 2766 2663 2505 2880 \n",
       "Q 2244 3097 1772 3097 \n",
       "Q 1472 3097 1187 3025 \n",
       "Q 903 2953 641 2809 \n",
       "L 641 3341 \n",
       "Q 956 3463 1253 3523 \n",
       "Q 1550 3584 1831 3584 \n",
       "Q 2591 3584 2966 3190 \n",
       "Q 3341 2797 3341 1997 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSans-52\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6f\" transform=\"translate(64.982422 0)\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-50\" transform=\"translate(126.164062 0)\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-45\" transform=\"translate(186.466797 0)\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-3a\" transform=\"translate(249.650391 0)\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-20\" transform=\"translate(283.341797 0)\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-50\" transform=\"translate(315.128906 0)\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6f\" transform=\"translate(371.806641 0)\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-73\" transform=\"translate(432.988281 0)\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-69\" transform=\"translate(485.087891 0)\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-74\" transform=\"translate(512.871094 0)\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-69\" transform=\"translate(552.080078 0)\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6f\" transform=\"translate(579.863281 0)\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6e\" transform=\"translate(641.044922 0)\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-2d\" transform=\"translate(704.423828 0)\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-64\" transform=\"translate(740.507812 0)\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-65\" transform=\"translate(803.984375 0)\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-70\" transform=\"translate(865.507812 0)\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-65\" transform=\"translate(928.984375 0)\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6e\" transform=\"translate(990.507812 0)\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-64\" transform=\"translate(1053.886719 0)\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-65\" transform=\"translate(1117.363281 0)\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6e\" transform=\"translate(1178.886719 0)\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-74\" transform=\"translate(1242.265625 0)\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-20\" transform=\"translate(1281.474609 0)\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-56\" transform=\"translate(1313.261719 0)\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-65\" transform=\"translate(1373.919922 0)\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-63\" transform=\"translate(1435.443359 0)\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-74\" transform=\"translate(1490.423828 0)\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6f\" transform=\"translate(1529.632812 0)\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-72\" transform=\"translate(1590.814453 0)\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-20\" transform=\"translate(1631.927734 0)\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-52\" transform=\"translate(1663.714844 0)\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6f\" transform=\"translate(1728.697266 0)\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-74\" transform=\"translate(1789.878906 0)\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-61\" transform=\"translate(1829.087891 0)\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-74\" transform=\"translate(1890.367188 0)\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-69\" transform=\"translate(1929.576172 0)\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6f\" transform=\"translate(1957.359375 0)\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6e\" transform=\"translate(2018.541016 0)\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"p36a63da2bb\">\n",
       "   <rect x=\"48.415312\" y=\"22.318125\" width=\"167.4\" height=\"166.32\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib_inline.backend_inline import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg')\n",
    "\n",
    "def visualize_rope_rotation_simple():\n",
    "    # Rotation angles for each position\n",
    "    positions = np.arange(4)  # 4 positions\n",
    "    angles = positions * np.pi/6  # increasing by 30 degrees each time\n",
    "    \n",
    "    # Original vector\n",
    "    vector = np.array([1, 0])  # Reference vector\n",
    "    \n",
    "    plt.figure(figsize=(3, 3))\n",
    "    for i, theta in enumerate(angles):\n",
    "        # Create rotation matrix\n",
    "        rotation = np.array([\n",
    "            [np.cos(theta), -np.sin(theta)],\n",
    "            [np.sin(theta), np.cos(theta)]\n",
    "        ])\n",
    "        \n",
    "        # Rotate the vector\n",
    "        rotated = rotation @ vector\n",
    "        \n",
    "        # Plot the rotated vector\n",
    "        plt.arrow(0, 0, rotated[0], rotated[1], \n",
    "                 head_width=0.05, head_length=0.1)\n",
    "        plt.text(rotated[0], rotated[1], f'pos {i}')\n",
    "    \n",
    "    plt.grid(True)\n",
    "    plt.axis('equal')\n",
    "    plt.title('RoPE: Position-dependent Vector Rotation')\n",
    "    plt.show()\n",
    "\n",
    "visualize_rope_rotation_simple()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La ventaja de este método es que el cálculo de distancias relativas es sencillo (diferencia de ángulos de rotación entre dos posiciones) y no hay límite en la longitud de la secuencia. Además, es posible procesar secuencias más largas que las aprendidas.\n",
    "\n",
    "**Principales cambios en `efficient_encoder_rope.py`**\n",
    "\n",
    "1.  **Clase `Embeddings`:** Se elimina `position_embeddings`, y se suprime el proceso de agregar incrustaciones de posición en `forward()`. No es necesario un incrustación de posición separada, ya que RoPE maneja la información de posición.\n",
    "\n",
    "2.  **Función `rotate_half`:** Es la parte central de la operación RoPE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # (efficient_encoder_rope.py)\n",
    "    def rotate_half(x):\n",
    "        \"\"\"Rotates half the hidden dims of the input.\"\"\"\n",
    "        x1 = x[..., :x.shape[-1] // 2]\n",
    "        x2 = x[..., x.shape[-1] // 2:]\n",
    "        return torch.cat((-x2, x1), dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.  **`apply_rotary_pos_emb` función:** aplica RoPE a la consulta (q) y la clave (k)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # (efficient_encoder_rope.py)\n",
    "    def apply_rotary_pos_emb(q, k, cos, sin):\n",
    "        \"\"\"Applies rotary position embeddings to query and key tensors.\"\"\"\n",
    "        q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "        k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "        return q_embed, k_embed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.  **`FlashAttention` clase:**\n",
    "\n",
    "*   `cos_cached`, `sin_cached`: Almacena (en caché) los valores de coseno y seno utilizados en RoPE, calculados previamente. Se generan en `_build_cache()`.\n",
    "*   `_build_cache()`: Calcula previamente los valores de las funciones trigonométricas necesarios para RoPE.\n",
    "*   `forward()`: Después de aplicar la transformación lineal a las consultas y claves, llama a `apply_rotary_pos_emb()` para aplicar RoPE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import Optional\n",
    "import torch.nn as nn\n",
    "\n",
    "def apply_rotary_pos_emb(q, k, cos, sin):\n",
    "    \"\"\"Applies Rotary Position Embeddings to query and key tensors.\"\"\"\n",
    "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "    return q_embed, k_embed\n",
    "\n",
    "def rotate_half(x):\n",
    "    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n",
    "    x1 = x[..., : x.shape[-1] // 2]\n",
    "    x2 = x[..., x.shape[-1] // 2 :]\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "class FlashAttention(nn.Module):\n",
    "    # ... (rest of the class definition, unchanged) ...\n",
    "\n",
    "    def _build_cache(self, device, dtype):\n",
    "        if self.cos_cached is not None and self.cos_cached.dtype == dtype: #Return if cache already exist.\n",
    "            return\n",
    "\n",
    "        # Create position indices\n",
    "        pos_seq = torch.arange(self.max_position_embeddings, device=device, dtype=dtype)\n",
    "\n",
    "        # Create freqs (theta in paper)\n",
    "        inv_freq = 1.0 / (10000 ** (torch.arange(0, self.attention_head_size, 2, device=device, dtype=dtype) / self.attention_head_size))\n",
    "\n",
    "        # Create freqs for each position in sequence.\n",
    "        freqs = torch.einsum(\"i,j->ij\", pos_seq, inv_freq)\n",
    "        # Expand the shape for later element-wise calculations\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "\n",
    "        # Create the cos and sin cache\n",
    "        self.cos_cached = emb.cos()[None, None, :, :]  # Add head and batch dimensions\n",
    "        self.sin_cached = emb.sin()[None, None, :, :]\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor, attention_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        # ... (rest of the forward method, unchanged) ...\n",
    "\n",
    "        # Apply RoPE\n",
    "        batch_size, num_heads, seq_len, head_dim = query_layer.shape\n",
    "        self._build_cache(query_layer.device, query_layer.dtype)\n",
    "\n",
    "        cos = self.cos_cached[:, :, :seq_len, :head_dim]\n",
    "        sin = self.sin_cached[:, :, :seq_len, :head_dim]\n",
    "\n",
    "        query_layer, key_layer = apply_rotary_pos_emb(query_layer, key_layer, cos, sin)\n",
    "\n",
    "        # ... (rest of the forward method, unchanged) ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.5.4 Resultados experimentales: Clasificación de texto AG News\n",
    "\n",
    "Utilizamos dos versiones del codificador eficiente (`efficient_encoder_rope.py` y `efficient_encoder.py`) para realizar experimentos de clasificación de texto en el conjunto de datos AG News (que clasifica artículos de noticias en 4 categorías). El código para ejecutar el entrenamiento es `train_ag_news.py`.\n",
    "\n",
    "El conjunto de datos AG News está compuesto por artículos de noticias equilibrados en cada categoría. Cada artículo se limita a una longitud máxima de 128 tokens, y realizamos un entrenamiento comparativo utilizando dos tokenizadores: BERT y T5. Clasificamos los textos de noticias en las categorías World, Sports, Business, Sci/Tech. La escala del modelo se configuró muy pequeña, como sigue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size: int = 30522,\n",
    "hidden_size: int = 256,\n",
    "num_hidden_layers: int = 4,\n",
    "num_attention_heads: int = 8,\n",
    "intermediate_size: int = 512,\n",
    "hidden_dropout_prob: float = 0.1,\n",
    "attention_probs_dropout_prob: float = 0.1,\n",
    "max_position_embeddings: int = 512,\n",
    "layer_norm_eps: float = 1e-12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo siguiente es la sección de ejecución para realizar experimentos de comparación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dldna.chapter_09.encoder.train_ag_news import train_and_test_all_versions\n",
    "\n",
    "train_and_test_all_versions(verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resultados de entrenamiento**\n",
    "\n",
    "| Versión del modelo | Tokenizador          | Precisión en prueba (%) |               Notas               |\n",
    "| -------- | ------------------- | --------------| ------------------------- |\n",
    "| v1 | bert-base-uncased     |       91.24       |           FlashAttention           |\n",
    "| v1        | t5-small              |      92.00    |       FlashAttention       |\n",
    "| v2   | bert-base-uncased     |       92.57       |         RoPE, FlashAttention         |\n",
    "| v2  | t5-small              |       92.07    |       RoPE, FlashAttention        |\n",
    "\n",
    "*   **v1**: `efficient_encoder.py` (sin RoPE)\n",
    "*   **v2**: `efficient_encoder_rope.py` (con RoPE)\n",
    "\n",
    "**Interpretación de resultados**\n",
    "\n",
    "1.  **Efecto de RoPE (v2):** Al usar el tokenizador `bert-base-uncased`, el modelo v2 con RoPE mostró una precisión 1.33%p mayor que el modelo v1. Esto sugiere que RoPE codifica la información posicional de manera más efectiva, mejorando el rendimiento del modelo. En particular, cuando se deben procesar secuencias más largas que los datos de entrenamiento (extrapolación de longitud), las ventajas de RoPE pueden ser más notables.\n",
    "\n",
    "2.  **Influencia del tokenizador:** Al usar el tokenizador `t5-small`, ambas versiones mostraron un nivel similar de precisión al utilizar `bert-base-uncased`. Sin embargo, v2 presentó un rendimiento ligeramente mejor.\n",
    "\n",
    "3.  **Rendimiento alto en general:** Ambas versiones lograron una alta precisión superior al 91% en el conjunto de datos AG News. Esto indica que la arquitectura del modelo es efectiva y que se han aplicado bien técnicas modernas de entrenamiento de Transformers, como el uso de `F.scaled_dot_product_attention` para FlashAttention (cuando el entorno lo soporta), Pre-LN, GELU, inicialización Xavier, AdamW y programación de tasa de aprendizaje.\n",
    "\n",
    "**Comparación con modelos similares (tabla)**\n",
    "\n",
    "La tabla siguiente compara el rendimiento del modelo con otros modelos de tamaño similar en el conjunto de datos AG News. (La precisión puede variar según la literatura y los resultados experimentales.)\n",
    "\n",
    "| Modelo            | Precisión (%) |\n",
    "| ----------------- | ------------- |\n",
    "| Nuestro modelo v1 | 91.24         |\n",
    "| Nuestro modelo v2 | 92.57         |\n",
    "| DistilBERT        | 90.36         |\n",
    "| BERT-base         | 91.10         |\n",
    "| RoBERTa-base      | 92.84         |\n",
    "| modelo                                  | hidden_size | num_hidden_layers | precisión AG News (aprox.) |               notas               |\n",
    "| ------------------------------------ |----------| ------------ | --------------- | ------------------------------ |\n",
    "| **Efficient Encoder (v2, bert)**    |     256     |         4         |        92.57       |         RoPE, FlashAttention         |\n",
    "| **Efficient Encoder (v2, t5)** |     256     |      4            |       92.07      |       RoPE, FlashAttention        |\n",
    "| **Efficient Encoder (v1, bert)**    |     256     |         4         |        91.24       |           FlashAttention           |\n",
    "| **Efficient Encoder (v1, t5)** |     256     |         4                   |         92.00     |       FlashAttention       |\n",
    "| TinyBERT (4 capas, hidden_size=312)  |     312     |         4         |       88-90%       |           Destilación           |\n",
    "| BERT-small                            |     512        |        4                    |      ~90.8%            |             |\n",
    "| DistilBERT-base                       |     768     |         6         |       90-92%       |  Destilación, más pequeño que BERT-base  |\n",
    "| BERT-base                             |     768     |        12                |       92-95%       |       modelo mucho más grande            |\n",
    "\n",
    "**mecanismos aplicados**\n",
    "| Mecanismo        | v1 (`efficient_encoder.py`) | v2 (`efficient_encoder_rope.py`) |                   Nota                   |\n",
    "| ------------------------ | ---------------------- | ------------------- | ------------------------------ |\n",
    "| FlashAttention             |               O               |                O                |    Optimización utilizando la estructura de memoria de GPU     |\n",
    "| Pre-LN                     |               O               |                O                |    Aplicación de Layer Normalization antes de la atención/FFN    |\n",
    "| RoPE                       |               X               |                O                |   Codificación de información posicional mediante matrices rotacionales   |\n",
    "| Incrustaciones de posición aprendibles     |               O               |                X                |       Representación de información posicional cuando no se usa RoPE       |\n",
    "| Inicialización Xavier              |               O               |                O                |             Método de inicialización de pesos             |\n",
    "| Función de activación GELU          |               O               |                O                |     Función de activación no lineal (usada en FFN)     |\n",
    "| Dropout                    |               O               |                O                |                 Mejora del rendimiento de generalización                 |\n",
    "| Normalización por capa          |               O                |                O                |     Estabilización y mejora del rendimiento durante el aprendizaje     |\n",
    "| Uso de tokenizador preentrenado |               O               |                O                | BERT-base-uncased, t5-small utilizado |\n",
    "\n",
    "**Conclusión**\n",
    "\n",
    "En esta sección, diseñamos un modelo de codificador Transformer (v2) más eficiente aplicando FlashAttention utilizando `F.scaled_dot_product_attention` de PyTorch y RoPE (Rotary Positional Embeddings) para aumentar la eficiencia. Entrenamos y probamos los modelos v1 (codificador Transformer básico) y v2 (con RoPE aplicado) con el conjunto de datos AG News para clasificación de texto, utilizando `bert-base-uncased` y `t5-small` como tokenizadores. Los resultados mostraron que el modelo v2 logró una precisión más alta (92.57%) con el tokenizador `bert-base-uncased`. Esto sugiere que RoPE codifica eficazmente la información de posición relativa, mejorando el rendimiento del modelo, especialmente en el procesamiento de textos largos.\n",
    "Ambos modelos lograron una alta precisión del 91-92%, lo que demuestra que la arquitectura Efficient Encoder es eficiente y puede generar un rendimiento potente. Además, al comparar los tokenizadores `bert-base-uncased` y `t5-small`, hubo una ligera diferencia, con v2 utilizando `bert-base-uncased` logrando un mejor rendimiento.\n",
    "\n",
    "Como se puede ver en la tabla, el modelo Efficient Encoder propuesto supera en rendimiento a modelos más pequeños como TinyBERT y alcanza un rendimiento competitivo en comparación con BERT-small. Es importante destacar que logra un rendimiento cercano al de modelos más grandes como DistilBERT-base o BERT-base, pero con un tamaño mucho menor. Esto se puede atribuir a la combinación de tokenizadores preentrenados, FlashAttention, estructura Pre-LN, RoPE, inicialización Xavier, función de activación GELU y una configuración adecuada del modelo (hidden_size, num_hidden_layers, etc.).\n",
    "\n",
    "En conclusión, el Efficient Encoder (v2) presentado en este capítulo no solo es útil para comprender los componentes clave del Transformer con fines educativos, sino que también ha demostrado ser un modelo eficiente capaz de generar un rendimiento competitivo en aplicaciones prácticas. En particular, la aplicación de RoPE se ha confirmado como un método efectivo para mejorar el rendimiento del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.6 Mistral: Implementación y análisis de una arquitectura de decodificador eficiente\n",
    "\n",
    "El modelo Mistral-7B publicado por Mistral AI en 2023 se basa en la arquitectura LLaMA e introduce **atención de consulta agrupada (GQA)** y **atención de ventana deslizante (SWA)**, lo que mejora significativamente la eficiencia de memoria y la velocidad de procesamiento. En particular, con solo 7B de parámetros, muestra un rendimiento comparable al de modelos con más de 13B de parámetros, demostrando la importancia del diseño de una arquitectura eficiente.\n",
    "\n",
    "En esta sección, implementamos y analizamos un modelo simplificado de Mistral basándonos en la implementación de Hugging Face Transformers, centrándonos en los elementos clave de optimización. Examinaremos detenidamente GQA, SWA, RoPE y el mecanismo de caché KV, y entenderemos cómo contribuyen a la eficiencia y rendimiento del modelo. El código está disponible en `chapter_09/mistral`.\n",
    "\n",
    "### 9.6.1 Arquitectura del modelo `simple_mistral`: Análisis detallado de los componentes\n",
    "\n",
    "El modelo `simple_mistral` implementa una versión simplificada de los componentes principales del modelo Mistral-7B, donde cada componente está modularizado y realiza funciones claras. A continuación, analizaremos en detalle cada uno de estos componentes.\n",
    "\n",
    "#### 1. MistralConfig: Configuración del modelo\n",
    "\n",
    "La clase `MistralConfig` define los hiperparámetros del modelo, lo cual es un papel crucial para determinar la estructura y el comportamiento del modelo.\n",
    "\n",
    "*   **Propiedades principales:**\n",
    "    *   vocab_size: Especifica el tamaño del diccionario de vocabulario (valor predeterminado: 32000).\n",
    "    *   hidden_size: Representa la dimensión de los embeddings y estados ocultos (valor predeterminado: 4096).\n",
    "    *   intermediate_size: Define la dimensión intermedia de la red FeedForward (valor predeterminado: 14336).\n",
    "    *   num_hidden_layers: Especifica el número de capas del decodificador Transformer (valor predeterminado: 32).\n",
    "    *   num_attention_heads: Indica el número de cabezas de atención (valor predeterminado: 32).\n",
    "    *   num_key_value_heads: Define el número de cabezas clave/valor utilizadas en GQA (valor predeterminado: 8).\n",
    "    *   hidden_act: Función de activación, se usa \"silu\" (predeterminada).\n",
    "    *   max_position_embeddings: Especifica la longitud máxima de secuencia (valor predeterminado: 4096 * 32).\n",
    "    *   rms_norm_eps: Representa el valor epsilon de RMSNorm (valor predeterminado: 1e-6).\n",
    "    *   use_cache: Determina si se utiliza caché KV (valor predeterminado: True).\n",
    "    *   rope_theta: Establece el valor theta de RoPE (valor predeterminado: 10000.0).\n",
    "    *   sliding_window: Especifica el tamaño de la ventana deslizante (valor predeterminado: 4096).\n",
    "    *   use_return_dict: Configura si se devuelve un diccionario (valor predeterminado: True).\n",
    "\n",
    "#### 2. MistralRMSNorm: Normalización RMS\n",
    "\n",
    "La clase `MistralRMSNorm` implementa la normalización RMS (Root Mean Square Layer Normalization). Mejora la eficiencia computacional al eliminar el promedio y normalizar mediante la raíz cuadrada de la media de los cuadrados (RMS) en lugar de usar LayerNorm tradicional.\n",
    "\n",
    "*   **Características:** Utiliza `variance_epsilon` para asegurar estabilidad numérica.\n",
    "\n",
    "#### 3. MistralAttention: Mecanismo de atención\n",
    "\n",
    "La clase `MistralAttention` implementa el mecanismo de atención central del modelo Mistral, integrando GQA, SWA y RoPE para mejorar la eficiencia y el rendimiento.\n",
    "*   **GQA (Grouped-Query Attention):**\n",
    "    *   Se mantienen múltiples cabezas de consulta (Q) y se configuran menos cabezas para clave (K) y valor (V) para reducir el uso de memoria y la cantidad de cálculos.\n",
    "    *   Se ajusta el número de cabezas K/V a través de `num_key_value_heads`.\n",
    "    *   Se replica el tensor K/V hasta coincidir con el número de cabezas Q utilizando la función `repeat_kv`.\n",
    "\n",
    "*   **SWA (Sliding Window Attention):**\n",
    "    *   Se reduce la complejidad computacional al permitir que cada token solo realice atención sobre tokens dentro de una ventana limitada.\n",
    "    *   Se ajusta el tamaño de la ventana a través del parámetro `sliding_window`.\n",
    "    *   Se modifica el `attention_mask` para bloquear la atención con tokens fuera de la ventana.\n",
    "\n",
    "*   **RoPE (Rotary Positional Embedding):**\n",
    "    *   Se codifican las posiciones utilizando matrices de rotación.\n",
    "    *   Se implementa a través de la clase `MistralRotaryEmbedding`.\n",
    "    *   Se aplica RoPE a las consultas y claves usando la función `apply_rotary_pos_emb`.\n",
    "\n",
    "#### 4. MistralRotaryEmbedding: Implementación de RoPE\n",
    "\n",
    "La clase `MistralRotaryEmbedding` implementa RoPE (Rotary Positional Embedding).\n",
    "\n",
    "*   **Método `__init__`:**\n",
    "    *   dim: se establece la dimensión del embedding.\n",
    "    *   max_position_embeddings: se especifica la longitud máxima de secuencia.\n",
    "    *   base: se define una constante para el cálculo de frecuencias (valor predeterminado: 10000).\n",
    "    *   inv_freq: se calcula la frecuencia inversa y se registra como un parámetro no entrenable.\n",
    "    *   cos_cached, sin_cached: se almacenan en caché los valores precalculados de coseno y seno.\n",
    "\n",
    "*   **Método `forward`:**\n",
    "    *   Recibe el tensor de entrada `x` y la longitud de secuencia `seq_len`.\n",
    "    *   Si `seq_len` es mayor que la longitud máxima almacenada en caché, se llama a `_set_cos_sin_cache` para actualizar la caché.\n",
    "    *   Devuelve los valores almacenados en caché de coseno y seno.\n",
    "\n",
    "*   **Método `_set_cos_sin_cache`:**\n",
    "    *   Se generan índices de posición hasta `seq_len`.\n",
    "    *   Se calcula la frecuencia multiplicando los índices de posición por la frecuencia inversa.\n",
    "    *   Se calculan y almacenan en caché los valores de coseno y seno utilizando las frecuencias calculadas.\n",
    "\n",
    "#### 5. MistralMLP: Red FeedForward\n",
    "\n",
    "La clase `MistralMLP` implementa la red FeedForward del modelo Mistral.\n",
    "\n",
    "*   **Composición:**\n",
    "    *   `gate_proj`, `up_proj`, `down_proj`: tres capas lineales se utilizan para expandir y luego reducir de nuevo la entrada.\n",
    "    *   `act_fn`: Se utiliza la función de activación SiLU (Sigmoid Linear Unit).\n",
    "\n",
    "#### 6. MistralDecoderLayer: Capa de decodificador\n",
    "\n",
    "La clase `MistralDecoderLayer` compone una capa de decodificador del modelo Mistral.\n",
    "\n",
    "*   **Componentes:**\n",
    "    *   `self_attn`: utiliza el módulo `MistralAttention` para realizar self-attention.\n",
    "    *   `mlp`: utiliza el módulo `MistralMLP` para realizar la red FeedForward.\n",
    "    *   `input_layernorm`, `post_attention_layernorm`: utilizan `MistralRMSNorm` para normalizar las entradas/salidas.\n",
    "\n",
    "#### 7. MistralPreTrainedModel: Clase abstracta de modelo preentrenado\n",
    "La clase `MistralPreTrainedModel` es una clase abstracta base que gestiona la inicialización y configuración de los pesos del modelo Mistral.\n",
    "\n",
    "*   **Métodos principales:**\n",
    "    *   `_init_weights`: Inicializa los pesos.\n",
    "    *   `_set_gradient_checkpointing`: Configura si el checkpointing de gradientes está activado o no.\n",
    "\n",
    "#### 8. MistralModel: Modelo Mistral\n",
    "\n",
    "La clase `MistralModel` define la estructura completa del modelo Mistral.\n",
    "\n",
    "*   **Componentes:**\n",
    "    *   `embed_tokens`: Convierte los tokens de entrada en vectores de embeddings.\n",
    "    *   `layers`: Compone varias capas de `MistralDecoderLayer`.\n",
    "    *   `norm`: Normaliza la salida de la última capa.\n",
    "\n",
    "#### 9. MistralForCausalLM: Mistral para modelado de lenguaje\n",
    "\n",
    "La clase `MistralForCausalLM` es una clase diseñada para ajustar el modelo Mistral a tareas de modelado de lenguaje causal (Causal Language Modeling).\n",
    "\n",
    "*   **Componentes principales:**\n",
    "    *   `lm_head`: Proyecta la salida del modelo al tamaño del vocabulario para calcular las probabilidades de predicción del siguiente token.\n",
    "    *   `prepare_inputs_for_generation`: Prepara las entradas durante el proceso de inferencia.\n",
    "    *   `_reorder_cache`: Reordena el caché KV durante la búsqueda por haz (beam search).\n",
    "\n",
    "---\n",
    "\n",
    "De esta manera, el modelo `simple_mistral` proporciona un diseño eficiente y flexible al modularizar cada componente. Entender el rol e interacción de cada componente permite comprender mejor los principios de funcionamiento del modelo.\n",
    "\n",
    "### 9.6.2 Análisis de elementos técnicos clave: El secreto de la eficiencia y el rendimiento\n",
    "\n",
    "El modelo `simple_mistral` maximiza su eficiencia y rendimiento mediante elementos técnicos clave como GQA, SWA y RoPE. Analizaremos detalladamente cómo funcionan estos elementos y sus ventajas.\n",
    "\n",
    "#### 1. GQA (Grouped-Query Attention): Innovación para la eficiencia de memoria y cálculo\n",
    "\n",
    "GQA es una variante del Multi-Head Attention que reduce el uso de memoria y la cantidad de cálculos mientras mantiene el rendimiento.\n",
    "\n",
    "*   **Funcionamiento:**\n",
    "    *   Las consultas (Q) se dividen en múltiples cabezas, pero las claves (K) y valores (V) se dividen en un número menor de cabezas.\n",
    "    *   Cada cabeza Q se asigna a un grupo específico de cabezas K/V.\n",
    "    *   Cada cabeza Q solo calcula la atención para el grupo de cabezas K/V al que está asignada.\n",
    "    *   La función `repeat_kv` replica los tensores K/V para ajustarlos al número de cabezas Q, implementando este mecanismo.\n",
    "\n",
    "*   **Ventajas:**\n",
    "    *   **Reducción del uso de memoria:** El tamaño de los tensores K/V se reduce, lo que permite reducir el tamaño del caché KV.\n",
    "    *   **Reducción de la cantidad de cálculos:** La cantidad de cálculos de atención se reduce, mejorando la velocidad de inferencia.\n",
    "    *   **Mantenimiento del rendimiento:** El número de cabezas Q permanece igual, por lo que la capacidad expresiva del modelo no disminuye significativamente.\n",
    "\n",
    "#### 2. SWA (Sliding Window Attention): Estrategia eficiente para el procesamiento de secuencias largas\n",
    "\n",
    "SWA es una técnica que reduce la complejidad computacional al permitir que cada token solo realice la atención dentro de un rango limitado (ventana).\n",
    "\n",
    "*   **Funcionamiento:**\n",
    "    *   Cada token realiza la atención solo en los tokens dentro de una ventana de tamaño fijo.\n",
    "    *   La ventana se desplaza a lo largo de la secuencia, calculando la atención en cada posición.\n",
    "    *   Se utiliza un `attention_mask` para enmascarar la atención con tokens fuera de la ventana.\n",
    "*   **Ventajas:**\n",
    "    *   **Reducción de la complejidad computacional:** La cantidad de cálculos de atención se reduce de O(N²) a O(N\\*W). (N: longitud de la secuencia, W: tamaño de la ventana)\n",
    "    *   **Procesamiento de secuencias largas:** Se puede procesar secuencias más largas debido a una menor utilización de memoria.\n",
    "\n",
    "#### 3. RoPE (Rotary Positional Embedding): codificación eficiente de información de posición relativa\n",
    "\n",
    "RoPE ya se revisó en el capítulo 9.5. Aquí solo revisaremos brevemente las partes implementadas en el modelo. \n",
    "\n",
    "*   **Implementación:**\n",
    "    *   **Función `rotate_half`:** divide la dimensión del tensor de entrada a la mitad y alterna los signos para simular el efecto de una multiplicación compleja."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_half(x):\n",
    "    x1 = x[..., : x.shape[-1] // 2]\n",
    "    x2 = x[..., x.shape[-1] // 2 :]\n",
    "    return torch.cat((-x2, x1), dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*   **`apply_rotary_pos_emb` función:** aplica RoPE a los tensores de consulta (q) y clave (k)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rotary_pos_emb(q, k, cos, sin, position_ids_q, position_ids_k=None):\n",
    "    cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
    "    sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
    "    cos_q = cos[position_ids_q].unsqueeze(1)  # [batch_size, 1, seq_len, dim]\n",
    "    sin_q = sin[position_ids_q].unsqueeze(1)  # [batch_size, 1, seq_len, dim]\n",
    "    cos_k = cos[position_ids_k].unsqueeze(1)  # [batch_size, 1, seq_len, dim]\n",
    "    sin_k = sin[position_ids_k].unsqueeze(1)  # [batch_size, 1, seq_len, dim]\n",
    "    q_embed = (q * cos_q) + (rotate_half(q) * sin_q)\n",
    "    k_embed = (k * cos_k) + (rotate_half(k) * sin_k)\n",
    "    return q_embed, k_embed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Clase `MistralRotaryEmbedding`:** Calcula y almacena en caché los valores de coseno y seno necesarios para RoPE.\n",
    "    - `cos_cached`, `sin_cached`: Valores precalculados de coseno y seno.\n",
    "    - `_set_cos_sin_cache`: Actualiza `cos_cached` y `sin_cached` según la longitud de la secuencia.\n",
    "\n",
    "*   **Ventajas:**\n",
    "    *   **Preservación de información de posición relativa:** Los pesos de atención varían naturalmente según las distancias relativas entre los tokens.\n",
    "    *   **Extrapolación de longitud (Length Extrapolation):** Funciona bien incluso con secuencias más largas que la longitud de secuencia de entrenamiento.\n",
    "    *   **Complejidad lineal:** No afecta la complejidad del cálculo de atención.\n",
    "\n",
    "GQA, SWA y RoPE son elementos técnicos clave que mejoran respectivamente la eficiencia de memoria, la eficiencia computacional y la capacidad de representación de información de posición, lo que eleva el rendimiento general del modelo `simple_mistral`.\n",
    "\n",
    "#### 4. Caché KV: Eliminación de cálculos redundantes\n",
    "\n",
    "La caché KV es una técnica de optimización crucial para mejorar la velocidad de inferencia en modelos generativos.\n",
    "\n",
    "*   **Concepto:**\n",
    "    *   La caché KV almacena los tensores de clave (K) y valor (V) calculados en cada capa del decodificador durante el proceso de inferencia, para reutilizarlos.\n",
    "    *   Cada vez que se genera un nuevo token, no es necesario recalcular K y V para los tokens anteriores; se utilizan los valores almacenados en caché para realizar las operaciones.\n",
    "    *   Se almacena la caché KV de pasos previos a través del parámetro `past_key_values`, y se activa esta funcionalidad configurando `use_cache=True`. Cada capa recibe un `past_key_value` como entrada y produce un `present_key_value` actualizado.\n",
    "\n",
    "*   **Ventajas:**\n",
    "    *   **Mejora de la velocidad de inferencia:** Elimina cálculos redundantes, lo que aumenta significativamente la velocidad de generación de tokens.\n",
    "    *   **Aumento del uso de memoria:** Se requiere memoria adicional para almacenar la caché KV, pero esta cantidad puede mitigarse con técnicas como GQA y SWA.\n",
    "\n",
    "La caché KV es especialmente efectiva al generar texto largo y contribuye significativamente a mejorar la experiencia del usuario.\n",
    "\n",
    "### 9.6.3 Entrenamiento del modelo: Guía de entrenamiento para `simple_mistral`\n",
    "\n",
    "El proceso de entrenar el modelo `simple_mistral` consta principalmente de dos etapas: preprocesamiento de datos y entrenamiento del modelo.\n",
    "\n",
    "#### 1. Preprocesamiento de datos: Conversión a un formato comprensible por el modelo\n",
    "\n",
    "Este es el proceso de convertir los datos de texto que se utilizarán para el entrenamiento en un formato que el modelo pueda procesar.\n",
    "\n",
    "*   **Tokenización (Tokenization):**\n",
    "    *   Se utiliza un tokenizador (Tokenizer) para convertir los datos de texto en una forma numérica (IDs de tokens) que el modelo pueda procesar.\n",
    "    *   El tokenizador divide el texto en unidades más pequeñas (tokens) y asigna a cada token un ID único.\n",
    "\n",
    "*   **Generación de `attention_mask`:**\n",
    "    *   El `attention_mask` distingue los tokens de relleno (padding) y asegura que solo se aplique atención a los datos reales.\n",
    "    *   Los tokens de relleno son adicionados para ajustar la longitud de la secuencia y deben ser excluidos en el cálculo de atención.\n",
    "\n",
    "#### 2. Entrenamiento del modelo: Búsqueda de parámetros óptimos\n",
    "\n",
    "Se utiliza el modelo `MistralForCausalLM` para entrenar el modelo mediante lenguaje modelado causal (Causal Language Modeling).\n",
    "*   **`MistralForCausalLM` modelo:** Clase que configura el modelo Mistral para tareas de modelado de lenguaje.\n",
    "*   **Función de pérdida (Loss Function):**\n",
    "    *   Se utiliza `CrossEntropyLoss` para calcular la diferencia entre las salidas del modelo (predicciones) y las etiquetas correctas.\n",
    "    *   El modelo se entrena en dirección a minimizar esta pérdida.\n",
    "*   **Optimizador (Optimizer):**\n",
    "    *   Se usa el optimizador `AdamW` para actualizar los pesos (parámetros) del modelo.\n",
    "    *   AdamW es una versión mejorada del optimizador Adam, que aplica eficazmente la decadencia de peso (weight decay).\n",
    "*   **Programador de tasa de aprendizaje (Learning Rate Scheduler):**\n",
    "    *   Se utiliza el programador `get_cosine_schedule_with_warmup` para reducir gradualmente la tasa de aprendizaje.\n",
    "    *   Al principio del entrenamiento, se aumenta la tasa de aprendizaje para converger rápidamente y en las etapas finales del entrenamiento, se reduce la tasa de aprendizaje para realizar ajustes finos (fine-tuning).\n",
    "*   **Recorte de gradientes (Gradient Clipping):**\n",
    "    *   Se aplica recorte de gradientes para prevenir el problema de los gradientes explosivos.\n",
    "    *   Si el tamaño del gradiente excede un cierto umbral, se trunca el valor para ayudar a un aprendizaje estable.\n",
    "\n",
    "### 9.6.4 Generación de texto usando la función `generate()`: Creación de oraciones creativas\n",
    "\n",
    "Proceso de generar nuevo texto utilizando un modelo entrenado. La función `generate()` puede ajustar el estilo y la diversidad del texto generado mediante diversos parámetros.\n",
    "\n",
    "#### Función `generate()`: El núcleo de la generación de texto\n",
    "\n",
    "*   **Funcionalidad:** Genera texto basándose en un prompt dado.\n",
    "*   **Uso de caché KV:** Utiliza `past_key_values` para aprovechar la caché KV y mejorar la velocidad de inferencia.\n",
    "*   **Parámetros principales:**\n",
    "    *   max_new_tokens: Especifica el número máximo de tokens a generar.\n",
    "    *   temperature: Ajusta la forma de la distribución de probabilidad para controlar la diversidad de los resultados generados. (Valores bajos: consistencia, valores altos: diversidad)\n",
    "    *   top_k: Considera solo los k tokens con mayor probabilidad para muestrear.\n",
    "    *   top_p: Considera solo los tokens cuya probabilidad acumulada es igual o superior a p para muestrear. (muestreo de núcleo)\n",
    "    *   repetition_penalty: Aplica una penalización a los tokens repetidos para reducir la repetición en el texto.\n",
    "\n",
    "#### Proceso de generación: Generación de texto paso a paso\n",
    "\n",
    "1.  **Entrada inicial:** Tokeniza el prompt y lo introduce en el modelo para obtener la salida inicial.\n",
    "2.  **Ajuste de distribución de probabilidad:** Aplica condiciones de restricción como `temperature`, `top_k`, `top_p`, `repetition_penalty` a los logits de salida para ajustar la distribución de probabilidad del siguiente token.\n",
    "3.  **Muestreo de tokens:** Muestra el siguiente token según la distribución de probabilidad ajustada.\n",
    "4.  **Adición de salida y actualización de caché KV:** Añade el token generado a la secuencia de salida y actualiza la caché KV.\n",
    "5.  **Iteración:** Repite los pasos 2-4 hasta que se cumplan las condiciones de terminación (alcanzar la longitud máxima o generar un token de terminación).\n",
    "\n",
    "En esta sección, hemos examinado en detalle el proceso de entrenamiento y generación de texto del modelo Mistral. En las siguientes secciones, exploraremos ejemplos prácticos para ilustrar el uso del modelo `simple_mistral` a través de tres ejemplos. Los ejemplos están en mistral/examples.\n",
    "1.  **Predicción de secuencia numérica (`train_seq_num.py`):** Se verifica la capacidad básica de aprendizaje y generación del modelo a través de una tarea simple de predicción de números consecutivos.\n",
    "2.  **Predicción de operaciones aritméticas (`train_math.py`):** Se examina si el modelo aprende razonamiento simbólico (symbolic reasoning) a través de una tarea de predicción de resultados de operaciones de suma, resta y multiplicación.\n",
    "3.  **Generación de consultas SQL (`train_sql.py`):** Se evalúa la capacidad del modelo para comprender y procesar estructuras lingüísticas complejas mediante una tarea de conversión de preguntas en lenguaje natural a consultas SQL. (Uso del conjunto de datos WikiSQL)\n",
    "\n",
    "Se puede ejecutar directamente desde la shell en la ubicación correspondiente. Por ejemplo, `python train_seq_num.py`. A continuación se muestra cómo ejecutarlo desde un cuaderno Jupyter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.6.5 Ejemplo de predicción de secuencia numérica: análisis de `train_seq_num.py`\n",
    "\n",
    "`train_seq_num.py` es un ejemplo que utiliza el modelo `simple_mistral` para realizar una tarea de predicción de secuencia numérica simple. A través de este ejemplo, podemos examinar cómo el modelo aprende a predecir el siguiente número en una secuencia dada.\n",
    "\n",
    "#### 1. Preparación del conjunto de datos y del cargador de datos: configuración de los datos de entrenamiento\n",
    "\n",
    "Esta es la etapa donde se preparan los datos que el modelo `simple_mistral` utilizará para aprender.\n",
    "\n",
    "*   **Clase `SimpleDataset`:**\n",
    "    *   Define un conjunto de datos de secuencia numérica simple heredando de la clase `Dataset` de PyTorch.\n",
    "    *   El método `__init__` inicializa el conjunto de datos con los datos (`data`) y la longitud de la secuencia (`seq_length`).\n",
    "    *   El método `__len__` devuelve el número total de muestras en el conjunto de datos.\n",
    "    *   El método `__getitem__` devuelve la secuencia de entrada y la secuencia de etiquetas correspondientes a un índice dado (`idx`). En este ejemplo, la entrada y las etiquetas son la misma secuencia. Internamente, el modelo desplaza automáticamente las etiquetas una posición hacia adelante para formar la tarea de predicción del siguiente token.\n",
    "\n",
    "*   **Función `create_simple_data`:**\n",
    "    *   Genera datos de secuencia numérica que se ajustan a un tamaño de vocabulario especificado (`vocab_size`), número de muestras (`num_examples`) y longitud de secuencia (`seq_length`).\n",
    "    *   Repite los números desde 0 hasta `vocab_size - 1` para crear una lista de longitud `num_examples`.\n",
    "\n",
    "*   **Cargador de datos (`DataLoader`):**\n",
    "    *   El `DataLoader` agrupa los datos generados por `SimpleDataset` en lotes (mini-batches) y los proporciona al modelo.\n",
    "    *   `batch_size` especifica el número de muestras que se ingresarán al modelo a la vez,\n",
    "    *   Si se establece `shuffle=True`, el orden de los datos se mezclará aleatoriamente cada época para mejorar el efecto de entrenamiento.\n",
    "\n",
    "    Los datos de entrenamiento generados por `SimpleDataset` tienen el siguiente formato:\n",
    "\n",
    "    ```text\n",
    "    Muestra 1: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] -> [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "    Muestra 2: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] -> [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "    ```\n",
    "\n",
    "\n",
    "    **Desplazamiento de etiquetas en la función `forward` del modelo**\n",
    "\n",
    "    En la función `forward` del modelo `simple_mistral`, las secuencias de etiquetas se desplazan internamente una posición hacia adelante. Es decir, el modelo funciona así:\n",
    "\n",
    "    1.  **Secuencia de entrada:** `[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]`\n",
    "    2.  **Entrada al modelo:** `[0, 1, 2, 3, 4, 5, 6, 7, 8]` (excluyendo el último token)\n",
    "    3.  **Predicción del modelo:** `[1, 2, 3, 4, 5, 6, 7, 8, 9]` (predicción del siguiente token en cada posición)\n",
    "    4.  **Etiquetas:** `[1, 2, 3, 4, 5, 6, 7, 8, 9]` (excluyendo el primer token de la secuencia de entrada, para comparar con la predicción del modelo)\n",
    "\n",
    "    A través de este proceso, el modelo aprende a predecir el siguiente token en cada posición de la secuencia de entrada.\n",
    "\n",
    "\n",
    "#### 2. Configuración y entrenamiento del modelo: entrenamiento de `simple_mistral`\n",
    "\n",
    "Esta es la etapa donde se configura el modelo `simple_mistral` y se procede con el entrenamiento utilizando los datos preparados.\n",
    "*   **Configuración de `MistralConfig`:**\n",
    "    *   `vocab_size` se establece agregando el token `<eos>` al tamaño del vocabulario definido por el tokenizador. Esto permite que el modelo reconozca el final de una oración.\n",
    "    *   `sliding_window` se configura igual que la longitud de la secuencia para que cada token pueda ver toda la secuencia.\n",
    "    *   Se establece `use_cache=False` para no utilizar el caché KV durante el entrenamiento.\n",
    "* **Compartir pesos (`tie_weights = True`):**\n",
    "    *   Al configurar `tie_weights` en `True`, se comparten los pesos de incrustación y los pesos del capa de salida (`lm_head`). Esto puede reducir el número de parámetros y ayudar a aprender patrones específicos (como la generación secuencial de números).\n",
    "\n",
    "*   **Creación del modelo (`MistralForCausalLM`) y optimizador (`AdamW`):**\n",
    "    *   Se crea un modelo `MistralForCausalLM` y se mueve al dispositivo especificado (CPU o GPU).\n",
    "    *   Se crea un optimizador `AdamW`, y se establecen los parámetros del modelo y la tasa de aprendizaje (`learning_rate`).\n",
    "\n",
    "*   **Función `train` (bucle de entrenamiento):**\n",
    "    *   Se configura el modelo en modo de entrenamiento (`model.train()`).\n",
    "    *   Se repite el entrenamiento por el número especificado de épocas.\n",
    "    *   En cada época, se obtienen mini lotes del cargador de datos, se ingresan al modelo y se calcula la pérdida.\n",
    "    *   A través del retropropagación, se calculan los gradientes y se actualizan los parámetros del modelo utilizando el optimizador.\n",
    "    *   Se imprime la pérdida por lote a intervalos regulares y se muestra la pérdida promedio al final de cada época para monitorear el progreso del entrenamiento.\n",
    "\n",
    "#### 3. Generación de texto: Predicción con el modelo entrenado\n",
    "\n",
    "Esta es la etapa en la que se utiliza el modelo entrenado para generar nuevo texto (secuencias numéricas).\n",
    "\n",
    "*   **Función `generate_text`:**\n",
    "    *   Se configura el modelo en modo de evaluación (`model.eval()`).\n",
    "    *   Se convierte el texto inicial (`start_text`, por ejemplo: `['1', '2', '3']`) a IDs de tokens y se ingresan al modelo.\n",
    "    *   Se genera el siguiente token iterativamente hasta alcanzar `max_length`.\n",
    "        *   Se aplica una `temperature` a los logits de salida del modelo para ajustar la distribución de probabilidad. Un valor bajo de `temperature` produce texto más coherente, mientras que un valor alto produce texto más diverso.\n",
    "        *   Se muestrean los IDs de tokens del siguiente token de la distribución de probabilidad ajustada (usando la función `torch.multinomial`).\n",
    "        *   Se convierten los IDs de tokens muestreados nuevamente a texto y se agregan a la lista de tokens generados.\n",
    "        *   Se agrega el nuevo token generado a la entrada para predecir el siguiente token en un proceso iterativo.\n",
    "    *   Finalmente, se devuelve el texto generado.\n",
    "\n",
    "#### 4. Análisis de resultados: Evaluación del entrenamiento y texto generado\n",
    "\n",
    "Esta es la etapa en la que se analizan los resultados del entrenamiento del modelo y el texto generado.\n",
    "\n",
    "*   **Resultados del entrenamiento:** Se puede observar una disminución constante de la pérdida (`loss`) durante el proceso de entrenamiento, lo que indica que el modelo está aprendiendo con éxito los patrones de las secuencias numéricas.\n",
    "*   **Resultados generados:**\n",
    "    *   Resultado de la generación de texto comenzando con `['1', '2', '3']`: `1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20`\n",
    "    *   Resultado de la generación de texto comenzando con `['40', '41', '42']`: `40 41 42 43 44 45 46 47 48 49`\n",
    "Podemos verificar que el modelo genera con precisión los números consecutivos que siguen al número de inicio dado. Esto demuestra que el modelo ha aprendido el patrón de la secuencia numérica y puede generar nuevas secuencias basándose en ello.\n",
    "\n",
    "\n",
    "El ejemplo `train_seq_num.py` muestra cómo se puede realizar exitosamente una tarea de predicción de secuencias numéricas sencilla pero clara utilizando el modelo `simple_mistral`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data before training (input sequence -> label sequence):\n",
      "Sample 1: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] -> [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Sample 2: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] -> [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Start training...\n",
      "Batch 100/124, Loss: 0.0020\n",
      "Epoch 1/5, Average Loss: 2.2763\n",
      "Batch 100/124, Loss: 0.0027\n",
      "Epoch 2/5, Average Loss: 0.0024\n",
      "Batch 100/124, Loss: 0.0006\n",
      "Epoch 3/5, Average Loss: 0.0011\n",
      "Batch 100/124, Loss: 0.0008\n",
      "Epoch 4/5, Average Loss: 0.0007\n",
      "Batch 100/124, Loss: 0.0005\n",
      "Epoch 5/5, Average Loss: 0.0005\n",
      "Generating text starting with tokens ['1', '2', '3']:\n",
      "Generated text: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\n",
      "Generating text starting with tokens ['40', '41', '42']:\n",
      "Generated text: 40 41 42 43 44 45 46 47 48 49\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from dldna.chapter_09.mistral.examples.train_seq_num import MistralConfig, MistralForCausalLM, SimpleDataset, create_simple_data, generate_text, train\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Hyperparameter settings\n",
    "base_vocab_size = 50    # Original vocab_size before the EOS token\n",
    "seq_length = 10         # Sequence length of each training sample\n",
    "batch_size = 8\n",
    "epochs = 5\n",
    "learning_rate = 5e-3\n",
    "num_train_examples = 1000\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# 1) Create tokenizer (string token -> token id)\n",
    "tokenizer_vocab = {str(i): i for i in range(base_vocab_size)}\n",
    "tokenizer_vocab[\"<eos>\"] = base_vocab_size\n",
    "updated_vocab_size = base_vocab_size + 1\n",
    "\n",
    "# 2) Model configuration: Apply the updated vocab_size and set sliding_window to seq_length\n",
    "config = MistralConfig(\n",
    "    vocab_size=updated_vocab_size,\n",
    "    hidden_size=32,\n",
    "    intermediate_size=64,\n",
    "    num_hidden_layers=2,\n",
    "    num_attention_heads=4,\n",
    "    num_key_value_heads=2,\n",
    "    max_position_embeddings=128,\n",
    "    sliding_window=seq_length,  # Set to the same as the sequence length\n",
    "    use_cache=False  # Do not use cache during training\n",
    ")\n",
    "config.eos_token_id = tokenizer_vocab[\"<eos>\"]\n",
    "\n",
    "# (Optional) Set up weight tying between embedding and lm_head -> Can help reproduce sequential patterns.\n",
    "tie_weights = True\n",
    "\n",
    "# 3) Create model and Optimizer\n",
    "model = MistralForCausalLM(config).to(device)\n",
    "if tie_weights:\n",
    "    model.lm_head.weight = model.model.embed_tokens.weight\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 4) Data generation and DataLoader preparation\n",
    "train_data = create_simple_data(updated_vocab_size, num_train_examples, seq_length)\n",
    "train_dataset = SimpleDataset(train_data, seq_length)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# --- For debugging: Output some data before training ---\n",
    "print(\"Sample data before training (input sequence -> label sequence):\")\n",
    "for i in range(2):\n",
    "    input_seq, label_seq = train_dataset[i]\n",
    "    print(f\"Sample {i+1}: {input_seq.tolist()} -> {label_seq.tolist()}\")\n",
    "\n",
    "# 5) Start training\n",
    "print(\"Start training...\")\n",
    "train(model, train_dataloader, optimizer, epochs, device)\n",
    "\n",
    "# 6) Text generation example\n",
    "print(\"Generating text starting with tokens ['1', '2', '3']:\")\n",
    "start_text = [\"1\", \"2\", \"3\"]\n",
    "generated = generate_text(model, start_text, tokenizer_vocab, max_length=20, device=device)\n",
    "print(\"Generated text:\", \" \".join(generated))\n",
    "\n",
    "print(\"Generating text starting with tokens ['40', '41', '42']:\")\n",
    "start_text = [\"40\", \"41\", \"42\"]\n",
    "generated = generate_text(model, start_text, tokenizer_vocab, max_length=20, device=device)\n",
    "print(\"Generated text:\", \" \".join(generated))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.6.6 Ejemplo de predicción de operaciones aritméticas: Análisis de `train_math.py`\n",
    "\n",
    "`train_math.py` es un ejemplo que utiliza el modelo `simple_mistral` para predecir los resultados de simples operaciones aritméticas (suma, resta, multiplicación). A través de este ejemplo, se evalúa si el modelo puede comprender números y símbolos de operación, y realizar inferencias matemáticas básicas. Los ejemplos de datos de entrenamiento son los siguientes.\n",
    "\n",
    "```text\n",
    "Muestra 1: 4*1=4<eos>\n",
    "Muestra 2: 9+8=17<eos>\n",
    "```\n",
    "#### Generación y preprocesamiento de datos: la armonía entre símbolos y números\n",
    "\n",
    "El ejemplo `train_math.py` presenta algunas diferencias importantes en comparación con el ejemplo anterior de predicción de secuencias numéricas, tanto en generación de datos, tokenización, como en configuración del modelo. La diferencia más significativa es que los datos tratados no son solo una lista simple de números, sino \"expresiones\" compuestas por números, símbolos de operación, signo igual y el token `<eos>` que indica el final de la oración.\n",
    "\n",
    "*   **Función `create_arithmetic_data`: generación de datos aritméticos**\n",
    "    *   Esta función genera expresiones aritméticas y sus resultados en formato de cadena para una cantidad especificada (`num_samples`) de operaciones aritméticas.\n",
    "    *   Cada expresión sigue el formato `f\"{num1}{op}{num2}={result}<eos>\"` (por ejemplo: `\"12+7=19<eos>\"`)\n",
    "        *   `num1`, `num2`: enteros seleccionados aleatoriamente entre 1 y `max_value`.\n",
    "        *   `op`: símbolo de operación seleccionado aleatoriamente entre suma (`+`), resta (`-`) y multiplicación (`*`).\n",
    "        *   `result`: el valor resultante calculado utilizando la función `eval` de Python.\n",
    "        *   **Importancia del token `<eos>`:** Es crucial agregar explícitamente el token `<eos>` (End-of-Sentence) al final de cada cadena. Este token especial actúa como un hito que indica al modelo el final de una oración. Sin el token `<eos>`, el modelo podría tener dificultades para determinar cuándo detener la generación, lo que podría resultar en una salida infinita de números o símbolos.\n",
    "\n",
    "*   **Función `create_tokenizer`: definición del vocabulario**\n",
    "    *   Se genera un vocabulario que incluye números (0-9), símbolos de operación (`+`, `-`, `\\*`), signo igual (`=`) y tokens especiales (`<pad>`, `<eos>`). Este vocabulario define los caracteres básicos que el modelo puede entender.\n",
    "        *   El token `<pad>` se usa para agrupar secuencias de diferentes longitudes en un solo lote (batch).\n",
    "\n",
    "*   **Función `create_reverse_tokenizer`: restauración de tokens ID a caracteres**\n",
    "    *   Se crea un diccionario inverso para convertir IDs de tokens nuevamente en tokens de cadena. Esto se utiliza para interpretar los resultados generados en un formato legible por humanos.\n",
    "\n",
    "*   **Función `tokenize_sample`: conversión de cadenas a listas de tokens**\n",
    "    *   La función `tokenize_sample` convierte una cadena de muestra en una lista de tokens que el modelo puede reconocer.\n",
    "        - Los tokens especiales como `<eos>` se tratan como un solo token para asegurar que el modelo los reconozca completamente.\n",
    "\n",
    "* **Clase `ArithmeticDataset`: conversión a datos aprendibles**\n",
    "*  La función `create_arithmetic_data` convierte los datos generados en el formato de PyTorch `Dataset`. `Dataset` es un método estandarizado para suministrar datos al modelo de manera eficiente.\n",
    "*   El método `__getitem__` realiza las siguientes tareas:\n",
    "    1.  Utiliza la función `tokenize_sample` para tokenizar primero las cadenas de muestra.\n",
    "    2.  Si la longitud de la secuencia tokenizada es menor que el `seq_length` especificado, se ajusta la longitud utilizando tokens `<pad>`. Esto se hace para asegurar que todas las secuencias de entrada tengan la misma longitud, lo que permite al modelo procesar datos en lotes (batch).\n",
    "    3.  Convierte los tokens a IDs enteros y devuelve las secuencias de entrada y las secuencias de etiquetas (iguales a las de entrada) en forma de tensores PyTorch.\n",
    "\n",
    "#### Configuración del modelo y entrenamiento\n",
    "\n",
    "*   **Configuración `MistralConfig`:** Dado que esta es una tarea un poco más compleja que el ejemplo de predicción de secuencias numéricas, se ha aumentado ligeramente el tamaño del modelo (`hidden_size=64`, `intermediate_size=128`, `num_hidden_layers=3`, `num_attention_heads=8`, `num_key_value_heads=4`). Además, se han configurado `pad_token_id` y `eos_token_id` para que el modelo reconozca los tokens de relleno y fin de oración.\n",
    "*   **Entrenamiento:** Se utiliza la función `train` de manera similar al ejemplo anterior para entrenar el modelo. Un programador de tasas de aprendizaje `CosineAnnealingLR` se usa para reducir gradualmente la tasa de aprendizaje, permitiendo una rápida convergencia inicial y ajustes finos en las etapas posteriores.\n",
    "\n",
    "#### Generación de texto\n",
    "\n",
    "*   **Función `generate_text`:** Permite que el modelo genere texto (resultado de operaciones aritméticas) basado en un prompt dado (por ejemplo, \"12+7=\"). La generación de la cadena de resultados se detiene cuando el modelo genera tokens `<eos>` o `<pad>`.\n",
    "\n",
    "#### Análisis de los resultados\n",
    "\n",
    "*   **Resultados del entrenamiento:** Se puede ver que el modelo está aprendiendo patrones aritméticos a medida que la pérdida (loss) disminuye gradualmente durante el proceso de entrenamiento.\n",
    "*   **Resultados generados:** A través de ejemplos de datos de evaluación, se verifica si el modelo genera resultados correctos para los prompts dados. (por ejemplo, \"4+20=\" -> \"4+20=24\")\n",
    "\n",
    "El ejemplo `train_math.py` demuestra que el modelo `simple_mistral` puede aprender habilidades de razonamiento simbólico, como las operaciones aritméticas, más allá de la simple predicción de secuencias numéricas. Además, se puede apreciar la importancia y el papel de tokens especiales como `<eos>`, así como la necesidad de ajustar el tamaño del modelo según la complejidad de la tarea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data examples:\n",
      "Sample 1: 4*1=4<eos>\n",
      "Sample 2: 9+8=17<eos>\n",
      "Sample 3: 5*4=20<eos>\n",
      "Sample 4: 18*3=54<eos>\n",
      "Sample 5: 14+2=16<eos>\n",
      "Sample 6: 3+7=10<eos>\n",
      "Sample 7: 17+20=37<eos>\n",
      "Sample 8: 18*7=126<eos>\n",
      "Sample 9: 18+14=32<eos>\n",
      "Sample 10: 15-19=-4<eos>\n",
      "Start training...\n",
      "Epoch 1/20, Average Loss: 2.4820, LR: 0.000994\n",
      "Epoch 2/20, Average Loss: 1.2962, LR: 0.000976\n",
      "Epoch 3/20, Average Loss: 1.1905, LR: 0.000946\n",
      "Epoch 4/20, Average Loss: 1.0831, LR: 0.000905\n",
      "Epoch 5/20, Average Loss: 0.9902, LR: 0.000855\n",
      "Epoch 6/20, Average Loss: 0.9112, LR: 0.000796\n",
      "Epoch 7/20, Average Loss: 0.8649, LR: 0.000730\n",
      "Epoch 8/20, Average Loss: 0.8362, LR: 0.000658\n",
      "Epoch 9/20, Average Loss: 0.8194, LR: 0.000582\n",
      "Epoch 10/20, Average Loss: 0.8128, LR: 0.000505\n",
      "Epoch 11/20, Average Loss: 0.8049, LR: 0.000428\n",
      "Epoch 12/20, Average Loss: 0.7971, LR: 0.000352\n",
      "Epoch 13/20, Average Loss: 0.7945, LR: 0.000280\n",
      "Epoch 14/20, Average Loss: 0.7918, LR: 0.000214\n",
      "Epoch 15/20, Average Loss: 0.7903, LR: 0.000155\n",
      "Epoch 16/20, Average Loss: 0.7884, LR: 0.000105\n",
      "Epoch 17/20, Average Loss: 0.7864, LR: 0.000064\n",
      "Epoch 18/20, Average Loss: 0.7854, LR: 0.000034\n",
      "Epoch 19/20, Average Loss: 0.7837, LR: 0.000016\n",
      "Epoch 20/20, Average Loss: 0.7831, LR: 0.000010\n",
      "\n",
      "Evaluation data examples:\n",
      "Generated result for prompt '4+20=': 4+20=24 (Original data: 4+20=24<eos>)\n",
      "Generated result for prompt '16-3=': 16-3=13 (Original data: 16-3=13<eos>)\n",
      "Generated result for prompt '10+15=': 10+15=25 (Original data: 10+15=25<eos>)\n",
      "Generated result for prompt '8+4=': 8+4=12 (Original data: 8+4=12<eos>)\n",
      "Generated result for prompt '16-13=': 16-13=3 (Original data: 16-13=3<eos>)\n",
      "Generated result for prompt '10*1=': 10*1=10 (Original data: 10*1=10<eos>)\n",
      "Generated result for prompt '18+13=': 18+13=31 (Original data: 18+13=31<eos>)\n",
      "Generated result for prompt '9+9=': 9+9=18 (Original data: 9+9=18<eos>)\n",
      "Generated result for prompt '1+15=': 1+15=16 (Original data: 1+15=16<eos>)\n",
      "Generated result for prompt '18-18=': 18-18=0 (Original data: 18-18=0<eos>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "from dldna.chapter_09.mistral.examples.train_math import MistralConfig, MistralForCausalLM, generate_text, train,create_arithmetic_data, ArithmeticDataset, create_tokenizer, create_reverse_tokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Hyperparameter settings\n",
    "num_samples = 10000   # Total number of samples in the dataset\n",
    "max_value = 20       # Maximum value of operands\n",
    "seq_length = 20      # Fixed sequence length including EOS token (e.g., 20)\n",
    "batch_size = 16\n",
    "epochs = 20\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# Data generation (including EOS token) and output training data examples\n",
    "arithmetic_data = create_arithmetic_data(num_samples, max_value)\n",
    "print(\"Training data examples:\")\n",
    "for i in range(10):\n",
    "    print(f\"Sample {i+1}: {arithmetic_data[i]}\")\n",
    "\n",
    "# Create tokenizer\n",
    "tokenizer = create_tokenizer()\n",
    "reverse_tokenizer = create_reverse_tokenizer(tokenizer)\n",
    "updated_vocab_size = len(tokenizer)\n",
    "\n",
    "# Configure Dataset and DataLoader\n",
    "dataset = ArithmeticDataset(arithmetic_data, seq_length, tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "config = MistralConfig(\n",
    "    vocab_size=updated_vocab_size,\n",
    "    hidden_size=64,\n",
    "    intermediate_size=128,\n",
    "    num_hidden_layers=3,\n",
    "    num_attention_heads=8,\n",
    "    num_key_value_heads=4,\n",
    "    max_position_embeddings=128,\n",
    "    sliding_window=seq_length,\n",
    "    use_cache=False,\n",
    "    use_return_dict=True,\n",
    "    pad_token_id=tokenizer[\"<pad>\"]  # Set the pad token id here.\n",
    ")\n",
    "config.eos_token_id = tokenizer[\"<eos>\"]  # Also update the eos token\n",
    "\n",
    "model = MistralForCausalLM(config).to(device)\n",
    "\n",
    "# weight tying (share weights between embedding and lm_head)\n",
    "tie_weights = True\n",
    "if tie_weights:\n",
    "    model.lm_head.weight = model.model.embed_tokens.weight\n",
    "\n",
    "# Create optimizer and add cosine annealing scheduler\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=1e-5)\n",
    "\n",
    "# Start training\n",
    "print(\"Start training...\")\n",
    "train(model, dataloader, optimizer, scheduler, epochs, device)\n",
    "\n",
    "# Evaluation: Output 10 random evaluation samples (terminate generation if EOS is included in the prompt)\n",
    "print(\"\\nEvaluation data examples:\")\n",
    "for i in range(10):\n",
    "    sample = random.choice(arithmetic_data)\n",
    "    # Use the part before '=' as a prompt in the entire expression, e.g., \"12+7=19<eos>\" (\"12+7=\")\n",
    "    prompt = sample.split('=')[0] + '='\n",
    "    generated = generate_text(model, prompt, tokenizer, reverse_tokenizer, max_length=seq_length, device=device)\n",
    "    print(f\"Generated result for prompt '{prompt}': {generated} (Original data: {sample})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.6.7 Ejemplo de generación de consultas SQL a partir del lenguaje natural: Análisis de `train_sql.py`\n",
    "\n",
    "`train_sql.py` utiliza el modelo `simple_mistral` para abordar una tarea de procesamiento de lenguaje natural más compleja, que consiste en convertir preguntas en lenguaje natural en consultas SQL. En este ejemplo, se examina cómo el modelo aprende a comprender el significado de oraciones complejas en lenguaje natural y a expresarlas en la estructurada lengua de consulta SQL, que va más allá de la generación simple de secuencias. El conjunto de entrenamiento está formado por ejemplos donde se proporciona una oración y se devuelve esta en forma de sentencia SQL. A continuación se presentan algunos ejemplos de datos de entrenamiento.\n",
    "\n",
    "```text\n",
    "Muestra 1: Dime cuáles son las notas para Australia del Sur sep> SELECT Notes FROM table WHERE Current slogan = SOUTH AUSTRALIA eos>\n",
    "Muestra 2: ¿Cuál es el formato para Australia del Sur? sep> SELECT Format FROM table WHERE State/territory = South Australia eos>\n",
    "```\n",
    "\n",
    "#### Conjunto de datos y preprocesamiento: Armonía entre WikiSQL y tokens especiales\n",
    "\n",
    "La clave en el ejemplo de `train_sql.py` es utilizar eficazmente el conjunto de datos WikiSQL y preparar los datos para que el modelo pueda aprender la relación entre el lenguaje natural y las consultas SQL.\n",
    "\n",
    "*   **Carga del conjunto de datos WikiSQL:** Se utiliza la biblioteca `datasets` para cargar el conjunto de datos WikiSQL. WikiSQL es un conjunto de datos compuesto por preguntas en lenguaje natural y sus correspondientes consultas SQL, ampliamente utilizado para tareas de conversión de lenguaje natural a SQL. Con la función `load_dataset`, se pueden especificar conjuntos de datos de entrenamiento (`train`) y validación (`validation`) utilizando el parámetro `split`.\n",
    "\n",
    "*   **Clase `WikiSQLDataset`:** Se hereda de la clase `Dataset` de PyTorch para procesar el conjunto de datos WikiSQL en un formato adecuado para el entrenamiento del modelo.\n",
    "    *   En el método `__init__`, se carga el conjunto de datos WikiSQL y se configura el tokenizador (`tokenizer`) y la longitud máxima de secuencia (`max_length`).\n",
    "    *   El método `__getitem__` procesa los ejemplos de datos para convertirlos en un formato apto para ser ingresado al modelo. La parte más importante de este proceso es combinar las preguntas en lenguaje natural con las consultas SQL y agregar tokens especiales.\n",
    "        1.  Primero, se obtienen la pregunta en lenguaje natural (`question`) y la consulta SQL escrita por humanos (`sql['human_readable']`) del ejemplo de datos.\n",
    "        2.  Se combinan la pregunta y la consulta SQL en el formato `\"pregunta <sep> SQL<eos>\"`. Aquí, `<sep>` es un token separador que distingue entre la pregunta y la consulta SQL, y `<eos>` es un token final de oración que indica el final de la secuencia. Estos tokens especiales juegan un papel crucial al proporcionar estructura al texto de entrada del modelo.\n",
    "        3.  Se utiliza el `tokenizer` para tokenizar el texto combinado. En este proceso, se establece `truncation=True` para cortar el texto si excede la longitud máxima (`max_length`), y `padding=\"max_length\"` para agregar relleno y asegurar que la secuencia tenga una longitud de `max_length`.\n",
    "        4.  Finalmente, se devuelve `input_ids` tokenizado. (las entradas y las etiquetas son idénticas)\n",
    "*   **Tokenizador (T5Tokenizer):** Utiliza `T5Tokenizer` de la biblioteca `transformers`. Las razones para elegir `T5Tokenizer` son las siguientes.\n",
    "    *   Soporta varios tokens especiales (`<pad>`, `<eos>`, `<sep>`, etc.) por defecto.\n",
    "    *   Es un tokenizador versátil que puede manejar tanto lenguaje natural como consultas SQL (código) de manera efectiva.\n",
    "    *   Se puede obtener fácilmente el tamaño del vocabulario del tokenizador a través de `tokenizer.vocab_size`, lo cual es conveniente para establecer el `vocab_size` del modelo.\n",
    "\n",
    "*   **Cargador de datos (`DataLoader`):** Desempeña el papel de agrupar los conjuntos de datos generados por `WikiSQLDataset` en lotes miniatura y suministrarlos al modelo de manera eficiente. `batch_size` es el número de muestras que se ingresan al modelo a la vez, y `shuffle=True` mezcla los datos antes de cada época para mejorar el rendimiento del entrenamiento.\n",
    "\n",
    "#### Configuración del modelo y entrenamiento\n",
    "\n",
    "*   **Configuración de `MistralConfig`:** Establece los hiperparámetros relacionados con la estructura del modelo. En particular, configura `pad_token_id`, `bos_token_id` y `eos_token_id` a las ID de tokens correspondientes en `tokenizer` para que el modelo procese correctamente los tokens de padding, inicio de oración y fin de oración.\n",
    "\n",
    "*   **Creación del modelo (`MistralForCausalLM`) y optimizador (`AdamW`):** Crea un modelo `MistralForCausalLM` y lo mueve al dispositivo especificado (CPU o GPU). Utiliza el optimizador `AdamW` y el programador de tasas de aprendizaje `get_cosine_schedule_with_warmup` para controlar la tasa de aprendizaje y optimizar el modelo.\n",
    "\n",
    "*   **Función `train`:** Al igual que las funciones utilizadas en `train_seq_num.py` y `train_math.py`, utiliza un bucle de entrenamiento estándar para entrenar el modelo.\n",
    "\n",
    "#### Generación de texto (`generate_sql`): Inferencia de consultas SQL a partir de preguntas\n",
    "\n",
    "*   **Función `generate_sql`:** Utiliza el modelo entrenado para generar una consulta SQL a partir de una pregunta en lenguaje natural.\n",
    "    *   Primero, agrega un token `<sep>` a la pregunta ingresada para formar un prompt en el formato `\"pregunta <sep> \"`. Este prompt informa claramente al modelo que la pregunta ha terminado y debe generar una consulta SQL.\n",
    "    *   **Importancia del procesamiento de tokens de padding:** Los datos de entrenamiento se completan hasta la longitud máxima (`max_length`) con un token `<eos>`. Sin embargo, si los datos de entrenamiento solo contienen `\"pregunta <sep>\"` sin la parte SQL ni el token `<eos>` (es decir, en el formato `\"pregunta <sep> <pad> <pad> ...\"`), el modelo no aprenderá qué generar después del token `<sep>`. Como resultado, durante la etapa de generación, el modelo podría generar solo tokens de padding después de `<sep>`, o incluso una cadena vacía. Para evitar esto, los datos de entrenamiento deben estar en el formato `\"pregunta <sep> SQL<eos>\"`.\n",
    "    *   Ajusta la diversidad de las consultas SQL generadas utilizando el parámetro `temperature`.\n",
    "    *   El modelo detiene la generación de la consulta cuando genera un token `<eos>` o `<pad>`.\n",
    "\n",
    "#### Análisis de resultados\n",
    "\n",
    "*   **Salida de muestra:** Antes del entrenamiento, se muestran 3 muestras del conjunto de datos WikiSQL para verificar el formato de los datos.\n",
    "*   **Resultado del entrenamiento:** Se puede confirmar que el modelo está aprendiendo a convertir preguntas en lenguaje natural en consultas SQL observando la disminución de la pérdida (loss) durante el proceso de entrenamiento.\n",
    "*   **Resultado de generación:** Se evalúa las consultas SQL generadas al ingresar preguntas del conjunto de datos de validación. Se examina principalmente si las consultas SQL generadas son gramaticalmente correctas y reflejan con precisión el significado de las preguntas.\n",
    "`train_sql.py` es un ejemplo que muestra cómo utilizar el modelo `simple_mistral` para realizar la tarea de procesamiento de lenguaje natural más compleja de conversión de lenguaje natural a SQL. Este ejemplo enfatiza la importancia de utilizar adecuadamente tokens especiales (`<sep>`, `<eos>`, `<pad>`) en el proceso de preprocesamiento de datos, y cómo la composición de los datos de entrenamiento afecta las capacidades de generación del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== WikiSQL Data Sample Output ===\n",
      "Sample 1: Tell me what the notes are for South Australia sep> SELECT Notes FROM table WHERE Current slogan = SOUTH AUSTRALIA eos>\n",
      "Sample 2: What is the current series where the new series began in June 2011? sep> SELECT Current series FROM table WHERE Notes = New series began in June 2011 eos>\n",
      "Sample 3: What is the format for South Australia? sep> SELECT Format FROM table WHERE State/territory = South Australia eos>\n",
      "Start training...\n",
      "Epoch 1/8, Average Loss: 10.5748, LR: 0.000000\n",
      "Epoch 2/8, Average Loss: 9.7000, LR: 0.000001\n",
      "Epoch 3/8, Average Loss: 7.2037, LR: 0.000001\n",
      "Epoch 4/8, Average Loss: 5.5372, LR: 0.000001\n",
      "Epoch 5/8, Average Loss: 4.5961, LR: 0.000001\n",
      "Epoch 6/8, Average Loss: 4.0102, LR: 0.000002\n",
      "Epoch 7/8, Average Loss: 3.6296, LR: 0.000002\n",
      "Epoch 8/8, Average Loss: 3.3907, LR: 0.000002\n",
      "\n",
      "=== Evaluation Examples ===\n",
      "Question: Who was the minister for the CSV party with a present day end date? <unk>\n",
      "Target SQL: SELECT Minister FROM table WHERE Party = csv AND End date = present day <unk> <eos></s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "Generated SQL: Who was the minister for the CSV party with a present day end date? sep> FROM table WHERE60ed = s eos>\n",
      "\n",
      "Question: What is the production number of From Hare to Heir? <unk>\n",
      "Target SQL: SELECT SUM Production Number FROM table WHERE Title = from hare to heir <unk> <eos></s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "Generated SQL: What is the production number of From Hare to Heir? sep>os FROM table WHERE Score = 0 eos>\n",
      "\n",
      "Question: What was the score on January 12? <unk>\n",
      "Target SQL: SELECT Score FROM table WHERE Date = january 12 <unk> <eos></s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "Generated SQL: What was the score on January 12? sep>a Record FROM table WHERE #  eos>\n",
      "\n",
      "Question: The race tony bettenhausen 200 has what smallest rd? <unk>\n",
      "Target SQL: SELECT MIN Rd FROM table WHERE Name = Tony Bettenhausen 200 <unk> <eos></s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "Generated SQL: The race tony bettenhausen 200 has what smallest rd? sep> Team FROM table WHERE Player = a ODi a eos>\n",
      "\n",
      "Question: what is the club that was founded before 2007, joined prsl in 2008 and the stadium is yldefonso solá morales stadium? <unk>\n",
      "Target SQL: SELECT Club FROM table WHERE Founded <unk> 2007 AND Joined PRSL = 2008 AND Stadium = yldefonso solá morales stadium <unk> <eos></s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "Generated SQL: what is the club that was founded before 2007, joined prsl in 2008 and the stadium is yldefonso solá morales stadium? sep> ( for  for the highest FROM table WHERE Team = Rank  of vir AND COUNT  eos>\n",
      "\n",
      "Question: Who is the co-contestant (yaar vs. Pyaar) with Vishal Singh as the main contestant? <unk>\n",
      "Target SQL: SELECT Co-contestant (Yaar vs. Pyaar) FROM table WHERE Main contestant = vishal singh <unk> <eos></s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "Generated SQL: Who is the co-contestant (yaar vs. Pyaar) with Vishal Singh as the main contestant? sep> SELECT  Record FROM table WHERE ts = 9kt AND Date = a eos>\n",
      "\n",
      "Question: What season did SV Darmstadt 98 end up at RL Süd (1st)? <unk>\n",
      "Target SQL: SELECT Season FROM table WHERE RL Süd (1st) = SV Darmstadt 98 <unk> <eos></s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "Generated SQL: What season did SV Darmstadt 98 end up at RL Süd (1st)? sep> FROM table WHERE Away team = s s eos>\n",
      "\n",
      "Question: What character was portrayed by the same actor for 12 years on Neighbours? <unk>\n",
      "Target SQL: SELECT Character FROM table WHERE Duration = 12 years AND Soap Opera = neighbours <unk> <eos></s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "Generated SQL: What character was portrayed by the same actor for 12 years on Neighbours? sep>FS Class FROM table WHERE Date = m ja eos>\n",
      "\n",
      "Question: What was the score between Marseille and Manchester United on the second leg of the Champions League Round of 16? <unk>\n",
      "Target SQL: SELECT 2nd leg score** FROM table WHERE Opponent = Marseille <unk> <eos></s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "Generated SQL: What was the score between Marseille and Manchester United on the second leg of the Champions League Round of 16? sep>hes> d FROM table WHERE Date =s eos>\n",
      "\n",
      "Question: Who was the Man of the Match when the opponent was Milton Keynes Lightning and the venue was Away? <unk>\n",
      "Target SQL: SELECT Man of the Match FROM table WHERE Opponent = milton keynes lightning AND Venue = away <unk> <eos></s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "Generated SQL: Who was the Man of the Match when the opponent was Milton Keynes Lightning and the venue was Away? sep> with Cap? sep> SELECT Home team score FROM table WHERE Wilson AND jump = s eos>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "from transformers import T5Tokenizer, get_cosine_schedule_with_warmup\n",
    "from dldna.chapter_09.mistral.examples.train_sql import MistralConfig, MistralForCausalLM, WikiSQLDataset, generate_sql\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Use T5Tokenizer as the tokenizer (use T5's vocab_size and pad/eos tokens)\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "\n",
    "# WikiSQL dataset (training: train, evaluation: validation)\n",
    "max_length = 128\n",
    "train_dataset = WikiSQLDataset(\"train\", tokenizer, max_length=max_length)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
    "\n",
    "valid_dataset = WikiSQLDataset(\"validation\", tokenizer, max_length=max_length)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# Model configuration: Use MistralConfig and MistralForCausalLM provided by simple_mistral.py\n",
    "# The model size is adjusted for educational purposes.\n",
    "config = MistralConfig(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    hidden_size=512,\n",
    "    intermediate_size=2048,\n",
    "    num_hidden_layers=4,\n",
    "    num_attention_heads=8,\n",
    "    num_key_value_heads=4,     # num_attention_heads % num_key_value_heads == 0 must be true\n",
    "    max_position_embeddings=max_length,\n",
    "    sliding_window=max_length,\n",
    "    use_cache=False,\n",
    "    use_return_dict=True,\n",
    "    pad_token_id=tokenizer.pad_token_id,  # Set the pad token id.\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "model = MistralForCausalLM(config).to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "num_epochs = 8  # Set the number of epochs small for the example\n",
    "total_training_steps = num_epochs * len(train_loader)\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=len(train_loader) // 5,\n",
    "    num_training_steps=total_training_steps\n",
    ")\n",
    "    # Added code: Output WikiSQL data samples\n",
    "print(\"=== WikiSQL Data Sample Output ===\")\n",
    "sample_count = 3  # Number of examples to output\n",
    "for i in range(sample_count):\n",
    "    input_ids, labels = train_dataset[i]\n",
    "    decoded_text = tokenizer.decode(input_ids, skip_special_tokens=True)\n",
    "    print(f\"Sample {i+1}: {decoded_text}\")\n",
    "\n",
    "\n",
    "print(\"Start training...\")\n",
    "train(model, train_loader, optimizer, scheduler, num_epochs, device)\n",
    "\n",
    "# Save the model: Save the final model to a file.\n",
    "torch.save(model.state_dict(), \"final_nl2sql_model.pth\")\n",
    "\n",
    "# Evaluation code part\n",
    "print(\"\\n=== Evaluation Examples ===\")\n",
    "for i, (input_ids, labels) in enumerate(valid_loader):\n",
    "    if i >= 10:\n",
    "        break\n",
    "    # Keep special tokens with skip_special_tokens=False.\n",
    "    full_text = tokenizer.decode(input_ids[0], skip_special_tokens=False)\n",
    "    # Unify the tokens \"sep>\" and \"eos>\" to \"<sep>\" and \"<eos>\" respectively.\n",
    "    full_text = full_text.replace(\"sep>\", \"<sep>\").replace(\"eos>\", \"<eos>\")\n",
    "    \n",
    "    if \"<sep>\" in full_text:\n",
    "        # Split based on the first <sep>, then join all subsequent parts to restore the complete SQL.\n",
    "        parts = full_text.split(\"<sep>\")\n",
    "        question = parts[0].strip()\n",
    "        target_sql = \"<sep>\".join(parts[1:]).strip()\n",
    "        # If target_sql ends with \"<eos>\", remove it.\n",
    "        if target_sql.endswith(\"<eos>\"):\n",
    "            target_sql = target_sql[:-len(\"<eos>\")].strip()\n",
    "    else:\n",
    "        question = full_text.strip()\n",
    "        target_sql = \"\"\n",
    "\n",
    "    generated_sql = generate_sql(model, tokenizer, question, max_length, device, temperature=0.7)\n",
    "    # If there is a \"sep>\" token in generated_sql, extract the part after that token to use.\n",
    "    # if \"sep>\" in generated_sql:\n",
    "    #     generated_sql = generated_sql.split(\"sep>\", 1)[1].strip()\n",
    "\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Target SQL: {target_sql}\")\n",
    "    print(f\"Generated SQL: {generated_sql}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.callout-note collapse=\"true\" title=\"Haga clic para ver el contenido (buceo profundo: diseño y depuración sólida de transformadores - guía práctica)\"}\n",
    "## Diseño y depuración sólida de transformers - guía práctica\n",
    "\n",
    "Incluir arquitecturas eficientes como `simple_mistral` hace que construir modelos de transformers desde cero sea una tarea difícil pero gratificante. Aunque la comprensión teórica es importante, en el proceso de implementación real a menudo surgen errores sutiles y cuellos de botella de rendimiento. En esta sección, examinamos profundamente estrategias prácticas para diseñar, implementar y depurar transformers, centrándonos especialmente en los componentes utilizados en `simple_mistral` (RoPE, RMSNorm, Attention). Tratamos extensivamente las pruebas unitarias y discutimos otras técnicas de depuración y diseño esenciales.\n",
    "\n",
    "### 1. El papel crucial de las pruebas unitarias\n",
    "\n",
    "Cuando se construyen modelos complejos como transformers, las pruebas unitarias no son una *opción* sino una *necesidad*. Ayudan a detectar errores temprano, prevenir regresiones y brindar confianza en la implementación. Un modelo bien probado es un modelo *confiable*.\n",
    "\n",
    "Cada fuente de modelos tiene **un directorio llamado tests con pruebas unitarias (por ejemplo: mistral/tests, phi3/tests)**\n",
    "\n",
    "**Por qué las pruebas unitarias son importantes para los transformers**\n",
    "\n",
    "*   **Complejidad:** Los transformers están compuestos por varios módulos interactivos (Attention, Redes Feedforward, Normalización, Embedding). Los errores pueden surgir fácilmente en cualquiera de estos componentes.\n",
    "*   **Errores sutiles:** Muchos errores de transformers no son *inmediatamente evidentes*. En lugar de causar un fallo, pueden resultar en una disminución del rendimiento o salidas incorrectas. Las pruebas unitarias pueden detectar estos errores sutiles.\n",
    "*   **Estabilidad numérica:** Los modelos de aprendizaje profundo, especialmente cuando se utilizan técnicas como la precisión mixta, son vulnerables a problemas numéricos (NaN, Inf, Gradientes desaparecidos/explotados). Las pruebas unitarias ayudan a detectar estos problemas.\n",
    "*   **Refactorización y modificaciones:** Es inevitable que el código cambie al mejorar y optimizar el modelo. Las pruebas unitarias aseguran que las funciones existentes no se dañen debido a los cambios.\n",
    "*   **Reproducibilidad:** Pruebas bien definidas contribuyen a la reproducibilidad de los resultados.\n",
    "*   **Caché (`past_key_value`):** Cuando el modelo utiliza cachés como `past_key_values`, es importante verificar mediante pruebas unitarias que no haya errores relacionados con shape, dtype o device.\n",
    "\n",
    "**Principios clave para pruebas unitarias efectivas**\n",
    "\n",
    "*   **Desarrollo guiado por pruebas (Test-Driven Development, TDD):** Idealmente, se deben escribir las pruebas unitarias *antes* de codificar el modelo. Esto permite pensar claramente en el comportamiento esperado de cada componente.\n",
    "*   **Modularidad:** Diseña el código de manera modular con funciones y clases pequeñas y bien definidas. Esto facilita aislar y probar componentes individuales.\n",
    "*   **Cobertura exhaustiva:** Apunta a una alta cobertura de pruebas. Prueba todas las funciones y métodos importantes de la clase.\n",
    "*   **Casos límite:** No solo pruebes los \"casos normales\". Prueba también casos límite, condiciones frontera y escenarios potencialmente erróneos (por ejemplo: secuencias de longitud 0, lotes con un solo elemento, varios tipos de datos).\n",
    "*   **Assertions:** Usa libremente las assertions (`assert`) para verificar que el código funcione como se espera. Escribe las assertions lo más específicas posible. No solo asegúrate de que el código se ejecute sin fallos, sino también de que la *salida* sea correcta.\n",
    "*   **Pytest:** Aunque los ejemplos de este capítulo utilizan el módulo `unittest`, el marco `pytest` es el más recomendado en Python.\n",
    "\n",
    "**Áreas de enfoque para pruebas unitarias de transformers**\n",
    "*   **Forma de entrada/salida:** El tipo más común de error en la implementación del transformer es un tensor con una forma incorrecta. Cada prueba debe incluir una afirmación para verificar la forma del tensor de salida.\n",
    "*   **Tipo de datos:** Verifique que los tensores tengan el tipo de datos esperado (por ejemplo, `torch.float32`, `torch.float16`, `torch.int64`).\n",
    "*   **Colocación de dispositivos:** Si está utilizando GPU, verifique que los tensores estén en el dispositivo correcto (CPU o GPU).\n",
    "*   **Estabilidad numérica:** Asegúrese de que no haya NaN (Not a Number) ni Inf en los tensores después de operaciones como softmax o normalización.\n",
    "*   **Cálculo de gradientes:** Verifique que los gradientes se calculen correctamente para todos los parámetros entrenables.\n",
    "*   **Caché (`past_key_value`):** Como se mencionó anteriormente, el mecanismo de caché es una causa frecuente de errores. Pruebe exhaustivamente la decodificación incremental (incremental decoding).\n",
    "\n",
    "**Ejemplos detallados de pruebas unitarias (RoPE, RMSNorm, Attention)**\n",
    "\n",
    "```python\n",
    "# test_rope.py\n",
    "import unittest\n",
    "import torch\n",
    "from dldna.chapter_09.mistral.simple_mistral import MistralRotaryEmbedding, apply_rotary_pos_emb, rotate_half\n",
    "\n",
    "# ...\n",
    "```\n",
    "\n",
    "```python\n",
    "# test_rms_norm.py\n",
    "import torch\n",
    "import pytest\n",
    "from dldna.chapter_09.mistral.simple_mistral import PhiMiniRMSNorm\n",
    "\n",
    "# ... \n",
    "\n",
    "```\n",
    "\n",
    "```python\n",
    "# test_attention.py\n",
    "import torch\n",
    "import pytest\n",
    "from dldna.chapter_09.mistral.simple_mistral import PhiMiniConfig, PhiMiniAttention\n",
    "\n",
    "# ... \n",
    "\n",
    "# Pruebas adicionales para la atención\n",
    "\n",
    "def test_phi_mini_attention_zero_length_initial():\n",
    "    # ... \n",
    "\n",
    "def test_phi_mini_attention_single_token_initial():\n",
    "    # ... \n",
    "@pytest.mark.parametrize(\"batch_size\", [1, 2, 4, 8])\n",
    "def test_phi_mini_attention_various_batch_sizes(batch_size):\n",
    "    # ...\n",
    "\n",
    "@pytest.mark.parametrize(\"num_heads, num_kv_heads\", [(8, 8), (8, 4), (8, 1)]) # Casos MHA, GQA\n",
    "def test_phi_mini_attention_different_head_configs(num_heads, num_kv_heads):\n",
    "    # ... \n",
    "\n",
    "@pytest.mark.parametrize(\"dtype\", [torch.float16, torch.bfloat16, torch.float32])\n",
    "def test_phi_mini_attention_mixed_precision(dtype):\n",
    "    # ... \n",
    "\n",
    "def test_phi_mini_attention_combined_mask():\n",
    "    # ... \n",
    "\n",
    "def test_phi_mini_attention_long_sequence():\n",
    "    # ... \n",
    "\n",
    "def test_phi_mini_attention_output_attentions_with_cache():\n",
    "    # ... \n",
    "```\n",
    "\n",
    "### 2. Más allá de las pruebas unitarias: otras estrategias de depuración\n",
    "\n",
    "Aunque las pruebas unitarias son fundamentales, no son la única herramienta en el conjunto de herramientas de depuración. Las siguientes son otras estrategias importantes.\n",
    "\n",
    "**1. Registro (Logging)**\n",
    "*   **Registro estratégico:** Agregar declaraciones de registro (sentencias `print` o, preferiblemente, el módulo `logging`) al código para rastrear los valores de las variables principales, la forma del tensor y el flujo de ejecución. Esto puede ayudar a identificar rápidamente dónde ocurre un problema.\n",
    "*   **Control de nivel de detalle:** Hacer que el registro sea detallado pero proporcionar una manera de controlar el nivel de detalle (por ejemplo, usando banderas de línea de comandos o variables de entorno). Esto permite obtener información detallada durante la depuración pero evitar demasiada salida cuando se está funcionando normalmente.\n",
    "\n",
    "**2. Visualización (Visualization)**\n",
    "\n",
    "*   **Pesos de atención:** Visualizar los pesos de atención para verificar en qué tokens el modelo está prestando atención. Esto puede ayudar a detectar problemas con el mecanismo de atención o las embeddings de posición.\n",
    "*   **Activaciones:** Visualizar las activaciones de las neuronas en el modelo. Esto puede ayudar a identificar neuronas muertas (neuronas que siempre están inactivas) o saturadas (neuronas que siempre están al máximo o mínimo).\n",
    "*   **Gradientes:** Visualizar los gradientes durante el entrenamiento. Esto puede ayudar a detectar problemas de desaparición o explosión de gradientes.\n",
    "\n",
    "**3. Depuración numérica (Numerical Debugging)**\n",
    "\n",
    "*   **Verificación de NaN/Inf:** Usar `torch.isnan()` y `torch.isinf()` para verificar si un tensor contiene valores NaN o Inf. Esto a menudo indica inestabilidad numérica.\n",
    "    ```python\n",
    "    if torch.isnan(tensor).any() or torch.isinf(tensor).any():\n",
    "        print(\"¡NaN o Inf detectados!\")\n",
    "    ```\n",
    "*   **Verificación de gradientes:** Usar `torch.autograd.gradcheck` para verificar si una función autograd personalizada está calculando los gradientes correctamente. Esto es particularmente importante al implementar mecanismos de atención personalizados u otras tareas complejas.\n",
    "*   **Casos de prueba pequeños:** Crear casos de prueba muy pequeños y simples (por ejemplo, una sola capa, un vocabulario pequeño, secuencias cortas) para los cuales se pueda calcular manualmente la salida esperada. Esto puede ayudar a aislar bugs.\n",
    "\n",
    "**4. Depuradores (pdb, depuradores de IDE)**\n",
    "\n",
    "*   **`pdb` (Python Debugger):** Usar el depurador integrado de Python (`pdb`) para ejecutar el código línea por línea, examinar variables y establecer puntos de interrupción.\n",
    "    ```python\n",
    "    import pdb; pdb.set_trace()  # Agregar esta línea para establecer un punto de interrupción.\n",
    "    ```\n",
    "*   **Depuradores de IDE:** La mayoría de los IDE (como PyCharm, VS Code, etc.) tienen depuradores integrados con interfaces más amigables para la depuración.\n",
    "\n",
    "**5. Perfilado (Profiling)**\n",
    "\n",
    "*   **PyTorch Profiler:** Usar el perfilador de PyTorch para identificar cuellos de botella de rendimiento en el código. Esto puede ayudar a encontrar áreas para optimizar la velocidad o el uso de memoria.\n",
    "*   **Perfilado de memoria:** Usar herramientas como `memory_profiler` para rastrear el uso de memoria y detectar posibles fugas de memoria.\n",
    "\n",
    "**6. Principios de diseño del modelo para facilitar la depuración**\n",
    "*   **Manténgalo simple (Keep it Simple):** Comience con un modelo simple y vaya aumentando gradualmente la complejidad. De esta manera, será más fácil aislar los errores.\n",
    "*   **Modularidad (Modularity):** Divida el código en módulos pequeños y bien definidos. Esto facilita la prueba y depuración de componentes individuales.\n",
    "*   **Assertions:** Use assertions para verificar las condiciones esperadas y detectar errores temprano.\n",
    "*   **Comentarios y documentación (Comments and Documentation):** Escribe comentarios claros y concisos, así como documentación, para explicar la lógica del código. Esto ayuda a los usuarios (y a otras personas) a comprender el código e identificar posibles problemas.\n",
    "*   **Reproducibilidad (Reproducibility):** Use una seed de random fija para que los resultados sean reproducibles. Esto es importante tanto para la depuración como para comparar diferentes configuraciones de modelos.\n",
    "*   **Sobreajuste a un solo lote/pequeño conjunto de datos (Overfitting):** Ajuste el modelo a un pequeño conjunto de datos antes de entrenarlo con un conjunto de datos grande.\n",
    "\n",
    "**7. Errores comunes y formas de evitarlos**\n",
    "\n",
    "*   **Incorrect Tensor Shapes:** Verifique nuevamente la forma esperada de los tensores, especialmente después de operaciones como reshape, transpose, concatenate. Utilice `tensor.shape` frecuentemente en el proceso de depuración.\n",
    "*   **Errores Off-by-One:** Preste atención a la indexación, especialmente al trabajar con secuencias y embeddings de posición.\n",
    "*   **Incompatibilidades de tipos de datos (Data Type Mismatches):** Asegúrese de que los tensores tengan el tipo de datos correcto (por ejemplo, `float32` vs `float16`).\n",
    "*   **Incompatibilidades de dispositivos (Device Mismatches):** Verifique que todos los tensores estén en el mismo dispositivo (CPU o GPU).\n",
    "*   **Variables no inicializadas (Uninitialized Variables):** Inicialice todas las variables antes de usarlas.\n",
    "*   **Máscaras incorrectas (Incorrect Masking):** Si está usando una máscara de atención, asegúrese de que se aplique correctamente y no esté ocultando información importante.\n",
    "*   **Uso incorrecto de `past_key_values`:** Asegúrese de seguir el método correcto para su uso.\n",
    "\n",
    "Combinar estas técnicas de depuración con un entendimiento sólido de los principios básicos de los modelos de transformers permitirá resolver incluso los problemas más difíciles de implementación. La depuración es un proceso iterativo, por lo que tenga paciencia y use todas las herramientas de manera sistemática.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.7 Gemma: Revisión del último modelo de código abierto\n",
    "\n",
    "Gemma es el más reciente modelo de código abierto publicado por Google en febrero de 2024. Aunque no introduce cambios revolucionarios en la estructura del modelo en comparación con Mistral, refleja las tendencias actuales de los modelos y tiene valor para ser estudiado debido a su utilidad en ciertos contextos. Gemma adopta una arquitectura de decodificador único (Decoder-only) basada en Transformer, similar a LLaMA y Mistral.\n",
    "\n",
    "#### Razones para revisar Gemma\n",
    "\n",
    "1.  **Reflejo de las tendencias actuales en modelos:** Gemma incluye componentes comúnmente utilizados en los últimos modelos, como RoPE (Rotary Positional Embedding), RMSNorm (Root Mean Square Layer Normalization) y la función de activación GeGLU. Estos elementos contribuyen al rendimiento y a la eficiencia del modelo, y ayudan a comprender las tendencias actuales. RoPE codifica de manera eficiente información de posición relativa para mejorar la capacidad de procesar secuencias largas, mientras que RMSNorm aumenta la eficiencia computacional eliminando la operación de centrado en Layer Normalization. GeGLU es una variante de GLU (Gated Linear Unit) que aumenta la capacidad expresiva del modelo a través de la no linealidad.\n",
    "\n",
    "2.  **Diversas tamaños de modelos:** Gemma está disponible en tamaños de 2B, 7B, 9B y 27B. Esto permite a los usuarios con recursos computacionales limitados experimentar con un modelo relativamente pequeño (2B), mientras que los modelos más grandes (27B) ofrecen un rendimiento superior pero requieren mayores recursos computacionales. Los usuarios pueden elegir el tamaño de modelo adecuado según su entorno y necesidades.\n",
    "\n",
    "3.  **Integración con el ecosistema de Google:** Gemma está asociada al proyecto Gemini de Google y puede integrarse fácilmente con Google Cloud, Vertex AI, entre otros. Para los desarrolladores que utilizan principalmente la plataforma de Google, Gemma puede ser una opción útil. Vertex AI de Google Cloud proporciona una plataforma integral para el entrenamiento, despliegue y gestión de modelos de aprendizaje automático, y Gemma puede aumentar la productividad del desarrollo a través de su compatibilidad con estas plataformas.\n",
    "\n",
    "4. **Accesibilidad de los modelos de código abierto:** Gemma se publica bajo la licencia Apache 2.0, permitiendo un uso, distribución y modificación libres, incluyendo el uso comercial.\n",
    "| Característica             | Gemma                           | Mistral                          |\n",
    "|------------------|---------------------------------|----------------------------------|\n",
    "| **Fecha de lanzamiento**    | Febrero de 2024                 | Septiembre de 2023               |\n",
    "| **Tamaño del modelo**       | 2B, 7B, 9B, 27B                | 7.3B                            |\n",
    "| **Arquitectura base**      | Transformer (solo Decoder)     | Transformer (solo Decoder)      |\n",
    "| **Embedding de posición**   | RoPE                           | RoPE                            |\n",
    "| **Normalización**           | RMSNorm                        | RMSNorm                         |\n",
    "| **Función de activación**   | GeGLU                          | SwiGLU                          |\n",
    "| **Atención**                | Multi-Head Attention (MHA), GQA| Grouped-Query Attention (GQA), SWA |\n",
    "| **Ventana de contexto**     | Máximo 8192 tokens             | Máximo 131,000 tokens           |\n",
    "| **Características principales**| Diversos tamaños, soporte del ecosistema Google, GeGLU, amplia ventana de contexto | GQA y SWA para inferencia eficiente, procesamiento de contexto largo |\n",
    "| **Innovación (comparativa)**| Baja                           | Alta                            |\n",
    "\n",
    "*   **Similitudes:** Gemma y Mistral son ambos modelos Transformer basados en solo Decoder y utilizan componentes similares como RoPE y RMSNorm. Estos componentes contribuyen a mejorar la eficiencia y el rendimiento del modelo.\n",
    "*   **Diferencias:**\n",
    "    *   Gemma utiliza GeGLU como función de activación, mientras que Mistral utiliza SwiGLU (una variante de SiLU). GeGLU separa la entrada en dos transformaciones lineales, una actúa como puerta y la otra se multiplica para generar el resultado.\n",
    "    *   Gemma emplea Multi-Head Attention (MHA) o Grouped-Query Attention (GQA), mientras que Mistral combina GQA con Sliding Window Attention (SWA) para aumentar la eficiencia. GQA reduce el número de cabezas K y V en comparación con las cabezas Q para disminuir el uso de memoria y cálculos. SWA genera máscaras para que cada token solo realice atención dentro de un rango fijo (ventana), lo que reduce la complejidad computacional.\n",
    "\n",
    "#### Conclusión\n",
    "\n",
    "Gemma, aunque no es tan innovadora en su estructura de modelo como Mistral, tiene las siguientes importancias como un modelo de código abierto reciente:\n",
    "\n",
    "*   **Comprensión de tendencias tecnológicas actuales:** A través de Gemma, se puede entender la implementación y el funcionamiento de componentes ampliamente utilizados en modelos modernos como RoPE, RMSNorm y GeGLU.\n",
    "*   **Opciones de modelo diversas:** Gemma ofrece modelos de diversos tamaños (2B, 7B, 27B) para adaptarse a diferentes entornos de cómputo.\n",
    "*   **Utilización del ecosistema Google:** Para usuarios de la plataforma Google, Gemma puede ofrecer mejor integración y soporte en comparación con otros modelos.\n",
    "* **Accesibilidad al modelo de código abierto**: Cualquiera puede acceder fácilmente y contribuir a la comunidad.\n",
    "Por lo tanto, es mejor centrarse en el valor práctico de Gemma como modelo abierto que refleja las últimas tendencias tecnológicas, y en la posibilidad de integración con el ecosistema de Google, más que en la innovación del propio modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.8 Phi-3 : un modelo de lenguaje pequeño pero potente\n",
    "\n",
    "En las secciones 9.6 y 9.7, examinamos los elementos clave de la arquitectura de modelos de lenguaje eficientes a través de los modelos Mistral y Gemma. En esta sección, implementaremos y analizaremos directamente el modelo Phi-3 Mini desarrollado por Microsoft, explorando el secreto detrás de su excelente rendimiento a pesar de su tamaño pequeño.\n",
    "\n",
    "Phi-3 Mini es un modelo de lenguaje pequeño (SLM, Small Language Model) que Microsoft presentó en abril de 2024. Con 3.8B de parámetros, Phi-3 Mini muestra un rendimiento competitivo en varios benchmarks frente a modelos más grandes como Mistral (7B) y Gemma (7B), demostrando la posibilidad de modelos ligeros. En particular, Phi-3 Mini enfatiza la importancia de **\"datos de alta calidad\"** y **\"arquitectura eficiente\"**, señalando una nueva dirección más allá de la simple competencia en el tamaño del modelo. Esta filosofía se refleja bien en el eslogan \"Textbooks Are All You Need\". `simple_phi3.py` es un código que implementa los componentes clave de Phi-3 Mini de manera simplificada. El código completo está en `chapter_09/phi3`.\n",
    "\n",
    "### 9.8.1 `simple_phi3` modelo\n",
    "\n",
    "`simple_phi3` es un modelo implementado con fines educativos basado en Phi-3 Mini. En comparación con simple mistral del Capítulo 9.6, se tiene lo siguiente.\n",
    "\n",
    "**Resumen de diferencias funcionales del modelo**\n",
    "\n",
    "| Función | Simple Phi-3 | Simple Mistral |\n",
    "|---|---|---|\n",
    "| Atención | Multi-Head Attention (MHA) | Grouped-Query Attention (GQA) + Sliding Window Attention (SWA) |\n",
    "| Activación | GELU (aproximación tanh) | SiLU |\n",
    "| Normalización | RMSNorm | RMSNorm |\n",
    "| Codificación posicional | RoPE | RoPE |\n",
    "| `past_key_value` | Soportado (caching) | Soportado (caching) |\n",
    "| Ventana deslizante | No soportado | Soportado |\n",
    "| GQA | No soportado (usa MHA, K=V=Q, configuración de `num_key_value_heads`) | Soportado |\n",
    "| Atención de producto punto escalado | Uso de `F.scaled_dot_product_attention` | Uso de `F.scaled_dot_product_attention` |\n",
    "| Mejora del caching RoPE | Gestión eficiente de cachés `cos`, `sin` dentro del método `forward`, actualización con `_set_cos_sin_cache` cuando es necesario. Lógica optimizada para aplicar RoPE en el decoding incremental mediante la función `apply_rotary_pos_emb_single`, minimizando cálculos redundantes. | Generación de `cos_cached`, `sin_cached` en el método `_set_cos_sin_cache`, uso en `forward`. Posibilidad de usar diferentes position IDs para queries y keys en `apply_rotary_pos_emb`. |\n",
    "| Optimización de la máscara de atención | Uso de la función `scaled_dot_product_attention`, combinación eficiente de `attention_mask` y `causal_mask`, reducción de cálculos innecesarios | Uso de la función `scaled_dot_product_attention`, manejo de `attention_mask`, `sliding_window_mask` |\n",
    "| `return_dict` | Retorno flexible y claro de output mediante `return_dict`. | Retorno de output a través de `return_dict`. |\n",
    "| Vinculación de pesos | Enlace (tying) de los pesos de embedding y la capa de salida en `post_init`, reducción de parámetros y mejora del rendimiento | No se menciona explícitamente el enlace de pesos |\n",
    "\n",
    "**Principales mejoras**\n",
    "*   **Multi-Head Attention (MHA):** En lugar de GQA (Grouped-Query Attention) de Mistral, utiliza MHA convencional. Phi-3 Mini demuestra que puede lograr un rendimiento adecuado sin GQA.\n",
    "*   **Mejora en el almacenamiento en caché de RoPE:** Gestiona eficientemente las cachés `cos` y `sin` dentro del método `forward`, y actualiza solo cuando es necesario a través de `_set_cos_sin_cache`. Además, durante la decodificación incremental, optimiza la aplicación de RoPE utilizando la función `apply_rotary_pos_emb_single` para minimizar cálculos redundantes.\n",
    "*   **Optimización de Attention Mask:** Combina eficientemente `attention_mask` y `causal_mask` al usar la función `scaled_dot_product_attention` para reducir cálculos innecesarios.\n",
    "*   **Weight Tying:** En `post_init`, vincula (tied) los pesos de embebido con los pesos del layer de salida para reducir el número de parámetros y mejorar el rendimiento.\n",
    "\n",
    "Ahora examinaremos en detalle los componentes clave del modelo `simple_phi3`.\n",
    "\n",
    "#### 1. PhiMiniConfig: Configuración del modelo\n",
    "\n",
    "La clase `PhiMiniConfig` define los hiperparámetros del modelo, siguiendo la configuración de Phi-3 Mini. Dado que ya se ha explicado con detalle en Mistral, lo omitiremos aquí.\n",
    "\n",
    "#### 2. PhiMiniRMSNorm: Normalización RMS\n",
    "\n",
    "La clase `PhiMiniRMSNorm` implementa RMSNorm (Root Mean Square Layer Normalization), similar a la versión de Mistral.\n",
    "\n",
    "#### 3. PhiMiniRotaryEmbedding: Implementación de RoPE (mejorada cacha)\n",
    "\n",
    "La clase `PhiMiniRotaryEmbedding` implementa RoPE (Rotary Positional Embedding). Aunque es similar a `MistralRotaryEmbedding` de Mistral, presenta las siguientes mejoras clave para maximizar la eficiencia del almacenamiento en caché.\n",
    "\n",
    "*   **Gestión de caché dentro del método `forward`:**\n",
    "    *   En el método `forward`, se utilizan directamente `cos_cached` y `sin_cached`. Es decir, si ya hay valores calculados, se utilizan inmediatamente.\n",
    "    *   Si `seq_len` es mayor que `max_seq_len_cached`, lo que indica que se necesita una nueva longitud de secuencia en caché, se llama al método `_set_cos_sin_cache` para actualizar la caché. Esto evita la creación innecesaria de cachés y maximiza el reuso de valores ya calculados.\n",
    "\n",
    "*   **Variables de instancia `max_seq_len_cached`, `cos_cached`, `sin_cached`:**\n",
    "    *   `max_seq_len_cached`: almacena la longitud máxima de secuencia en caché hasta ahora.\n",
    "    *   `cos_cached`, `sin_cached`: almacenan los valores de coseno y seno precalculados.\n",
    "    *   Al gestionar estas variables como variables de instancia, se evita su creación nueva cada vez que se llama al método `forward` y se reutilizan para mejorar la eficiencia.\n",
    "\n",
    "* **Optimización de decodificación incremental:**\n",
    "    *   `apply_rotary_pos_emb_single`: permite aplicar RoPE solo a los **nuevos tokens** en situaciones de decodificación incremental utilizando `past_key_value`. Como los resultados de RoPE para los tokens anteriores ya están almacenados en `past_key_value`, se evitan cálculos redundantes.\n",
    "\n",
    "Estas mejoras aumentan significativamente la eficiencia de las operaciones RoPE, proporcionando ventajas de rendimiento especialmente al procesar secuencias largas o generar texto.\n",
    "\n",
    "#### 4. PhiMiniAttention: Mecanismo de atención (MHA, aplicación eficiente de RoPE)\n",
    "La clase `PhiMiniAttention` implementa el mecanismo de atención de Phi-3 Mini. Aunque utiliza la atención multi-cabeza (MHA) convencional en lugar del GQA de Mistral, optimiza la aplicación de RoPE para mejorar la eficiencia.\n",
    "\n",
    "*   **MHA (Multi-Head Attention):** El número de cabezas de consulta(Q), clave(K) y valor(V) son todos iguales.\n",
    "*   **Aplicación eficiente de RoPE:**\n",
    "    *   Se generan IDs de posición diferentes según la presencia o ausencia de `past_key_value`.\n",
    "        *   Si no hay `past_key_value` (caso común): se generan IDs de posición para toda la secuencia (`0` a `q_len - 1`).\n",
    "        *   Si hay `past_key_value` (decodificación incremental): se generan IDs de posición para el nuevo token (`past_len` a `past_len + q_len - 1`) y para toda la secuencia clave (`0` a `past_len + q_len - 1`).\n",
    "    *   A través de la función `apply_rotary_pos_emb_single`, cuando hay `past_key_value` (decodificación incremental), se aplica RoPE solo al nuevo token(query).\n",
    "*   **Caché KV:** Al igual que Mistral, utiliza `past_key_value` para almacenar en caché los tensores de clave/valor de pasos anteriores y mejorar la velocidad de inferencia.\n",
    "\n",
    "#### 5. Funciones auxiliares: `rotate_half`, `apply_rotary_pos_emb`, `apply_rotary_pos_emb_single`\n",
    "\n",
    "*   `rotate_half`: Es una función auxiliar necesaria para implementar RoPE, similar a Mistral.\n",
    "*   `apply_rotary_pos_emb`: Aplica RoPE a los tensores de consulta(q) y clave(k). A diferencia de Mistral, solo recibe un conjunto de position_ids (aplicado igualmente a la consulta y la clave).\n",
    "*   `apply_rotary_pos_emb_single`: En situaciones de decodificación incremental con `past_key_value`, aplica RoPE al tensor de entrada `x` (consulta o clave).\n",
    "\n",
    "#### 6. PhiMiniMLP: Red FeedForward\n",
    "\n",
    "La clase `PhiMiniMLP` implementa una red FeedForward, similar a Mistral, utilizando la función de activación GELU.\n",
    "\n",
    "#### 7. PhiMiniDecoderLayer: Capa decodificadora\n",
    "\n",
    "La clase `PhiMiniDecoderLayer` utiliza la estructura Pre-Norm y las conexiones residuales, al igual que Mistral.\n",
    "\n",
    "#### 8. PhiMiniModel: Modelo completo\n",
    "\n",
    "La clase `PhiMiniModel` compone el modelo Phi-3 Mini completo, similar a Mistral.\n",
    "\n",
    "#### 9. PhiMiniForCausalLM: Adición de cabeza para modelado de lenguaje\n",
    "\n",
    "La clase `PhiMiniForCausalLM` agrega una cabeza (`lm_head`) para modelado de lenguaje al `PhiMiniModel`.\n",
    "\n",
    "*   **Método `post_init`:**\n",
    "    *   Realiza la inicialización de pesos. (Similar a Mistral)\n",
    "    *   **Empate de Pesos:** Une los pesos de incrustación (`self.transformer.embed_tokens.weight`) con los pesos del último nivel (`self.lm_head.weight`). Esto reduce el número de parámetros, evita el sobreajuste y generalmente mejora el rendimiento.\n",
    "*   **Función `generate`:** Función para generar texto, que resuelve problemas relacionados con RoPE durante la decodificación incremental al pasar solo el último token en lugar de toda la secuencia a `forward()` cuando existen `past_key_values`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.8.2 Ejemplo del modelo `simple_phi3`: cálculo de expresiones complejas\n",
    "\n",
    "Como ejemplo práctico del modelo `simple_phi3` revisado en la sección 9.8.1, probaremos su capacidad para calcular expresiones matemáticas complejas. A través de este ejemplo, evaluaremos si modelos de lenguaje pequeños (SLM) como el Phi-3 Mini pueden manejar no solo operaciones básicas de suma y resta, sino también multiplicación y expresiones más complejas con paréntesis, analizando su rendimiento y limitaciones.\n",
    "\n",
    "La ubicación del código de ejemplo es **chapter_09/phi3/examples/train_math.py**.\n",
    "\n",
    "**Significado del ejemplo**\n",
    "\n",
    "*   **Verificación de la capacidad del SLM:** muestra que incluso un modelo de tamaño pequeño puede resolver problemas complejos a través de datos de alta calidad y una arquitectura eficiente.\n",
    "*   **Evaluación de las habilidades de inferencia:** evalúa la capacidad de inferir respuestas para nuevas expresiones basadas en reglas operacionales aprendidas, más allá del simple memorizado.\n",
    "*   **Exploración de posibilidades prácticas:** el cálculo de expresiones complejas es una habilidad fundamental que puede ser aplicada en diversos campos como procesamiento de lenguaje natural y análisis de datos. Este ejemplo permite vislumbrar las posibilidades prácticas del SLM.\n",
    "\n",
    "**Forma de los datos de entrenamiento**\n",
    "\n",
    "Usando la función `create_complex_arithmetic_data`, generamos datos de expresiones matemáticas complejas con las siguientes características:\n",
    "\n",
    "*   Dos o tres números (1 ~ 50)\n",
    "*   Uso de dos operadores entre +, - y \\*\n",
    "*   Uso opcional de paréntesis (())\n",
    "*   Formato `expresión=resultado<eos>` (ej: `(12+7)*3=57<eos>`, `12+7*3=33<eos>`)\n",
    "\n",
    "**Resultados del entrenamiento**\n",
    "\n",
    "```python\n",
    "Muestra 1: 41*8-2=326<eos>\n",
    "Muestra 2: 15+(9*48)=447<eos>\n",
    "Muestra 3: 35-6+38=67<eos>\n",
    "Muestra 4: 6*14*15=1260<eos>\n",
    "Muestra 5: 36*(13*46)=21528<eos>\n",
    "\n",
    "...(omisión del registro de entrenamiento)...\n",
    "\n",
    "Prompt: '23-23-50=' --> Resultado generado: '23-23-50=-50'  (Respuesta correcta: 23-23-50=-50<eos>)\n",
    "Prompt: '39-46-15=' --> Resultado generado: '39-46-15=-22'  (Respuesta correcta: 39-46-15=-22<eos>)\n",
    "Prompt: '(33-30)+30=' --> Resultado generado: '(33-30)+30=33'  (Respuesta correcta: (33-30)+30=33<eos>)\n",
    "Prompt: '6*14*15=' --> Resultado generado: '6*14*15=1260'  (Respuesta correcta: 6*14*15=1260<eos>)\n",
    "```\n",
    "\n",
    "**Análisis**\n",
    "\n",
    "*   **Resultados precisos o cercanos a la respuesta correcta:** en la mayoría de los casos, el modelo `simple_phi3` generó respuestas que coinciden con las correctas o son muy similares. Esto indica que el modelo ha aprendido bien las reglas operacionales para expresiones matemáticas complejas.\n",
    "*   **Errores en algunos casos:** tendencia a cometer errores cuando la multiplicación está involucrada o los números son grandes, lo cual puede deberse al tamaño limitado del modelo y a la falta de diversidad en los datos de entrenamiento.\n",
    "*   **Manejo de paréntesis:** el modelo también muestra una habilidad decente para manejar expresiones con paréntesis, indicando que tiene una comprensión contextual y puede seguir el orden correcto de las operaciones.\n",
    "\n",
    "**Conclusión**\n",
    "\n",
    "A pesar de tener solo alrededor de 120,000 parámetros, el modelo `simple_phi3` logró un alto nivel de precisión (aproximadamente 80%) en el cálculo de expresiones matemáticas complejas. Esto sugiere que ha aprendido reglas complejas como el manejo de paréntesis y el orden de las operaciones. En comparación con modelos de lenguaje grandes (LLM) que tienen miles de millones de parámetros, `simple_phi3` demuestra un rendimiento impresionante a pesar de su tamaño extremadamente pequeño (0.12M)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data examples:\n",
      "Sample 1: 41*8-2=326<eos>\n",
      "Sample 2: 15+(9*48)=447<eos>\n",
      "Sample 3: 35-6+38=67<eos>\n",
      "Sample 4: 6*14*15=1260<eos>\n",
      "Sample 5: 36*(13*46)=21528<eos>\n",
      "Total Trainable Parameters: 126208\n",
      "Start training...\n",
      "Epoch 1/30, Avg Loss: 0.7439, LR: 0.000997\n",
      "Epoch 2/30, Avg Loss: 0.6393, LR: 0.000989\n",
      "Epoch 3/30, Avg Loss: 0.6139, LR: 0.000976\n",
      "Epoch 4/30, Avg Loss: 0.5919, LR: 0.000957\n",
      "Epoch 5/30, Avg Loss: 0.5825, LR: 0.000934\n",
      "Epoch 6/30, Avg Loss: 0.5753, LR: 0.000905\n",
      "Epoch 7/30, Avg Loss: 0.5696, LR: 0.000873\n",
      "Epoch 8/30, Avg Loss: 0.5649, LR: 0.000836\n",
      "Epoch 9/30, Avg Loss: 0.5599, LR: 0.000796\n",
      "Epoch 10/30, Avg Loss: 0.5558, LR: 0.000753\n",
      "Epoch 11/30, Avg Loss: 0.5522, LR: 0.000706\n",
      "Epoch 12/30, Avg Loss: 0.5479, LR: 0.000658\n",
      "Epoch 13/30, Avg Loss: 0.5443, LR: 0.000608\n",
      "Epoch 14/30, Avg Loss: 0.5409, LR: 0.000557\n",
      "Epoch 15/30, Avg Loss: 0.5370, LR: 0.000505\n",
      "Epoch 16/30, Avg Loss: 0.5339, LR: 0.000453\n",
      "Epoch 17/30, Avg Loss: 0.5307, LR: 0.000402\n",
      "Epoch 18/30, Avg Loss: 0.5280, LR: 0.000352\n",
      "Epoch 19/30, Avg Loss: 0.5242, LR: 0.000304\n",
      "Epoch 20/30, Avg Loss: 0.5217, LR: 0.000258\n",
      "Epoch 21/30, Avg Loss: 0.5189, LR: 0.000214\n",
      "Epoch 22/30, Avg Loss: 0.5161, LR: 0.000174\n",
      "Epoch 23/30, Avg Loss: 0.5137, LR: 0.000137\n",
      "Epoch 24/30, Avg Loss: 0.5120, LR: 0.000105\n",
      "Epoch 25/30, Avg Loss: 0.5101, LR: 0.000076\n",
      "Epoch 26/30, Avg Loss: 0.5085, LR: 0.000053\n",
      "Epoch 27/30, Avg Loss: 0.5073, LR: 0.000034\n",
      "Epoch 28/30, Avg Loss: 0.5062, LR: 0.000021\n",
      "Epoch 29/30, Avg Loss: 0.5055, LR: 0.000013\n",
      "Epoch 30/30, Avg Loss: 0.5050, LR: 0.000010\n",
      "Model saved: phimini_complex_math.pt\n",
      "\n",
      "Test sample generation results:\n",
      "Prompt: '23-23-50=' --> Generated result: '23-23-50=-50'  (Correct answer: 23-23-50=-50<eos>)\n",
      "Prompt: '39-46-15=' --> Generated result: '39-46-15=-22'  (Correct answer: 39-46-15=-22<eos>)\n",
      "Prompt: '(33-30)+30=' --> Generated result: '(33-30)+30=33'  (Correct answer: (33-30)+30=33<eos>)\n",
      "Prompt: '30+14*27=' --> Generated result: '30+14*27=408'  (Correct answer: 30+14*27=408<eos>)\n",
      "Prompt: '(13-22)-18=' --> Generated result: '(13-22)-18=-27'  (Correct answer: (13-22)-18=-27<eos>)\n",
      "Prompt: '9-15+12=' --> Generated result: '9-15+12=6'  (Correct answer: 9-15+12=6<eos>)\n",
      "Prompt: '28*(3+31)=' --> Generated result: '28*(3+31)=960'  (Correct answer: 28*(3+31)=952<eos>)\n",
      "Prompt: '24*(12+1)=' --> Generated result: '24*(12+1)=320'  (Correct answer: 24*(12+1)=312<eos>)\n",
      "Prompt: '(1-33)+26=' --> Generated result: '(1-33)+26=-6'  (Correct answer: (1-33)+26=-6<eos>)\n",
      "Prompt: '24+47+6=' --> Generated result: '24+47+6=77'  (Correct answer: 24+47+6=77<eos>)\n",
      "\n",
      "Overall accuracy: 80.00% (8/10)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "from dldna.chapter_09.phi3.examples.train_complex_math import PhiMiniConfig, PhiMiniForCausalLM, ComplexArithmeticDataset, train, create_complex_arithmetic_data, create_tokenizer, create_reverse_tokenizer, generate_text\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Hyperparameters\n",
    "num_samples = 100000      # Sufficiently large amount of data\n",
    "max_value = 50           # Maximum value of operands (for slightly complex calculations)\n",
    "seq_length = 30          # Complex arithmetic problems can have somewhat long expressions\n",
    "batch_size = 128\n",
    "epochs = 30\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# Data generation\n",
    "complex_data = create_complex_arithmetic_data(num_samples, max_value)\n",
    "print(\"Training data examples:\")\n",
    "for i in range(5):\n",
    "    print(f\"Sample {i+1}: {complex_data[i]}\")\n",
    "\n",
    "# Create tokenizer and reverse tokenizer\n",
    "tokenizer = create_tokenizer()\n",
    "reverse_tokenizer = create_reverse_tokenizer(tokenizer)\n",
    "updated_vocab_size = len(tokenizer)\n",
    "\n",
    "# Configure Dataset and DataLoader\n",
    "dataset = ComplexArithmeticDataset(complex_data, seq_length, tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# PhiMini Model Configuration\n",
    "config = PhiMiniConfig(\n",
    "    vocab_size=updated_vocab_size,\n",
    "    hidden_size=64,              # Small model size for experimentation\n",
    "    intermediate_size=128,\n",
    "    num_hidden_layers=3,\n",
    "    num_attention_heads=8,\n",
    "    num_key_value_heads=8,        # K=V=Q\n",
    "    max_position_embeddings=128,\n",
    "    use_cache=False,\n",
    "    use_return_dict=True,\n",
    ")\n",
    "config.pad_token_id = tokenizer[\"<pad>\"]\n",
    "config.eos_token_id = tokenizer[\"<eos>\"]\n",
    "\n",
    "# Create PhiMini For CausalLM Model\n",
    "model = PhiMiniForCausalLM(config).to(device)\n",
    "print(\"Total Trainable Parameters:\", sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "\n",
    "# weight tying (share weights between embedding and lm_head)\n",
    "model.lm_head.weight = model.transformer.embed_tokens.weight\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=1e-5)\n",
    "\n",
    "# Model Training\n",
    "print(\"Start training...\")\n",
    "train(model, dataloader, optimizer, scheduler, epochs, device)\n",
    "\n",
    "# Save Model\n",
    "save_path = \"phimini_complex_math.pt\"\n",
    "torch.save(model.state_dict(), save_path)\n",
    "print(f\"Model saved: {save_path}\")\n",
    "\n",
    "# Load Saved Model (create a new model object before testing and load_state_dict)\n",
    "loaded_model = PhiMiniForCausalLM(config).to(device)\n",
    "loaded_model.load_state_dict(torch.load(save_path, map_location=device))\n",
    "loaded_model.eval()\n",
    "\n",
    "# Generate and Print Results with Test Set, Calculate Accuracy\n",
    "print(\"\\nTest sample generation results:\")\n",
    "test_samples = random.sample(complex_data, 10)\n",
    "correct_count = 0\n",
    "for sample in test_samples:\n",
    "    prompt = sample.split('=')[0] + '='\n",
    "    generated = generate_text(loaded_model, prompt, tokenizer, reverse_tokenizer, seq_length, device, temperature=0.1)  # Reduce temperature for testing\n",
    "    answer = sample.split('=')[1].replace('<eos>', '')\n",
    "\n",
    "    if generated.split('=')[1] == answer:\n",
    "        correct_count += 1\n",
    "    print(f\"Prompt: '{prompt}' --> Generated result: '{generated}'  (Correct answer: {sample})\")\n",
    "\n",
    "accuracy = (correct_count / len(test_samples)) * 100\n",
    "print(f\"\\nOverall accuracy: {accuracy:.2f}% ({correct_count}/{len(test_samples)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusión\n",
    "\n",
    "En el Capítulo 9, hemos seguido la evolución de los Transformers desde su presentación en 2017 con el paper pionero \"Attention is All You Need\" hasta la actualidad en 2025, centrándonos en las dos fuerzas impulsoras clave del desarrollo de los Transformers: **eficiencia** y **escalabilidad**.\n",
    "\n",
    "Los primeros Transformers mostraron un rendimiento revolucionario, pero se enfrentaron a una limitación fundamental: el aumento exponencial del cálculo y el uso de memoria con la longitud de las secuencias. El Capítulo 9 aborda en profundidad los esfuerzos constantes para superar estas restricciones, incluyendo enfoques puramente software (Sección 9.2), combinaciones de hardware y software (Sección 9.3) y diversas innovaciones técnicas para mejorar la escalabilidad del modelo (Sección 9.4). Desde los ejemplos de implementación de RoPE y FlashAttention (Sección 9.5) hasta el análisis de las arquitecturas de modelos recientes como Mistral, Gemma y Phi-3 Mini (Secciones 9.6, 9.7, 9.8), exploramos exhaustivamente tanto la teoría como la implementación práctica para iluminar las arquitecturas Transformers más eficientes.\n",
    "\n",
    "Gracias a estos avances tecnológicos, los Transformers se han convertido en una herramienta poderosa capaz de comprender contextos más largos, resolver problemas más complejos y aplicarse a un rango más amplio de campos. Es evidente el papel crucial que han jugado la **eficiencia y escalabilidad** en el crecimiento de los Transformers desde simples modelos de lenguaje hasta convertirse en un motor central para el desarrollo de tecnologías de inteligencia artificial.\n",
    "\n",
    "Por supuesto, aún quedan desafíos por resolver. El aumento del consumo energético asociado al agrandamiento de los modelos, problemas de sesgo y daño, y la cuestión de la interpretabilidad de los modelos son desafíos importantes que debemos superar en el futuro. La investigación para desarrollar sistemas de IA más seguros, confiables y capaces de colaborar armónicamente con humanos continuará.\n",
    "\n",
    "A partir del Capítulo 10 y 11, iniciaremos un viaje hacia el mundo **multimodal** de los Transformers, que va más allá del dominio único del texto para integrar diversas tipos de datos como imágenes, audio y video. Los modelos multimodales que combinan información de múltiples modalidades ofrecen representaciones más ricas y potentes, lo que permite inferencias más complejas. Exploraremos los mecanismos de atención multimodal y sus infinitas posibilidades de aplicación a través de pioneros modelos como ViT, CLIP, DALL-E, Stable Diffusion, Flamingo, GATO y Gemini. Las innovaciones en eficiencia y escalabilidad discutidas en el Capítulo 9 sentarán las bases sólidas para el futuro de los Transformers multimodales que se presentará en los Capítulos 10 y 11."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.callout-note collapse=\"true\" title=\"Haga clic para ver el contenido (análisis detallado: evolución teórica de la arquitectura MoE y las últimas tendencias tecnológicas)\"}\n",
    "## Evolución teórica y tendencias tecnológicas más recientes de la arquitectura MoE (Mixture of Experts)\n",
    "\n",
    "En el desarrollo de los modelos de lenguaje a gran escala (LLM), Mixture of Experts (MoE) ha surgido como un marco que resuelve innovadoramente el equilibrio entre la capacidad del modelo y la eficiencia computacional. MoE funciona combinando varias redes \"expertas\" y activando selectivamente las expertas apropiadas para cada entrada a través de una red de puerta. Aquí se desglosan en profundidad los mecanismos clave de MoE, y se organiza sistemáticamente la teoría extendida basada en las tendencias de investigación más recientes.\n",
    "\n",
    "### 1. Fundamentos teóricos de MoE\n",
    "\n",
    "#### 1.1 Componentes básicos\n",
    "\n",
    "*   **Redes expertas:** Existen *N* redes expertas $\\{E_i\\}_{i=1}^N$, generalmente compuestas por Redes Neuronales Feedforward (FFN). Cada experta recibe una entrada $x$ y genera una salida $E_i(x)$.\n",
    "*   **Red de puerta:** La red de puerta $G$ toma la entrada $x$ y produce pesos (probabilidades) para cada experta. Estos pesos indican qué experta es la más adecuada para la entrada $x$. La salida de la red de puerta $G(x)$ es un vector de *N* dimensiones, donde cada elemento $G(x)_i$ representa el peso para la *i*-ésima experta.\n",
    "*   **Salida final:** La salida final del modelo MoE $y$ se calcula como una suma ponderada de las salidas de las expertas.\n",
    "\n",
    "    $y = \\sum_{i=1}^{N} G(x)_i E_i(x)$\n",
    "\n",
    "#### 1.2 MoE Denso y MoE Disperso\n",
    "\n",
    "*   **MoE Denso:** Todas las expertas realizan cálculos para todas las entradas, y la red de puerta determina los pesos de las salidas de las expertas a través de una función softmax. ($G(x) = \\text{softmax}(W_g x)$)\n",
    "*   **MoE Disperso:** Para cada entrada, solo se activan un pequeño número de expertas. La red de puerta utiliza Top-k gating (seleccionando los *k* valores más altos) o Noisy Top-k gating (GShard, Switch Transformer).\n",
    "\n",
    "#### 1.3 Formalización matemática y perspectiva de inferencia variacional\n",
    "\n",
    "Al reinterpretar el sistema MoE como un modelo gráfico probabilístico, la distribución conjunta de los datos observados $\\mathbf{x}$ y las variables latentes $\\mathbf{z}$ (indicadoras de selección de experta) se modela como sigue:\n",
    "\n",
    "$p(\\mathbf{x}, \\mathbf{z}|\\theta) = p(\\mathbf{z}|\\theta_g)p(\\mathbf{x}|\\mathbf{z},\\theta_e)$\n",
    "\n",
    "donde $\\theta_g$ son los parámetros de la red de puerta y $\\theta_e$ son los parámetros de las redes expertas. En el marco de inferencia variacional, la Cota Inferior del Evidencia (ELBO) se deriva como:\n",
    "\n",
    "$\\mathcal{L}(\\theta, \\phi) = \\mathbb{E}_{q_\\phi(\\mathbf{z}|\\mathbf{x})}[\\log p_\\theta(\\mathbf{x}|\\mathbf{z})] - D_{KL}(q_\\phi(\\mathbf{z}|\\mathbf{x}) \\parallel p(\\mathbf{z}))$\n",
    "\n",
    "Este enfoque redefine el proceso de aprendizaje de MoE dentro de un sistema de inferencia bayesiana, proporcionando una base teórica para la división del conocimiento entre las expertas. En particular, la técnica de reparametrización Gumbel-Softmax aproxima continuamente el proceso discreto de selección de experta, permitiendo la aplicación del descenso por gradiente.\n",
    "\n",
    "$\\mathbf{z} = \\text{softmax}((\\log \\boldsymbol{\\pi} + \\mathbf{g})/\\tau)$\n",
    "\n",
    "donde $\\mathbf{g}$ es ruido Gumbel y $\\tau$ es un parámetro de temperatura.\n",
    "\n",
    "### 2. Innovaciones estructurales en MoE Disperso\n",
    "#### 2.1 Particionamiento Jerárquico de Expertos (Hierarchical Expert Partitioning)\n",
    "\n",
    "Multi-Head Latent Attention (MLA) introducido en DeepSeek-V2 reduce significativamente la caché Key-Value [5, 6]. Esto se implementa a través de un enfoque que bifurca la jerarquía de expertos en partición espacial (Spatial Partitioning) y partición funcional (Functional Partitioning).\n",
    "\n",
    "$E_i(\\mathbf{x}) = \\sum_{h=1}^H W_{h,i}^o \\cdot \\text{GeLU}(W_{h,i}^k \\mathbf{x} \\oplus W_{h,i}^v \\mathbf{x})$\n",
    "\n",
    "Dentro de cada experto, las cabezas de atención actúan como subexpertos independientes y maximizan la eficiencia paramétrica a través de matrices base compartidas (shared basis matrices).\n",
    "\n",
    "#### 2.2 Adaptación Dinámica de Topología\n",
    "\n",
    "El modelo Mixtral 8x7B introdujo un mecanismo para reconstruir dinámicamente la estructura de conexión de los expertos en función de los datos de entrada. La red de enrutamiento ha evolucionado más allá de una simple selección de expertos, convirtiéndose en una red neuronal gráfica (Graph Neural Network) que ajusta la intensidad de las conexiones entre expertos.\n",
    "\n",
    "$A_{ij}^{(l)} = \\sigma(f_\\phi(\\mathbf{h}_i^{(l)}, \\mathbf{h}_j^{(l)}))$\n",
    "\n",
    "Aquí, $A_{ij}$ representa el peso de conexión entre los expertos $i$ y $j$, lo que permite la extracción de características multi-escala a través del mecanismo de atención jerárquica.\n",
    "\n",
    "### 3. Ventajas y Optimización de Modelos MoE\n",
    "\n",
    "#### 3.1 Ventajas\n",
    "\n",
    "*   **Aumento de Capacidad del Modelo:** Se puede aumentar significativamente el número de parámetros al incrementar el número de expertos, pero el costo computacional aumenta relativamente poco.\n",
    "*   **Eficiencia Computacional (Sparse MoE):** Dado que solo un pequeño número de expertos se activan por token, los FLOPs son bajos.\n",
    "*   **Ley de Escalabilidad:** Los modelos MoE tienden a seguir una ley de escalabilidad más favorable en comparación con los modelos densos.\n",
    "*   **Ajuste Fino (Fine-tuning):** Se pueden ajustar finamente expertos específicos para especializarse en tareas específicas.\n",
    "\n",
    "#### 3.2 Innovaciones en Teoría de Optimización\n",
    "\n",
    "*   **Optimización Equilibrada (Balanced Optimization):** Para abordar el problema del desequilibrio de carga entre los expertos, se introdujo la técnica de descomposición dual (Dual Decomposition), utilizando el método de multiplicadores de Lagrange para restringir explícitamente la desviación estándar de la utilización de los expertos.\n",
    "\n",
    "    $\\mathcal{L}_{\\text{total}} = \\mathcal{L}_{\\text{task}} + \\lambda \\sum_{i=1}^N (\\mathbb{E}[u_i] - \\bar{u})^2$\n",
    "\n",
    "    Aquí, $u_i$ es la utilización del experto $i$, y $\\bar{u}$ es la tasa de utilización promedio objetivo.\n",
    "\n",
    "*   **Distilación de Conocimiento en Múltiples Capas:** Se propuso una distilación de conocimiento jerárquica (Hierarchical Knowledge Distillation) que refleja la estructura jerárquica de los modelos MoE.\n",
    "    $\\mathcal{L}_{KD} = \\sum_{l=1}^{L}\\alpha_{l}D_{KL}(g^{\\text{teacher}}_{l} || g^{\\text{student}}_{l})$\n",
    "    Al minimizar individualmente la divergencia KL de las distribuciones de puerta $g_{l}$ en cada capa MoE, se posibilita la transferencia de conocimientos especializados de los expertos.\n",
    "\n",
    "### 4. Ejemplos y Limitaciones de Modelos MoE\n",
    "\n",
    "#### 4.1 Ejemplo\n",
    "*   **GShard:** Modelo MoE disperso de Google. Utiliza el gating Noisy Top-k.\n",
    "*   **Switch Transformer:** Modelo MoE disperso de Google. Cada token es procesado por un solo experto (k=1).\n",
    "*   **GLaM:** Modelo MoE disperso de Google (1.2T parámetros).\n",
    "*   **Mistral 8x7B:** Modelo MoE disperso de Mistral AI. Utiliza el gating Top-2.\n",
    "\n",
    "#### 4.2 Limitaciones y desafíos\n",
    "\n",
    "*   **Desbalance de expertos (Load Imbalance):** Asignación excesiva de tokens a ciertos expertos. (Solución: Gating Noisy Top-k, Loss de balanceo de carga, Límite de capacidad del experto)\n",
    "*   **Dificultad en el aprendizaje de la red de gating:** Dificultad para aprender una selección/combinação efectiva de expertos.\n",
    "*   **Costos de comunicación (aprendizaje distribuido):** Posible aumento de los costos de comunicación entre expertos.\n",
    "*   **Dificultad en el enriquecimiento del conocimiento**: Debido al tamaño del modelo MoE, es difícil destilar conocimientos a modelos más pequeños.\n",
    "\n",
    "### 5. Avances en la implementación física\n",
    "#### 5.1 Hardware para activaciones de expertos dispersos\n",
    "La GPU NVIDIA H100 Tensor Core introduce una Unidad de Ejecución Dispersa (Sparse Execution Unit) dedicada a MoE, que acelera las operaciones de routing Top-k.\n",
    "* Control dinámico de warp: Gestión independiente del flujo de ejecución por grupo de expertos\n",
    "* Memoria compartida jerárquica: Optimización del intercambio de resultados intermedios entre expertos\n",
    "* Paralelismo de modelo asincrónico: Minimización de latencia durante la ejecución distribuida de expertos\n",
    "\n",
    "#### 5.2 Intercambio cuantificado de expertos\n",
    "Investigaciones recientes han desarrollado técnicas para reducir el ancho de banda de comunicación al cuantizar los parámetros del experto a 4 bits[5]. Se aplica la técnica de cuantización diferencial (Differential Quantization).\n",
    "$\\Delta W_{i} = \\text{sign}(W_{i}-\\hat{W})\\cdot 2^{\\lfloor \\log_{2}|W_{i}-\\hat{W}|\\rfloor}$\n",
    "Aquí, $\\hat{W}$ representa la matriz base compartida, y solo se cuantizan las desviaciones por experto para minimizar la pérdida de precisión.\n",
    "\n",
    "### 6. Últimas tendencias en expansión teórica\n",
    "\n",
    "#### 6.1 Espacio de expertos continuo (Continuous Expert Space)\n",
    "\n",
    "La investigación más reciente de Google DeepMind en 2025 propuso CES-MoE, que modela los expertos como una distribución en un espacio continuo en lugar de objetos discretos. Utiliza un modelo de difusión de expertos basado en movimiento browniano.\n",
    "\n",
    "$dE_t = \\mu(E_t,t)dt + \\sigma(t)dW_t$\n",
    "\n",
    "Este enfoque modela la evolución gradual de las características del experto y muestra un rendimiento excelente en adaptación dinámica de dominio (Dynamic Domain Adaptation).\n",
    "\n",
    "#### 6.2 Expertos basados en ecuaciones diferenciales neuronales\n",
    "\n",
    "En las arquitecturas MoE de próxima generación, se están realizando investigaciones para reemplazar las redes de expertos con ecuaciones diferenciales neuronales (Neural ODEs)\n",
    "\n",
    "$\\frac{d\\mathbf{h}(t)}{dt} = f_\\theta(\\mathbf{h}(t), t)$\n",
    "\n",
    "Esto permite modelar las características evolutivas temporales de los expertos y ha logrado una mejora en el rendimiento en tareas de inferencia a largo plazo (Long-horizon Inference).\n",
    "\n",
    "### 7. Desafíos y direcciones futuras\n",
    "#### 7.1 Análisis profundo de las limitaciones teóricas\n",
    "* Cuello de botella informativo: Sesgo en la selección de expertos debido a las limitaciones de capacidad de procesamiento de información del router\n",
    "* Optimización no convexa: Problema de múltiples mínimos locales en el espacio de combinación experto-gate\n",
    "* Redundancia de conocimiento: Falta de base teórica para el aprendizaje de características redundantes entre expertos\n",
    "#### 7.2 Marco de trabajo para la próxima generación de investigación\n",
    "* Geometría diferencial estocástica (Stochastic Differential Geometry)\n",
    "    - Estrategias de exploración eficiente a través del análisis de curvatura de variedades especializadas\n",
    "* Expertos en superposición cuántica (Quantum Superposition Experts)\n",
    "     - Utilización de estados de superposición basados en qubits para expertos\n",
    "* Imitación biológica de la plasticidad\n",
    "    - Reconstrucción dinámica de expertos aplicando el principio de plasticidad sináptica\n",
    "\n",
    "### 8. Estudios de caso de aplicación práctica \n",
    "#### 8.1 Diseño de sistemas de inferencia a gran escala\n",
    "El sistema HyperClova X-MoE de Naver distribuyó 1,024 expertos mediante clústerización jerárquica.\n",
    "\n",
    "* Enrutamiento jerárquico en tres etapas: filtrado de expertos por etapa (clúster→rack→nodo)\n",
    "* Reconstrucción dinámica de la disposición: optimización de la ubicación de los expertos basada en RL\n",
    "* Caché de precisión mixta: gestión de FP8 para expertos calientes y FP16 para expertos fríos\n",
    "\n",
    "#### 8.2 Expansión de aplicaciones multimodales\n",
    "GPT-4o de OpenAI aplicó MoE al aprendizaje multimodal.\n",
    "\n",
    "$\\mathbf{h}_{\\text{fused}} = \\sum_{i=1}^N G(\\mathbf{x}_{\\text{text}} \\oplus \\mathbf{x}_{\\text{image}})_i E_i(\\mathbf{x}_{\\text{text}}, \\mathbf{x}_{\\text{image}})$\n",
    "\n",
    "Operaron expertos en un espacio de incrustación conjunto texto-imagen, mejorando el rendimiento de la inferencia cruzada modal.\n",
    "\n",
    "---\n",
    "**Referencias:**\n",
    "\n",
    "[1] Fedus, W., Zoph, B., & Shazeer, N. (2021). Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity. *arXiv preprint arXiv:2101.03961*.\n",
    "\n",
    "[2] Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q., Hinton, G., & Dean, J. (2017). Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer. *arXiv preprint arXiv:1701.06538*.\n",
    "\n",
    "[3] Jacobs, R. A., Jordan, M. I., Nowlan, S. J., & Hinton, G. E. (1991). Adaptive mixtures of local experts. *Neural computation*, *3*(1), 79-87.\n",
    "\n",
    "[4] NVIDIA Developer Blog. (2024). Applying Mixture of Experts in LLM Architectures. [https://developer.nvidia.com/blog/applying-mixture-of-experts-in-llm-architectures/](https://developer.nvidia.com/blog/applying-mixture-of-experts-in-llm-architectures/)\n",
    "\n",
    "[5] Materiales relacionados con DeepSeek-V2:\n",
    "    * Modu Labs Blog. [https://modulabs.co.kr/blog/deepseek-r1-introduction](https://modulabs.co.kr/blog/deepseek-r1-introduction)\n",
    "    * HyperLab. [https://hyperlab.hits.ai/blog/ai-deepseek](https://hyperlab.hits.ai/blog/ai-deepseek)\n",
    "    * Wikidocs. [https://wikidocs.net/275230](https://wikidocs.net/275230)\n",
    "[6] Chung, E. (2023). Arquitectura de próxima generación después del Transformer de tendencias - MoE, SSM, RetNet, V-JEPA. *Velog*. [https://velog.io/@euisuk-chung/%ED%8A%B8%EB%A0%8C%EB%93%9C-%ED%8A%B8%EB%A0%8C%EC%8A%A4%ED%8F%AC%EB%A8%B8-%EC%9D%B4%ED%9B%84%EC%9D%98-%EC%B0%A8%EC%84%B8%EB%8C%80-%EC%95%84%ED%82%A4%ED%85%8D%EC%B3%90-MoE-SSM-RetNet-V-JEPA](https://velog.io/@euisuk-chung/%ED%8A%B8%EB%A0%8C%EB%93%9C-%ED%8A%B8%EB%A0%8C%EC%8A%A4%ED%8F%AC%EB%A8%B8-%EC%9D%B4%ED%9B%84%EC%9D%98-%EC%B0%A8%EC%84%B8%EB%8C%80-%EC%95%84%ED%82%A4%ED%85%8D%EC%B3%90-MoE-SSM-RetNet-V-JEPA)\n",
    "\n",
    "[7] The Moonlight. (2024). GG MoE vs MLP en Datos Tabulares. [https://www.themoonlight.io/ko/review/gg-moe-vs-mlp-on-tabular-data](https://www.themoonlight.io/ko/review/gg-moe-vs-mlp-on-tabular-data)\n",
    "\n",
    "[8] Unite.AI. (2024). Modelo MoE 8x7B de la Última Versión de Mistral AI. [https://www.unite.ai/ko/mistral-ais-latest-mixture-of-experts-moe-8x7b-model/](https://www.unite.ai/ko/mistral-ais-latest-mixture-of-experts-moe-8x7b-model/)\n",
    "\n",
    "[9] Turing Post (2024) Benchmark MS EUREKA. [[https://turingpost.co.kr/p/ms-eureka-benchmark](https://turingpost.co.kr/p/ms-eureka-benchmark)]([https://turingpost.co](https://www.google.com/search?q=https://turingpost.co)\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Ejercicios de práctica\n",
    "\n",
    "**Problemas básicos**\n",
    "\n",
    "1.  Explique por qué la complejidad computacional del mecanismo de atención en los transformadores aumenta cuadráticamente con la longitud de la secuencia.\n",
    "2.  Explique cómo FlashAttention optimiza las operaciones de atención utilizando la jerarquía de memoria de GPU.\n",
    "3.  Explique la diferencia entre MQA (Multi-Query Attention) y GQA (Grouped-Query Attention), y compare sus ventajas y desventajas.\n",
    "4.  Explique el principio por el cual PagedAttention y vLLM mejoran la velocidad de inferencia y el rendimiento en modelos de lenguaje a gran escala.\n",
    "5.  Compare el funcionamiento de la atención jerárquica (Hierarchical Attention) y del Recurrent Memory Transformer para el procesamiento de contextos largos, y explique sus respectivas ventajas y desventajas.\n",
    "\n",
    "**Problemas de aplicación**\n",
    "\n",
    "1.  Escriba un código para realizar una tarea de clasificación de texto (text classification) en un conjunto de datos de texto dado utilizando un modelo de transformador. (Use el ejemplo de efficient\\_encoder del capítulo 9.5, aplicando técnicas de optimización como FlashAttention, Pre-LN structure y Gradient Checkpointing).\n",
    "2.  Escriba un código para realizar una tarea de conversión de números a palabras usando el modelo Simple Mistral descrito en el capítulo 9.5, y evalúe el rendimiento del modelo.\n",
    "3.  Escriba un código para realizar una tarea de conversión de lenguaje natural a SQL utilizando el modelo Simple Mistral descrito en el capítulo 9.5, y evalúe el rendimiento del modelo.\n",
    "4.  Explique el concepto de Constitutional AI y proponga métodos para aplicarlo a los modelos de transformador con el fin de reforzar las restricciones éticas/seguras del modelo. (No es necesario implementar).\n",
    "\n",
    "**Problemas avanzados**\n",
    "\n",
    "1.  Analice matemáticamente cómo el procesamiento por bloques de FlashAttention mejora la eficiencia de memoria y compare su complejidad computacional con los mecanismos de atención tradicionales.\n",
    "2.  Investigue otros métodos para reducir el tamaño del caché KV además de MQA y GQA, y compare sus ventajas y desventajas.\n",
    "3.  Proponga un nuevo mecanismo de atención para el procesamiento de contextos largos y explique cómo se diferencia de los métodos existentes. (Un nivel de propuesta de ideas es suficiente).\n",
    "4.  Identifique las limitaciones de Constitutional AI y proponga soluciones para superarlas. (Un nivel de propuesta de ideas es suficiente)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.callout-note collapse=\"true\" title=\"Haga clic para ver el contenido (soluciones de los ejercicios)\"}\n",
    "## Soluciones de Ejercicios\n",
    "\n",
    "### Problemas Básicos\n",
    "\n",
    "1.  **Complejidad computacional del mecanismo de atención:** El mecanismo de atención calcula la relación entre cada par de tokens. Cuando la longitud de la secuencia es n, para cada uno de los n tokens se deben calcular las relaciones con los otros (n-1) tokens, por lo que se necesitan aproximadamente n * (n-1) ≈ n² operaciones. Por lo tanto, la complejidad computacional es O(n²).\n",
    "\n",
    "2.  **Optimización FlashAttention:** FlashAttention aprovecha al máximo el SRAM (memoria rápida) de la GPU. Divide las entradas en pequeños bloques, los carga en el SRAM y realiza cálculos de atención a nivel de bloque, luego escribe los resultados de nuevo en el HBM (memoria lenta). De esta manera, se reduce el número de accesos al HBM, minimizando la I/O de memoria y aumentando la velocidad de cálculo.\n",
    "\n",
    "3.  **MQA vs. GQA:**\n",
    "    *   **MQA (Multi-Query Attention):** Todos los cabezales comparten las mismas matrices Key, Value. Reduce el tamaño del caché KV, disminuyendo el uso de memoria y mejorando la velocidad, pero puede reducir la expresividad.\n",
    "    *   **GQA (Grouped-Query Attention):** Divide las consultas en varios grupos, y cada grupo comparte las matrices Key, Value. Tiene mayor expresividad que MQA y es más eficiente en términos de memoria que Multi-Head Attention.\n",
    "\n",
    "4.  **PagedAttention & vLLM:** PagedAttention adopta el concepto de paginación del sistema operativo para almacenar el caché KV en bloques de memoria no contiguos (páginas). vLLM utiliza PagedAttention para reducir el desperdicio de memoria, gestionando dinámicamente el caché KV y mejorando la velocidad de inferencia y el rendimiento.\n",
    "\n",
    "5.  **Hierarchical Attention vs. Recurrent Memory Transformer:**\n",
    "    *   **Hierarchical Attention:** Procesa las entradas en múltiples niveles (por ejemplo, palabra -> oración -> párrafo). Calcula la atención en cada nivel y agrega la información al nivel superior para capturar dependencias a largo plazo. Puede aumentar significativamente la cantidad de cálculos.\n",
    "    *   **Recurrent Memory Transformer (RMT):** Almacena la información del segmento anterior en forma de vectores de memoria, y utiliza esta memoria al procesar el segmento actual. Divide las secuencias largas en pequeños segmentos que se procesan secuencialmente, lo que reduce el uso de memoria pero dificulta el procesamiento paralelo.\n",
    "\n",
    "### Problemas Aplicados\n",
    "\n",
    "1.  **Escribir código para clasificación de texto:** (Se omite la escritura del código) Basándose en el ejemplo del Capítulo 9.5, utiliza la función `efficient_encoder` en lugar de `nn.TransformerEncoderLayer`, y aplica FlashAttention, Pre-LN, Gradient Checkpointing. Agrega código para cargar y preprocesar el conjunto de datos, entrenar y evaluar el modelo.\n",
    "\n",
    "2.  **Conversión número-palabra:** (Se omite la escritura del código) Carga un modelo Simple Mistral y prepara los datos de entrenamiento como pares de números y sus representaciones en palabras. Entrena el modelo y evalúa su rendimiento con datos de prueba. (Ejemplo: puntuación BLEU)\n",
    "\n",
    "3.  **Conversión lenguaje natural-SQL:** (Se omite la escritura del código) Carga un modelo Simple Mistral y prepara los datos de entrenamiento como pares de preguntas en lenguaje natural y consultas SQL. Entrena el modelo y evalúa su rendimiento con datos de prueba. (Ejemplo: precisión, viabilidad)\n",
    "\n",
    "4.  **Propuesta de Constitutional AI:** (Se omite la implementación) Constitutional AI define una serie de reglas (constitución) para las respuestas del modelo y evalúa y modifica las respuestas según estas reglas. Para aplicar esto a un modelo transformer, se pueden: (1) definir reglas éticas/seguras, (2) agregar un módulo separado para evaluar la salida del modelo, o (3) utilizar una función de pérdida que refleje las reglas durante el proceso de fine-tuning.\n",
    "\n",
    "### Problemas Avanzados\n",
    "1.  **Análisis matemático de FlashAttention:** (se omite el análisis matemático) FlashAttention reduce la cantidad de accesos a HBM mediante operaciones en bloques. Mientras que la atención tradicional requiere acceso a memoria O(n²), FlashAttention solo necesita acceso a HBM O(n²/B), donde B es el tamaño del bloque (limitado por el tamaño de SRAM de GPU).\n",
    "\n",
    "2.  **Métodos para reducir el tamaño del caché KV:**\n",
    "    *   **Cuantización (Quantization):** Representar los valores del caché KV con baja precisión (por ejemplo, 8-bit) para reducir el uso de memoria.\n",
    "    *   **Dispersidad (Sparsity):** Eliminar o convertir en cero las partes de los pesos de atención con un valor bajo para comprimir el caché KV.\n",
    "    *   **Aproximación de baja rango (Low-Rank Approximation):** Aproximar y almacenar la matriz KV como una matriz de baja dimensión.\n",
    "\n",
    "3.  **Nuevos mecanismos de atención propuestos:** (sugerencias de ideas)\n",
    "    *   **Atención Local + Global:** Procesar el contexto local (por ejemplo, palabras cercanas) con atención tradicional y las dependencias a largo plazo con atención dispersa o mecanismos de memoria.\n",
    "    *   **Amplitud de atención adaptable (Adaptive Attention Span):** Asignar una amplitud de atención diferente para cada token para reducir cálculos innecesarios.\n",
    "\n",
    "4.  **Limitaciones y soluciones para el Constitutional AI:**\n",
    "    *   **Limitaciones:** Ambigüedad en la constitución (reglas), posibilidad de conflictos entre reglas, dificultad para abordar nuevos tipos de respuestas perjudiciales.\n",
    "    *   **Soluciones:** Jerarquizar y especificar las reglas, introducir mecanismos de resolución de conflictos, actualizar y verificar continuamente las reglas, aprendizaje por refuerzo a través del feedback humano.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## material de referencia\n",
    "1.  **Attention Is All You Need (Paper Original del Transformer):** Artículo que propone por primera vez la estructura básica del modelo de transformador y el mecanismo de atención. [https://arxiv.org/abs/1706.03762](https://www.google.com/url?sa=E&source=gmail&q=https://arxiv.org/abs/1706.03762)\n",
    "2.  **FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness:** Artículo que propone FlashAttention, una optimización de las operaciones de atención utilizando la jerarquía de memoria de GPU. [https://arxiv.org/abs/2205.14135](https://www.google.com/url?sa=E&source=gmail&q=https://arxiv.org/abs/2205.14135)\n",
    "3.  **FlashAttention-v2: Faster Attention with Better Parallelism and Work Partitioning:** Versión mejorada de FlashAttention que ofrece mayor velocidad y mejor paralelismo. [https://arxiv.org/abs/2307.08691](https://www.google.com/url?sa=E&source=gmail&q=https://arxiv.org/abs/2307.08691)\n",
    "4.  **Scaling Transformer to 1M tokens and beyond with RMT:** Método para escalar la longitud del contexto de los modelos de transformador a más de 1 millón de tokens utilizando Recurrent Memory Transformers (RMT). [https://arxiv.org/abs/2311.02394](https://www.google.com/url?sa=E&source=gmail&q=https://arxiv.org/abs/2311.02394)\n",
    "5.  **Mistral-7B:** Descripción del modelo de lenguaje de alto rendimiento Mistral-7B, que tiene 7 mil millones de parámetros. [https://arxiv.org/abs/2310.06825](https://www.google.com/url?sa=E&source=gmail&q=https://arxiv.org/abs/2310.06825)\n",
    "6.  **The Illustrated Transformer:** Recurso de blog que explica fácilmente el funcionamiento del modelo de transformador mediante ilustraciones. [http://jalammar.github.io/illustrated-transformer/](https://www.google.com/url?sa=E&source=gmail&q=http://jalammar.github.io/illustrated-transformer/)\n",
    "7.  **Hugging Face Transformers Documentation:** Documentación oficial de la biblioteca Hugging Face Transformers, que ayuda a usar y aprender modelos de transformador fácilmente. [https://huggingface.co/transformers/](https://www.google.com/url?sa=E&source=gmail&q=https://huggingface.co/transformers/)\n",
    "8.  **PyTorch Documentation:** Documentación oficial del marco de aprendizaje profundo PyTorch, que proporciona funciones necesarias para implementar y entrenar modelos de transformador. [https://pytorch.org/docs/stable/index.html](https://www.google.com/url?sa=E&source=gmail&q=https://pytorch.org/docs/stable/index.html)\n",
    "9.  **TensorFlow Documentation:** Documentación oficial del marco de aprendizaje profundo TensorFlow, que proporciona API para implementar y entrenar modelos de transformador. [https://www.tensorflow.org/api_docs](https://www.google.com/url?sa=E&source=gmail&q=https://www.tensorflow.org/api_docs)\n",
    "10. **The Annotated Transformer:** Recurso del grupo Harvard NLP que explica detalladamente el artículo \"Attention is all you need\" con código de PyTorch. [http://nlp.seas.harvard.edu/2018/04/03/attention.html](https://www.google.com/url?sa=E&source=gmail&q=http://nlp.seas.harvard.edu/2018/04/03/attention.html)\n",
    "11. **DeepMind's Blog on AlphaFold:** Artículo de blog de DeepMind sobre el modelo de predicción de estructura de proteínas AlphaFold, que utiliza técnicas basadas en transformadores. [https://www.deepmind.com/blog/alphafold-a-solution-to-a-50-year-old-grand-challenge-in-biology](https://www.google.com/url?sa=E&source=gmail&q=https://www.deepmind.com/blog/alphafold-a-solution-to-a-50-year-old-grand-challenge-in-biology)\n",
    "12. **LongLoRA:** Método LongLoRA para ajustar eficientemente modelos de lenguaje a gran escala con técnicas de LoRA. [https://arxiv.org/abs/2311.02394](https://www.google.com/url?sa=E&source=gmail&q=https://arxiv.org/abs/2311.02394) \n",
    "\n",
    "Note: The last item (LongLoRA) seems to be a duplicate of the fourth item in the list, which also links to [https://arxiv.org/abs/2311.02394](https://www.google.com/url?sa=E&source=gmail&q=https://arxiv.org/abs/2311.02394). If this is not intended, please clarify or provide the correct reference.\n",
    "1. **Attention Is All You Need (Paper Original del Transformer):** Paper que propuso la estructura básica del modelo de transformador y el mecanismo de atención. [https://arxiv.org/abs/1706.03762](https://www.google.com/url?sa=E&source=gmail&q=https://arxiv.org/abs/1706.03762)\n",
    "2. **FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness:** Paper que propone FlashAttention, que optimiza el cálculo de atención utilizando la jerarquía de memoria de GPU. [https://arxiv.org/abs/2205.14135](https://www.google.com/url?sa=E&source=gmail&q=https://arxiv.org/abs/2205.14135)\n",
    "3. **FlashAttention-v2: Faster Attention with Better Parallelism and Work Partitioning:** Versión mejorada de FlashAttention que ofrece mayor velocidad y procesamiento paralelo mejorado. [https://arxiv.org/abs/2307.08691](https://www.google.com/url?sa=E&source=gmail&q=https://arxiv.org/abs/2307.08691)\n",
    "4. **Scaling Transformer to 1M tokens and beyond with RMT:** Propone cómo escalar la longitud de contexto del modelo de transformador a más de 1M de tokens utilizando Recurrent Memory Transformer (RMT). [https://arxiv.org/abs/2304.11062](https://www.google.com/url?sa=E&source=gmail&q=https://arxiv.org/abs/2304.11062)\n",
    "5. **Constitutional AI: Harmlessness from AI Feedback:** Propone el marco de Constitutional AI para controlar las respuestas del modelo de IA según principios éticos. [https://arxiv.org/abs/2212.08073](https://www.google.com/url?sa=E&source=gmail&q=https://arxiv.org/abs/2212.08073)\n",
    "6. **vLLM: Easy, Fast, and Cheap LLM Serving with PagedAttention:** Introduce la biblioteca vLLM que mejora la velocidad de inferencia y el rendimiento del procesamiento de modelos de lenguaje a gran escala utilizando PagedAttention. [https://arxiv.org/abs/2309.06180](https://www.google.com/url?sa=E&source=gmail&q=https://arxiv.org/abs/2309.06180), [https://vllm.ai/](https://www.google.com/url?sa=E&source=gmail&q=https://vllm.ai/)\n",
    "7. **GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints:** Introduce la técnica GQA para entrenar eficientemente modelos de atención multi-query a partir de puntos de control de atención multi-cabeza. [https://arxiv.org/abs/2305.13245](https://www.google.com/url?sa=E&source=gmail&q=https://arxiv.org/abs/2305.13245)\n",
    "8. **LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models:** Propone el método LongLoRA para afinar eficientemente modelos de lenguaje a gran escala con contexto largo.\n",
    "[https://arxiv.org/abs/2311.02394](https://www.google.com/url?sa=E&source=gmail&q=https://arxiv.org/abs/2311.02394) 9. **Mistral-7B:** Descripción del modelo de lenguaje de alto rendimiento Mistral-7B, que cuenta con 7 mil millones de parámetros. [https://arxiv.org/abs/2310.06825](https://www.google.com/url?sa=E&source=gmail&q=https://arxiv.org/abs/2310.06825)\n",
    "10. **The Illustrated Transformer:** Material de blog que explica fácilmente el funcionamiento del modelo Transformer con ilustraciones. [http://jalammar.github.io/illustrated-transformer/](https://www.google.com/url?sa=E&source=gmail&q=http://jalammar.github.io/illustrated-transformer/)\n",
    "11. **Hugging Face Transformers Documentation:** Documentación oficial de la biblioteca Hugging Face Transformers, que ayuda a usar y aprender modelos Transformer de manera sencilla. [https://huggingface.co/transformers/](https://www.google.com/url?sa=E&source=gmail&q=https://huggingface.co/transformers/)\n",
    "12. **PyTorch Documentation:** Documentación oficial del marco de deep learning PyTorch, que proporciona funciones necesarias para la implementación y aprendizaje de modelos Transformer. [https://pytorch.org/docs/stable/index.html](https://www.google.com/url?sa=E&source=gmail&q=https://pytorch.org/docs/stable/index.html)\n",
    "13. **TensorFlow Documentation:** Documentación oficial del marco de deep learning TensorFlow, que proporciona API para la implementación y aprendizaje de modelos Transformer. [https://www.tensorflow.org/api_docs](https://www.google.com/url?sa=E&source=gmail&q=https://www.tensorflow.org/api_docs)\n",
    "14. **The Annotated Transformer:** Material del grupo Harvard NLP que explica en detalle el artículo \"Attention is all you need\" con código PyTorch. [http://nlp.seas.harvard.edu/2018/04/03/attention.html](https://www.google.com/url?sa=E&source=gmail&q=http://nlp.seas.harvard.edu/2018/04/03/attention.html)\n",
    "15. **DeepMind's Blog on AlphaFold:** Artículo de blog de DeepMind sobre el modelo de predicción de estructura proteica AlphaFold, que es un caso de uso basado en Transformer. [https://www.deepmind.com/blog/alphafold-a-solution-to-a-50-year-old-grand-challenge-in-biology](https://www.google.com/url?sa=E&source=gmail&q=https://www.deepmind.com/blog/alphafold-a-solution-to-a-50-year-old-grand-challenge-in-biology)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
