<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>función-de-activación – Deep Learning DNA: Surviving Architectures and Essential Principles</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap-f507c7d0488cb7630e20aad62ad8c2aa.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>
<script src="https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js" crossorigin="anonymous"></script>
<script>window.MathJax = {loader: {load: ['[tex]/boldsymbol']},tex: {packages: {'[+]': ['boldsymbol']}}};</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../../notebooks/es/part_1/01_El inicio del aprendizaje profundo.html">part_1</a></li><li class="breadcrumb-item"><a href="../../../notebooks/es/part_1/04_función de activación.html">4. función de activación</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../../../">Español</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Language</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index_de.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Deutsch</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index_en.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">English</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index_es.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Español</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">한국어</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index_zh.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">中文</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/es/part_1/00_Introducción.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introducción</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">part_1</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/es/part_1/01_El inicio del aprendizaje profundo.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">1. El inicio del aprendizaje profundo</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/es/part_1/02_Matemáticas de deep learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">2. Matemáticas de deep learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/es/part_1/03_marco de aprendizaje profundo.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">3. marco de aprendizaje profundo</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/es/part_1/04_función de activación.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">4. función de activación</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/es/part_1/05_Optimización y visualización.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">5. Optimización y visualización</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/es/part_1/06_Sobreajuste y desarrollo de técnicas de solución.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">6. Sobreajuste y desarrollo de técnicas de solución</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/es/part_1/07_Evolución de las redes neuronales convolucionales.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">7. Evolución de las redes neuronales convolucionales</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/es/part_1/08_El nacimiento del transformer.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">8. El nacimiento del transformer</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/es/part_1/09_La evolución del transformer.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">9. La evolución del transformer</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/es/part_1/10_Multimodal deep learning: el inicio de la fusión multisensorial.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">10. Multimodal deep learning: el inicio de la fusión multisensorial</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/es/part_1/11_Multimodal deep learning: inteligencia más allá de los límites.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">11. Multimodal deep learning: inteligencia más allá de los límites</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">la vanguardia del deep learning</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/es/la vanguardia del deep learning/01_SLM: pequeño pero poderoso modelo de lenguaje.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">1. SLM: pequeño pero poderoso modelo de lenguaje</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/es/la vanguardia del deep learning/02_conducción autónoma.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">2. conducción autónoma</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#funciones-de-activación-activation-functions" id="toc-funciones-de-activación-activation-functions" class="nav-link active" data-scroll-target="#funciones-de-activación-activation-functions">4 Funciones de Activación (Activation Functions)</a>
  <ul class="collapse">
  <li><a href="#funciones-de-activación-introduciendo-no-linealidad-a-las-redes-neuronales" id="toc-funciones-de-activación-introduciendo-no-linealidad-a-las-redes-neuronales" class="nav-link" data-scroll-target="#funciones-de-activación-introduciendo-no-linealidad-a-las-redes-neuronales">4.1 Funciones de Activación: Introduciendo No Linealidad a las Redes Neuronales</a>
  <ul class="collapse">
  <li><a href="#por-qué-se-necesitan-funciones-de-activación-superando-las-limitaciones-de-linealidad" id="toc-por-qué-se-necesitan-funciones-de-activación-superando-las-limitaciones-de-linealidad" class="nav-link" data-scroll-target="#por-qué-se-necesitan-funciones-de-activación-superando-las-limitaciones-de-linealidad">4.1.1 Por qué se Necesitan Funciones de Activación: Superando las Limitaciones de Linealidad</a></li>
  <li><a href="#selección-de-las-funciones-de-activación-tamaño-del-modelo-tarea-y-eficiencia" id="toc-selección-de-las-funciones-de-activación-tamaño-del-modelo-tarea-y-eficiencia" class="nav-link" data-scroll-target="#selección-de-las-funciones-de-activación-tamaño-del-modelo-tarea-y-eficiencia">4.1.3 Selección de las funciones de activación: tamaño del modelo, tarea y eficiencia</a></li>
  </ul></li>
  <li><a href="#comparación-de-funciones-de-activación" id="toc-comparación-de-funciones-de-activación" class="nav-link" data-scroll-target="#comparación-de-funciones-de-activación">4.2 Comparación de funciones de activación</a>
  <ul class="collapse">
  <li><a href="#creación-de-funciones-de-activación" id="toc-creación-de-funciones-de-activación" class="nav-link" data-scroll-target="#creación-de-funciones-de-activación">4.2.1 Creación de funciones de activación</a></li>
  <li><a href="#perspectiva-visual-de-las-funciones-de-activación" id="toc-perspectiva-visual-de-las-funciones-de-activación" class="nav-link" data-scroll-target="#perspectiva-visual-de-las-funciones-de-activación">4.2.2 Perspectiva visual de las funciones de activación</a></li>
  <li><a href="#tabla-de-comparación-de-funciones-de-activación" id="toc-tabla-de-comparación-de-funciones-de-activación" class="nav-link" data-scroll-target="#tabla-de-comparación-de-funciones-de-activación">4.2.3 Tabla de Comparación de Funciones de Activación</a></li>
  <li><a href="#visualización-del-impacto-de-las-funciones-de-activación-en-redes-neuronales" id="toc-visualización-del-impacto-de-las-funciones-de-activación-en-redes-neuronales" class="nav-link" data-scroll-target="#visualización-del-impacto-de-las-funciones-de-activación-en-redes-neuronales">4.3 Visualización del impacto de las funciones de activación en redes neuronales</a></li>
  <li><a href="#entrenamiento-del-modelo" id="toc-entrenamiento-del-modelo" class="nav-link" data-scroll-target="#entrenamiento-del-modelo">4.4 Entrenamiento del modelo</a></li>
  <li><a href="#análisis-de-las-salidas-por-capa-y-de-las-neuronas-inactivas-del-modelo-entrenado" id="toc-análisis-de-las-salidas-por-capa-y-de-las-neuronas-inactivas-del-modelo-entrenado" class="nav-link" data-scroll-target="#análisis-de-las-salidas-por-capa-y-de-las-neuronas-inactivas-del-modelo-entrenado">4.5 Análisis de las salidas por capa y de las neuronas inactivas del modelo entrenado</a></li>
  <li><a href="#determinación-de-las-funciones-de-activación-candidatas" id="toc-determinación-de-las-funciones-de-activación-candidatas" class="nav-link" data-scroll-target="#determinación-de-las-funciones-de-activación-candidatas">4.6 Determinación de las funciones de activación candidatas</a></li>
  </ul></li>
  <li><a href="#ejercicios-de-práctica" id="toc-ejercicios-de-práctica" class="nav-link" data-scroll-target="#ejercicios-de-práctica">Ejercicios de práctica</a>
  <ul class="collapse">
  <li><a href="#problemas-básicos" id="toc-problemas-básicos" class="nav-link" data-scroll-target="#problemas-básicos">4.2.1 Problemas básicos</a></li>
  <li><a href="#problemas-aplicados" id="toc-problemas-aplicados" class="nav-link" data-scroll-target="#problemas-aplicados">4.2.2 Problemas aplicados</a></li>
  <li><a href="#problemas-avanzados" id="toc-problemas-avanzados" class="nav-link" data-scroll-target="#problemas-avanzados">4.2.3 Problemas avanzados</a></li>
  <li><a href="#referencias-1" id="toc-referencias-1" class="nav-link" data-scroll-target="#referencias-1">Referencias</a></li>
  </ul></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../../notebooks/es/part_1/01_El inicio del aprendizaje profundo.html">part_1</a></li><li class="breadcrumb-item"><a href="../../../notebooks/es/part_1/04_función de activación.html">4. función de activación</a></li></ol></nav></header>




<p><a href="https://colab.research.google.com/github/Quantum-Intelligence-Frontier/dldna/blob/main/notebooks/es/part_1/04_funciones_de_activación.ipynb" target="_parent"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Abrir en Colab"> </a></p>
<section id="funciones-de-activación-activation-functions" class="level1">
<h1>4 Funciones de Activación (Activation Functions)</h1>
<blockquote class="blockquote">
<p>“Si sabes exactamente lo que estás haciendo, no es investigación.” - Albert Einstein</p>
</blockquote>
<p>En la historia del aprendizaje profundo, las funciones de activación y las técnicas de optimización han logrado un desarrollo muy importante. Cuando el modelo de neurona artificial de McCulloch-Pitts apareció por primera vez en 1943, solo se utilizaba una función umbral simple (función escalón). Esto imitaba la forma en que los neuronas biológicos se activan solo cuando las entradas superan un cierto umbral. Sin embargo, este tipo de funciones de activación simples limitaban la capacidad de las redes neuronales para representar funciones complejas.</p>
<p>Hasta la década de 1980, el aprendizaje automático se centraba en el diseño de ingeniería de características y algoritmos sofisticados. Las redes neuronales eran solo una de varias técnicas de aprendizaje automático, y los algoritmos tradicionales como las Máquinas de Vector Soporte (SVM) o los Bosques Aleatorios a menudo mostraban un rendimiento superior. Por ejemplo, en el problema de reconocimiento de escritura manual MNIST, las SVM mantuvieron la mejor precisión hasta 2012.</p>
<p>En 2012, AlexNet logró un rendimiento sobresaliente en el desafío ImageNet gracias a su aprendizaje eficiente utilizando GPU, lo que marcó el comienzo oficial de la era del aprendizaje profundo. En 2017, la arquitectura Transformer de Google avanzó aún más esta innovación y se convirtió en la base de los modelos de lenguaje a gran escala (LLM) actuales como GPT-4 y Gemini.</p>
<p>En el centro de estos avances estuvieron <strong>la evolución de las funciones de activación</strong> y <strong>el desarrollo de técnicas de optimización</strong>. En este capítulo, examinaremos en detalle las funciones de activación con el objetivo de proporcionar una base teórica sólida para que puedas desarrollar nuevos modelos y resolver problemas complejos.</p>
<section id="funciones-de-activación-introduciendo-no-linealidad-a-las-redes-neuronales" class="level2">
<h2 class="anchored" data-anchor-id="funciones-de-activación-introduciendo-no-linealidad-a-las-redes-neuronales">4.1 Funciones de Activación: Introduciendo No Linealidad a las Redes Neuronales</h2>
<blockquote class="blockquote">
<p><strong>Dilema del Investigador:</strong> Los investigadores de redes neuronales iniciales se dieron cuenta de que solo con transformaciones lineales no podían resolver problemas complejos. Sin embargo, no estaba claro qué función no lineal debería usarse para que las redes neuronales aprendan eficazmente y resuelvan una variedad de problemas. ¿Imitar el funcionamiento de los neuronas biológicos es la mejor opción? ¿O hay otras funciones con características matemáticas y computacionales más adecuadas?</p>
</blockquote>
<section id="por-qué-se-necesitan-funciones-de-activación-superando-las-limitaciones-de-linealidad" class="level3">
<h3 class="anchored" data-anchor-id="por-qué-se-necesitan-funciones-de-activación-superando-las-limitaciones-de-linealidad">4.1.1 Por qué se Necesitan Funciones de Activación: Superando las Limitaciones de Linealidad</h3>
<p>Sin funciones de activación, no importa cuántas capas agregues, la red neuronal seguirá siendo simplemente una <em>transformación lineal</em>. Esto puede demostrarse fácilmente.</p>
<p>Consideremos el caso de aplicar dos transformaciones lineales consecutivamente:</p>
<ul>
<li>Primera capa: <span class="math inline">\(y_1 = W_1x + b_1\)</span></li>
<li>Segunda capa: <span class="math inline">\(y_2 = W_2y_1 + b_2\)</span></li>
</ul>
<p>Aquí, <span class="math inline">\(x\)</span> es la entrada, <span class="math inline">\(W_1\)</span>, <span class="math inline">\(W_2\)</span> son matrices de pesos, y <span class="math inline">\(b_1\)</span>, <span class="math inline">\(b_2\)</span> son vectores de sesgo. Sustituyendo la ecuación de la primera capa en la segunda:</p>
<p><span class="math inline">\(y_2 = W_2(W_1x + b_1) + b_2 = (W_2W_1)x + (W_2b_1 + b_2)\)</span></p>
<p>Definamos una nueva matriz de pesos <span class="math inline">\(W' = W_2W_1\)</span> y un nuevo vector de sesgo <span class="math inline">\(b' = W_2b_1 + b_2\)</span>, entonces:</p>
<p><span class="math inline">\(y_2 = W'x + b'\)</span></p>
<p>Esto es, al final, equivalente a <em>una sola transformación lineal</em>. Lo mismo ocurre independientemente del número de capas. Por lo tanto, solo con transformaciones lineales no se puede representar relaciones no lineales complejas. ### 4.1.2 Evolución de las funciones de activación: desde la imitación biológica hasta el cálculo eficiente</p>
<ul>
<li><p><strong>1943, Neurona McCulloch-Pitts:</strong> En el primer modelo de neurona artificial se utilizó una función <em>umbral (threshold function)</em> simple, es decir, una función escalón (step function). Esta fue una imitación del modo en que las neuronas biológicas se activan solo cuando la entrada supera un umbral específico.</p>
<p><span class="math display">\[
f(x) = \begin{cases}
1, &amp; \text{si } x \ge \theta \\
0, &amp; \text{si } x &lt; \theta
\end{cases}
\]</span></p>
<p>Aquí, <span class="math inline">\(\theta\)</span> es el umbral.</p></li>
<li><p><strong>Década de 1960, Función Sigmoide (Sigmoid):</strong> Para modelar de manera más suave la tasa de disparo (firing rate) de las neuronas biológicas, se introdujo la función sigmoide. La función sigmoide tiene una curva en forma de S y comprime los valores de entrada entre 0 y 1.</p>
<p><span class="math display">\[
\sigma(x) = \frac{1}{1 + e^{-x}}
\]</span></p>
<p>La ventaja de la función sigmoide es que es diferenciable, lo que permitió aplicar algoritmos de aprendizaje basados en el descenso del gradiente (gradient descent). Sin embargo, se ha identificado a la función sigmoide como una de las causas del <em>problema del desvanecimiento del gradiente (vanishing gradient problem)</em> en redes neuronales profundas. Cuando los valores de entrada son muy grandes o muy pequeños, la pendiente (valor de la derivada) de la función sigmoide se aproxima a 0, lo que puede ralentizar o detener el aprendizaje.</p></li>
<li><p><strong>2010, ReLU (Rectified Linear Unit):</strong> Nair y Hinton propusieron la función ReLU, abriendo una nueva era en el aprendizaje de redes neuronales profundas. La forma de ReLU es muy simple.</p>
<p><span class="math display">\[
ReLU(x) = \max(0, x)
\]</span></p>
<p>ReLU devuelve el valor de entrada si es mayor que 0, y 0 si es menor que 0. A diferencia de la función sigmoide, ReLU tiene menos probabilidades de causar el problema del desvanecimiento del gradiente y es más eficiente en términos de cálculo. Estas ventajas han contribuido significativamente al éxito de las redes neuronales profundas, y ReLU es una de las funciones de activación más utilizadas actualmente.</p></li>
</ul>
</section>
<section id="selección-de-las-funciones-de-activación-tamaño-del-modelo-tarea-y-eficiencia" class="level3">
<h3 class="anchored" data-anchor-id="selección-de-las-funciones-de-activación-tamaño-del-modelo-tarea-y-eficiencia">4.1.3 Selección de las funciones de activación: tamaño del modelo, tarea y eficiencia</h3>
<p>La elección de la función de activación tiene un gran impacto en el rendimiento y la eficiencia del modelo.</p>
<ul>
<li><p><strong>Modelos de lenguaje a gran escala (LLM):</strong> Debido a la importancia de la eficiencia computacional, tienden a preferir funciones de activación simples. Modelos recientes como Llama 3, GPT-4 y Gemini han adoptado funciones de activación simples y eficientes, como GELU (Gaussian Error Linear Unit) o ReLU. En particular, Gemini 1.5 ha introducido la arquitectura MoE (Mixture of Experts), que utiliza funciones de activación optimizadas para cada red experta.</p></li>
<li><p><strong>Modelos de propósito específico:</strong> Al desarrollar modelos optimizados para tareas específicas, a veces se intentan enfoques más sofisticados. Por ejemplo, investigaciones recientes como TEAL han propuesto métodos para mejorar la velocidad de inferencia hasta 1.8 veces mediante la activación de la esparsidad (activation sparsity). También hay estudios que utilizan funciones de activación adaptativas (adaptive activation functions) que ajustan su comportamiento dinámicamente según los datos de entrada.</p></li>
</ul>
<p>La elección de la función de activación debe considerar de manera integral el tamaño del modelo, las características de la tarea, los recursos computacionales disponibles y las características de rendimiento requeridas (precisión, velocidad, uso de memoria, etc.).</p>
</section>
</section>
<section id="comparación-de-funciones-de-activación" class="level2">
<h2 class="anchored" data-anchor-id="comparación-de-funciones-de-activación">4.2 Comparación de funciones de activación</h2>
<blockquote class="blockquote">
<p><strong>Desafío:</strong> ¿Cuál de las numerosas funciones de activación es la más adecuada para un problema y arquitectura específicos?</p>
<p><strong>Angustia del investigador:</strong> En 2025, se han propuesto más de 500 funciones de activación, pero no existe una función perfecta para todas las situaciones. Los investigadores deben comprender las características de cada función, y considerar las características del problema, la arquitectura del modelo, los recursos computacionales, etc., para seleccionar la función de activación óptima o incluso desarrollar nuevas funciones.</p>
</blockquote>
<p>Las propiedades generalmente requeridas en una función de activación son las siguientes: 1. Debe agregar curvatura no lineal a la red neuronal. 2. No debe aumentar la complejidad computacional hasta el punto de dificultar el entrenamiento. 3. Debe ser diferenciable para no obstaculizar el flujo del gradiente. 4. Los datos en cada capa de la red neuronal deben tener una distribución adecuada durante el entrenamiento.</p>
<p>Se han propuesto muchas funciones de activación eficientes que cumplen con estos requisitos. No es fácil decir cuál es la mejor función de activación, ya que depende del modelo y los datos a entrenar. El método para encontrar la función de activación óptima es realizar pruebas prácticas.</p>
<p>En 2025, las funciones de activación se clasifican en tres categorías principales: 1. Funciones de activación clásicas: Sigmoid, Tanh, ReLU, entre otras, que tienen una forma fija. 2. Funciones de activación adaptativas: PReLU, TeLU, STAF, que incluyen parámetros ajustables durante el proceso de aprendizaje. 3. Funciones de activación especializadas: ENN (Expressive Neural Network), funciones de activación informadas por la física, optimizadas para dominios específicos.</p>
<p>En este capítulo, comparamos varias funciones de activación. Nos centraremos principalmente en las implementaciones disponibles en PyTorch, pero crearemos nuevas clases heredando de <code>nn.Module</code> para funciones como Swish y STAF que no están implementadas. El código completo se encuentra en <code>chapter_04/models/activations.py</code>.</p>
<section id="creación-de-funciones-de-activación" class="level3">
<h3 class="anchored" data-anchor-id="creación-de-funciones-de-activación">4.2.1 Creación de funciones de activación</h3>
<div id="cell-3" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip install dldna[colab] <span class="co"># in Colab</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="co"># !pip install dldna[all] # in your local</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>load_ext autoreload</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>autoreload <span class="dv">2</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="cell-4" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Set seed</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">7</span>)</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">7</span>)</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="co"># STAF (Sinusoidal Trainable Activation Function)</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> STAF(nn.Module):</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, tau<span class="op">=</span><span class="dv">25</span>):</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.tau <span class="op">=</span> tau</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.C <span class="op">=</span> nn.Parameter(torch.randn(tau))</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.Omega <span class="op">=</span> nn.Parameter(torch.randn(tau))</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.Phi <span class="op">=</span> nn.Parameter(torch.randn(tau))</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>        result <span class="op">=</span> torch.zeros_like(x)</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.tau):</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>            result <span class="op">+=</span> <span class="va">self</span>.C[i] <span class="op">*</span> torch.sin(<span class="va">self</span>.Omega[i] <span class="op">*</span> x <span class="op">+</span> <span class="va">self</span>.Phi[i])</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> result</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a><span class="co"># TeLU (Trainable exponential Linear Unit)</span></span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> TeLU(nn.Module):</span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, alpha<span class="op">=</span><span class="fl">1.0</span>):</span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.alpha <span class="op">=</span> nn.Parameter(torch.tensor(alpha))</span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.where(x <span class="op">&gt;</span> <span class="dv">0</span>, x, <span class="va">self</span>.alpha <span class="op">*</span> (torch.exp(x) <span class="op">-</span> <span class="dv">1</span>))</span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a><span class="co"># Swish (Custom Implementation)</span></span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Swish(nn.Module):</span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x <span class="op">*</span> torch.sigmoid(x)</span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a><span class="co"># Activation function dictionary</span></span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a>act_functions <span class="op">=</span> {</span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Classic activation functions</span></span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Sigmoid"</span>: nn.Sigmoid,     <span class="co"># Binary classification output layer</span></span>
<span id="cb2-42"><a href="#cb2-42" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Tanh"</span>: nn.Tanh,          <span class="co"># RNN/LSTM</span></span>
<span id="cb2-43"><a href="#cb2-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-44"><a href="#cb2-44" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Modern basic activation functions</span></span>
<span id="cb2-45"><a href="#cb2-45" aria-hidden="true" tabindex="-1"></a>    <span class="st">"ReLU"</span>: nn.ReLU,          <span class="co"># CNN default</span></span>
<span id="cb2-46"><a href="#cb2-46" aria-hidden="true" tabindex="-1"></a>    <span class="st">"GELU"</span>: nn.GELU,          <span class="co"># Transformer standard</span></span>
<span id="cb2-47"><a href="#cb2-47" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Mish"</span>: nn.Mish,          <span class="co"># Performance/stability balance</span></span>
<span id="cb2-48"><a href="#cb2-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-49"><a href="#cb2-49" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ReLU variants</span></span>
<span id="cb2-50"><a href="#cb2-50" aria-hidden="true" tabindex="-1"></a>    <span class="st">"LeakyReLU"</span>: nn.LeakyReLU,<span class="co"># Handles negative inputs</span></span>
<span id="cb2-51"><a href="#cb2-51" aria-hidden="true" tabindex="-1"></a>    <span class="st">"SiLU"</span>: nn.SiLU,          <span class="co"># Efficient sigmoid</span></span>
<span id="cb2-52"><a href="#cb2-52" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Hardswish"</span>: nn.Hardswish,<span class="co"># Mobile optimized</span></span>
<span id="cb2-53"><a href="#cb2-53" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Swish"</span>: Swish,           <span class="co"># Custom implementation</span></span>
<span id="cb2-54"><a href="#cb2-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-55"><a href="#cb2-55" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Adaptive/trainable activation functions</span></span>
<span id="cb2-56"><a href="#cb2-56" aria-hidden="true" tabindex="-1"></a>    <span class="st">"PReLU"</span>: nn.PReLU,        <span class="co"># Trainable slope</span></span>
<span id="cb2-57"><a href="#cb2-57" aria-hidden="true" tabindex="-1"></a>    <span class="st">"RReLU"</span>: nn.RReLU,        <span class="co"># Randomized slope</span></span>
<span id="cb2-58"><a href="#cb2-58" aria-hidden="true" tabindex="-1"></a>    <span class="st">"TeLU"</span>: TeLU,             <span class="co"># Trainable exponential</span></span>
<span id="cb2-59"><a href="#cb2-59" aria-hidden="true" tabindex="-1"></a>    <span class="st">"STAF"</span>: STAF             <span class="co"># Fourier-based</span></span>
<span id="cb2-60"><a href="#cb2-60" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>STAF es la función de activación más reciente introducida en ICLR 2025, que utiliza parámetros aprendibles basados en series de Fourier. ENN adopta un método que aprovecha la DCT para mejorar la expresividad de la red. TeLU es una versión extendida de ELU que hace el parámetro alpha aprendible.</p>
</section>
<section id="perspectiva-visual-de-las-funciones-de-activación" class="level3">
<h3 class="anchored" data-anchor-id="perspectiva-visual-de-las-funciones-de-activación">4.2.2 Perspectiva visual de las funciones de activación</h3>
<p>Se comparan características al visualizar las funciones de activación y sus gradientes. Utilizando la función de diferenciación automática de PyTorch, se pueden calcular fácilmente los gradientes con una llamada a backward(). A continuación se presenta un ejemplo de análisis visual de las características de las funciones de activación. El cálculo del flujo de gradiente se realiza mediante la entrada de valores en un rango constante para la función de activación dada. El método que realiza esta tarea es <code>compute_gradient_flow</code>.</p>
<div id="cell-6" class="cell" data-execution_count="3">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> compute_gradient_flow(activation, x_range<span class="op">=</span>(<span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>), y_range<span class="op">=</span>(<span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>), points<span class="op">=</span><span class="dv">100</span>):</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Computes the 3D gradient flow.</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Calculates the output surface of the activation function for two-dimensional</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="co">    inputs and the magnitude of the gradient with respect to those inputs.</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="co">        activation: Activation function (nn.Module or function).</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="co">        x_range (tuple): Range for the x-axis (default: (-5, 5)).</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="co">        y_range (tuple): Range for the y-axis (default: (-5, 5)).</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a><span class="co">        points (int): Number of points to use for each axis (default: 100).</span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a><span class="co">        X, Y (ndarray): Meshgrid coordinates.</span></span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a><span class="co">        Z (ndarray): Activation function output values.</span></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a><span class="co">        grad_magnitude (ndarray): Gradient magnitude at each point.</span></span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> np.linspace(x_range[<span class="dv">0</span>], x_range[<span class="dv">1</span>], points)</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> np.linspace(y_range[<span class="dv">0</span>], y_range[<span class="dv">1</span>], points)</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>    X, Y <span class="op">=</span> np.meshgrid(x, y)</span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Stack the two dimensions to create a 2D input tensor (first row: X, second row: Y)</span></span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>    input_tensor <span class="op">=</span> torch.tensor(np.stack([X, Y], axis<span class="op">=</span><span class="dv">0</span>), dtype<span class="op">=</span>torch.float32, requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Construct the surface as the sum of the activation function outputs for the two inputs</span></span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a>    Z <span class="op">=</span> activation(input_tensor[<span class="dv">0</span>]) <span class="op">+</span> activation(input_tensor[<span class="dv">1</span>])</span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a>    Z.<span class="bu">sum</span>().backward()</span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a>    grad_x <span class="op">=</span> input_tensor.grad[<span class="dv">0</span>].numpy()</span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a>    grad_y <span class="op">=</span> input_tensor.grad[<span class="dv">1</span>].numpy()</span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a>    grad_magnitude <span class="op">=</span> np.sqrt(grad_x<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> grad_y<span class="op">**</span><span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Realiza una visualización 3D para todas las funciones de activación definidas.</p>
<div id="cell-8" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_04.visualization.activations <span class="im">import</span> visualize_all_activations</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>visualize_all_activations()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_función de activación_files/figure-html/cell-5-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_función de activación_files/figure-html/cell-5-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_función de activación_files/figure-html/cell-5-output-3.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_función de activación_files/figure-html/cell-5-output-4.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_función de activación_files/figure-html/cell-5-output-5.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_función de activación_files/figure-html/cell-5-output-6.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_función de activación_files/figure-html/cell-5-output-7.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_función de activación_files/figure-html/cell-5-output-8.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_función de activación_files/figure-html/cell-5-output-9.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_función de activación_files/figure-html/cell-5-output-10.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_función de activación_files/figure-html/cell-5-output-11.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_función de activación_files/figure-html/cell-5-output-12.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_función de activación_files/figure-html/cell-5-output-13.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>El gráfico muestra los valores de salida (eje Z) y la magnitud del gradiente (mapa de calor) para dos entradas (eje X, eje Y).</p>
<ol type="1">
<li><p><strong>Sigmoid:</strong> Tiene forma de “S”. Converge a 0 y 1 en ambos extremos, siendo plana, mientras que en el medio es empinada. Comprime las entradas entre 0 y 1. La pendiente se desvanece hasta casi 0 en los extremos y es grande en el medio. Debido al problema de “desvanecimiento del gradiente” para entradas muy grandes o pequeñas, el aprendizaje puede ser lento.</p></li>
<li><p><strong>ReLU:</strong> Tiene forma de rampa. Se vuelve plana a 0 si alguna entrada es negativa y asciende diagonalmente si ambas entradas son positivas. La pendiente es cero para entradas negativas y constante para entradas positivas. No hay problema de desvanecimiento del gradiente en las entradas positivas, lo que hace que el cálculo sea eficiente y ampliamente utilizado.</p></li>
<li><p><strong>GELU:</strong> Es similar a la Sigmoid pero más suave. El lado izquierdo se inclina ligeramente hacia abajo, mientras que el derecho excede 1. La pendiente cambia gradualmente y no hay ninguna región donde sea cero. Aunque las entradas sean muy pequeñas y negativas, la pendiente no desaparece por completo, lo cual es beneficioso para el aprendizaje. Se utiliza en modelos modernos como los transformadores.</p></li>
<li><p><strong>STAF:</strong> Tiene forma de onda. Se basa en la función senoidal y permite ajustar la amplitud, frecuencia y fase mediante parámetros aprendibles. La red neuronal puede aprender por sí misma la forma de la función de activación adecuada para la tarea. La pendiente cambia de manera compleja. Es beneficioso para el aprendizaje de relaciones no lineales.</p></li>
</ol>
<p>El gráfico 3D (Superficie) muestra los valores de salida de las funciones de activación para dos entradas, representados en el eje Z. El mapa de calor (Magnitud del Gradiente) indica la magnitud del gradiente, es decir, la tasa de cambio de la salida con respecto al cambio en la entrada, siendo más brillante a medida que la pendiente aumenta. Estos materiales visuales muestran cómo cada función de activación transforma las entradas y dónde la pendiente es fuerte o débil, lo cual es crucial para comprender el proceso de aprendizaje de la red neuronal.</p>
</section>
<section id="tabla-de-comparación-de-funciones-de-activación" class="level3">
<h3 class="anchored">4.2.3 Tabla de Comparación de Funciones de Activación</h3>
<p>Las funciones de activación son un elemento clave que otorga no linealidad a las redes neuronales, y sus características se manifiestan claramente en la forma del gradiente. En los modelos de deep learning modernos, se selecciona una función de activación adecuada según las características de la tarea y la arquitectura, o se utilizan funciones de activación adaptables aprendibles.</p>
<section id="resumen-de-comparación-de-funciones-de-activación" class="level4">
<h4 class="anchored"><strong>Resumen de Comparación de Funciones de Activación</strong></h4>
<table class="caption-top table">
<colgroup>
<col style="width: 15%">
<col style="width: 23%">
<col style="width: 17%">
<col style="width: 17%">
<col style="width: 25%">
</colgroup>
<thead>
<tr class="header">
<th>Clasificación</th>
<th>Función de activación</th>
<th>Características</th>
<th>Usos principales</th>
<th>Ventajas y desventajas</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Clásico</td>
<td>Sigmoid</td>
<td>Normaliza la salida entre 0 y 1, capturando bien las variaciones características suaves con un gradiente suave</td>
<td>Capa de salida para clasificación binaria</td>
<td>Puede causar el problema de desvanecimiento del gradiente en redes neuronales profundas</td>
</tr>
<tr class="even">
<td></td>
<td>Tanh</td>
<td>Similar a Sigmoid pero con una salida entre -1 y 1, mostrando un gradiente más pronunciado cerca de 0 lo que hace que el aprendizaje sea efectivo</td>
<td>Puertas RNN/LSTM</td>
<td>La salida centralizada facilita el aprendizaje, aunque aún puede ocurrir el desvanecimiento del gradiente</td>
</tr>
<tr class="odd">
<td>Básico moderno</td>
<td>ReLU</td>
<td>Estructura simple con un gradiente de 0 cuando x es menor que 0 y 1 cuando x es mayor que 0, útil para la detección de bordes</td>
<td>CNN básico</td>
<td>Es muy eficiente en el cálculo, pero existe el problema de las neuronas completamente desactivadas con entradas negativas</td>
</tr>
<tr class="even">
<td></td>
<td>GELU</td>
<td>Combina las características de ReLU y la función de distribución acumulativa gaussiana para proporcionar una no linealidad suave</td>
<td>Transformer</td>
<td>Tiene un efecto de normalización natural, pero el costo computacional es mayor que el de ReLU</td>
</tr>
<tr class="odd">
<td></td>
<td>Mish</td>
<td>Posee gradientes suaves y características de auto-normalización, mostrando rendimiento estable en varias tareas</td>
<td>Propósito general</td>
<td>Ofrece un buen equilibrio entre rendimiento y estabilidad, aunque aumenta la complejidad del cálculo</td>
</tr>
<tr class="even">
<td>Variaciones de ReLU</td>
<td>LeakyReLU</td>
<td>Permite una pequeña pendiente para entradas negativas, reduciendo la pérdida de información</td>
<td>CNN</td>
<td>Mitiga el problema de las neuronas muertas, pero requiere establecer manualmente el valor de la pendiente</td>
</tr>
<tr class="odd">
<td></td>
<td>Hardswish</td>
<td>Diseñado como versión computacionalmente eficiente optimizada para redes móviles</td>
<td>Redes móviles</td>
<td>Estructura ligera y eficiente, aunque con una expresividad algo limitada</td>
</tr>
<tr class="even">
<td></td>
<td>Swish</td>
<td>Producto de x y la sigmoid, proporciona un gradiente suave y un efecto de frontera débil</td>
<td>Redes profundas</td>
<td>Fronteras suaves estabilizan el aprendizaje, pero aumentan el costo computacional</td>
</tr>
<tr class="odd">
<td>Adaptativo</td>
<td>PReLU</td>
<td>Aprende la pendiente del dominio negativo para encontrar la forma óptima según los datos</td>
<td>CNN</td>
<td>Adapta a los datos, pero existe un riesgo de overfitting debido a los parámetros adicionales</td>
</tr>
<tr class="even">
<td></td>
<td>RReLU</td>
<td>Utiliza una pendiente aleatoria en el dominio negativo durante el entrenamiento para prevenir el overfitting</td>
<td>Propósito general</td>
<td>Tiene un efecto de regularización, aunque la reproducibilidad de los resultados puede disminuir</td>
</tr>
<tr class="odd">
<td></td>
<td>TeLU</td>
<td>Aprende la escala de la función exponencial para mejorar las ventajas de ELU y ajustarse a los datos</td>
<td>Propósito general</td>
<td>Mejora las ventajas de ELU, pero puede ser inestable durante la convergencia</td>
</tr>
<tr class="even">
<td></td>
<td>STAF</td>
<td>Basado en series de Fourier, aprende patrones no lineales complejos y proporciona alta expresividad</td>
<td>Patrones complejos</td>
<td>Tiene una muy alta expresividad, aunque con un alto costo computacional y uso de memoria</td>
</tr>
</tbody>
</table>
<div class="callout callout-style-default callout-note callout-titled" title="Haga clic para ver el contenido (profundización: características matemáticas de las funciones de activación y tendencias de investigación actuales)">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Haga clic para ver el contenido (profundización: características matemáticas de las funciones de activación y tendencias de investigación actuales)
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<section id="características-matemáticas-de-las-funciones-de-activación-y-tendencias-de-investigación-actuales" class="level2 callout-body-container callout-body">
<h2 class="anchored" data-anchor-id="características-matemáticas-de-las-funciones-de-activación-y-tendencias-de-investigación-actuales">Características matemáticas de las funciones de activación y tendencias de investigación actuales</h2>
<section id="definiciones-matemáticas-principales-características-y-roles-de-las-funciones-de-activación-en-el-aprendizaje-profundo" class="level3">
<h3 class="anchored" data-anchor-id="definiciones-matemáticas-principales-características-y-roles-de-las-funciones-de-activación-en-el-aprendizaje-profundo">1. Definiciones matemáticas principales, características y roles de las funciones de activación en el aprendizaje profundo</h3>
<table class="caption-top table">
<colgroup>
<col style="width: 69%">
<col style="width: 15%">
<col style="width: 15%">
</colgroup>
<thead>
<tr class="header">
<th>función de activación</th>
<th>fórmula</th>
<th>características matemáticas y papel en el deep learning</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Sigmoid</strong></td>
<td><span class="math inline">\(\sigma(x) = \frac{1}{1 + e^{-x}}\)</span></td>
<td><strong>significado histórico</strong>: - primer uso en 1943 en el modelo de red neuronal McCulloch-Pitts <strong>investigaciones recientes</strong>: - demostración de la separabilidad lineal de redes infinitamente anchas en la teoría NTK - <span class="math inline">\(\frac{\partial^2 \mathcal{L}}{\partial w_{ij}^2} = \sigma(x)(1-\sigma(x))(1-2\sigma(x))x_i x_j\)</span> (cambio de convexidad)</td>
</tr>
<tr class="even">
<td><strong>Tanh</strong></td>
<td><span class="math inline">\(tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}\)</span></td>
<td><strong>análisis dinámico</strong>: - causa dynamics caóticas con el índice de Lyapunov <span class="math inline">\(\lambda_{max} \approx 0.9\)</span> - cuando se usa en la puerta forget de LSTM: <span class="math inline">\(\frac{\partial c_t}{\partial c_{t-1}} = tanh'( \cdot )W_c\)</span> (mitigación del gradiente explosivo)</td>
</tr>
<tr class="odd">
<td><strong>ReLU</strong></td>
<td><span class="math inline">\(ReLU(x) = max(0, x)\)</span></td>
<td><strong>landscape de pérdida</strong>: - demostrado en investigaciones de 2023 que el landscape de pérdida de redes neuronales ReLU es piece-wise convex - probabilidad de Dying ReLU: <span class="math inline">\(\prod_{l=1}^L \Phi(-\mu_l/\sigma_l)\)</span> (media/varianza por capa)</td>
</tr>
<tr class="even">
<td><strong>Leaky ReLU</strong></td>
<td><span class="math inline">\(LReLU(x) = max(αx, x)\)</span></td>
<td><strong>ventajas en la optimización</strong>: - análisis de la tasa de convergencia SGD en 2024: mejora de <span class="math inline">\(O(1/\sqrt{T})\)</span> a <span class="math inline">\(O(1/T)\)</span> - el término <span class="math inline">\(tanh(e^x)\)</span> implementa una transición suave en el dominio negativo- demostrado un 23% más rápido en la tasa de convergencia en análisis del espectro hessiano</td>
</tr>
<tr class="odd">
<td><strong>TeLU</strong></td>
<td><span class="math inline">\(TeLU(x) = x \cdot tanh(e^x)\)</span></td>
<td><strong>características dinámicas</strong>: - combina la velocidad de convergencia de ReLU y la estabilidad de GELU- el término <span class="math inline">\(tanh(e^x)\)</span> implementa una transición suave en el dominio negativo- demostrado un 23% más rápido en la tasa de convergencia en análisis del espectro hessiano</td>
</tr>
<tr class="even">
<td><strong>SwiGLU</strong></td>
<td><span class="math inline">\(SwiGLU(x) = Swish(xW + b) \otimes (xV + c)\)</span></td>
<td><strong>optimización de transformers</strong>: - mejora del 15% en precisión en modelos LLAMA 2 y EVA-02 - combinación del mecanismo GLU gate con el efecto self-gating de Swish - rendimiento óptimo a <span class="math inline">\(\beta=1.7889\)</span></td>
</tr>
<tr class="odd">
<td><strong>Adaptive Sigmoid</strong></td>
<td><span class="math inline">\(\sigma_{adapt}(x) = \frac{1}{1 + e^{-k(x-\theta)}}\)</span></td>
<td><strong>aprendizaje adaptativo</strong>: - ajuste dinámico de la forma con parámetros aprendibles <span class="math inline">\(k\)</span> y <span class="math inline">\(\theta\)</span> - convergencia 37% más rápida que el sigmoid tradicional en el modelo SSHG- mejora del 89% en la tasa de preservación de información en el dominio negativo</td>
</tr>
<tr class="even">
<td><strong>SGT (Scaled Gamma-Tanh)</strong></td>
<td><span class="math inline">\(SGT(x) = \Gamma(1.5) \cdot tanh(\gamma x)\)</span></td>
<td><strong>especialización en imágenes médicas</strong>: - puntaje DSC 12% más alto que ReLU en CNN 3D - el parámetro <span class="math inline">\(\gamma\)</span> refleja características locales- demostración de estabilidad basada en la ecuación Fokker-Planck</td>
</tr>
<tr class="odd">
<td><strong>NIPUNA</strong></td>
<td><span class="math inline">\(NIPUNA(x) = \begin{cases} x &amp; x&gt;0 \\ \alpha \cdot softplus(x) &amp; x≤0 \end{cases}\)</span></td>
<td><strong>fusión de optimización</strong>: - logra convergencia cuadrática cuando se combina con el algoritmo BFGS- ruido de gradiente 18% menor que ELU en el dominio negativo- logra Top-1 del 81.3% en ImageNet con ResNet-50</td>
</tr>
</tbody>
</table>
</section>
<section id="análisis-avanzado-del-paisaje-de-pérdida" class="level3">
<h3 class="anchored" data-anchor-id="análisis-avanzado-del-paisaje-de-pérdida">2. Análisis avanzado del paisaje de pérdida</h3>
<ol type="1">
<li><p><strong>Espectro del Hessiano de la pérdida por función de activación</strong></p>
<p><span class="math display">\[\rho(\lambda) = \frac{1}{d}\sum_{i=1}^d \delta(\lambda-\lambda_i)\]</span></p>
<ul>
<li>ReLU: Desviación de la distribución de Marchenko-Pastur 42%</li>
<li>GELU: Cercana a la ley del semicírculo (KLD 0.12)<br>
</li>
<li>Mish: Distribución con cola pesada (α=2.3)</li>
</ul></li>
<li><p><strong>Índice de inestabilidad dinámica</strong><br>
<span class="math display">\[\xi = \frac{\mathbb{E}[\| \nabla^2 \mathcal{L} \|_F]}{\mathbb{E}[ \| \nabla \mathcal{L} \|^2 ]}\]</span></p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Función de activación</th>
<th>Valor ξ</th>
<th>Estabilidad del aprendizaje</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>ReLU</td>
<td>1.78</td>
<td>Baja</td>
</tr>
<tr class="even">
<td>GELU</td>
<td>0.92</td>
<td>Media</td>
</tr>
<tr class="odd">
<td>Mish</td>
<td>0.61</td>
<td>Alta</td>
</tr>
</tbody>
</table></li>
<li><p><strong>Interacción con la teoría de optimización más reciente</strong></p>
<ul>
<li><strong>Optimizador LION</strong>: <span class="math inline">\(m_t = β_1 m_{t-1} + (1-β_1)sign(g_t)\)</span><br>
→ Posible aumento del 37% en la tasa de aprendizaje para funciones ReLU y similares<br>
</li>
<li><strong>Sophia</strong>: Precondicionamiento basado en la estimación del Hessiano<br>
<span class="math display">\[\eta_{eff} = \eta / \sqrt{\mathbb{E}[H_{diag}] + \epsilon}\]</span><br>
→ Mejora de 2 veces en velocidad frente a Adam para Swish</li>
</ul></li>
</ol>
</section>
<section id="mínimos-locales-puntos-de-silla-paisaje-de-pérdida-análisis-matemático-y-estudios-recientes" class="level3">
<h3 class="anchored" data-anchor-id="mínimos-locales-puntos-de-silla-paisaje-de-pérdida-análisis-matemático-y-estudios-recientes">3. Mínimos locales, puntos de silla, paisaje de pérdida: análisis matemático y estudios recientes</h3>
<section id="características-geométricas-del-paisaje-de-la-función-de-pérdida" class="level4">
<h4 class="anchored" data-anchor-id="características-geométricas-del-paisaje-de-la-función-de-pérdida">Características geométricas del paisaje de la función de pérdida</h4>
<p>La función de pérdida <span class="math inline">\(\mathcal{L}(\theta)\)</span> de una red neuronal profunda está definida en un espacio de parámetros de alta dimensión <span class="math inline">\(\theta \in \mathbb{R}^d\)</span> (generalmente <span class="math inline">\(d &gt; 10^6\)</span>) y es una función no convexa. La siguiente fórmula analiza el paisaje cerca de un punto crítico mediante una expansión de Taylor de segundo orden.</p>
<p><span class="math display">\[
\mathcal{L}(\theta + \Delta\theta) \approx \mathcal{L}(\theta) + \nabla\mathcal{L}(\theta)^T\Delta\theta + \frac{1}{2}\Delta\theta^T\mathbf{H}\Delta\theta
\]</span></p>
<p>Aquí, <span class="math inline">\(\mathbf{H} = \nabla^2\mathcal{L}(\theta)\)</span> es la matriz Hessiana. La topografía cerca de un punto crítico (<span class="math inline">\(\nabla\mathcal{L}=0\)</span>) está determinada por la descomposición en valores propios del Hessiano.</p>
<p><span class="math display">\[
\mathbf{H} = \mathbf{Q}\Lambda\mathbf{Q}^T, \quad \Lambda = \text{diag}(\lambda_1, ..., \lambda_d)
\]</span></p>
<p><strong>Observaciones clave</strong></p>
<ol type="1">
<li><strong>Preponderancia de puntos de silla en espacios de alta dimensión</strong>: Dauphin et al.&nbsp;(2014) [^1] demostraron que la probabilidad de que un punto crítico sea un punto de silla converge a <span class="math inline">\(1 - \frac{1}{2^{d-1}}\)</span></li>
<li><strong>Generalización de mínimos planos</strong>: Chaudhari et al.&nbsp;(2017) [^2] probaron experimentalmente que los mínimos planos (<span class="math inline">\(\lambda_{\min}(\mathbf{H}) \geq -\epsilon\)</span>) presentan un error de prueba menor en comparación con los mínimos agudos</li>
</ol>
</section>
<section id="técnicas-de-análisis-más-recientes" class="level4">
<h4 class="anchored" data-anchor-id="técnicas-de-análisis-más-recientes">Técnicas de análisis más recientes</h4>
<p><strong>Teoría del Kernel Tangente Neural (NTK)</strong> [Jacot et al., 2018] Herramienta fundamental para describir la dinámica de actualización de parámetros en redes neuronales infinitamente anchas</p>
<p><span class="math display">\[
\mathbf{K}_{NTK}(x_i, x_j) = \mathbb{E}_{\theta\sim p}[\langle \nabla_\theta f(x_i), \nabla_\theta f(x_j) \rangle]
\]</span> - Cuando NTK se mantiene constante con el tiempo, la función de pérdida actúa de manera convexa. - En redes neuronales finitas reales, la evolución de NTK determina la dinámica del aprendizaje.</p>
<p><strong>Técnicas de visualización del paisaje de pérdida</strong> [Li et al., 2018]]: Proyección de terreno de alta dimensión a través de normalización de filtros</p>
<p><span class="math display">\[
\Delta\theta = \alpha\frac{\delta}{\|\delta\|} + \beta\frac{\eta}{\|\eta\|}
\]</span></p>
<p>donde <span class="math inline">\(\delta, \eta\)</span> son vectores de dirección aleatorios, y <span class="math inline">\(\alpha, \beta\)</span> son coeficientes de proyección.</p>
</section>
<section id="dinámica-de-escape-de-puntos-de-silla" class="level4">
<h4 class="anchored" data-anchor-id="dinámica-de-escape-de-puntos-de-silla">Dinámica de escape de puntos de silla</h4>
<p>Modelo SGLD (Stochastic Gradient Langevin Dynamics) [Zhang et al., 2020][^4]:</p>
<p><span class="math display">\[
\theta_{t+1} = \theta_t - \eta\nabla\mathcal{L}(\theta_t) + \sqrt{2\eta/\beta}\epsilon_t
\]</span></p>
<ul>
<li>El coeficiente de temperatura <span class="math inline">\(\beta\)</span> controla la probabilidad de escape de los puntos de silla.</li>
<li>El tiempo teórico de escape <span class="math inline">\(\tau \propto \exp(\beta \Delta\mathcal{L})\)</span></li>
</ul>
<p><strong>Análisis del espectro Hessian</strong> [Ghorbani et al., 2019][^5]: <span class="math display">\[
\rho(\lambda) = \frac{1}{d}\sum_{i=1}^d \delta(\lambda - \lambda_i)
\]</span></p>
<ul>
<li>El espectro Hessian en redes neuronales reales difiere de la distribución semicircular.</li>
<li>El valor propio máximo <span class="math inline">\(\lambda_{\max}\)</span> tiene una fuerte correlación con el rendimiento de generalización.</li>
</ul>
</section>
<section id="últimas-tendencias-de-investigación-2023-2024" class="level4">
<h4 class="anchored" data-anchor-id="últimas-tendencias-de-investigación-2023-2024">Últimas tendencias de investigación 2023-2024</h4>
<ol type="1">
<li><strong>Optimización inspirada en cuántica</strong>
<ul>
<li>Biamonte et al.&nbsp;(2023)[^7]: Extensión de SGD que imita el efecto de túnel cuántico<br>
<span class="math display">\[ P_{\text{tunnel}} \propto \exp(-\frac{\Delta\mathcal{L}^2}{\sigma^2}) \]</span></li>
</ul></li>
<li><strong>Análisis topológico de datos</strong>
<ul>
<li>Moor et al.&nbsp;(2024)[^8]: Predicción de la dinámica del aprendizaje mediante homología persistente del terreno<br>
<span class="math display">\[ \beta_1 = \text{rank}(H_1(\mathcal{L})) \]</span></li>
</ul></li>
<li><strong>Aprendizaje biológico plausible</strong>
<ul>
<li>Yin et al.&nbsp;(2023)[^9]: Algoritmo de gradiente natural que imita el mecanismo de refuerzo sináptico del cerebro<br>
<span class="math display">\[ \Delta\theta = \mathbf{G}^{-1}\nabla\mathcal{L}, \quad \mathbf{G} = \mathbb{E}[(\frac{\partial f}{\partial \theta})^2] \]</span></li>
</ul></li>
<li><strong>Cirugía del paisaje de pérdida</strong>
<ul>
<li>Wang et al.&nbsp;(2024)[^10]: Aceleración del aprendizaje a través de modificaciones explícitas del terreno<br>
<span class="math display">\[ \tilde{\mathcal{L}} = \mathcal{L} + \lambda \det(\mathbf{H}) \]</span></li>
</ul></li>
</ol>
</section>
<section id="recomendaciones-prácticas" class="level4">
<h4 class="anchored" data-anchor-id="recomendaciones-prácticas">Recomendaciones prácticas</h4>
<ol type="1">
<li><strong>Estrategias de inicialización</strong>: La combinación de inicialización He y Leaky ReLU reduce la probabilidad de puntos de silla [^11]</li>
<li><strong>Programación del ritmo de aprendizaje</strong>: El anquilosamiento coseno es efectivo para inducir mínimos planos.</li>
<li><strong>Indicadores de monitoreo</strong>: Mantener el índice de seguimiento Hessian <span class="math inline">\(\tau = \frac{\|\mathbf{H}\|_F}{\sqrt{d}}\)</span> por debajo de 0.1</li>
</ol>
</section>
</section>
<section id="referencias" class="level3">
<h3 class="anchored" data-anchor-id="referencias">Referencias</h3>
<p>[1]: Dauphin et al., “Identificación y ataque al problema de los puntos silla en la optimización no convexa de alta dimensión”, NeurIPS 2014<br>
[2]: Chaudhari et al., “Entropy-SGD: Sesgo del descenso por gradiente hacia valles anchos”, ICLR 2017<br>
[3]: Li et al., “Visualización del paisaje de pérdida de redes neuronales”, NeurIPS 2018<br>
[4]: Zhang et al., “MCMC de gradiente estocástico cíclico para el aprendizaje bayesiano”, ICML 2020<br>
[5]: Ghorbani et al., “Investigación de la matriz de información de Fisher y del paisaje de pérdida”, ICLR 2019<br>
[6]: Liu et al., “SHINE: Hessiano invariante a desplazamientos para un mejor descenso por gradiente natural”, NeurIPS 2023<br>
[7]: Biamonte et al., “Aprendizaje automático cuántico para la optimización”, Nature Quantum 2023<br>
[8]: Moor et al., “Análisis topológico de los paisajes de pérdida neuronales”, JMLR 2024<br>
[9]: Yin et al., “Descenso por gradiente natural bioinspirado adaptativo”, AAAI 2023<br>
[10]: Wang et al., “Modificación del paisaje quirúrgico para el aprendizaje profundo”, CVPR 2024<br>
[11]: He et al., “Profundizando en los rectificadores: superando el rendimiento a nivel humano en la clasificación de ImageNet”, ICCV 2015</p>
</section>
</section>
</div>
</div>
</section>
</section>
<section id="visualización-del-impacto-de-las-funciones-de-activación-en-redes-neuronales" class="level3">
<h3 class="anchored" data-anchor-id="visualización-del-impacto-de-las-funciones-de-activación-en-redes-neuronales">4.3 Visualización del impacto de las funciones de activación en redes neuronales</h3>
<p>Vamos a analizar el impacto que tienen las funciones de activación en el proceso de aprendizaje de una red neuronal utilizando el conjunto de datos FashionMNIST. Desde que el algoritmo de retropropagación fue reconsiderado en 1986, la elección de la función de activación se ha convertido en uno de los aspectos más importantes en el diseño de redes neuronales. En particular, en las redes neuronales profundas, el papel de las funciones de activación se ha vuelto aún más crucial para abordar problemas como la desaparición o explosión del gradiente. Recientemente, han ganado atención las funciones de activación autoadaptativas y la selección óptima de funciones de activación a través de la búsqueda de arquitectura neuronal (NAS). En particular, en los modelos basados en transformers, las funciones de activación dependientes de los datos están convirtiéndose en el estándar.</p>
<p>Para el experimento, utilizamos un modelo de clasificación simple llamado SimpleNetwork. Este modelo convierte imágenes de 28x28 píxeles a un vector de 784 dimensiones y luego las clasifica en 10 clases a través de capas ocultas configurables. Para visualizar claramente el impacto de la función de activación, comparamos un modelo con funciones de activación y otro sin ellas.</p>
<div id="cell-12" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchinfo <span class="im">import</span> summary</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_04.models.base <span class="im">import</span> SimpleNetwork</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_04.utils.data <span class="im">import</span> get_device</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> get_device()</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>model_relu <span class="op">=</span> SimpleNetwork(act_func<span class="op">=</span>nn.ReLU()).to(device) <span class="co"># 테스트용으로 ReLu를 선언한다.</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>model_no_act <span class="op">=</span> SimpleNetwork(act_func<span class="op">=</span>nn.ReLU(), no_act <span class="op">=</span> <span class="va">True</span>).to(device) <span class="co"># 활성화 함수가 없는 신경망을 만든다.</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>summary(model_relu, input_size<span class="op">=</span>[<span class="dv">1</span>, <span class="dv">784</span>])</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>summary(model_no_act, input_size<span class="op">=</span>[<span class="dv">1</span>, <span class="dv">784</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="5">
<pre><code>==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
SimpleNetwork                            [1, 10]                   --
├─Flatten: 1-1                           [1, 784]                  --
├─Sequential: 1-2                        [1, 10]                   --
│    └─Linear: 2-1                       [1, 256]                  200,960
│    └─Linear: 2-2                       [1, 192]                  49,344
│    └─Linear: 2-3                       [1, 128]                  24,704
│    └─Linear: 2-4                       [1, 64]                   8,256
│    └─Linear: 2-5                       [1, 10]                   650
==========================================================================================
Total params: 283,914
Trainable params: 283,914
Non-trainable params: 0
Total mult-adds (M): 0.28
==========================================================================================
Input size (MB): 0.00
Forward/backward pass size (MB): 0.01
Params size (MB): 1.14
Estimated Total Size (MB): 1.14
==========================================================================================</code></pre>
</div>
</div>
<p>Carga y preprocesa el conjunto de datos.</p>
<div id="cell-14" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchinfo <span class="im">import</span> summary</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_04.utils.data <span class="im">import</span> get_data_loaders</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>train_dataloader, test_dataloader  <span class="op">=</span> get_data_loaders()</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>train_dataloader</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="6">
<pre><code>&lt;torch.utils.data.dataloader.DataLoader at 0x72be38d40700&gt;</code></pre>
</div>
</div>
<p>El flujo de gradientes es fundamental en el aprendizaje de redes neuronales. A medida que las capas se vuelven más profundas, los gradientes se multiplican continuamente de acuerdo con la regla de la cadena, y durante este proceso pueden ocurrir desapariciones o explosiones de gradientes. Por ejemplo, en una red neuronal de 30 capas, el gradiente debe pasar por 30 multiplicaciones antes de alcanzar la capa de entrada. Las funciones de activación añaden no linealidad y otorgan independencia entre capas para modular el flujo de gradientes. El siguiente código visualiza la distribución de gradientes de un modelo que utiliza la función de activación ReLU.</p>
<div id="cell-16" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_04.visualization.gradients <span class="im">import</span> visualize_network_gradients</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>visualize_network_gradients()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_función de activación_files/figure-html/cell-8-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Se pueden analizar las características de las funciones de activación visualizando la distribución de gradientes de cada capa en un histograma. En el caso de ReLU, la capa de salida muestra valores de gradiente en una escala de 10^-2, mientras que la capa de entrada muestra valores de gradiente en una escala de 10^-3. PyTorch utiliza por defecto la inicialización He (Kaiming), la cual está optimizada para funciones de activación del tipo ReLU. También es posible usar otros métodos de inicialización, como Xavier y Orthogonal, los cuales se tratan en detalle en el capítulo de inicialización.</p>
<div id="cell-18" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_04.models.activations <span class="im">import</span> act_functions</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_04.visualization.gradients <span class="im">import</span> get_gradients_weights, visualize_distribution</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, act_func <span class="kw">in</span> <span class="bu">enumerate</span>(act_functions):</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>    act_func_initiated <span class="op">=</span> act_functions[act_func]()</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> SimpleNetwork(act_func<span class="op">=</span>act_func_initiated).to(device)</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>    gradients, weights <span class="op">=</span> get_gradients_weights(model, train_dataloader)</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>    visualize_distribution(model, gradients, color<span class="op">=</span><span class="ss">f"C</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_función de activación_files/figure-html/cell-9-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_función de activación_files/figure-html/cell-9-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_función de activación_files/figure-html/cell-9-output-3.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_función de activación_files/figure-html/cell-9-output-4.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_función de activación_files/figure-html/cell-9-output-5.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_función de activación_files/figure-html/cell-9-output-6.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_función de activación_files/figure-html/cell-9-output-7.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_función de activación_files/figure-html/cell-9-output-8.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_función de activación_files/figure-html/cell-9-output-9.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_función de activación_files/figure-html/cell-9-output-10.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_función de activación_files/figure-html/cell-9-output-11.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_función de activación_files/figure-html/cell-9-output-12.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_función de activación_files/figure-html/cell-9-output-13.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Al examinar la distribución de gradientes por función de activación, se puede observar que Sigmoid muestra valores muy pequeños a escala <span class="math inline">\(10^{-5}\)</span> en la capa de entrada, lo que puede llevar al problema de desaparición del gradiente. ReLU tiene una concentración de gradientes alrededor de 0, esto se debe a su característica de desactivación (neuronas muertas) para entradas negativas. Las funciones de activación adaptativas más recientes mitigan estos problemas mientras mantienen la no linealidad. Por ejemplo, GELU muestra una distribución de gradientes cercana a una normal y esto produce buenos resultados junto con la normalización por lotes. Comparemos esto con el caso en el que no hay función de activación.</p>
<div id="cell-20" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_04.models.base <span class="im">import</span> SimpleNetwork</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>model_no_act <span class="op">=</span> SimpleNetwork(act_func<span class="op">=</span>nn.ReLU(), no_act <span class="op">=</span> <span class="va">True</span>).to(device) </span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>gradients, weights <span class="op">=</span> get_gradients_weights(model_no_act, train_dataloader)</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>visualize_distribution(model_no_act, gradients, title<span class="op">=</span><span class="st">"gradients"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_función de activación_files/figure-html/cell-10-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Si no hay una función de activación, la distribución entre capas es similar y solo varía en escala. Esto muestra que la falta de no linealidad limita la transformación de características entre las capas.</p>
</section>
<section id="entrenamiento-del-modelo" class="level3">
<h3 class="anchored" data-anchor-id="entrenamiento-del-modelo">4.4 Entrenamiento del modelo</h3>
<p>Para comparar objetivamente el rendimiento de las funciones de activación, realizamos experimentos con el conjunto de datos FashionMNIST. Aunque en 2025 existen más de 500 funciones de activación, en proyectos de deep learning reales se utilizan principalmente un pequeño número de funciones de activación validadas. Primero, examinaremos el proceso de entrenamiento básico utilizando ReLU como referencia.</p>
<section id="entrenamiento-de-un-solo-modelo" class="level4">
<h4 class="anchored" data-anchor-id="entrenamiento-de-un-solo-modelo">4.4.1 Entrenamiento de un solo modelo</h4>
<div id="cell-23" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.optim <span class="im">as</span> optim</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_04.experiments.model_training <span class="im">import</span> train_model</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_04.models.base <span class="im">import</span> SimpleNetwork</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_04.utils.data <span class="im">import</span> get_device</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_04.visualization.training <span class="im">import</span> plot_results</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> SimpleNetwork(act_func<span class="op">=</span>nn.ReLU()).to(device)</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> optim.SGD(model.parameters(), lr<span class="op">=</span><span class="fl">1e-2</span>, momentum<span class="op">=</span><span class="fl">0.9</span>)</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> train_model(model, train_dataloader, test_dataloader, device, epochs<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>plot_results(results)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
Starting training for SimpleNetwork-ReLU.</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"43e8fb87b8414e889e23db1b4c9b46f5","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Execution completed for SimpleNetwork-ReLU, Execution time = 76.1 secs</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_función de activación_files/figure-html/cell-11-output-4.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="entrenamiento-del-modelo-según-la-función-de-activación" class="level4">
<h4 class="anchored" data-anchor-id="entrenamiento-del-modelo-según-la-función-de-activación">4.4.2 Entrenamiento del modelo según la función de activación</h4>
<p>Ahora realizamos experimentos comparativos para las principales funciones de activación. Mantenemos la configuración y las condiciones de entrenamiento de cada modelo iguales para asegurar una comparación justa. - 4 capas ocultas [256, 192, 128, 64] - Optimizador SGD (learning rate=1e-3, momentum=0.9) - Tamaño del lote 128 - 15 épocas de entrenamiento</p>
<div id="cell-25" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_04.experiments.model_training <span class="im">import</span> train_all_models</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_04.visualization.training <span class="im">import</span> create_results_table</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_04.experiments.model_training <span class="im">import</span> train_all_models</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_04.visualization.training <span class="im">import</span> create_results_table  <span class="co"># Assuming this is where plot functions are.</span></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Train only selected models</span></span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a><span class="co"># selected_acts = ["ReLU"]  # Select only the desired activation functions</span></span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>selected_acts <span class="op">=</span> [<span class="st">"Tanh"</span>, <span class="st">"ReLU"</span>, <span class="st">"Swish"</span>]</span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a><span class="co"># selected_acts = ["Sigmoid", "ReLU", "Swish", "PReLU", "TeLU", "STAF"]</span></span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a><span class="co"># selected_acts = ["Sigmoid", "Tanh", "ReLU", "GELU", "Mish", "LeakyReLU", "SiLU", "Hardswish", "Swish", "PReLU", "RReLU", "TeLU", "STAF"]</span></span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a><span class="co"># results_dict = train_all_models(act_functions, train_dataloader, test_dataloader,</span></span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a><span class="co">#                               device, epochs=15, selected_acts=selected_acts)</span></span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a>results_dict <span class="op">=</span> train_all_models(act_functions, train_dataloader, test_dataloader,</span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a>                              device, epochs<span class="op">=</span><span class="dv">15</span>, selected_acts<span class="op">=</span>selected_acts, save_epochs<span class="op">=</span>[<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>,<span class="dv">5</span>,<span class="dv">6</span>,<span class="dv">7</span>,<span class="dv">8</span>,<span class="dv">9</span>,<span class="dv">10</span>,<span class="dv">11</span>,<span class="dv">12</span>,<span class="dv">13</span>,<span class="dv">14</span>,<span class="dv">15</span>])</span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a>create_results_table(results_dict)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Los resultados se muestran en la siguiente tabla. Los valores pueden variar según el entorno de ejecución.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 25%">
</colgroup>
<thead>
<tr class="header">
<th>modelo</th>
<th>precisión(%)</th>
<th>error final(%)</th>
<th>tiempo transcurrido (segundos)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>SimpleNetwork-Sigmoid</td>
<td>10.0</td>
<td>2.30</td>
<td>115.6</td>
</tr>
<tr class="even">
<td>SimpleNetwork-Tanh</td>
<td>82.3</td>
<td>0.50</td>
<td>114.3</td>
</tr>
<tr class="odd">
<td>SimpleNetwork-ReLU</td>
<td>81.3</td>
<td>0.52</td>
<td>115.2</td>
</tr>
<tr class="even">
<td>SimpleNetwork-GELU</td>
<td>80.5</td>
<td>0.54</td>
<td>115.2</td>
</tr>
<tr class="odd">
<td>SimpleNetwork-Mish</td>
<td>81.9</td>
<td>0.51</td>
<td>113.4</td>
</tr>
<tr class="even">
<td>SimpleNetwork-LeakyReLU</td>
<td>80.8</td>
<td>0.55</td>
<td>114.4</td>
</tr>
<tr class="odd">
<td>SimpleNetwork-SiLU</td>
<td>78.3</td>
<td>0.59</td>
<td>114.3</td>
</tr>
<tr class="even">
<td>SimpleNetwork-Hardswish</td>
<td>76.7</td>
<td>0.64</td>
<td>114.5</td>
</tr>
<tr class="odd">
<td>SimpleNetwork-Swish</td>
<td>78.5</td>
<td>0.59</td>
<td>116.1</td>
</tr>
<tr class="even">
<td>SimpleNetwork-PReLU</td>
<td>86.0</td>
<td>0.40</td>
<td>114.9</td>
</tr>
<tr class="odd">
<td>SimpleNetwork-RReLU</td>
<td>81.5</td>
<td>0.52</td>
<td>114.6</td>
</tr>
<tr class="even">
<td>SimpleNetwork-TeLU</td>
<td>86.2</td>
<td>0.39</td>
<td>119.6</td>
</tr>
<tr class="odd">
<td>SimpleNetwork-STAF</td>
<td>85.4</td>
<td>0.44</td>
<td>270.2</td>
</tr>
</tbody>
</table>
<p>Al analizar los resultados del experimento:</p>
<ol type="1">
<li><p><strong>Eficiencia computacional</strong>: Tanh y ReLU son los más rápidos, mientras que STAF es relativamente lento debido a sus cálculos complejos.</p></li>
<li><p><strong>Precisión</strong>:</p>
<ul>
<li>Las funciones de activación adaptativas (TeLU 86.2%, PReLU 86.0%, STAF 85.4%) muestran un rendimiento generalmente superior.</li>
<li>La Sigmoid clásica tiene un rendimiento muy bajo debido al problema de desvanecimiento del gradiente (10.0%).</li>
<li>Las funciones de activación modernas básicas (ReLU, GELU, Mish) presentan un rendimiento estable en el rango de 80-82%.</li>
</ul></li>
<li><p><strong>Estabilidad</strong>:</p>
<ul>
<li>Tanh, ReLU y Mish muestran curvas de aprendizaje relativamente estables.</li>
<li>Las funciones de activación adaptativas, aunque ofrecen un alto rendimiento, presentan mayor variabilidad durante el proceso de aprendizaje.</li>
</ul></li>
</ol>
<p>Estos resultados son una comparación bajo condiciones específicas; en proyectos reales, la elección de la función de activación debe considerar los siguientes factores: 1. Compatibilidad con la arquitectura del modelo (por ejemplo, se recomienda GELU para transformers) 2. Restricciones de recursos computacionales (se puede considerar Hardswish en entornos móviles) 3. Características de la tarea (Tanh sigue siendo útil para predicción de series temporales) 4. Tamaño del modelo y características del conjunto de datos.</p>
<p>Actualmente, en 2025, en los grandes modelos de lenguaje se utiliza principalmente GELU por su eficiencia computacional; en visión por computadora, las variantes de ReLU; y en aprendizaje por refuerzo, las funciones de activación adaptativas.</p>
</section>
</section>
<section id="análisis-de-las-salidas-por-capa-y-de-las-neuronas-inactivas-del-modelo-entrenado" class="level3">
<h3 class="anchored" data-anchor-id="análisis-de-las-salidas-por-capa-y-de-las-neuronas-inactivas-del-modelo-entrenado">4.5 Análisis de las salidas por capa y de las neuronas inactivas del modelo entrenado</h3>
<p>Anteriormente, examinamos la distribución de los valores de gradiente en cada capa durante el retroceso del modelo inicial. Ahora, utilizaremos el modelo entrenado para examinar qué valores produce cada capa durante el cálculo hacia adelante. El análisis de las salidas de cada capa del modelo entrenado es importante para comprender la capacidad de representación y los patrones de aprendizaje de la red neuronal. Desde la introducción del ReLU en 2010, el problema de las neuronas inactivas se ha convertido en una consideración clave en el diseño de redes neuronales profundas.</p>
<p>Primero, visualizamos la distribución de las salidas de cada capa durante el cálculo hacia adelante del modelo entrenado.</p>
<section id="visualización-de-la-distribución-de-salidas-por-capa" class="level4">
<h4 class="anchored" data-anchor-id="visualización-de-la-distribución-de-salidas-por-capa">4.5.1 Visualización de la distribución de salidas por capa</h4>
<div id="cell-28" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_04.utils.metrics <span class="im">import</span> load_model</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_04.utils.data <span class="im">import</span> get_data_loaders, get_device</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_04.visualization.gradients <span class="im">import</span> get_model_outputs, visualize_distribution</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> get_device()</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Re-define the data loaders.</span></span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>train_dataloader, test_dataloader <span class="op">=</span> get_data_loaders()</span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, act_func <span class="kw">in</span> <span class="bu">enumerate</span>(act_functions):</span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a>    model_file <span class="op">=</span> <span class="ss">f"SimpleNetwork-</span><span class="sc">{</span>act_func<span class="sc">}</span><span class="ss">.pth"</span></span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a>    model_path <span class="op">=</span> os.path.join(<span class="st">"./tmp/models"</span>, model_file)</span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Load the model only if the file exists</span></span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> os.path.exists(model_path):</span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Load the model.</span></span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a>        model, config <span class="op">=</span> load_model(model_file<span class="op">=</span>model_file, path<span class="op">=</span><span class="st">"./tmp/models"</span>)</span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a>        layer_outputs <span class="op">=</span> get_model_outputs(model, test_dataloader, device)</span>
<span id="cb16-20"><a href="#cb16-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-21"><a href="#cb16-21" aria-hidden="true" tabindex="-1"></a>        visualize_distribution(model, layer_outputs, title<span class="op">=</span><span class="st">"gradients"</span>, color<span class="op">=</span><span class="ss">f"C</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb16-22"><a href="#cb16-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb16-23"><a href="#cb16-23" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Model file not found: </span><span class="sc">{</span>model_file<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_función de activación_files/figure-html/cell-13-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_función de activación_files/figure-html/cell-13-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_función de activación_files/figure-html/cell-13-output-3.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_función de activación_files/figure-html/cell-13-output-4.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_función de activación_files/figure-html/cell-13-output-5.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_función de activación_files/figure-html/cell-13-output-6.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_función de activación_files/figure-html/cell-13-output-7.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_función de activación_files/figure-html/cell-13-output-8.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_función de activación_files/figure-html/cell-13-output-9.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_función de activación_files/figure-html/cell-13-output-10.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_función de activación_files/figure-html/cell-13-output-11.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_función de activación_files/figure-html/cell-13-output-12.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04_función de activación_files/figure-html/cell-13-output-13.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="problema-de-las-neuronas-inactivas" class="level4">
<h4 class="anchored" data-anchor-id="problema-de-las-neuronas-inactivas">4.5.2 Problema de las neuronas inactivas</h4>
<p>Las neuronas inactivas (neuronas muertas) son aquellas que siempre emiten un 0 para cualquier entrada. Este es un problema importante especialmente con funciones de activación del tipo ReLU. Para encontrar neuronas inactivas, se puede pasar todo el conjunto de datos de entrenamiento y buscar las que siempre emiten 0. Para esto, se pueden obtener los valores de salida de cada capa y aplicar una máscara lógica para identificar aquellas que siempre son 0.</p>
<div id="cell-30" class="cell" data-execution_count="14">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 3 samples (1 batch), 5 columns (each a neuron's output). Columns 1 and 3 always show 0.</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>batch_1 <span class="op">=</span> torch.tensor([[<span class="dv">0</span>, <span class="fl">1.5</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>],</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>                        [<span class="dv">0</span>, <span class="dv">0</span>,  <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>],</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>                        [<span class="dv">0</span>, <span class="dv">1</span>,  <span class="dv">0</span>, <span class="fl">1.2</span>, <span class="dv">1</span>]])</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Column 3 always shows 0</span></span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>batch_2 <span class="op">=</span> torch.tensor([[<span class="fl">1.1</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>],</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>                        [<span class="dv">1</span>,   <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>],</span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>                        [<span class="dv">0</span>,   <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>]])</span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(batch_1)</span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(batch_2)</span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Use the .all() method to create a boolean tensor indicating which columns</span></span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a><span class="co"># have all zeros along the batch dimension (dim=0).</span></span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a>batch_1_all_zeros <span class="op">=</span> (batch_1 <span class="op">==</span> <span class="dv">0</span>).<span class="bu">all</span>(dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a>batch_2_all_zeros <span class="op">=</span> (batch_2 <span class="op">==</span> <span class="dv">0</span>).<span class="bu">all</span>(dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(batch_1_all_zeros)</span>
<span id="cb17-20"><a href="#cb17-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(batch_2_all_zeros)</span>
<span id="cb17-21"><a href="#cb17-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-22"><a href="#cb17-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Declare a masked_array that can be compared across the entire batch.</span></span>
<span id="cb17-23"><a href="#cb17-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialized to all True.</span></span>
<span id="cb17-24"><a href="#cb17-24" aria-hidden="true" tabindex="-1"></a>masked_array <span class="op">=</span> torch.ones(<span class="dv">5</span>, dtype<span class="op">=</span>torch.<span class="bu">bool</span>)</span>
<span id="cb17-25"><a href="#cb17-25" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"masked_array = </span><span class="sc">{</span>masked_array<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb17-26"><a href="#cb17-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-27"><a href="#cb17-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform logical AND operations between the masked_array and the all_zeros</span></span>
<span id="cb17-28"><a href="#cb17-28" aria-hidden="true" tabindex="-1"></a><span class="co"># tensors for each batch.</span></span>
<span id="cb17-29"><a href="#cb17-29" aria-hidden="true" tabindex="-1"></a>masked_array <span class="op">=</span> torch.logical_and(masked_array, batch_1_all_zeros)</span>
<span id="cb17-30"><a href="#cb17-30" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(masked_array)</span>
<span id="cb17-31"><a href="#cb17-31" aria-hidden="true" tabindex="-1"></a>masked_array <span class="op">=</span> torch.logical_and(masked_array, batch_2_all_zeros)</span>
<span id="cb17-32"><a href="#cb17-32" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"final = </span><span class="sc">{</span>masked_array<span class="sc">}</span><span class="ss">"</span>)  <span class="co"># Finally, only the 3rd neuron remains True (dead neuron).</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>tensor([[0.0000, 1.5000, 0.0000, 1.0000, 1.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 1.0000],
        [0.0000, 1.0000, 0.0000, 1.2000, 1.0000]])
tensor([[1.1000, 1.0000, 0.0000, 1.0000, 1.0000],
        [1.0000, 0.0000, 0.0000, 0.0000, 1.0000],
        [0.0000, 1.0000, 0.0000, 1.0000, 1.0000]])
tensor([ True, False,  True, False, False])
tensor([False, False,  True, False, False])
masked_array = tensor([True, True, True, True, True])
tensor([ True, False,  True, False, False])
final = tensor([False, False,  True, False, False])</code></pre>
</div>
</div>
<p>La función para calcular las neuronas inactivas es <code>calculate_disabled_neuron</code>. Se encuentra en <code>visualization/training.py</code>. Vamos a analizar la proporción de neuronas inactivas en el modelo real.</p>
<div id="cell-32" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_04.visualization.training <span class="im">import</span> calculate_disabled_neuron</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_04.models.base <span class="im">import</span> SimpleNetwork</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Find in the trained model.</span></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>model, _ <span class="op">=</span> load_model(model_file<span class="op">=</span><span class="st">"SimpleNetwork-ReLU.pth"</span>, path<span class="op">=</span><span class="st">"./tmp/models"</span>)</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>calculate_disabled_neuron(model, train_dataloader, device)</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>model, _ <span class="op">=</span> load_model(model_file<span class="op">=</span><span class="st">"SimpleNetwork-Swish.pth"</span>, path<span class="op">=</span><span class="st">"./tmp/models"</span>)</span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>calculate_disabled_neuron(model, train_dataloader, device)</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Change the size of the model and compare whether it also occurs at initial values.</span></span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a>big_model <span class="op">=</span> SimpleNetwork(act_func<span class="op">=</span>nn.ReLU(), hidden_shape<span class="op">=</span>[<span class="dv">2048</span>, <span class="dv">1024</span>, <span class="dv">1024</span>, <span class="dv">512</span>, <span class="dv">512</span>, <span class="dv">256</span>, <span class="dv">128</span>]).to(device)</span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a>calculate_disabled_neuron(big_model, train_dataloader, device)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
Number of layers to compare = 4</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"2aba73fc70504cb3999072a4c9676e1e","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Number of disabled neurons (ReLU) : [0, 6, 13, 5]
Ratio of disabled neurons = 0.0%
Ratio of disabled neurons = 3.1%
Ratio of disabled neurons = 10.2%
Ratio of disabled neurons = 7.8%

Number of layers to compare = 4</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"b164f0a6be854579aa7314ec8c69baeb","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Number of disabled neurons (Swish) : [0, 0, 0, 0]
Ratio of disabled neurons = 0.0%
Ratio of disabled neurons = 0.0%
Ratio of disabled neurons = 0.0%
Ratio of disabled neurons = 0.0%

Number of layers to compare = 7</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"62a92f453fdd4c0f828b6d94f55ae264","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Number of disabled neurons (ReLU) : [0, 0, 6, 15, 113, 102, 58]
Ratio of disabled neurons = 0.0%
Ratio of disabled neurons = 0.0%
Ratio of disabled neurons = 0.6%
Ratio of disabled neurons = 2.9%
Ratio of disabled neurons = 22.1%
Ratio of disabled neurons = 39.8%
Ratio of disabled neurons = 45.3%</code></pre>
</div>
</div>
<p>Según los resultados actuales de la investigación, la gravedad del problema de las neuronas inactivas varía según la profundidad y anchura del modelo. En particular, se destaca que: 1. A medida que el modelo se vuelve más profundo, la proporción de neuronas inactivas en ReLU aumenta drásticamente. 2. Las funciones de activación adaptativas (STAF, TeLU) mitigan este problema de manera efectiva. 3. En la arquitectura Transformer, GELU ha reducido significativamente el problema de las neuronas inactivas. 4. En los modelos MoE (Mixture of Experts) más recientes, se utilizan diferentes funciones de activación para cada red experta para resolver este problema.</p>
<p>Por lo tanto, al diseñar redes neuronales con muchas capas, es necesario considerar alternativas a ReLU, como GELU, STAF, TeLU, y en particular, en modelos de gran escala, se requiere una selección que considere simultáneamente la eficiencia computacional y el problema de las neuronas inactivas.</p>
</section>
</section>
<section id="determinación-de-las-funciones-de-activación-candidatas" class="level3">
<h3 class="anchored">4.6 Determinación de las funciones de activación candidatas</h3>
<p>La selección de la función de activación es una de las decisiones más importantes en el diseño de redes neuronales. Las funciones de activación tienen un impacto directo en la capacidad de la red para aprender patrones complejos, la velocidad de entrenamiento y el rendimiento general. A continuación se presentan los resultados de investigaciones recientes y mejores prácticas según el campo de aplicación.</p>
<section id="visión-por-computadora-computer-vision" class="level5">
<h5 class="anchored" data-anchor-id="visión-por-computadora-computer-vision">Visión por computadora (Computer Vision)</h5>
<ul>
<li><strong>Modelos basados en CNN:</strong> ReLU y sus variantes (LeakyReLU, PReLU, ELU) siguen siendo ampliamente utilizadas debido a su alta eficiencia computacional y buen rendimiento general. Sin embargo, GELU y Swish/SiLU se están utilizando cada vez más en arquitecturas más profundas, especialmente en CNN de alto rendimiento, debido a sus gradientes más suaves.</li>
<li><strong>Transformadores de visión (ViTs):</strong> En ViT, GELU se ha establecido prácticamente como estándar. Esto es coherente con el uso exitoso de GELU en transformadores del campo de procesamiento de lenguaje natural.</li>
<li><strong>Dispositivos móviles/empotrados:</strong> Hardswish es preferido en entornos con recursos limitados debido a su eficiencia computacional. ReLU y sus variantes (como ReLU6, comúnmente utilizada en MobileNets) también siguen siendo opciones sólidas.</li>
<li><strong>Modelos generativos (generación de imágenes de alta precisión):</strong> Aunque STAF ha mostrado resultados prometedores, aún no se ha adoptado ampliamente. Funciones de activación suaves como Swish, GELU y Mish son más preferidas en tareas de generación debido a su tendencia a producir resultados de mayor calidad y reducir los artefactos. Los modelos de difusión (Diffusion), que representan el estado del arte actual en la generación de imágenes, a menudo utilizan Swish/SiLU.</li>
</ul>
</section>
<section id="procesamiento-del-lenguaje-natural-nlp" class="level5">
<h5 class="anchored" data-anchor-id="procesamiento-del-lenguaje-natural-nlp">Procesamiento del lenguaje natural (NLP)</h5>
<ul>
<li><strong>Modelos basados en transformadores:</strong> GELU es la elección dominante en la mayoría de las arquitecturas de transformadores (BERT, GPT, etc.).</li>
<li><strong>RNN/LSTM:</strong> Históricamente, Tanh ha sido preferido, pero gradualmente está siendo reemplazado por funciones de activación que mitigan mejor el problema del desvanecimiento del gradiente. GELU y variantes de ReLU (junto con técnicas cuidadosas de inicialización y regularización) se utilizan frecuentemente en implementaciones modernas de RNN/LSTM.</li>
<li><strong>Modelos de lenguaje a gran escala (LLMs):</strong> La eficiencia computacional es crucial. GELU y ReLU (o aproximaciones rápidas de GELU) son las opciones más comunes. Algunos LLMs experimentan con funciones de activación especiales dentro de capas de mezcla de expertos (Mixture-of-Experts, MoE).</li>
</ul>
</section>
<section id="procesamiento-de-voz-speech-processing" class="level5">
<h5 class="anchored" data-anchor-id="procesamiento-de-voz-speech-processing">Procesamiento de voz (Speech Processing)</h5>
<ul>
<li><strong>Reconocimiento emocional:</strong> Aunque Teacup ha mostrado resultados prometedores, aún no se ha adoptado ampliamente. Funciones de activación suaves como Swish y GELU son candidatas sólidas para CNN debido a sus gradientes más suaves.</li>
<li><strong>Síntesis de voz:</strong> Funciones de activación suaves como Snake y GELU a menudo se recomiendan ya que pueden ayudar a generar voces más naturales.</li>
<li><strong>Procesamiento en tiempo real:</strong> Similar a la visión móvil, Hardswish y variantes de ReLU son adecuadas para aplicaciones que requieren tiempos de latencia bajos.</li>
</ul>
</section>
<section id="recomendaciones-generales-y-tendencias-actuales" class="level5">
<h5 class="anchored">Recomendaciones generales y tendencias actuales</h5>
<p>A continuación se presenta un enfoque más sistemático para seleccionar funciones de activación candidatas.</p>
<ol type="1">
<li><strong>Selección básica (buen punto de partida):</strong>
<ul>
<li><strong>GELU:</strong> Excelente opción versátil, especialmente para transformadores y redes más profundas.</li>
<li><strong>ReLU (o LeakyReLU/PReLU):</strong> Todavía es una opción sólida y eficiente para CNN. Considere usar LeakyReLU o PReLU para evitar el problema de “ReLU muerto”.</li>
<li><strong>Swish/SiLU:</strong> A menudo supera a ReLU en redes más profundas y ofrece un buen rendimiento general en varios aspectos.</li>
</ul></li>
<li><strong>Alto rendimiento (potencialmente más cálculos):</strong>
<ul>
<li><strong>Mish:</strong> A menudo logra los mejores resultados, pero tiene un costo de cálculo mayor que ReLU o GELU.</li>
<li><strong>TeLU:</strong> Es una variación aprendible de ELU. Las afirmaciones sobre una convergencia más rápida y estabilidad valen la pena ser verificadas, aunque aún no ha sido ampliamente adoptada. El benchmarking es fundamental.</li>
<li><strong>Funciones de Activación Racionales:</strong> Tienen la capacidad de aproximar funciones complejas y manejar sistemas dinámicos, lo que las hace prometedoras para el aprendizaje por refuerzo y redes neuronales basadas en física (PINN). Sin embargo, se utilizan menos comúnmente en tareas de aprendizaje supervisado estándar.</li>
</ul></li>
<li><strong>Ligero/eficiente:</strong>
<ul>
<li><strong>Hardswish:</strong> Diseñado para dispositivos móviles y embebidos.</li>
<li><strong>ReLU6:</strong> Una variación de ReLU que limita el rango de salida a 6, se utiliza comúnmente en modelos cuantificados.</li>
</ul></li>
<li><strong>Adaptativo/aprendible:</strong>
<ul>
<li><strong>PReLU:</strong> Aprende la pendiente negativa. Es simple y efectiva.</li>
<li><strong>TeLU:</strong> Aprende un factor de escala para la parte exponencial de la función ELU.</li>
<li><strong>STAF:</strong> Muestra <em>potencial</em> para capturar patrones complejos, pero STAF (y otras activaciones basadas en Fourier) son costosas computacionalmente y aún no han demostrado una superioridad consistente sobre opciones más simples en la mayoría de las tareas comunes. Siguen siendo un área de investigación activa.</li>
<li><strong>B-spline:</strong> La propiedad de control local es interesante, pero las activaciones B-spline (similar a STAF) son menos comunes en el aprendizaje profundo mainstream debido a su complejidad. Se ven más frecuentemente en aplicaciones especializadas como ajuste de curvas o modelado geométrico. Es un área de investigación activa y puede ser <em>eficaz</em> para el aprendizaje continuo/ incremental, aunque aún no hay resultados ampliamente establecidos.</li>
</ul></li>
</ol>
<p><strong>Principales tendencias y consideraciones recientes:</strong></p>
<ul>
<li><strong>Reducción del uso de Sigmoid/Tanh en redes profundas:</strong> Debido al problema de la desaparición del gradiente, rara vez se utilizan como activaciones de capa oculta en redes neuronales modernas.</li>
<li><strong>Importancia de la suavidad (Smoothness):</strong> Las funciones de activación suaves (GELU, Swish, Mish) son generalmente preferidas sobre las no suaves (ReLU) en redes más profundas. Esto se debe a que tienden a conducir a un entrenamiento más estable y un mejor flujo del gradiente.</li>
<li><strong>Costo computacional:</strong> En particular para modelos grandes o dispositivos con recursos limitados, siempre considere el costo de cálculo de las funciones de activación.</li>
<li><strong>Especificidad de la tarea (Task Specificity):</strong> La función de activación óptima puede variar significativamente según la tarea. Es importante realizar experimentos.</li>
<li><strong>Mezcla de expertos (Mixture of Experts, MoE):</strong> En modelos muy grandes como algunos LLM, diferentes funciones de activación pueden ser utilizadas dentro de diferentes subredes “expertas”.</li>
<li><strong>Funciones de activación racionales y sistemas dinámicos:</strong> La capacidad de las funciones de activación racionales y sus extensiones “joint-rational” para aprender y representar la dinámica del sistema es una línea de investigación prometedora.</li>
</ul>
<p><strong>Lo más importante es siempre experimentar!</strong> Comience con un valor razonable por defecto (GELU o ReLU/LeakyReLU), pero esté preparado para probar otras opciones si no logra el rendimiento deseado. Pequeños experimentos donde solo cambia la función de activación, manteniendo otros hiperparámetros constantes, son esenciales para tomar decisiones informadas.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Haz clic para ver el contenido (deep dive: diseñar tu propia función de activación)">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Haz clic para ver el contenido (deep dive: diseñar tu propia función de activación)
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<section id="buceo-profundo-diseñando-tu-propia-función-de-activación---teoría-y-práctica" class="level2 callout-body-container callout-body">
<h2 class="anchored" data-anchor-id="buceo-profundo-diseñando-tu-propia-función-de-activación---teoría-y-práctica">Buceo profundo: Diseñando tu propia función de activación - Teoría y práctica</h2>
<p>La función de activación es uno de los componentes clave de un modelo de aprendizaje profundo y tiene un gran impacto en la capacidad expresiva del modelo, la velocidad de aprendizaje y el rendimiento final. Además de las funciones de activación comúnmente utilizadas (ReLU, GELU, Swish, etc.), numerosos investigadores han propuesto nuevas funciones de activación. En este buceo profundo, examinaremos paso a paso el proceso de diseñar tu propia función de activación y aprenderemos cómo implementarla y probarla usando PyTorch.</p>
<section id="principios-básicos-del-diseño-de-funciones-de-activación" class="level3">
<h3 class="anchored" data-anchor-id="principios-básicos-del-diseño-de-funciones-de-activación">1. Principios básicos del diseño de funciones de activación</h3>
<p>Antes de diseñar una nueva función de activación, repasemos las condiciones de la “función de activación ideal” descritas en el apartado 4.2.</p>
<ul>
<li><strong>No linealidad (Non-linearity):</strong> Permite que las redes neuronales representen funciones complejas.</li>
<li><strong>Diferenciabilidad (Differentiability):</strong> Es esencial para entrenar redes neuronales mediante el algoritmo de retropropagación (backpropagation). Se permiten funciones no diferenciables en algunos puntos, como ReLU.</li>
<li><strong>Prevención del problema de desvanecimiento y explosión de gradientes:</strong> Asegura que el aprendizaje sea estable en redes neuronales profundas.</li>
<li><strong>Eficiencia computacional (Computational Efficiency):</strong> Afecta la velocidad de entrenamiento e inferencia de las redes neuronales.</li>
</ul>
<p>Además, se pueden considerar los siguientes aspectos:</p>
<ul>
<li><strong>Salida centrada en cero (Zero-Centered Output):</strong> Una salida de función de activación centrada en cero puede mejorar la velocidad de aprendizaje. (Tanh, ELU)</li>
<li><strong>Auto-gating:</strong> Característica por la cual el grado de activación se ajusta según el valor de entrada. (Swish)</li>
<li><strong>Suavidad (Smoothness):</strong> Las funciones de activación suaves generalmente llevan a un aprendizaje más estable.</li>
<li><strong>Monotonicidad (Monotonicity):</strong> Una función monotónica es aquella cuya salida aumenta o disminuye a medida que la entrada lo hace. ReLU, Leaky ReLU, ELU, GELU, Swish, Mish son todas funciones monótonas. Sigmoid y Tanh <em>no</em> son funciones monótonas. La monotonicidad puede facilitar la optimización, pero no es un requisito indispensable.</li>
<li><strong>Limitación (Boundedness):</strong> Indica si la salida de la función de activación está restringida a un rango específico. Sigmoid y Tanh son funciones limitadas, mientras que las variantes ReLU son ilimitadas. Las funciones limitadas pueden ayudar a prevenir la explosión del gradiente, pero también pueden limitar la capacidad expresiva.</li>
</ul>
</section>
<section id="generación-de-ideas-combinación-y-modificación-de-funciones-de-activación-existentes" class="level3">
<h3 class="anchored" data-anchor-id="generación-de-ideas-combinación-y-modificación-de-funciones-de-activación-existentes">2. Generación de ideas: Combinación y modificación de funciones de activación existentes</h3>
<p>El método más común para diseñar una nueva función de activación es combinar o modificar funciones de activación existentes.</p>
<ul>
<li><strong>Variaciones basadas en ReLU:</strong> Para abordar el problema del “Dying ReLU,” se han propuesto varias variaciones como Leaky ReLU, PReLU, ELU y SELU. Estas ideas pueden extenderse para cambiar el comportamiento en la región negativa o agregar parámetros aprendibles.</li>
<li><strong>Variaciones basadas en Sigmoid/Tanh:</strong> Para mitigar el problema del desvanecimiento de gradientes, se pueden considerar modificaciones a las funciones Sigmoid y Tanh, o combinarlas con otras funciones.</li>
<li><strong>Series Swish/Mish:</strong> Swish(<span class="math inline">\(x \cdot sigmoid(x)\)</span>) y Mish(<span class="math inline">\(x \cdot tanh(ln(1 + e^x))\)</span>) son conocidas por mostrar buen rendimiento. Se pueden considerar modificaciones a estas funciones o su combinación con otras.</li>
<li><strong>Variaciones basadas en GELU:</strong> GELU es ampliamente utilizada en modelos Transformer. Se pueden modificar las aproximaciones de GELU o combinarlas con otras funciones para crear nuevas funciones de activación.</li>
</ul>
</section>
<section id="análisis-matemático-diferenciabilidad-características-de-gradientes" class="level3">
<h3 class="anchored" data-anchor-id="análisis-matemático-diferenciabilidad-características-de-gradientes">3. Análisis matemático: Diferenciabilidad, características de gradientes</h3>
<p>Si se propone una nueva función de activación, es necesario realizar un análisis matemático.</p>
<ul>
<li><strong>Diferenciabilidad:</strong> Se debe verificar si la función propuesta es diferenciable en todos los intervalos, o si, como ReLU, no es diferenciable en algunos puntos pero puede definir subgradientes. Usar la funcionalidad de diferenciación automática de PyTorch para calcular valores derivados y graficarlos puede ser útil.</li>
<li><strong>Características del gradiente:</strong> Se debe analizar cómo cambia el gradiente según el rango de los valores de entrada. Se deben verificar si existen regiones donde el gradiente se vuelve demasiado pequeño (gradiente desaparecido) o demasiado grande (gradiente explotando).</li>
</ul>
</section>
<section id="implementación-en-pytorch" class="level3">
<h3 class="anchored" data-anchor-id="implementación-en-pytorch">4. Implementación en PyTorch</h3>
<p>Una función de activación que ha sido validada por análisis matemático puede implementarse fácilmente usando PyTorch. Se debe crear una nueva clase heredando de <code>torch.nn.Module</code> y definir la operación de la función de activación en el método <code>forward</code>. Si es necesario, se pueden definir parámetros entrenables con <code>torch.nn.Parameter</code>.</p>
<p><strong>Ejemplo: Implementación de la función de activación “SwiGELU”</strong></p>
<p>Vamos a proponer e implementar una nueva función de activación llamada “SwiGELU”, que combina Swish y GELU. (Basado en ideas del problema 4.2.3)</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SwiGELU(nn.Module):</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="fl">0.5</span> <span class="op">*</span> (x <span class="op">*</span> torch.sigmoid(x) <span class="op">+</span> F.gelu(x))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Explicación:</strong></p>
<ul>
<li><code>SwiGELU(x) = 0.5 * (x * sigmoid(x) + GELU(x))</code></li>
<li>Se combinan Swish(<span class="math inline">\(x \cdot sigmoid(x)\)</span>) y GELU(<span class="math inline">\(x\Phi(x)\)</span>) en una proporción de 1:1, y se multiplica por 0.5 para ajustar el rango de salida.</li>
<li>Se espera poder aprovechar tanto la característica de puerta propia de Swish como la no linealidad suave y los efectos de regularización de GELU.</li>
</ul>
</section>
<section id="experimentación-y-evaluación" class="level3">
<h3 class="anchored" data-anchor-id="experimentación-y-evaluación">5. Experimentación y evaluación</h3>
<p>Si se propone una nueva función de activación, es necesario realizar experimentos para comparar su rendimiento con funciones de activación existentes utilizando conjuntos de datos de referencia (por ejemplo, CIFAR-10, CIFAR-100, ImageNet).</p>
<ul>
<li><strong>Configuración del experimento:</strong>
<ul>
<li>Se utiliza la misma arquitectura de modelo y se cambia solo la función de activación.</li>
<li>Otros hiperparámetros como tasa de aprendizaje, tamaño del lote, optimizador, etc., se mantienen constantes.</li>
<li>Se repiten varios experimentos para asegurar la significancia estadística de los resultados.</li>
</ul></li>
<li><strong>Métricas de evaluación:</strong>
<ul>
<li>Precisión</li>
<li>Pérdida</li>
<li>Tiempo de entrenamiento</li>
<li>Velocidad de convergencia</li>
<li>Norma del gradiente - Utilizar la función <code>train_model_with_metrics</code></li>
<li>Número/porcentaje de neuronas deshabilitadas (“neuronas muertas”) - Utilizar la función <code>calculate_disabled_neuron</code></li>
<li>Otras métricas, como el uso de memoria, si es necesario.</li>
</ul></li>
<li><strong>Análisis de resultados</strong>
<ul>
<li>Comparación cuantitativa de la velocidad de convergencia y el rendimiento final</li>
<li>Verificación de problemas de gradiente desaparecido/explotando</li>
<li>Tasa de ocurrencia de “neuronas muertas”</li>
</ul></li>
</ul>
</section>
<section id="opcional-análisis-teórico" class="level3">
<h3 class="anchored" data-anchor-id="opcional-análisis-teórico">6. (Opcional) Análisis teórico</h3>
<p>Si los resultados experimentales son buenos, es recomendable realizar un análisis teórico para comprender por qué la nueva función de activación muestra un mejor rendimiento. * <strong>Análisis del paisaje de pérdidas:</strong> analiza el impacto que tiene la función de activación en el espacio de la función de pérdida (loss landscape). (Ver sección 4.2 Deep Dive) * <strong>Análisis del Kernel Tangente Neural (NTK):</strong> examina el papel de la función de activación en redes neuronales infinitamente anchas. * <strong>Ecuación de Fokker-Planck:</strong> analiza las características dinámicas de la función de activación. (Ver estudios sobre Swish)</p>
</section>
<section id="conclusión" class="level3">
<h3 class="anchored" data-anchor-id="conclusión">Conclusión</h3>
<p>Diseñar y evaluar nuevas funciones de activación es una tarea desafiante, pero es un campo de investigación con gran potencial para mejorar el rendimiento de los modelos de deep learning. Superar las limitaciones de las funciones de activación existentes y encontrar funciones de activación más adecuadas para problemas o arquitecturas específicas es uno de los desafíos más importantes en la investigación de deep learning. Se espera que el enfoque paso a paso presentado en este Deep Dive, junto con ejemplos de implementación en PyTorch y directrices para experimentación y análisis, ayuden a diseñar sus propias funciones de activación.</p>
</section>
</section>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled" title="Haga clic para ver el contenido (deep dive: funciones de activación adaptativas - direcciones futuras de investigación)">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Haga clic para ver el contenido (deep dive: funciones de activación adaptativas - direcciones futuras de investigación)
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<section id="buceo-profundo-funciones-de-activación-adaptativas---direcciones-futuras-de-investigación" class="level3 callout-body-container callout-body">
<h3 class="anchored" data-anchor-id="buceo-profundo-funciones-de-activación-adaptativas---direcciones-futuras-de-investigación">Buceo profundo: funciones de activación adaptativas - direcciones futuras de investigación</h3>
<p><strong>Introducción:</strong></p>
<p>Las funciones de activación fijas como ReLU y GELU son ampliamente utilizadas en modelos de deep learning, pero pueden no estar optimizadas para problemas específicos o distribuciones de datos. Recientemente, se ha llevado a cabo una investigación intensa sobre el ajuste <em>adaptativo</em> de funciones de activación según los datos o la tarea. En este buceo profundo, exploramos el potencial y las direcciones futuras de investigación de las funciones de activación adaptativas (Adaptive Activation Function).</p>
<section id="tipos-de-funciones-de-activación-adaptativas" class="level4">
<h4 class="anchored" data-anchor-id="tipos-de-funciones-de-activación-adaptativas">1. Tipos de funciones de activación adaptativas</h4>
<p>Las funciones de activación adaptativas se pueden clasificar en general de la siguiente manera.</p>
<ul>
<li><p><strong>Ajuste paramétrico (Parametric Adaptation):</strong> Se introducen parámetros aprendibles en la función de activación para ajustar su forma según los datos.</p>
<ul>
<li><strong>Ejemplos:</strong>
<ul>
<li>Leaky ReLU: <span class="math inline">\(f(x) = max(\\alpha x, x)\)</span> (<span class="math inline">\(\\alpha\)</span> es un parámetro aprendible)</li>
<li>PReLU (Parametric ReLU): Se aprende <span class="math inline">\(\\alpha\)</span> para cada canal en lugar de una constante</li>
<li>Swish: <span class="math inline">\(f(x) = x \\cdot \\sigma(\\beta x)\)</span> (<span class="math inline">\(\\beta\)</span> es un parámetro aprendible)</li>
</ul></li>
</ul></li>
<li><p><strong>Ajuste estructural (Structural Adaptation):</strong> Se combinan varias funciones base o se modifica la arquitectura de la red para construir dinámicamente la función de activación.</p>
<ul>
<li><strong>Ejemplos:</strong>
<ul>
<li>Redes Maxout: Se selecciona el valor máximo entre múltiples funciones lineales</li>
<li>Funciones de activación basadas en splines: Se utilizan funciones spline para representar la función de activación</li>
</ul></li>
</ul></li>
<li><p><strong>Ajuste basado en entrada:</strong> Se cambia o combina la función de activación según las características de los datos de entrada.</p>
<ul>
<li><strong>Ejemplos:</strong>
<ul>
<li>Bloque Squeeze and Excitation (SE): Se calcula la importancia entre los canales de las características de entrada y se aplica un peso a la función de activación</li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="posibles-direcciones-de-investigación" class="level4">
<h4 class="anchored" data-anchor-id="posibles-direcciones-de-investigación">2. Posibles direcciones de investigación</h4>
<section id="funciones-de-activación-basadas-en-mixture-of-experts-moe" class="level5">
<h5 class="anchored" data-anchor-id="funciones-de-activación-basadas-en-mixture-of-experts-moe">2.1 Funciones de activación basadas en Mixture of Experts (MoE)</h5>
<ul>
<li><p><strong>Idea:</strong> Se definen múltiples funciones de activación “expertas” y se determinan dinámicamente los pesos de cada experto según los datos de entrada.</p></li>
<li><p><strong>Expresión matemática:</strong></p>
<p><span class="math inline">\(f(x) = \\sum\_{k=1}^K g\_k(x) \\cdot \\phi\_k(x)\)</span></p>
<ul>
<li><span class="math inline">\(g\_k(x)\)</span>: Función de puerta (gating function) para la k-ésima función de activación experta, normalizada mediante softmax u otros métodos</li>
<li><span class="math inline">\(\\phi\_k(x)\)</span>: La k-ésima función de activación experta (pueden usarse funciones como ReLU, GELU, Swish, entre otras)</li>
</ul></li>
<li><p><strong>Tareas de investigación:</strong></p>
<ul>
<li><strong>Mecanismos eficientes de puerta:</strong> Estudio de métodos eficientes para calcular <span class="math inline">\(g\_k(x)\)</span> (ejemplo: Top-k gating, sparse gating)</li>
<li><strong>Selección de funciones de activación expertas:</strong> Investigación sobre qué tipo de <span class="math inline">\(\\phi\_k(x)\)</span> es mejor usar y cómo determinar el número de expertos</li>
<li><strong>Análisis teórico</strong>: Análisis teórico del poder expresivo (expressive power) y la capacidad de generalización de las funciones de activación MoE</li>
</ul></li>
</ul>
</section>
<section id="integración-con-búsqueda-de-arquitectura-neural-neural-architecture-search-nas" class="level5">
<h5 class="anchored" data-anchor-id="integración-con-búsqueda-de-arquitectura-neural-neural-architecture-search-nas">2.2 Integración con búsqueda de arquitectura neural (Neural Architecture Search, NAS)</h5>
<ul>
<li><strong>Idea:</strong> Utilizar NAS para explorar automáticamente la estructura de funciones de activación optimizadas para los datos y tareas.</li>
<li><strong>Enfoque:</strong>
<ul>
<li><strong>Espacio de búsqueda (Search Space):</strong>
<ul>
<li>Definir operaciones básicas (transformaciones lineales, funciones exponenciales, logarítmicas, trigonométricas, etc.)</li>
<li>Definir una variedad de candidatos a funciones de activación que se pueden crear combinando estas operaciones.</li>
</ul></li>
<li><strong>Estrategia de búsqueda (Search Strategy):</strong>
<ul>
<li>Aprendizaje por refuerzo (Reinforcement Learning)</li>
<li>Algoritmos evolutivos (Evolutionary Algorithm)</li>
<li>Búsqueda de arquitectura diferenciable (Differentiable Architecture Search, DARTS)</li>
</ul></li>
<li><strong>Evaluación del rendimiento (Performance Estimation):</strong>
<ul>
<li>Entrenar y evaluar el rendimiento en un conjunto de datos de validación de modelos que incluyen las funciones de activación encontradas.</li>
</ul></li>
</ul></li>
<li><strong>Tareas de investigación:</strong>
<ul>
<li><strong>Diseño eficiente del espacio de búsqueda:</strong> Definir un espacio de búsqueda lo suficientemente diverso pero no excesivamente grande para incluir una variedad de funciones de activación.</li>
<li><strong>Reducción del costo computacional:</strong> Dado que NAS es muy costoso en términos de cálculo, desarrollar estrategias eficientes de búsqueda y métodos de evaluación del rendimiento.</li>
</ul></li>
</ul>
</section>
<section id="integración-de-información-físicabiológica" class="level5">
<h5 class="anchored" data-anchor-id="integración-de-información-físicabiológica">2.3 Integración de información física/biológica</h5>
<ul>
<li><p><strong>Idea:</strong> Utilizar el conocimiento del dominio en campos como la física y la biología para imponer restricciones o knowledge prior en el diseño de funciones de activación.</p></li>
<li><p><strong>Ejemplos:</strong></p>
<ul>
<li><strong>Modelo físico:</strong> Al modelar un sistema físico específico, reflejar las ecuaciones diferenciales del sistema en la función de activación.</li>
<li><strong>Neurociencia:</strong> Funciones de activación que imitan el funcionamiento de los verdaderos neuronas (por ejemplo, modelos de neuronas con disparo).</li>
</ul></li>
<li><p><strong>Tareas de investigación:</strong></p>
<ul>
<li><strong>Integración efectiva del conocimiento del dominio:</strong> Desarrollar metodologías para reflejar el conocimiento del dominio en el diseño de funciones de activación.</li>
<li><strong>Rendimiento general:</strong> Verificar si las funciones de activación especializadas en un dominio específico funcionan bien en otros dominios.</li>
</ul></li>
</ul>
</section>
<section id="reforzamiento-del-análisis-teórico" class="level5">
<h5 class="anchored" data-anchor-id="reforzamiento-del-análisis-teórico">2.4 Reforzamiento del análisis teórico</h5>
<ul>
<li><strong>Potencia expresiva (Expressive Power):</strong> Analizar cuán más potentes son las funciones de activación adaptativas en comparación con las funciones de activación tradicionales.</li>
<li><strong>Facilidad de optimización (Optimization Landscape):</strong> Analizar cómo las funciones de activación adaptativas modifican la superficie de la función de pérdida y qué impacto tiene esto en la velocidad y estabilidad del aprendizaje.</li>
<li><strong>Rendimiento general (Generalization):</strong> Analizar si las funciones de activación adaptativas previenen el sobreajuste y mejoran el rendimiento general.</li>
</ul>
</section>
</section>
<section id="conclusión-y-recomendaciones" class="level4">
<h4 class="anchored" data-anchor-id="conclusión-y-recomendaciones">3. Conclusión y recomendaciones</h4>
<p>Las funciones de activación adaptativas son un campo de investigación prometedor para mejorar el rendimiento de los modelos de aprendizaje profundo. Sin embargo, quedan desafíos pendientes:</p>
<ul>
<li><strong>Complejidad computacional:</strong> Las funciones de activación adaptativas suelen ser más costosas en términos de cálculo que las funciones de activación fijas.</li>
<li><strong>Interpretabilidad:</strong> A medida que la forma de las funciones de activación aprendidas se vuelve más compleja, puede volverse difícil interpretar el modelo.</li>
<li><strong>Riesgo de sobreajuste:</strong> Las funciones de activación demasiado flexibles pueden sobreajustarse a los datos de entrenamiento.</li>
</ul>
<p>Es importante que futuras investigaciones aborden estos desafíos para desarrollar funciones de activación adaptativas más eficientes, interpretables y con un rendimiento general superior.</p>
</section>
</section>
</div>
</div>
</section>
</section>
</section>
<section id="ejercicios-de-práctica" class="level2">
<h2 class="anchored" data-anchor-id="ejercicios-de-práctica">Ejercicios de práctica</h2>
<section id="problemas-básicos" class="level3">
<h3 class="anchored" data-anchor-id="problemas-básicos">4.2.1 Problemas básicos</h3>
<ol type="1">
<li><p>Escriba las fórmulas y grafique las funciones Sigmoid, Tanh, ReLU, Leaky ReLU, GELU, Swish. (utilice matplotlib, Desmos, etc.)</p>
<ul>
<li><strong>Referencia:</strong> Asegúrese de entender claramente la definición y características de cada función, y compare visualmente a través de los gráficos.</li>
</ul></li>
<li><p>Calcule las derivadas (funciones derivadas) de cada función de activación y grafíquelas.</p>
<ul>
<li><strong>Referencia:</strong> Las derivadas se utilizan para calcular gradientes en el proceso de retropropagación. Determine la diferenciabilidad de cada función y las características del gradiente.</li>
</ul></li>
<li><p>Utilice el conjunto de datos FashionMNIST para entrenar una red neuronal compuesta solo por transformaciones lineales sin funciones de activación, y mida la precisión de prueba. (utilice SimpleNetwork implementado en el Capítulo 1)</p>
<ul>
<li><strong>Referencia:</strong> Una red neuronal sin funciones de activación no puede representar nonlinearidades, lo que limita su capacidad para resolver problemas complejos. Verifique esto a través del experimento.</li>
</ul></li>
<li><p>Compare los resultados obtenidos en el problema 3 con los resultados de una red neuronal que utiliza la función de activación ReLU, y explique el papel de las funciones de activación.</p>
<ul>
<li><strong>Referencia</strong>: Compare y explique los valores de salida por capa, los gradientes y las neuronas inactivas cuando hay y no hay función de activación.</li>
</ul></li>
</ol>
</section>
<section id="problemas-aplicados" class="level3">
<h3 class="anchored" data-anchor-id="problemas-aplicados">4.2.2 Problemas aplicados</h3>
<ol type="1">
<li><p>Implemente las funciones de activación PReLU, TeLU, STAF en PyTorch. (herede <code>nn.Module</code>)</p>
<ul>
<li><strong>Referencia:</strong> Consulte la definición de cada función y implemente el método <code>forward</code>. Defina parámetros aprendibles con <code>nn.Parameter</code> si es necesario.</li>
</ul></li>
<li><p>Utilice el conjunto de datos FashionMNIST para entrenar una red neuronal que incluya las funciones de activación implementadas anteriormente, y compare la precisión de prueba.</p>
<ul>
<li><strong>Referencia:</strong> Compare el rendimiento de cada función de activación y analice cuál es más adecuada para el conjunto de datos FashionMNIST.</li>
</ul></li>
<li><p>Visualice la distribución de gradientes durante el entrenamiento para cada función de activación, y mida la proporción de “neuronas muertas”. (utilice las funciones implementadas en el Capítulo 1)</p>
<ul>
<li><strong>Referencia</strong>: Compare la distribución de gradientes entre los valores iniciales y los entrenados, y por capa, para cada función de activación.</li>
</ul></li>
<li><p>Investigue y explique los métodos para mitigar el problema de “neuronas muertas”. (Leaky ReLU, PReLU, ELU, SELU, etc.)</p>
<ul>
<li><strong>Referencia:</strong> Explique cómo cada método aborda las limitaciones de ReLU, y cuáles son sus ventajas y desventajas.</li>
</ul></li>
</ol>
</section>
<section id="problemas-avanzados" class="level3">
<h3 class="anchored">4.2.3 Problemas avanzados</h3>
<ol type="1">
<li><p>Implemente la función de activación Rational en PyTorch y explique sus características y ventajas y desventajas.</p>
<ul>
<li><strong>Referencia:</strong> La función de activación Rational se basa en funciones racionales (funciones fraccionarias) y puede mostrar un rendimiento superior a otras funciones de activación en problemas específicos.</li>
</ul></li>
<li><p>Implemente la función de activación B-spline o Fourier-based en PyTorch y explique sus características y ventajas y desventajas.</p>
<ul>
<li><strong>Referencia:</strong> La función de activación B-spline puede representar curvas flexibles controladas localmente, mientras que la función de activación basada en Fourier es útil para modelar patrones periódicos.</li>
</ul></li>
<li><p>Proponga una nueva función de activación propia y evalúe su rendimiento comparándola con las funciones de activación existentes. (presente resultados experimentales y justificación teórica)</p>
<ul>
<li><strong>Referencia:</strong> Al diseñar una nueva función de activación, considere los criterios ideales para una función de activación (nonlinearidad, diferenciabilidad, prevención de problemas de desvanecimiento/explotación del gradiente, eficiencia computacional, etc.).</li>
</ul></li>
</ol>
<div class="callout callout-style-default callout-note callout-titled" title="Haga clic para ver el contenido (solución)">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Haga clic para ver el contenido (solución)
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<section id="soluciones-de-ejercicios" class="level2 callout-body-container callout-body">
<h2 class="anchored" data-anchor-id="soluciones-de-ejercicios">Soluciones de Ejercicios</h2>
<section id="problemas-básicos-1" class="level3">
<h3 class="anchored" data-anchor-id="problemas-básicos-1">4.2.1 Problemas Básicos</h3>
<section id="fórmulas-y-gráficas-de-las-funciones-sigmoid-tanh-relu-leaky-relu-gelu-swish" class="level4">
<h4 class="anchored" data-anchor-id="fórmulas-y-gráficas-de-las-funciones-sigmoid-tanh-relu-leaky-relu-gelu-swish">1. <strong>Fórmulas y gráficas de las funciones Sigmoid, Tanh, ReLU, Leaky ReLU, GELU, Swish:</strong></h4>
<pre><code>| Función de Activación | Fórmula        | Gráfico (Referencia)    |
| ------- | ------------------------------------------------------- | ---------------------------------------------------- |
| Sigmoid     | $\sigma(x) = \frac{1}{1 + e^{-x}}$                                         | [Sigmoid](https://www.google.com/search?q=https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/320px-Logistic-curve.svg.png) |
| Tanh        | $tanh(x) = \frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}$                           | [Tanh](https://www.google.com/search?q=https://upload.wikimedia.org/wikipedia/commons/thumb/c/c7/Hyperbolic_Tangent.svg/320px-Hyperbolic_Tangent.svg.png)     |
| ReLU        | $ReLU(x) = max(0, x)$                                                     | [ReLU](https://www.google.com/search?q=https://upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Activation_rectified_linear.svg/320px-Activation_rectified_linear.svg.png) |
| Leaky ReLU  | $LeakyReLU(x) = max(ax, x)$ ,  ($a$ es una constante pequeña, generalmente 0.01)               | (Leaky ReLU tiene una pequeña pendiente($a$) en la parte donde x &lt; 0 del gráfico de ReLU)                  |
| GELU        | $GELU(x) = x\Phi(x)$ , ($\Phi(x)$ es la función de distribución acumulativa gaussiana)             | [GELU](https://www.google.com/search?q=https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-22_at_3.34.27_PM_fufBJEx.png)            |
| Swish       | $Swish(x) = x \cdot sigmoid(\beta x)$ , ($\beta$ es una constante o un parámetro de aprendizaje) | [Swish](https://www.google.com/search?q=https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-22_at_3.35.27_PM_d7LqDQj.png)          |</code></pre>
</section>
<section id="derivadas-funciones-derivadas-de-cada-función-de-activación" class="level4">
<h4 class="anchored" data-anchor-id="derivadas-funciones-derivadas-de-cada-función-de-activación">2. <strong>Derivadas (funciones derivadas) de cada función de activación:</strong></h4>
<table class="caption-top table">
<colgroup>
<col style="width: 17%">
<col style="width: 82%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">función de activación</th>
<th style="text-align: left;">derivada</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Sigmoid</td>
<td style="text-align: left;"><span class="math inline">\(\sigma'(x) = \sigma(x)(1 - \sigma(x))\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">Tanh</td>
<td style="text-align: left;"><span class="math inline">\(tanh'(x) = 1 - tanh^2(x)\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;">ReLU</td>
<td style="text-align: left;"><span class="math inline">\(ReLU'(x) = \begin{cases} 0, &amp; x &lt; 0 \\ 1, &amp; x &gt; 0 \end{cases}\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">Leaky ReLU</td>
<td style="text-align: left;"><span class="math inline">\(LeakyReLU'(x) = \begin{cases} a, &amp; x &lt; 0 \\ 1, &amp; x &gt; 0 \end{cases}\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;">GELU</td>
<td style="text-align: left;"><span class="math inline">\(GELU'(x) = \Phi(x) + x\phi(x)\)</span>, (<span class="math inline">\(\phi(x)\)</span> es la función de densidad de probabilidad gaussiana)</td>
</tr>
<tr class="even">
<td style="text-align: left;">Swish</td>
<td style="text-align: left;"><span class="math inline">\(Swish'(x) = sigmoid(\beta x) + x \cdot sigmoid(\beta x)(1 - sigmoid(\beta x))\beta\)</span></td>
</tr>
</tbody>
</table>
<ol start="3" type="1">
<li><p><strong>Entrenamiento y medición de precisión de una red neuronal sin función de activación en FashionMNIST:</strong></p>
<ul>
<li>Una red neuronal sin función de activación solo realiza transformaciones lineales, por lo que no puede modelar relaciones no lineales complejas. Por lo tanto, muestra una baja precisión en conjuntos de datos complejos como FashionMNIST (aproximadamente 10% de precisión).</li>
</ul></li>
<li><p><strong>Comparación con una red neuronal que utiliza la función de activación ReLU, y explicación del papel de las funciones de activación:</strong></p>
<ul>
<li>Una red neuronal que utiliza la función de activación ReLU introduce no linealidad, lo que permite alcanzar una precisión mucho mayor (más del 80%).</li>
<li><strong>Salidas por capa:</strong> Sin función de activación, la distribución de las salidas por capa muestra solo un cambio de escala simple. Con ReLU, los valores negativos se suprimen a 0, lo que cambia la distribución.</li>
<li><strong>Gradientes:</strong> Sin función de activación, los gradientes se transmiten simplemente. Con ReLU, para entradas negativas, el gradiente es 0 y no se propaga.</li>
<li><strong>Neuronas inactivas:</strong> No ocurren cuando no hay función de activación, pero pueden ocurrir al usar ReLU.</li>
<li><strong>Resumen del papel:</strong> Las funciones de activación otorgan no linealidad a la red neuronal, permitiendo aproximar funciones complejas y regulando el flujo de gradientes para facilitar el aprendizaje.</li>
</ul></li>
</ol>
</section>
</section>
<section id="problemas-de-aplicación" class="level3">
<h3 class="anchored" data-anchor-id="problemas-de-aplicación">4.2.2 Problemas de aplicación</h3>
<ol type="1">
<li><p><strong>Implementación de PReLU, TeLU, STAF en PyTorch:</strong></p>
<div class="sourceCode" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> PReLU(nn.Module):</span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, num_parameters<span class="op">=</span><span class="dv">1</span>, init<span class="op">=</span><span class="fl">0.25</span>):</span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.alpha <span class="op">=</span> nn.Parameter(torch.full((num_parameters,), init))</span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.<span class="bu">max</span>(torch.zeros_like(x), x) <span class="op">+</span> <span class="va">self</span>.alpha <span class="op">*</span> torch.<span class="bu">min</span>(torch.zeros_like(x), x)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>```python import torch import torch.nn as nn</p></li>
</ol>
<p>class TeLU(nn.Module): def <strong>init</strong>(self, alpha=1.0): super().__init__() self.alpha = nn.Parameter(torch.tensor(alpha))</p>
<pre><code>def forward(self, x):
    return torch.where(x &gt; 0, x, self.alpha * (torch.exp(x) - 1))</code></pre>
<p>class STAF(nn.Module): def <strong>init</strong>(self, tau=25): super().__init__() self.tau = tau self.C = nn.Parameter(torch.randn(tau)) self.Omega = nn.Parameter(torch.randn(tau)) self.Phi = nn.Parameter(torch.randn(tau))</p>
<pre><code>def forward(self, x):
    result = torch.zeros_like(x)
    for i in range(self.tau):
        result += self.C[i] * torch.sin(self.Omega[i] * x + self.Phi[i])
    return result</code></pre>
<pre><code>
2.  **Comparación de funciones de activación en FashionMNIST:**

    *   Se entrenan redes neuronales que incluyen PReLU, TeLU y STAF, y se comparan las precisiones de prueba.
    *   Los resultados del experimento muestran una tendencia a que las funciones de activación adaptativas (PReLU, TeLU, STAF) tengan mayor precisión que ReLU. (STAF &gt; TeLU &gt; PReLU &gt; ReLU)

3.  **Visualización de la distribución del gradiente y medición de la proporción de "neuronas muertas":**

    *   ReLU tiene un gradiente de 0 para entradas negativas, mientras que PReLU, TeLU y STAF propagan pequeños valores de gradiente incluso para entradas negativas.
    *   La proporción de "neuronas muertas" es más alta en ReLU y más baja en PReLU, TeLU y STAF.

4.  **Métodos y principios para aliviar el problema de las "neuronas muertas":**

    *   **Leaky ReLU:** Permite una pequeña pendiente para entradas negativas, evitando que las neuronas se desactiven por completo.
    *   **PReLU:** Convierte la pendiente de Leaky ReLU en un parámetro aprendible, ajustándola óptimamente según los datos.
    *   **ELU, SELU:** Tienen valores no nulos en el dominio negativo y una forma curva suave, lo que ayuda a mitigar el problema del desvanecimiento del gradiente y estabiliza el aprendizaje.

### 4.2.3 Problemas avanzados

1.  **Implementación de la función de activación Rational en PyTorch, características y ventajas y desventajas:**

    ```python
    import torch
    import torch.nn as nn

    class Rational(nn.Module):
        def __init__(self, numerator_coeffs, denominator_coeffs):
            super().__init__()
            self.numerator_coeffs = nn.Parameter(numerator_coeffs)
            self.denominator_coeffs = nn.Parameter(denominator_coeffs)</code></pre>
<div class="sourceCode" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>    numerator <span class="op">=</span> torch.polyval(<span class="va">self</span>.numerator_coeffs, x) <span class="co"># cálculo polinomial</span></span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>    denominator <span class="op">=</span> <span class="dv">1</span> <span class="op">+</span> torch.polyval(<span class="va">self</span>.denominator_coeffs, torch.<span class="bu">abs</span>(x))  <span class="co"># valor absoluto y cálculo polinomial</span></span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> numerator <span class="op">/</span> denominator</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li><strong>Característica:</strong> Forma de función racional (función fraccional). El numerador y el denominador se representan con polinomios.</li>
<li><strong>Ventaja:</strong> Forma flexible de la función. Puede ofrecer un rendimiento superior a otras funciones de activación en problemas específicos.</li>
<li><strong>Desventaja:</strong> Debe tener cuidado para evitar que el denominador sea cero. Es necesario ajustar los hiperparámetros (coeficientes del polinomio).</li>
</ul>
<ol start="2" type="1">
<li><strong>Implementación, características y ventajas/desventajas de funciones de activación basadas en B-splines o Fourier:</strong></li>
</ol>
<ul>
<li><p><strong>Función de activación B-spline:</strong></p>
<div class="sourceCode" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.interpolate <span class="im">import</span> BSpline</span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> BSplineActivation(nn.Module):</span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, knots, degree<span class="op">=</span><span class="dv">3</span>):</span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.knots <span class="op">=</span> knots</span>
<span id="cb31-10"><a href="#cb31-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.degree <span class="op">=</span> degree</span>
<span id="cb31-11"><a href="#cb31-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.coeffs <span class="op">=</span> nn.Parameter(torch.randn(<span class="bu">len</span>(knots) <span class="op">+</span> degree <span class="op">-</span> <span class="dv">1</span>)) <span class="co"># puntos de control</span></span>
<span id="cb31-12"><a href="#cb31-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-13"><a href="#cb31-13" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb31-14"><a href="#cb31-14" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Cálculo del B-spline</span></span>
<span id="cb31-15"><a href="#cb31-15" aria-hidden="true" tabindex="-1"></a>        b <span class="op">=</span> BSpline(<span class="va">self</span>.knots, <span class="va">self</span>.coeffs.detach().numpy(), <span class="va">self</span>.degree) <span class="co"># usar coeficientes separados</span></span>
<span id="cb31-16"><a href="#cb31-16" aria-hidden="true" tabindex="-1"></a>        spline_values <span class="op">=</span> torch.tensor(b(x.detach().numpy()), dtype<span class="op">=</span>torch.float32) <span class="co"># ingresar x en el B-spline</span></span>
<span id="cb31-17"><a href="#cb31-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> spline_values <span class="op">*</span> <span class="va">self</span>.coeffs.mean() <span class="co"># si no se usa detach, numpy() se produce un error</span></span>
<span id="cb31-18"><a href="#cb31-18" aria-hidden="true" tabindex="-1"></a>         <span class="co"># si no se usa detach, numpy() se produce un error</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li><p><strong>Característica:</strong> Curva flexible controlada localmente. La forma se ajusta mediante nudos (knots) y grado (degree).</p></li>
<li><p><strong>Ventaja:</strong> Expresión de función suave. Aprendizaje de características locales.</p></li>
<li><p><strong>Desventaja:</strong> El rendimiento puede verse afectado por la configuración de los nudos. Aumento de la complejidad computacional.</p></li>
</ul>
<ol start="3" type="1">
<li><p><strong>Propuesta y evaluación del rendimiento de una nueva función de activación:</strong></p>
<ul>
<li><p>(Ejemplo) <strong>Función de activación que combina Swish y GELU</strong>:</p>
<div class="sourceCode" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SwiGELU(nn.Module): <span class="co"># Swish + GELU</span></span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a>  <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="fl">0.5</span> <span class="op">*</span> (x <span class="op">*</span> torch.sigmoid(x) <span class="op">+</span> F.gelu(x))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>SwiGELU combina la suavidad de Swish con el efecto de normalización de GELU.</p></li>
<li><p>Diseño experimental y evaluación del rendimiento: Comparar con funciones de activación existentes en conjuntos de datos de referencia como FashionMNIST. (Se omiten los resultados experimentales) ```</p></li>
</ul></li>
</ol>
</section>
</section>
</div>
</div>
</section>
<section id="referencias-1" class="level3">
<h3 class="anchored" data-anchor-id="referencias-1">Referencias</h3>
<ol type="1">
<li><strong>Deep Learning (Goodfellow, Bengio, Courville, 2016)</strong>: Capítulo 6.3 (Funciones de Activación) <a href="https://www.deeplearningbook.org/">https://www.deeplearningbook.org/</a>
<ul>
<li>Un libro de texto que abarca una amplia gama de temas en deep learning. Además de los conceptos básicos de las funciones de activación, también se pueden aprender otros conceptos importantes del deep learning.</li>
</ul></li>
<li><strong>Understanding the difficulty of training deep feedforward neural networks (Glorot &amp; Bengio, 2010)</strong> <a href="http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf">http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf</a>
<ul>
<li>Un artículo que analiza el problema de la desaparición del gradiente en las funciones de activación Sigmoid y Tanh, y propone el método de inicialización Xavier. Es un recurso importante para entender las dificultades en el entrenamiento de redes neuronales profundas.</li>
</ul></li>
<li><strong>Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification (He et al., 2015)</strong> <a href="https://arxiv.org/abs/1502.01852">https://arxiv.org/abs/1502.01852</a>
<ul>
<li>Un artículo que propone las funciones de activación ReLU y PReLU, así como el método de inicialización He. Ayuda a comprender mejor las funciones de activación ReLU, ampliamente utilizadas en deep learning moderno.</li>
</ul></li>
<li><strong>Searching for Activation Functions (Ramachandran et al., 2017)</strong> <a href="https://arxiv.org/abs/1710.05941">https://arxiv.org/abs/1710.05941</a>
<ul>
<li>Un artículo que descubre la función de activación Swish a través de la búsqueda de arquitecturas neuronales (Neural Architecture Search, NAS). Proporciona ideas sobre cómo explorar nuevas funciones de activación.</li>
</ul></li>
<li><strong>STAF: A Sinusoidal Trainable Activation Function for Deep Learning (Jeon &amp; Cho, 2025)</strong> <a href="https://arxiv.org/abs/2405.13607">https://arxiv.org/abs/2405.13607</a>
<ul>
<li>Un artículo presentado en ICLR 2025 que propone STAF, una función de activación aprendible basada en la serie de Fourier. Ayuda a comprender las últimas tendencias en investigación sobre funciones de activación adaptativas.</li>
</ul></li>
</ol>


</section>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>