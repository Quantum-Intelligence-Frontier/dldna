<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>la-evolución-del-transformer – Deep Learning DNA: Surviving Architectures and Essential Principles</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap-f507c7d0488cb7630e20aad62ad8c2aa.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script>window.MathJax = {loader: {load: ['[tex]/boldsymbol']},tex: {packages: {'[+]': ['boldsymbol']}}};</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../../notebooks/es/part_1/01_El inicio del aprendizaje profundo.html">part_1</a></li><li class="breadcrumb-item"><a href="../../../notebooks/es/part_1/09_La evolución del transformer.html">9. La evolución del transformer</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../../../">Español</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Language</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index_de.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Deutsch</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index_en.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">English</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index_es.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Español</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">한국어</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index_zh.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">中文</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/es/part_1/00_Introducción.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introducción</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">part_1</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/es/part_1/01_El inicio del aprendizaje profundo.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">1. El inicio del aprendizaje profundo</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/es/part_1/02_Matemáticas de deep learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">2. Matemáticas de deep learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/es/part_1/03_marco de aprendizaje profundo.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">3. marco de aprendizaje profundo</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/es/part_1/04_función de activación.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">4. función de activación</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/es/part_1/05_Optimización y visualización.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">5. Optimización y visualización</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/es/part_1/06_Sobreajuste y desarrollo de técnicas de solución.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">6. Sobreajuste y desarrollo de técnicas de solución</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/es/part_1/07_Evolución de las redes neuronales convolucionales.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">7. Evolución de las redes neuronales convolucionales</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/es/part_1/08_El nacimiento del transformer.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">8. El nacimiento del transformer</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/es/part_1/09_La evolución del transformer.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">9. La evolución del transformer</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/es/part_1/10_Multimodal deep learning: el inicio de la fusión multisensorial.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">10. Multimodal deep learning: el inicio de la fusión multisensorial</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/es/part_1/11_Multimodal deep learning: inteligencia más allá de los límites.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">11. Multimodal deep learning: inteligencia más allá de los límites</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">la vanguardia del deep learning</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/es/la vanguardia del deep learning/01_SLM: pequeño pero poderoso modelo de lenguaje.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">1. SLM: pequeño pero poderoso modelo de lenguaje</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/es/la vanguardia del deep learning/02_conducción autónoma.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">2. conducción autónoma</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#capítulo-la-evolución-de-los-transformers-hacia-la-eficiencia-y-escalabilidad" id="toc-capítulo-la-evolución-de-los-transformers-hacia-la-eficiencia-y-escalabilidad" class="nav-link active" data-scroll-target="#capítulo-la-evolución-de-los-transformers-hacia-la-eficiencia-y-escalabilidad">9 Capítulo La evolución de los Transformers: hacia la eficiencia y escalabilidad</a>
  <ul class="collapse">
  <li><a href="#limitaciones-y-desafíos-de-los-transformers" id="toc-limitaciones-y-desafíos-de-los-transformers" class="nav-link" data-scroll-target="#limitaciones-y-desafíos-de-los-transformers">9.1 Limitaciones y desafíos de los Transformers</a>
  <ul class="collapse">
  <li><a href="#limitaciones-básicas-de-la-arquitectura-transformer-complejidad-computacional" id="toc-limitaciones-básicas-de-la-arquitectura-transformer-complejidad-computacional" class="nav-link" data-scroll-target="#limitaciones-básicas-de-la-arquitectura-transformer-complejidad-computacional">9.1.1 Limitaciones básicas de la arquitectura Transformer: complejidad computacional</a></li>
  <li><a href="#limitaciones-básicas-de-la-arquitectura-del-transformer-eficiencia-de-memoria" id="toc-limitaciones-básicas-de-la-arquitectura-del-transformer-eficiencia-de-memoria" class="nav-link" data-scroll-target="#limitaciones-básicas-de-la-arquitectura-del-transformer-eficiencia-de-memoria">9.1.2 Limitaciones básicas de la arquitectura del transformer: eficiencia de memoria</a></li>
  <li><a href="#tendencias-temporales-en-el-desarrollo-del-transformer-y-la-estructura-de-este-capítulo" id="toc-tendencias-temporales-en-el-desarrollo-del-transformer-y-la-estructura-de-este-capítulo" class="nav-link" data-scroll-target="#tendencias-temporales-en-el-desarrollo-del-transformer-y-la-estructura-de-este-capítulo">9.1.3 Tendencias temporales en el desarrollo del transformer y la estructura de este capítulo</a></li>
  </ul></li>
  <li><a href="#reducción-de-complejidad-optimización-atencional-en-software-2019-2020" id="toc-reducción-de-complejidad-optimización-atencional-en-software-2019-2020" class="nav-link" data-scroll-target="#reducción-de-complejidad-optimización-atencional-en-software-2019-2020">9.2 Reducción de complejidad: optimización atencional en software (2019-2020)</a>
  <ul class="collapse">
  <li><a href="#enfoques-iniciales-aproximación-y-esparsificación" id="toc-enfoques-iniciales-aproximación-y-esparsificación" class="nav-link" data-scroll-target="#enfoques-iniciales-aproximación-y-esparsificación">9.2.1 Enfoques iniciales: aproximación y esparsificación</a></li>
  <li><a href="#atención-local-global-resolución-del-problema-de-dependencias-a-larga-distancia" id="toc-atención-local-global-resolución-del-problema-de-dependencias-a-larga-distancia" class="nav-link" data-scroll-target="#atención-local-global-resolución-del-problema-de-dependencias-a-larga-distancia">9.2.3 Atención local-global: Resolución del problema de dependencias a larga distancia</a></li>
  </ul></li>
  <li><a href="#eficiencia-de-memoria-combinación-de-hardware-y-software-2021-2022" id="toc-eficiencia-de-memoria-combinación-de-hardware-y-software-2021-2022" class="nav-link" data-scroll-target="#eficiencia-de-memoria-combinación-de-hardware-y-software-2021-2022">9.3 Eficiencia de memoria: combinación de hardware y software (2021-2022)</a>
  <ul class="collapse">
  <li><a href="#flashattention-optimización-de-atención-utilizando-la-jerarquía-de-memoria-de-gpu" id="toc-flashattention-optimización-de-atención-utilizando-la-jerarquía-de-memoria-de-gpu" class="nav-link" data-scroll-target="#flashattention-optimización-de-atención-utilizando-la-jerarquía-de-memoria-de-gpu">9.3.1 FlashAttention: optimización de atención utilizando la jerarquía de memoria de GPU</a></li>
  <li><a href="#optimización-de-consultas-mejora-de-la-estructura-de-atención" id="toc-optimización-de-consultas-mejora-de-la-estructura-de-atención" class="nav-link" data-scroll-target="#optimización-de-consultas-mejora-de-la-estructura-de-atención">9.3.2 Optimización de consultas: Mejora de la estructura de atención</a></li>
  <li><a href="#gestión-y-optimización-de-la-caché-kv" id="toc-gestión-y-optimización-de-la-caché-kv" class="nav-link" data-scroll-target="#gestión-y-optimización-de-la-caché-kv">9.3.3 Gestión y optimización de la caché KV</a></li>
  </ul></li>
  <li><a href="#escalabilidad-y-arquitecturas-de-propósito-especial-2023-2024" id="toc-escalabilidad-y-arquitecturas-de-propósito-especial-2023-2024" class="nav-link" data-scroll-target="#escalabilidad-y-arquitecturas-de-propósito-especial-2023-2024">9.4 Escalabilidad y arquitecturas de propósito especial (2023-2024)</a>
  <ul class="collapse">
  <li><a href="#procesamiento-de-contexto-largo-extensión-de-la-longitud-de-contexto" id="toc-procesamiento-de-contexto-largo-extensión-de-la-longitud-de-contexto" class="nav-link" data-scroll-target="#procesamiento-de-contexto-largo-extensión-de-la-longitud-de-contexto">9.4.1 Procesamiento de contexto largo: extensión de la longitud de contexto</a></li>
  <li><a href="#restricciones-éticasseguridad-constitutional-ai" id="toc-restricciones-éticasseguridad-constitutional-ai" class="nav-link" data-scroll-target="#restricciones-éticasseguridad-constitutional-ai">9.4.2 Restricciones éticas/seguridad: Constitutional AI</a></li>
  <li><a href="#atención-de-propósito-especial-optimización-por-dominio-y-tarea" id="toc-atención-de-propósito-especial-optimización-por-dominio-y-tarea" class="nav-link" data-scroll-target="#atención-de-propósito-especial-optimización-por-dominio-y-tarea">9.4.3 Atención de propósito especial: optimización por dominio y tarea</a></li>
  </ul></li>
  <li><a href="#implementación-y-aplicación-de-un-codificador-eficiente-centrado-en-rope-y-flashattention" id="toc-implementación-y-aplicación-de-un-codificador-eficiente-centrado-en-rope-y-flashattention" class="nav-link" data-scroll-target="#implementación-y-aplicación-de-un-codificador-eficiente-centrado-en-rope-y-flashattention">9.5 Implementación y Aplicación de un Codificador Eficiente: Centrado en RoPE y FlashAttention</a>
  <ul class="collapse">
  <li><a href="#filosofía-de-diseño-del-codificador-eficiente-velocidad-y-memoria" id="toc-filosofía-de-diseño-del-codificador-eficiente-velocidad-y-memoria" class="nav-link" data-scroll-target="#filosofía-de-diseño-del-codificador-eficiente-velocidad-y-memoria">9.5.1 Filosofía de Diseño del Codificador Eficiente: Velocidad y Memoria</a></li>
  <li><a href="#análisis-detallado-del-código-efficient_encoder.py-sin-usar-rope" id="toc-análisis-detallado-del-código-efficient_encoder.py-sin-usar-rope" class="nav-link" data-scroll-target="#análisis-detallado-del-código-efficient_encoder.py-sin-usar-rope">9.5.2 Análisis detallado del código <code>efficient_encoder.py</code> (Sin usar RoPE)</a></li>
  <li><a href="#análisis-detallado-del-código-efficient_encoder_rope.py-con-rope" id="toc-análisis-detallado-del-código-efficient_encoder_rope.py-con-rope" class="nav-link" data-scroll-target="#análisis-detallado-del-código-efficient_encoder_rope.py-con-rope">9.5.3 Análisis detallado del código <code>efficient_encoder_rope.py</code> (con RoPE)</a></li>
  <li><a href="#resultados-experimentales-clasificación-de-texto-ag-news" id="toc-resultados-experimentales-clasificación-de-texto-ag-news" class="nav-link" data-scroll-target="#resultados-experimentales-clasificación-de-texto-ag-news">9.5.4 Resultados experimentales: Clasificación de texto AG News</a></li>
  </ul></li>
  <li><a href="#mistral-implementación-y-análisis-de-una-arquitectura-de-decodificador-eficiente" id="toc-mistral-implementación-y-análisis-de-una-arquitectura-de-decodificador-eficiente" class="nav-link" data-scroll-target="#mistral-implementación-y-análisis-de-una-arquitectura-de-decodificador-eficiente">9.6 Mistral: Implementación y análisis de una arquitectura de decodificador eficiente</a>
  <ul class="collapse">
  <li><a href="#arquitectura-del-modelo-simple_mistral-análisis-detallado-de-los-componentes" id="toc-arquitectura-del-modelo-simple_mistral-análisis-detallado-de-los-componentes" class="nav-link" data-scroll-target="#arquitectura-del-modelo-simple_mistral-análisis-detallado-de-los-componentes">9.6.1 Arquitectura del modelo <code>simple_mistral</code>: Análisis detallado de los componentes</a></li>
  <li><a href="#análisis-de-elementos-técnicos-clave-el-secreto-de-la-eficiencia-y-el-rendimiento" id="toc-análisis-de-elementos-técnicos-clave-el-secreto-de-la-eficiencia-y-el-rendimiento" class="nav-link" data-scroll-target="#análisis-de-elementos-técnicos-clave-el-secreto-de-la-eficiencia-y-el-rendimiento">9.6.2 Análisis de elementos técnicos clave: El secreto de la eficiencia y el rendimiento</a></li>
  <li><a href="#entrenamiento-del-modelo-guía-de-entrenamiento-para-simple_mistral" id="toc-entrenamiento-del-modelo-guía-de-entrenamiento-para-simple_mistral" class="nav-link" data-scroll-target="#entrenamiento-del-modelo-guía-de-entrenamiento-para-simple_mistral">9.6.3 Entrenamiento del modelo: Guía de entrenamiento para <code>simple_mistral</code></a></li>
  <li><a href="#generación-de-texto-usando-la-función-generate-creación-de-oraciones-creativas" id="toc-generación-de-texto-usando-la-función-generate-creación-de-oraciones-creativas" class="nav-link" data-scroll-target="#generación-de-texto-usando-la-función-generate-creación-de-oraciones-creativas">9.6.4 Generación de texto usando la función <code>generate()</code>: Creación de oraciones creativas</a></li>
  <li><a href="#ejemplo-de-predicción-de-secuencia-numérica-análisis-de-train_seq_num.py" id="toc-ejemplo-de-predicción-de-secuencia-numérica-análisis-de-train_seq_num.py" class="nav-link" data-scroll-target="#ejemplo-de-predicción-de-secuencia-numérica-análisis-de-train_seq_num.py">9.6.5 Ejemplo de predicción de secuencia numérica: análisis de <code>train_seq_num.py</code></a></li>
  <li><a href="#ejemplo-de-predicción-de-operaciones-aritméticas-análisis-de-train_math.py" id="toc-ejemplo-de-predicción-de-operaciones-aritméticas-análisis-de-train_math.py" class="nav-link" data-scroll-target="#ejemplo-de-predicción-de-operaciones-aritméticas-análisis-de-train_math.py">9.6.6 Ejemplo de predicción de operaciones aritméticas: Análisis de <code>train_math.py</code></a></li>
  <li><a href="#ejemplo-de-generación-de-consultas-sql-a-partir-del-lenguaje-natural-análisis-de-train_sql.py" id="toc-ejemplo-de-generación-de-consultas-sql-a-partir-del-lenguaje-natural-análisis-de-train_sql.py" class="nav-link" data-scroll-target="#ejemplo-de-generación-de-consultas-sql-a-partir-del-lenguaje-natural-análisis-de-train_sql.py">9.6.7 Ejemplo de generación de consultas SQL a partir del lenguaje natural: Análisis de <code>train_sql.py</code></a></li>
  </ul></li>
  <li><a href="#gemma-revisión-del-último-modelo-de-código-abierto" id="toc-gemma-revisión-del-último-modelo-de-código-abierto" class="nav-link" data-scroll-target="#gemma-revisión-del-último-modelo-de-código-abierto">9.7 Gemma: Revisión del último modelo de código abierto</a></li>
  <li><a href="#phi-3-un-modelo-de-lenguaje-pequeño-pero-potente" id="toc-phi-3-un-modelo-de-lenguaje-pequeño-pero-potente" class="nav-link" data-scroll-target="#phi-3-un-modelo-de-lenguaje-pequeño-pero-potente">9.8 Phi-3 : un modelo de lenguaje pequeño pero potente</a>
  <ul class="collapse">
  <li><a href="#simple_phi3-modelo" id="toc-simple_phi3-modelo" class="nav-link" data-scroll-target="#simple_phi3-modelo">9.8.1 <code>simple_phi3</code> modelo</a></li>
  <li><a href="#ejemplo-del-modelo-simple_phi3-cálculo-de-expresiones-complejas" id="toc-ejemplo-del-modelo-simple_phi3-cálculo-de-expresiones-complejas" class="nav-link" data-scroll-target="#ejemplo-del-modelo-simple_phi3-cálculo-de-expresiones-complejas">9.8.2 Ejemplo del modelo <code>simple_phi3</code>: cálculo de expresiones complejas</a></li>
  </ul></li>
  <li><a href="#conclusión-1" id="toc-conclusión-1" class="nav-link" data-scroll-target="#conclusión-1">Conclusión</a></li>
  <li><a href="#ejercicios-de-práctica" id="toc-ejercicios-de-práctica" class="nav-link" data-scroll-target="#ejercicios-de-práctica">Ejercicios de práctica</a></li>
  <li><a href="#material-de-referencia" id="toc-material-de-referencia" class="nav-link" data-scroll-target="#material-de-referencia">material de referencia</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../../notebooks/es/part_1/01_El inicio del aprendizaje profundo.html">part_1</a></li><li class="breadcrumb-item"><a href="../../../notebooks/es/part_1/09_La evolución del transformer.html">9. La evolución del transformer</a></li></ol></nav></header>




<p><a href="https://colab.research.google.com/github/Quantum-Intelligence-Frontier/dldna/blob/main/notebooks/es/part_1/09_la_evolución_del_transformer.ipynb" target="_parent"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Abrir en Colab"> </a></p>
<section id="capítulo-la-evolución-de-los-transformers-hacia-la-eficiencia-y-escalabilidad" class="level1">
<h1>9 Capítulo La evolución de los Transformers: hacia la eficiencia y escalabilidad</h1>
<blockquote class="blockquote">
<p>“La eficiencia es el puente hacia la inteligencia.” - Alan Turing</p>
</blockquote>
<p>Desde la aparición de los Transformers en 2017, han surgido sucesivamente modelos de lenguaje grandes representados por BERT y GPT. Estos modelos han abierto una nueva era en la inteligencia artificial con sus sorprendentes rendimientos. Sin embargo, detrás de este éxito se encontraban las limitaciones fundamentales de la arquitectura de los Transformers y los esfuerzos para superarlas. Se realizaron mejoras constantes y propuestas estructurales para superar problemas de complejidad computacional y limitaciones en el procesamiento de textos largos. En particular, desde 2019, con el rápido aumento del tamaño de los modelos, se ha intensificado la investigación sobre eficiencia.</p>
<p><strong>Cambios principales por período:</strong></p>
<ul>
<li>2019-2020: Focalizado en reducir complejidad</li>
<li>2021-2022: Focalizado en eficiencia de memoria</li>
<li>2023-2024: Focalizado en escalabilidad y propósitos especiales (ética, modelos abiertos, etc.)</li>
</ul>
<p>En este capítulo examinaremos las limitaciones de los Transformers y detallaremos diversas soluciones para superarlas.</p>
<section id="limitaciones-y-desafíos-de-los-transformers" class="level2">
<h2 class="anchored" data-anchor-id="limitaciones-y-desafíos-de-los-transformers">9.1 Limitaciones y desafíos de los Transformers</h2>
<blockquote class="blockquote">
<p><strong>Desafío:</strong> ¿Cómo reducir la complejidad computacional y el uso de memoria del modelo Transformer, procesar contextos más largos y entrenar modelos más grandes?</p>
<p><strong>Angustia del investigador:</strong> Aunque el rendimiento de los modelos Transformer era sobresaliente, su costo computacional era enorme. En particular, el mecanismo de atención tenía una complejidad proporcional al cuadrado de la longitud de la secuencia, lo que limitaba severamente la escalabilidad del modelo. Los investigadores tenían que encontrar formas de aumentar la eficiencia computacional mientras mantenían las funciones esenciales de la atención. No se trataba simplemente de reducir el tamaño del modelo, sino de buscar soluciones innovadoras a nivel de algoritmos y hardware. Esto era un desafío similar a construir un edificio enorme mientras se reducía el peso y el costo de cada ladrillo.</p>
</blockquote>
<p>La complejidad cuadrática de la operación de atención, las longitudes limitadas de contexto y los problemas de eficiencia en memoria fueron los principales obstáculos para la expansión del modelo. Estas limitaciones se convirtieron en factores cruciales que determinaron la dirección del desarrollo de los Transformers.</p>
<section id="limitaciones-básicas-de-la-arquitectura-transformer-complejidad-computacional" class="level3">
<h3 class="anchored" data-anchor-id="limitaciones-básicas-de-la-arquitectura-transformer-complejidad-computacional">9.1.1 Limitaciones básicas de la arquitectura Transformer: complejidad computacional</h3>
<p>Durante el proceso de escalado de modelos Transformer, la complejidad de las operaciones de atención, particularmente la proporcional a la longitud cuadrática de la secuencia, fue un gran problema.</p>
<p><strong>Análisis de la complejidad de la operación de atención:</strong></p>
<p><span class="math inline">\(Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V\)</span></p>
<ol type="1">
<li><span class="math inline">\(QK^T\)</span> cálculo: <span class="math inline">\(O(N^2d)\)</span> (d: dimensión de embedding)</li>
<li>Operación Softmax: <span class="math inline">\(O(N^2)\)</span></li>
<li>Multiplicación del resultado de softmax con V: <span class="math inline">\(O(N^2d)\)</span></li>
</ol>
<p>Vamos a ver esto en código para observar la velocidad de ejecución y el uso de memoria.</p>
<div id="cell-2" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip install dldna[colab] <span class="co"># in Colab</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="co"># !pip install dldna[all] # in your local</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>load_ext autoreload</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>autoreload <span class="dv">2</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="cell-3" class="cell" data-execution_count="1">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_09.complexity_benchmark <span class="im">import</span> measure_attention_complexity, plot_complexity_analysis, measure_attention_complexity_gpu</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>seq_lengths <span class="op">=</span> [<span class="dv">100</span>, <span class="dv">500</span>, <span class="dv">1000</span>, <span class="dv">2000</span>, <span class="dv">4000</span>, <span class="dv">8000</span>, <span class="dv">10000</span>, <span class="dv">15000</span>]</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> measure_attention_complexity(seq_lengths<span class="op">=</span>seq_lengths)</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">=== Complexity Analysis of Attention Operation ==="</span>)</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Memory usage and execution time by sequence length:"</span>)</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Length</span><span class="ch">\t\t</span><span class="st">Memory (MB)</span><span class="ch">\t</span><span class="st">Time (seconds)"</span>)</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"-"</span> <span class="op">*</span> <span class="dv">40</span>)</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> seq_len, mem, time_taken <span class="kw">in</span> results:</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>seq_len<span class="sc">}</span><span class="ch">\t\t</span><span class="sc">{</span>mem<span class="sc">:.2f}</span><span class="ch">\t\t</span><span class="sc">{</span>time_taken<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize with a graph</span></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>plot_complexity_analysis(results)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
=== Complexity Analysis of Attention Operation ===

Memory usage and execution time by sequence length:
Length      Memory (MB) Time (seconds)
----------------------------------------
100     18.75       0.0037
500     96.58       0.0388
1000        317.00      0.1187
2000        1119.00     0.4228
4000        4188.14     1.6553
8000        16142.53        6.5773
10000       25039.31        10.2601
15000       55868.54        25.1265</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="09_La evolución del transformer_files/figure-html/cell-3-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>En los modelos de transformador reales, esta operación se repite a través de múltiples capas. El aumento del tamaño del lote también aumenta la cantidad de cálculos.</p>
<div id="cell-5" class="cell" data-execution_count="2">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compare theoretical complexity with actual measurements</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">=== Comparison of Theoretical Complexity and Actual Measurements ==="</span>)</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>base_seq <span class="op">=</span> seq_lengths[<span class="dv">0</span>]</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>base_mem <span class="op">=</span> results[<span class="dv">0</span>][<span class="dv">1</span>]</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>base_time <span class="op">=</span> results[<span class="dv">0</span>][<span class="dv">2</span>]</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Theoretical vs Actual Growth Rate (Base: First Sequence Length)"</span>)</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Length      Theoretical(N²)      Actual Memory      Actual Time"</span>)</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"-"</span> <span class="op">*</span> <span class="dv">60</span>)</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> seq_len, mem, time_taken <span class="kw">in</span> results:</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>    theoretical <span class="op">=</span> (seq_len<span class="op">/</span>base_seq) <span class="op">**</span> <span class="dv">2</span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>    actual_mem <span class="op">=</span> mem<span class="op">/</span>base_mem</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>    actual_time <span class="op">=</span> time_taken<span class="op">/</span>base_time</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>seq_len<span class="sc">:6d}</span><span class="ss">    </span><span class="sc">{</span>theoretical<span class="sc">:10.2f}</span><span class="ss">x    </span><span class="sc">{</span>actual_mem<span class="sc">:10.2f}</span><span class="ss">x    </span><span class="sc">{</span>actual_time<span class="sc">:10.2f}</span><span class="ss">x"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
=== Comparison of Theoretical Complexity and Actual Measurements ===

Theoretical vs Actual Growth Rate (Base: First Sequence Length)
Length      Theoretical(N²)      Actual Memory      Actual Time
------------------------------------------------------------
   100          1.00x          1.00x          1.00x
   500         25.00x          5.15x          8.05x
  1000        100.00x         16.91x         32.49x
  2000        400.00x         59.71x        124.52x
  4000       1600.00x        223.34x        474.71x
  8000       6400.00x        860.92x       1882.04x
 10000      10000.00x       1335.43x       2976.84x
 15000      22500.00x       2979.67x       7280.40x</code></pre>
</div>
</div>
<p>La complejidad cuadrática es particularmente grave en modelos a gran escala como GPT-3. Esto ha llevado a numerosas limitaciones, como la restricción de procesamiento de documentos largos y el límite de tamaño de lote durante el entrenamiento. Esta situación se convirtió en un motivo principal para desarrollar mecanismos de atención más eficientes.</p>
<p>Los primeros intentos para abordar el problema de complejidad cuadrática en los transformadores tomaron principalmente tres direcciones.</p>
<p><strong>Atención con ventana deslizante</strong></p>
<p>Calcula la atención solo dentro de una ventana de tamaño fijo.</p>
<div id="cell-7" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sliding_window_attention(q, k, v, window_size):</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Sliding window attention"""</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>    batch_size, seq_len, dim <span class="op">=</span> q.shape</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>    attention_weights <span class="op">=</span> np.zeros((batch_size, seq_len, seq_len))</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(seq_len):</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>        start <span class="op">=</span> <span class="bu">max</span>(<span class="dv">0</span>, i <span class="op">-</span> window_size <span class="op">//</span> <span class="dv">2</span>)</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>        end <span class="op">=</span> <span class="bu">min</span>(seq_len, i <span class="op">+</span> window_size <span class="op">//</span> <span class="dv">2</span> <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>        scores <span class="op">=</span> np.matmul(q[:, i:i<span class="op">+</span><span class="dv">1</span>], k[:, start:end].transpose(<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">1</span>))</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>        attention_weights[:, i, start:end] <span class="op">=</span> softmax(scores, axis<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.matmul(attention_weights, v)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Este método reduce la complejidad a <span class="math inline">\(O(N \cdot w)\)</span> (w: tamaño de la ventana).</p>
<p><strong>Patrón de atención dispersa</strong></p>
<p>El patrón de atención dispersa es un enfoque que, en lugar de calcular las relaciones entre todas las parejas de tokens, solo calcula algunas relaciones según ciertos patrones. Por ejemplo, si tenemos una secuencia compuesta por 10 tokens, la atención normal calcularía 100 (10×10) relaciones, pero la atención dispersa solo calculará un subconjunto de estas.</p>
<div id="cell-9" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sparse_block_attention(q, k, v, block_size):</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Block sparse attention</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Example: seq_len=8, block_size=2</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="co">    Process the sequence in 4 blocks of 2 tokens each</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Block 1 (0,1), Block 2 (2,3), Block 3 (4,5), Block 4 (6,7)</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>    batch_size, seq_len, dim <span class="op">=</span> q.shape  <span class="co"># e.g., (1, 8, 64)</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>    num_blocks <span class="op">=</span> seq_len <span class="op">//</span> block_size  <span class="co"># e.g., 8/2 = 4 blocks</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>    attention_weights <span class="op">=</span> np.zeros((batch_size, seq_len, seq_len))</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_blocks):</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># e.g., when i=0, process Block 1 (0,1)</span></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>        start_q <span class="op">=</span> i <span class="op">*</span> block_size  <span class="co"># 0</span></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>        end_q <span class="op">=</span> (i <span class="op">+</span> <span class="dv">1</span>) <span class="op">*</span> block_size  <span class="co"># 2</span></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(num_blocks):</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>            <span class="co"># e.g., when j=0, attention with Block 1 (0,1)</span></span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>            start_k <span class="op">=</span> j <span class="op">*</span> block_size  <span class="co"># 0</span></span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>            end_k <span class="op">=</span> (j <span class="op">+</span> <span class="dv">1</span>) <span class="op">*</span> block_size  <span class="co"># 2</span></span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Calculate attention between tokens in Block 1 (0,1) and Block 1 tokens (0,1)</span></span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a>            scores <span class="op">=</span> np.matmul(</span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a>                q[:, start_q:end_q],  <span class="co"># (1, 2, 64)</span></span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a>                k[:, start_k:end_k].transpose(<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">1</span>)  <span class="co"># (1, 64, 2)</span></span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a>            )  <span class="co"># Result: (1, 2, 2)</span></span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Store weights block by block</span></span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a>            attention_weights[:, start_q:end_q, start_k:end_k] <span class="op">=</span> softmax(scores, axis<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Generate the final context vectors</span></span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.matmul(attention_weights, v)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Aproximación de rango bajo</p>
<p>La aproximación de rango bajo es una técnica que representa matrices grandes como el producto de matrices más pequeñas. Por ejemplo, en una oración con 10 tokens, la atención general calcula 10×10=100 relaciones, mientras que la aproximación de rango bajo las representa como el producto de dos matrices de 10×4 y 4×10 (rango=4). De esta manera, se obtienen resultados similares con solo 80 operaciones en lugar de 100.</p>
<div id="cell-11" class="cell" data-execution_count="4">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> low_rank_attention(q, k, v, rank):</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Low-rank attention</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Example: seq_len=10, dim=64, rank=16</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="co">    Project Q, K from 64 dimensions to 16 dimensions to reduce computation</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>    batch_size, seq_len, dim <span class="op">=</span> q.shape  <span class="co"># e.g., (1, 10, 64)</span></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create projection matrices to project from 64 dimensions to 16 dimensions</span></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>    projection_q <span class="op">=</span> np.random.randn(dim, rank) <span class="op">/</span> np.sqrt(rank)  <span class="co"># (64, 16)</span></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>    projection_k <span class="op">=</span> np.random.randn(dim, rank) <span class="op">/</span> np.sqrt(rank)</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Project Q, K to 16 dimensions</span></span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>    q_low <span class="op">=</span> np.matmul(q, projection_q)  <span class="co"># (1, 10, 16)</span></span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>    k_low <span class="op">=</span> np.matmul(k, projection_k)  <span class="co"># (1, 10, 16)</span></span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate attention in the lower dimension (operations on 10x16 matrices)</span></span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>    attention <span class="op">=</span> np.matmul(q_low, k_low.transpose(<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">1</span>))  <span class="co"># (1, 10, 10)</span></span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>    attention_weights <span class="op">=</span> softmax(attention, axis<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Generate the final context vectors</span></span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.matmul(attention_weights, v)  <span class="co"># (1, 10, 64)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Este método puede reducir la complejidad a <span class="math inline">\(O(N \cdot r)\)</span>. Aquí, <span class="math inline">\(r\)</span> es el rango utilizado en la aproximación. Vamos a calcular la eficiencia de cada método.</p>
<div id="cell-13" class="cell" data-execution_count="2">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_09.attention_complexity_examples <span class="im">import</span> calcualte_efficieny</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>calcualte_efficieny()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Original input shape: (2, 8, 4)

1. Sliding Window Attention
Output shape: (2, 8, 4)
Output of the first batch, first token: [-0.78236164  0.22592055 -1.03027549  1.13998368]

2. Block Sparse Attention
Output shape: (2, 8, 4)
Output of the first batch, first token: [-1.66095776  0.76700744 -0.45857165 -0.77422867]

3. Low-Rank Attention
Output shape: (2, 8, 4)
Output of the first batch, first token: [ 0.51121005  0.66772692 -0.77623488 -0.0323534 ]

Memory Usage Comparison (Relative Size):
Full Attention: 64
Sliding Window: 32
Block Sparse: 64
Low Rank: 32</code></pre>
</div>
</div>
<p>Sin embargo, los primeros intentos mostraron limitaciones como la pérdida de información, la complejidad de implementación y el deterioro del rendimiento. Google se enfocó en la aproximación de rango bajo, mientras que Microsoft centró sus esfuerzos en el desarrollo de patrones dispersos. Posteriormente, estos enfoques iniciales evolucionaron hacia un método híbrido que aprovecha tanto la dispersidad como las características de rango bajo.</p>
</section>
<section id="limitaciones-básicas-de-la-arquitectura-del-transformer-eficiencia-de-memoria" class="level3">
<h3 class="anchored" data-anchor-id="limitaciones-básicas-de-la-arquitectura-del-transformer-eficiencia-de-memoria">9.1.2 Limitaciones básicas de la arquitectura del transformer: eficiencia de memoria</h3>
<p>Otra limitación importante es la eficiencia de memoria. Particularmente en modelos de lenguaje a gran escala, existen las siguientes cargas de memoria.</p>
<p>En primer lugar, la carga de memoria causada por el caché KV (Key-Value). Durante el proceso de generación autoregresiva, se deben almacenar los valores de Key y Value de los pasos temporales anteriores, lo cual aumenta linealmente con la longitud de la secuencia. Por ejemplo, en el caso de GPT-3, al procesar 2048 tokens, se requiere aproximadamente 16MB de caché KV por cada capa. En segundo lugar, los requisitos de memoria del proceso de retropropagación. El transformer almacena los valores de activación intermedios (activation value) - los resultados de cálculos intermedios que ocurren en la capa de atención (valores transformados Q, K, V, scores de atención, salidas softmax, etc.) - lo cual aumenta drásticamente a medida que se incrementa el número de capas. En el caso de BERT-large, incluso con un solo lote, se requerían aproximadamente 24GB de memoria. En tercer lugar, el uso de memoria por las operaciones de atención en sí mismas. La matriz de scores de atención tiene un tamaño proporcional al cuadrado de la longitud de la secuencia, lo que puede ser un cuello de botella grave al procesar documentos largos.</p>
<p>Para abordar estos problemas de memoria, se han propuesto técnicas de optimización como el checkpointing de gradientes, el entrenamiento con precisión mixta y FlashAttention.</p>
</section>
<section id="tendencias-temporales-en-el-desarrollo-del-transformer-y-la-estructura-de-este-capítulo" class="level3">
<h3 class="anchored" data-anchor-id="tendencias-temporales-en-el-desarrollo-del-transformer-y-la-estructura-de-este-capítulo">9.1.3 Tendencias temporales en el desarrollo del transformer y la estructura de este capítulo</h3>
<p>Para superar las limitaciones de complejidad computacional y eficiencia de memoria discutidas en las secciones 9.1.1 y 9.1.2, los investigadores han desarrollado diversas técnicas para mejorar la eficiencia y escalabilidad. Estas técnicas han hecho que los modelos transformer sean más potentes y prácticos, y han tenido un gran impacto en el campo del aprendizaje profundo.</p>
<p>En este capítulo, presentamos un resumen de las tendencias temporales en el desarrollo del transformer, introduciendo las principales tecnologías y modelos de cada período, como se muestra en la siguiente tabla.</p>
<p><strong>Tabla: Tendencias temporales en el desarrollo del transformer, modelos/técnicas clave, contenido principal, ADN del aprendizaje profundo</strong> | Sección | Período (aproximado) | Principales modelos/técnicas | Contenido y explicación clave | ADN del aprendizaje profundo | |———–|———————–|————————————|————————————–|———————————–| | <strong>9.1</strong> | 2017-2018 | Transformer | Introducción de un mecanismo de Attention para superar las limitaciones de los RNN y CNN tradicionales.<br>Innovación en modelos sequence-to-sequence | <strong>Mecanismo de Attention</strong>: Proporciona una nueva forma de enfocarse en partes importantes de los datos | | <strong>9.2</strong> | 2019-2020 | Performer, Sparse Transformer, Longformer <br> Reformer, BigBird | Enfoque de software para <strong>reducir la complejidad computacional</strong>.<br><strong>Atención lineal</strong>: Aproximación del cálculo de atención (Performer).<br><strong>Atención dispersa</strong>: Aplicación de atención solo a ciertas pares de tokens (Sparse Transformer, Longformer).<br><strong>Atención local-global</strong>: Combinación de información local y global (Reformer, BigBird) | <strong>Atención eficiente</strong>: Esfuerzos para mantener las ventajas de la atención mientras se reduce la complejidad computacional.<br><strong>Dependencias a larga distancia</strong>: Mejoras en la estructura para manejar contextos largos efectivamente | | <strong>9.3</strong> | 2021-2022 | FlashAttention, MQA, GQA, PagedAttention, vLLM | Enfoques de hardware y software para <strong>mejorar la eficiencia en memoria</strong>.<br><strong>FlashAttention</strong>: Uso de la jerarquía de memoria GPU, tiling, procesamiento por bloques.<br><strong>MQA/GQA</strong>: Optimización de consultas, compartición Key/Value.<br><strong>Optimización del caché KV</strong>: PagedAttention, vLLM | <strong>Optimización de hardware</strong>: Métodos eficientes de cálculo considerando la estructura de memoria GPU.<br><strong>Procesamiento paralelo</strong>: Aumento de la eficiencia en el procesamiento a través de compartición de consultas | | <strong>9.4</strong> | 2022-2023 | Claude-2, LongLoRA, Constitutional AI, RLHF, <br>RLAIF, Atención jerárquica, Memoria recurrente | Arquitecturas para la <strong>escalabilidad y fines específicos</strong>.<br><strong>Contexto largo</strong>: Atención jerárquica, Transformador de memoria recurrente.<br><strong>Ética/seguridad</strong>: Atención basada en reglas, ajustes basados en aprendizaje por refuerzo | <strong>Contexto largo</strong>: Evolución de la estructura del modelo para manejar contextos más largos.<br><strong>Ajuste fino</strong>: Métodos para ajustar el modelo a fines específicos | | <strong>9.5</strong> | 2022-2023 | Codificador eficiente (basado en FlashAttention) | Clasificación de texto (AG News), FlashAttention, Pre-LN, Gradient Checkpointing, Entrenamiento con precisión mixta | <strong>Implementación:</strong> Uso de un codificador eficiente | | <strong>9.6</strong> | 2023 | Mistral, Decodificador eficiente (basado en GQA y Atención de ventana deslizante) | Análisis del modelo Mistral: GQA, Atención de ventana deslizante, RoPE, caché KV.<br>Ejemplos de aplicación: Conversión número-texto, conversión de lenguaje natural-SQL (generación de código), generación de texto-código. | <strong>Implementación:</strong> Arquitectura de decodificador eficiente | | <strong>9.7</strong> | 2024 | Gemma | Modelo abierto para mejorar la eficiencia y el acceso | <strong>Modelo abierto</strong>: Mejora del acceso a la investigación y al desarrollo | | <strong>9.8</strong> | 2024 | Phi-3 | LLM pequeño pero eficiente | <strong>Implementación:</strong> Potente SLM (Small Language Model) | La estructura de este capítulo es la siguiente:</p>
<ul>
<li><strong>Sección 9.2:</strong> Trata los enfoques de software para reducir la complejidad computacional de las operaciones de atención (aproximación, dispersión, atención local-global).</li>
<li><strong>Sección 9.3:</strong> Examina los enfoques de hardware y software para mejorar la eficiencia de memoria (FlashAttention, optimización de consultas, gestión de caché KV).</li>
<li><strong>Sección 9.4:</strong> Discute sobre la escalabilidad del modelo y las arquitecturas especiales con fines específicos (procesamiento de contexto largo, restricciones éticas/seguridad).</li>
<li><strong>Sección 9.5:</strong> Implementa un modelo de codificador eficiente y compara su eficiencia con otros modelos similares mediante un ejemplo de clasificación de AG news.</li>
<li><strong>Sección 9.6:</strong> Implementa un modelo de decodificador eficiente, el modelo Mistral simple, y presenta ejemplos de aplicaciones.</li>
<li><strong>Sección 9.7:</strong> Introduce a gemma, un ejemplo representativo de modelos abiertos.</li>
<li><strong>Sección 9.8:</strong> Implementa un modelo simple del potente SLM phi-3 y examina ejemplos de aplicaciones.</li>
</ul>
</section>
</section>
<section id="reducción-de-complejidad-optimización-atencional-en-software-2019-2020" class="level2">
<h2 class="anchored" data-anchor-id="reducción-de-complejidad-optimización-atencional-en-software-2019-2020">9.2 Reducción de complejidad: optimización atencional en software (2019-2020)</h2>
<section id="enfoques-iniciales-aproximación-y-esparsificación" class="level3">
<h3 class="anchored" data-anchor-id="enfoques-iniciales-aproximación-y-esparsificación">9.2.1 Enfoques iniciales: aproximación y esparsificación</h3>
<p>Desde 2019 hasta 2020, se realizaron varios intentos para reducir la complejidad computacional de los transformers. Particularmente, el desarrollo durante este período liderado por Google Research y DeepMind mejoró significativamente la eficiencia de las operaciones de atención.</p>
<section id="atención-lineal-performer" class="level4">
<h4 class="anchored" data-anchor-id="atención-lineal-performer">9.2.1.1 Atención lineal: Performer</h4>
<p>A principios de 2020, el equipo de Google Research logró reducir la complejidad de la atención de O(N²) a O(N) mediante FAVOR+ (Fast Attention Via positive Orthogonal Random features). FAVOR+ es el mecanismo central del modelo Performer y fue el primer método que permitió procesar secuencias largas de manera práctica.</p>
<p>La idea clave detrás de FAVOR+ comienza con el <strong>truco del kernel</strong>. El truco del kernel reinterpreta la atención softmax de la siguiente manera:</p>
<p><span class="math inline">\(Attention(Q,K,V) = softmax(\frac{QK^T}{\sqrt{d}})V\)</span></p>
<p>Esto puede aproximarse utilizando una función kernel φ(x) que toma valores positivos de la siguiente manera:</p>
<p><span class="math inline">\(Attention(Q,K,V) ≈ \frac{\phi(Q)\phi(K)^TV}{\phi(Q)\phi(K)^T\mathbf{1}}\)</span></p>
<p>La clave es reinterpretar la atención softmax en forma fraccionaria y utilizar una función kernel φ(x) para reorganizar el orden de las multiplicaciones matriciales. Es similar a cambiar <span class="math inline">\((a \times b) \times c\)</span> por <span class="math inline">\(a \times (b \times c)\)</span>.</p>
<div id="cell-17" class="cell" data-execution_count="3">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> kernel_attention(Q, K, V, feature_dim<span class="op">=</span><span class="dv">256</span>): <span class="co"># Q: (seq_len, d_model) K: (seq_len, d_model) V: (seq_len, d_model)</span></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 1. Generate random projection matrix</span></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>    projection <span class="op">=</span> np.random.randn(Q.shape[<span class="op">-</span><span class="dv">1</span>], feature_dim) <span class="op">/</span> np.sqrt(feature_dim)  </span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># projection: (d_model, feature_dim)</span></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 2. Project Q, K to lower dimension and apply ReLU</span></span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>    Q_mapped <span class="op">=</span> np.maximum(<span class="dv">0</span>, np.dot(Q, projection))  <span class="co"># phi(Q)</span></span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Q_mapped: (seq_len, feature_dim)</span></span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>    K_mapped <span class="op">=</span> np.maximum(<span class="dv">0</span>, np.dot(K, projection))  <span class="co"># phi(K)</span></span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># K_mapped: (seq_len, feature_dim)</span></span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 3. Calculate numerator: phi(Q)phi(K)^TV</span></span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a>    KV <span class="op">=</span> np.dot(K_mapped.T, V)  <span class="co"># (feature_dim, V_dim)</span></span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># KV: (feature_dim, d_model)</span></span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a>    numerator <span class="op">=</span> np.dot(Q_mapped, KV)  <span class="co"># (seq_len, V_dim)</span></span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># numerator: (seq_len, d_model)</span></span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 4. Calculate denominator: phi(Q)phi(K)^T1</span></span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a>    K_sum <span class="op">=</span> np.<span class="bu">sum</span>(K_mapped, axis<span class="op">=</span><span class="dv">0</span>, keepdims<span class="op">=</span><span class="va">True</span>)  <span class="co"># (1, feature_dim)</span></span>
<span id="cb11-23"><a href="#cb11-23" aria-hidden="true" tabindex="-1"></a>    <span class="co"># K_sum: (1, feature_dim)</span></span>
<span id="cb11-24"><a href="#cb11-24" aria-hidden="true" tabindex="-1"></a>    denominator <span class="op">=</span> np.dot(Q_mapped, K_sum.T)  <span class="co"># (seq_len, 1)</span></span>
<span id="cb11-25"><a href="#cb11-25" aria-hidden="true" tabindex="-1"></a>    <span class="co"># denominator: (seq_len, 1)</span></span>
<span id="cb11-26"><a href="#cb11-26" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-27"><a href="#cb11-27" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 5. Final attention output</span></span>
<span id="cb11-28"><a href="#cb11-28" aria-hidden="true" tabindex="-1"></a>    attention_output <span class="op">=</span> numerator <span class="op">/</span> (denominator <span class="op">+</span> <span class="fl">1e-6</span>)</span>
<span id="cb11-29"><a href="#cb11-29" aria-hidden="true" tabindex="-1"></a>    <span class="co"># attention_output: (seq_len, d_model)</span></span>
<span id="cb11-30"><a href="#cb11-30" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-31"><a href="#cb11-31" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> attention_output</span>
<span id="cb11-32"><a href="#cb11-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-33"><a href="#cb11-33" aria-hidden="true" tabindex="-1"></a><span class="co"># Example usage</span></span>
<span id="cb11-34"><a href="#cb11-34" aria-hidden="true" tabindex="-1"></a>seq_len, d_model <span class="op">=</span> <span class="dv">1000</span>, <span class="dv">64</span></span>
<span id="cb11-35"><a href="#cb11-35" aria-hidden="true" tabindex="-1"></a>Q <span class="op">=</span> np.random.randn(seq_len, d_model)</span>
<span id="cb11-36"><a href="#cb11-36" aria-hidden="true" tabindex="-1"></a>K <span class="op">=</span> np.random.randn(seq_len, d_model)</span>
<span id="cb11-37"><a href="#cb11-37" aria-hidden="true" tabindex="-1"></a>V <span class="op">=</span> np.random.randn(seq_len, d_model)</span>
<span id="cb11-38"><a href="#cb11-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-39"><a href="#cb11-39" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate attention with O(N) complexity</span></span>
<span id="cb11-40"><a href="#cb11-40" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> kernel_attention(Q, K, V)</span>
<span id="cb11-41"><a href="#cb11-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-42"><a href="#cb11-42" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(output)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>[[-0.00705502 -0.01553617 -0.01976792 ... -0.00906909  0.02983678
   0.0424082 ]
 [-0.00201811 -0.01741265 -0.00458378 ... -0.02578894  0.04247468
   0.03793401]
 [-0.01130314 -0.02011524 -0.00962334 ... -0.01348429  0.04382548
   0.01967338]
 ...
 [ 0.00180466 -0.01818735 -0.02244794 ... -0.01978542  0.03202302
   0.03887265]
 [-0.00421543 -0.01679868 -0.00537492 ... -0.00314385  0.05363415
   0.03304721]
 [ 0.00107896 -0.02042812 -0.01947976 ... -0.00557582  0.04534007
   0.04408479]]</code></pre>
</div>
</div>
<p>Las tres cambios clave introducidos por FAVOR+ son los siguientes:</p>
<ol type="1">
<li><strong>Estimación imparcial:</strong> al usar características aleatorias ortogonales estándar para calcular los valores de atención, se asegura que la media de los valores aproximados coincida con el valor de atención real.</li>
<li><strong>Características positivas:</strong> se utilizan las funciones de activación ReLU para hacer que todos los valores de características sean positivos. Esto aumenta la estabilidad numérica.</li>
<li><strong>Proyección ortogonal estándar:</strong> se utiliza una matriz ortogonal estándar para proyectar la entrada a un espacio de menor dimensión. Esto preserva al máximo las distancias y ángulos entre los vectores, minimizando el error de aproximación.</li>
</ol>
<p>Los pasos de procesamiento de FAVOR+ son los siguientes:</p>
<ol type="1">
<li><strong>Transformación de datos y reducción de dimensiones:</strong> se transforman los datos de entrada (Q, K, V) a un espacio de características ortogonales estándar de menor dimensión.
<ul>
<li>Proyección al espacio de características ortogonales estándar: cada vector de entrada se transforma en una forma independiente y equilibrada.</li>
<li>Reducción de dimensiones: se comprimen las entradas de alta dimensión a una dimensión más baja.</li>
<li>Preservación de información: se reduce la dimensionalidad manteniendo las relaciones importantes.</li>
<li>Cambio de dimensiones: (longitud de secuencia × dimensión de incrustación) → (longitud de secuencia × dimensión de características)</li>
</ul></li>
<li><strong>Operación de atención lineal:</strong> se calcula eficientemente la atención en el espacio de características transformado.
<ul>
<li>Operación en el espacio de características: se calcula la similitud entre los vectores proyectados.</li>
<li>Eficiencia de memoria: uso de memoria lineal proporcional a la longitud de la secuencia (O(N × d), N: longitud de la secuencia, d: dimensión de características).</li>
<li>Optimización de cálculo: se reorganiza el orden de las multiplicaciones matriciales para reducir la complejidad a O(N × d) (anteriormente O(N²)).</li>
</ul></li>
</ol>
<div id="cell-19" class="cell" data-execution_count="4">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> favor_plus_attention(q, k, v, feature_dim<span class="op">=</span><span class="dv">256</span>):</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""FAVOR+ attention implementation</span></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a><span class="co">        q: Query tensor (batch_size, seq_len, d_model)</span></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a><span class="co">        k: Key tensor (batch_size, seq_len, d_model)</span></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a><span class="co">        v: Value tensor (batch_size, seq_len, d_model)</span></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a><span class="co">        feature_dim: The number of dimensions of the low-dimensional feature space</span></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>    d_model <span class="op">=</span> q.shape[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 1. Generate an orthonormal random projection matrix</span></span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>    random_matrix <span class="op">=</span> np.random.randn(d_model, feature_dim)</span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>    q_orth, _ <span class="op">=</span> np.linalg.qr(random_matrix)</span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>    projection <span class="op">=</span> q_orth <span class="op">/</span> np.sqrt(feature_dim)  <span class="co"># (d_model, feature_dim)</span></span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 2. Project Q, K to the low-dimensional feature space and apply ReLU</span></span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a>    q_prime <span class="op">=</span> np.maximum(<span class="dv">0</span>, np.matmul(q, projection))  <span class="co"># (batch_size, seq_len, feature_dim)</span></span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a>    k_prime <span class="op">=</span> np.maximum(<span class="dv">0</span>, np.matmul(k, projection))  <span class="co"># (batch_size, seq_len, feature_dim)</span></span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 3. Calculate linear-time attention</span></span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Use einsum to perform matrix multiplication while maintaining the batch dimension</span></span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a>    kv <span class="op">=</span> np.einsum(<span class="st">'bsf,bsd-&gt;bfd'</span>, k_prime, v)  <span class="co"># (batch_size, feature_dim, d_model)</span></span>
<span id="cb13-25"><a href="#cb13-25" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-26"><a href="#cb13-26" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate the numerator</span></span>
<span id="cb13-27"><a href="#cb13-27" aria-hidden="true" tabindex="-1"></a>    numerator <span class="op">=</span> np.einsum(<span class="st">'bsf,bfd-&gt;bsd'</span>, q_prime, kv)  <span class="co"># (batch_size, seq_len, d_model)</span></span>
<span id="cb13-28"><a href="#cb13-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-29"><a href="#cb13-29" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate the denominator (normalization term)</span></span>
<span id="cb13-30"><a href="#cb13-30" aria-hidden="true" tabindex="-1"></a>    k_sum <span class="op">=</span> np.<span class="bu">sum</span>(k_prime, axis<span class="op">=</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)  <span class="co"># (batch_size, 1, feature_dim)</span></span>
<span id="cb13-31"><a href="#cb13-31" aria-hidden="true" tabindex="-1"></a>    denominator <span class="op">=</span> np.einsum(<span class="st">'bsf,bof-&gt;bso'</span>, q_prime, k_sum)  <span class="co"># (batch_size, seq_len, 1)</span></span>
<span id="cb13-32"><a href="#cb13-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-33"><a href="#cb13-33" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 4. Calculate the final attention output</span></span>
<span id="cb13-34"><a href="#cb13-34" aria-hidden="true" tabindex="-1"></a>    attention_output <span class="op">=</span> numerator <span class="op">/</span> (denominator <span class="op">+</span> <span class="fl">1e-6</span>)  <span class="co"># (batch_size, seq_len, d_model)</span></span>
<span id="cb13-35"><a href="#cb13-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-36"><a href="#cb13-36" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> attention_output</span>
<span id="cb13-37"><a href="#cb13-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-38"><a href="#cb13-38" aria-hidden="true" tabindex="-1"></a><span class="co"># Example usage</span></span>
<span id="cb13-39"><a href="#cb13-39" aria-hidden="true" tabindex="-1"></a>batch_size, seq_len, d_model <span class="op">=</span> <span class="dv">2</span>, <span class="dv">100</span>, <span class="dv">512</span></span>
<span id="cb13-40"><a href="#cb13-40" aria-hidden="true" tabindex="-1"></a>q <span class="op">=</span> np.random.randn(batch_size, seq_len, d_model)</span>
<span id="cb13-41"><a href="#cb13-41" aria-hidden="true" tabindex="-1"></a>k <span class="op">=</span> np.random.randn(batch_size, seq_len, d_model)</span>
<span id="cb13-42"><a href="#cb13-42" aria-hidden="true" tabindex="-1"></a>v <span class="op">=</span> np.random.randn(batch_size, seq_len, d_model)</span>
<span id="cb13-43"><a href="#cb13-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-44"><a href="#cb13-44" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> favor_plus_attention(q, k, v)</span>
<span id="cb13-45"><a href="#cb13-45" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Output tensor shape:"</span>, output.shape)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Output tensor shape: (2, 100, 512)</code></pre>
</div>
</div>
<p>FAVOR+ tiene las siguientes ventajas:</p>
<ol type="1">
<li>Reduce la complejidad computacional de O(N²) a O(N).</li>
<li>Mantiene la capacidad clave del attention para capturar relaciones entre tokens, mientras reduce el uso de memoria.</li>
<li>Hace posible procesar secuencias largas de manera práctica.</li>
</ol>
<p><strong>Fundamento matemático</strong></p>
<p>El fundamento matemático de FAVOR+ se basa en el <strong>teorema auxiliar de Johnson-Lindenstrauss</strong>. El punto clave es que las relaciones de distancia entre los datos <em>se mantienen casi</em> intactas cuando se proyectan de una dimensión alta a una dimensión baja. Es decir, si se reduce la dimensionalidad de 1000 a 100, las distancias relativas entre los datos no cambian significativamente.</p>
<p>El éxito de FAVOR+ ha impulsado el desarrollo de diversas formas de attention lineal, como Linear Transformer y Linear Attention Transformer, desempeñando un papel crucial en el procesamiento de secuencias largas.</p>
</section>
<section id="atención-dispersa-sparse-transformer-longformer" class="level4">
<h4 class="anchored" data-anchor-id="atención-dispersa-sparse-transformer-longformer">9.2.1.2 Atención dispersa: Sparse Transformer, Longformer</h4>
<p>En 2019, OpenAI introdujo patrones de dispersión <strong>fijos</strong> con el Sparse Transformer. Este método calcula solo algunas relaciones según un patrón específico en lugar de calcular las relaciones entre todas las parejas de tokens.</p>
<p><strong>Patrones fijos del Sparse Transformer</strong></p>
<p>El Sparse Transformer utiliza dos patrones de dispersión principales:</p>
<ol type="1">
<li><strong>Patrón de stride:</strong> Calcula la atención solo con tokens separados por intervalos regulares.</li>
<li><strong>Patrón local:</strong> Calcula la atención solo con tokens dentro de una ventana de tamaño fijo y adyacente.</li>
</ol>
<p>Estos patrones se pueden expresar matemáticamente como sigue:</p>
<p><span class="math inline">\(Attention(Q,K,V) = softmax(\frac{QK^T \odot M}{\sqrt{d_k}})V\)</span></p>
<p>Aquí, M es la matriz de máscara dispersa, y ⊙ denota el producto elemento a elemento. La matriz de máscara indica qué pares de tokens deben (1) o no deben (0) recibir atención.</p>
<p>Este enfoque mejoró la eficiencia computacional, pero presentaba la desventaja de que los patrones eran fijos y no podían adaptarse flexiblemente según el contexto.</p>
<p><strong>Combinación local-global del Longformer</strong></p>
<p>En 2020, Allen AI propuso un patrón disperso más flexible con el Longformer. El Longformer utiliza un enfoque híbrido que combina <strong>atención local</strong> y <strong>atención global</strong>:</p>
<ol type="1">
<li><strong>Atención local:</strong> Todos los tokens calculan la atención con w tokens cercanos (enfoque de ventana deslizante).</li>
<li><strong>Atención global:</strong> Tokens especiales (por ejemplo, [CLS]) calculan la atención con todos los tokens.</li>
</ol>
<p>Este enfoque permite considerar tanto el contexto local como el global, lo que facilita una comprensión más rica del contexto.</p>
<p>Traducción:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 50%">
<col style="width: 50%">
</colgroup>
<thead>
<tr class="header">
<th>Título</th>
<th>Descripción</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Ejemplo 1</td>
<td>Este es un ejemplo de texto en español. No se deben traducir las expresiones matemáticas como <span class="math inline">\(x^2 + y^2 = z^2\)</span>.</td>
</tr>
<tr class="even">
<td>Ejemplo 2</td>
<td>Otra fila con contenido que no debe ser modificado, incluyendo <code>código</code> o <strong>negrita</strong>.</td>
</tr>
</tbody>
</table>
<div id="cell-22" class="cell" data-execution_count="5">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> longformer_attention(q, k, v, window_size<span class="op">=</span><span class="dv">3</span>, global_tokens<span class="op">=</span>[<span class="dv">0</span>]):</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Longformer attention implementation</span></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a><span class="co">        q, k, v: (batch_size, seq_len, d_model)</span></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a><span class="co">        window_size: Size of the local attention window</span></span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a><span class="co">        global_tokens: List of token indices to perform global attention on</span></span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>    batch_size, seq_len, d_model <span class="op">=</span> q.shape</span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a>    attention_weights <span class="op">=</span> np.zeros((batch_size, seq_len, seq_len))</span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 1. Local attention: sliding window</span></span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(seq_len):</span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculate window range</span></span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a>        window_start <span class="op">=</span> <span class="bu">max</span>(<span class="dv">0</span>, i <span class="op">-</span> window_size)</span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a>        window_end <span class="op">=</span> <span class="bu">min</span>(seq_len, i <span class="op">+</span> window_size <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a>        window_size_current <span class="op">=</span> window_end <span class="op">-</span> window_start</span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculate attention scores within the window</span></span>
<span id="cb15-22"><a href="#cb15-22" aria-hidden="true" tabindex="-1"></a>        scores <span class="op">=</span> np.matmul(q[:, i:i<span class="op">+</span><span class="dv">1</span>], k[:, window_start:window_end].transpose(<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">1</span>))</span>
<span id="cb15-23"><a href="#cb15-23" aria-hidden="true" tabindex="-1"></a>        <span class="co"># scores: (batch_size, 1, window_size_current)</span></span>
<span id="cb15-24"><a href="#cb15-24" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb15-25"><a href="#cb15-25" aria-hidden="true" tabindex="-1"></a>        attention_weights[:, i:i<span class="op">+</span><span class="dv">1</span>, window_start:window_end] <span class="op">=</span> scores</span>
<span id="cb15-26"><a href="#cb15-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-27"><a href="#cb15-27" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 2. Global attention: specific tokens attend to all tokens</span></span>
<span id="cb15-28"><a href="#cb15-28" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> global_idx <span class="kw">in</span> global_tokens:</span>
<span id="cb15-29"><a href="#cb15-29" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculate attention scores for global tokens</span></span>
<span id="cb15-30"><a href="#cb15-30" aria-hidden="true" tabindex="-1"></a>        scores <span class="op">=</span> np.matmul(q[:, global_idx:global_idx<span class="op">+</span><span class="dv">1</span>], k.transpose(<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">1</span>))</span>
<span id="cb15-31"><a href="#cb15-31" aria-hidden="true" tabindex="-1"></a>        <span class="co"># scores: (batch_size, 1, seq_len)</span></span>
<span id="cb15-32"><a href="#cb15-32" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb15-33"><a href="#cb15-33" aria-hidden="true" tabindex="-1"></a>        attention_weights[:, global_idx:global_idx<span class="op">+</span><span class="dv">1</span>, :] <span class="op">=</span> scores</span>
<span id="cb15-34"><a href="#cb15-34" aria-hidden="true" tabindex="-1"></a>        attention_weights[:, :, global_idx:global_idx<span class="op">+</span><span class="dv">1</span>] <span class="op">=</span> scores.transpose(<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb15-35"><a href="#cb15-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-36"><a href="#cb15-36" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 3. Apply softmax (row-wise)</span></span>
<span id="cb15-37"><a href="#cb15-37" aria-hidden="true" tabindex="-1"></a>    attention_weights <span class="op">=</span> np.exp(attention_weights) <span class="op">/</span> np.<span class="bu">sum</span>(np.exp(attention_weights), axis<span class="op">=-</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb15-38"><a href="#cb15-38" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb15-39"><a href="#cb15-39" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 4. Calculate the final output by applying weights</span></span>
<span id="cb15-40"><a href="#cb15-40" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> np.matmul(attention_weights, v)  <span class="co"># (batch_size, seq_len, d_model)</span></span>
<span id="cb15-41"><a href="#cb15-41" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb15-42"><a href="#cb15-42" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> output</span>
<span id="cb15-43"><a href="#cb15-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-44"><a href="#cb15-44" aria-hidden="true" tabindex="-1"></a><span class="co"># Example usage</span></span>
<span id="cb15-45"><a href="#cb15-45" aria-hidden="true" tabindex="-1"></a>batch_size, seq_len, d_model <span class="op">=</span> <span class="dv">2</span>, <span class="dv">10</span>, <span class="dv">64</span></span>
<span id="cb15-46"><a href="#cb15-46" aria-hidden="true" tabindex="-1"></a>q <span class="op">=</span> np.random.randn(batch_size, seq_len, d_model)</span>
<span id="cb15-47"><a href="#cb15-47" aria-hidden="true" tabindex="-1"></a>k <span class="op">=</span> np.random.randn(batch_size, seq_len, d_model)</span>
<span id="cb15-48"><a href="#cb15-48" aria-hidden="true" tabindex="-1"></a>v <span class="op">=</span> np.random.randn(batch_size, seq_len, d_model)</span>
<span id="cb15-49"><a href="#cb15-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-50"><a href="#cb15-50" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> longformer_attention(q, k, v, window_size<span class="op">=</span><span class="dv">2</span>, global_tokens<span class="op">=</span>[<span class="dv">0</span>])</span>
<span id="cb15-51"><a href="#cb15-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-52"><a href="#cb15-52" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(output)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>[[[-0.72195324  0.03196266 -0.06067346 ...  0.57106283  1.31438
    0.63673636]
  [-1.72619367 -0.39122625  0.91285828 ... -1.4031466   1.2081069
    0.95934394]
  [ 0.07427921  0.42596224 -0.44545069 ...  0.154228    0.37435003
   -0.01884786]
  ...
  [ 1.26169539 -0.58215291  2.00334263 ...  1.15338425  0.31404728
   -1.33672458]
  [ 0.96005607  0.39904084  0.5703471  ... -0.2168805   0.93570179
    0.05680507]
  [ 0.61648602 -0.12874142  1.09736967 ...  0.32421211  1.23082505
    0.4141766 ]]

 [[ 0.92762851  0.26334678 -0.81047846 ... -0.19186621  0.42534117
    0.57313974]
  [ 1.01307261  0.61571205 -1.26925081 ... -0.56016688 -0.19707427
    2.49452497]
  [-1.0071559   2.81291178  2.5010486  ...  1.63559632 -0.60892113
   -1.40952186]
  ...
  [-1.96615634  1.85881047  0.19361453 ...  1.21044747 -0.00772792
   -0.68961122]
  [ 0.09090778  1.94770672 -0.990489   ... -0.09841141  0.65195305
    0.11634795]
  [-2.43256801  1.66319642  0.23557316 ...  2.39325846  0.8750332
    0.66295002]]]</code></pre>
</div>
</div>
<p><strong>Optimización de operaciones de matrices dispersas por bloques</strong></p>
<p>Para implementar eficientemente el enfoque híbrido del Longformer, es necesario optimizar las operaciones de matrices dispersas por bloques.</p>
<ol type="1">
<li><strong>Procesamiento por bloques:</strong> Aumenta la eficiencia de la caché a través de accesos de memoria contiguos.</li>
<li><strong>Núcleos personalizados CUDA:</strong> Optimiza el procesamiento paralelo especializado para patrones dispersos.</li>
<li><strong>Balanceo de carga dinámico:</strong> Distribuye el trabajo considerando la cantidad de cálculos por bloque.</li>
</ol>
<p>El enfoque basado en patrones dispersos redujo la complejidad a O(N log N) o O(N), pero presentó desafíos de implementación y optimización de hardware.</p>
</section>
</section>
<section id="atención-local-global-resolución-del-problema-de-dependencias-a-larga-distancia" class="level3">
<h3 class="anchored" data-anchor-id="atención-local-global-resolución-del-problema-de-dependencias-a-larga-distancia">9.2.3 Atención local-global: Resolución del problema de dependencias a larga distancia</h3>
<p>A principios de 2020, Google Research y Allen AI propusieron un enfoque híbrido que combinaba atención local y global para abordar la pérdida de información en la atención lineal y la complejidad de implementación en patrones dispersos.</p>
<section id="reformer-atención-lsh" class="level4">
<h4 class="anchored" data-anchor-id="reformer-atención-lsh">9.2.3.1 Reformer: Atención LSH</h4>
<p>Reformer utiliza <strong>hashing sensible a la localidad (Locality-Sensitive Hashing, LSH)</strong> para agrupar eficientemente vectores similares. El principio clave del LSH es el siguiente.</p>
<p><span class="math inline">\(h(x) = \text{argmax}( [xR; -xR] )\)</span></p>
<p>Aquí, R es una matriz de proyección aleatoria y los vectores similares tienen una alta probabilidad de tener el mismo valor hash. Reformer sigue los siguientes pasos.</p>
<ol type="1">
<li>Asigna vectores de consulta a cubos usando una función hash.</li>
<li>Calcula la atención solo con vectores clave en el mismo cubo.</li>
<li>Reduce la complejidad de O(N²) a O(N log N).</li>
</ol>
<p>Este método es eficiente para procesar secuencias largas, pero existe la posibilidad de pérdida de información debido a colisiones hash.</p>
</section>
<section id="bigbird-combinación-de-atención-local-global-y-aleatoria" class="level4">
<h4 class="anchored" data-anchor-id="bigbird-combinación-de-atención-local-global-y-aleatoria">9.2.3.2 BigBird: Combinación de atención local, global y aleatoria</h4>
<p>BigBird combinó tres patrones de atención para superar las limitaciones del Reformer.</p>
<ol type="1">
<li><strong>Ventana local:</strong> Calcula la atención con w tokens adyacentes para capturar el contexto local.</li>
<li><strong>Tokens globales:</strong> g tokens especiales atienden a toda la secuencia para mantener información global.</li>
<li><strong>Bloques aleatorios:</strong> Calcula la atención con r tokens aleatorios para capturar relaciones a diversas distancias.</li>
</ol>
<p>Esta estrategia híbrida se expresa mediante la siguiente fórmula.</p>
<p><span class="math inline">\(Attention(Q,K,V) = softmax(\frac{QK^T \odot (M_{local} + M_{global} + M_{random})}{\sqrt{d_k}})V\)</span></p>
<p>Aquí, M son las matrices de máscara respectivas. Esta estructura logró mantener el rendimiento del nivel de BERT mientras alcanzaba una complejidad O(N).</p>
<p><strong>Impacto del patrón híbrido</strong></p>
<p>El éxito de BigBird demostró el potencial del enfoque local-global, lo que tuvo un gran impacto en los modelos de transformers modernos.</p>
<ol type="1">
<li><strong>Eficiencia computacional:</strong>
<ul>
<li>Redujo la complejidad mediante atención selectiva.</li>
<li>Optimizó el uso de memoria GPU.</li>
<li>Permitió procesamiento paralelo.</li>
</ul></li>
<li><strong>Rendimiento del modelo:</strong>
<ul>
<li>Equilibró información detallada local y contexto global.</li>
<li>Mejoró la capacidad para capturar dependencias a larga distancia.</li>
<li>Mostró rendimiento estable en diversas tareas.</li>
</ul></li>
<li><strong>Aplicaciones prácticas:</strong>
<ul>
<li>Influyó en la estructura de Sparse Transformer de GPT-3.</li>
<li>Contribuyó al desarrollo de atención multiquery en PaLM.</li>
<li>Se utilizó en la implementación de Constitutional AI en Claude de Anthropic. Este enfoque híbrido se convirtió en la base de varios modelos posteriores, como Longformer y ETC. En particular, logró un gran éxito en tareas que implican el procesamiento de documentos largos, como clasificación de documentos y respuestas a consultas. Sin embargo, aún quedaban problemas de uso de memoria y eficiencia computacional. Especialmente, la optimización del uso de memoria de GPU en modelos de lenguaje grandes se convirtió en un nuevo desafío, lo cual lleva a las mejoras de eficiencia de memoria que se discuten en el Capítulo 9.3.</li>
</ul></li>
</ol>
</section>
</section>
</section>
<section id="eficiencia-de-memoria-combinación-de-hardware-y-software-2021-2022" class="level2">
<h2 class="anchored" data-anchor-id="eficiencia-de-memoria-combinación-de-hardware-y-software-2021-2022">9.3 Eficiencia de memoria: combinación de hardware y software (2021-2022)</h2>
<p>Desde 2021 hasta 2022, el enfoque se centró en mejorar la eficiencia de memoria de los transformadores. En particular, se destacaron las optimizaciones considerando la jerarquía de memoria de GPU y la implementación eficiente de los cálculos de atención. Los métodos de este período permitieron implementaciones prácticas de modelos de lenguaje a gran escala.</p>
<section id="flashattention-optimización-de-atención-utilizando-la-jerarquía-de-memoria-de-gpu" class="level3">
<h3 class="anchored" data-anchor-id="flashattention-optimización-de-atención-utilizando-la-jerarquía-de-memoria-de-gpu">9.3.1 FlashAttention: optimización de atención utilizando la jerarquía de memoria de GPU</h3>
<p>En 2022, el equipo de investigación dirigido por Tri Dao de Stanford propuso FlashAttention, considerando la jerarquía de memoria de GPU. Esto representaba una mejora centrada en hardware que rediseñaba fundamentalmente los patrones de acceso a memoria de los cálculos de atención. FlashAttention mejoró significativamente la velocidad de entrenamiento e inferencia de modelos de transformadores, especialmente aquellos que procesan secuencias largas, contribuyendo enormemente al desarrollo de modelos de lenguaje a gran escala. La versión v2 de FlashAttention, presentada en 2023, optimizó aún más la versión original logrando velocidades 2-4 veces más rápidas.</p>
<section id="estructura-de-memoria-de-gpu-y-optimización-de-es" class="level4">
<h4 class="anchored" data-anchor-id="estructura-de-memoria-de-gpu-y-optimización-de-es">9.3.1.1 Estructura de memoria de GPU y optimización de E/S</h4>
<p>Una ventaja de FlashAttention es que considera explícitamente la jerarquía de memoria de GPU. Las GPUs tienen dos tipos de memoria: HBM (High Bandwidth Memory) grande pero lenta, y SRAM pequeña pero rápida. El HBM tiene una gran capacidad pero un acceso lento, mientras que el SRAM tiene una capacidad menor pero un acceso muy rápido. FlashAttention aprovecha estas características.</p>
<ol type="1">
<li><strong>Minimización del movimiento de datos entre HBM y SRAM:</strong> En los mecanismos de atención tradicionales, después de calcular el producto punto entre las consultas y claves, se debía almacenar toda la matriz de puntuaciones de atención en HBM. Esto consumía una gran cantidad de ancho de banda de memoria y ralentizaba el proceso. FlashAttention minimiza estos movimientos de datos innecesarios.</li>
<li><strong>No almacenamiento de resultados intermedios (matriz de puntuaciones de atención) en HBM:</strong> En lugar de almacenar los resultados de cálculos intermedios en HBM, FlashAttention mantiene y realiza las operaciones necesarias en SRAM.</li>
<li><strong>Cálculo gradual de softmax en SRAM:</strong> En lugar de realizar la operación de softmax sobre toda la matriz de puntuaciones de atención a la vez, se calcula el softmax por bloques y los resultados se acumulan. Esto reduce el proceso de almacenar y leer valores intermedios desde HBM.</li>
</ol>
<p>Este diseño orientado al hardware redujo significativamente el acceso a memoria.</p>
</section>
<section id="tiling-y-procesamiento-por-bloques" class="level4">
<h4 class="anchored" data-anchor-id="tiling-y-procesamiento-por-bloques">9.3.1.2 Tiling y procesamiento por bloques</h4>
<p>Para implementar la optimización de memoria, se introdujo la técnica de tiling (mosaico). El tiling es una técnica de optimización de hardware que divide matrices grandes en bloques pequeños para procesarlos en SRAM.</p>
<ol type="1">
<li>División de las matrices de entrada (Q, K, V) en bloques que encajan en el tamaño de SRAM.</li>
<li>Carga de datos desde HBM a SRAM por bloques.</li>
<li>Realización de cálculos de atención por bloques dentro de SRAM.</li>
<li>Al completar los cálculos de atención para cada bloque, se almacena solo el resultado del bloque (es decir, el promedio ponderado de los valores del bloque) en HBM. No se almacena toda la matriz de puntuaciones de atención.</li>
</ol>
<p>Esta estrategia de procesamiento por bloques permitió calcular resultados de atención precisos mientras minimizaba el uso de ancho de banda de memoria.</p>
</section>
<section id="flashattention-v2-maximización-del-aprovechamiento-del-hardware" class="level4">
<h4 class="anchored" data-anchor-id="flashattention-v2-maximización-del-aprovechamiento-del-hardware">9.3.1.3 FlashAttention v2: maximización del aprovechamiento del hardware</h4>
<p>FlashAttention v2 mantuvo la idea básica de v1 pero agregó varias optimizaciones a nivel bajo para maximizar el aprovechamiento del hardware. Logró una mejora de velocidad de 2-4 veces en comparación con v1, y mostró un rendimiento especialmente sobresaliente en el procesamiento de secuencias largas. * <strong>Fusión de núcleos:</strong> FlashAttention v2 integra varias operaciones del mecanismo de atención, como las transformaciones de consulta, clave y valor, el cálculo de puntuaciones de atención, softmax y el cálculo de promedio ponderado en un solo kernel CUDA. Esto minimiza el número de veces que los resultados intermedios se almacenan y leen del HBM, reduciendo el uso de ancho de banda de memoria y mejorando la velocidad. * <strong>Procesamiento no secuencial (no-secuencial) de cabezas de atención</strong>: A diferencia del procesamiento secuencial de cabezas de atención en versiones anteriores, FlashAttention V2 las procesa en paralelo siempre que los recursos GPU lo permitan, reduciendo el retardo. * <strong>Diseño de memoria amigable con la caché:</strong> Se ha diseñado una estructura de datos más adecuada para las líneas de caché GPU, como almacenar los datos en orden de columna (column-major). Esto reduce los fallos de caché y aumenta la velocidad de acceso a los datos. * <strong>Paralelización a nivel de warp:</strong> Se ha optimizado el uso de 32 hilos dentro de un warp CUDA para procesar cada parte del cálculo de atención en paralelo. Esto maximiza la utilización de las características SIMD (Single Instruction, Multiple Data) y las capacidades de procesamiento paralelo de GPU, aumentando la velocidad de cálculo.</p>
<p>Gracias a estas optimizaciones integrales, FlashAttention v2 logró una mejora de eficiencia de memoria hasta 20 veces mayor y un aumento de velocidad de 2-4 veces en comparación con las implementaciones de atención de PyTorch existentes en ciertos entornos. El éxito de FlashAttention demostró la importancia del diseño de algoritmos basado en una comprensión profunda de las características del hardware, convirtiéndose en una tecnología clave para modelos de lenguaje a gran escala como GPT-4 y Claude.</p>
<p>La implementación oficial de FlashAttention se proporciona en código CUDA de NVIDIA. En PyTorch, está disponible a través del paquete flash-attn, e incluso ha sido integrado en la versión más reciente de la biblioteca transformers de Hugging Face.</p>
</section>
</section>
<section id="optimización-de-consultas-mejora-de-la-estructura-de-atención" class="level3">
<h3 class="anchored" data-anchor-id="optimización-de-consultas-mejora-de-la-estructura-de-atención">9.3.2 Optimización de consultas: Mejora de la estructura de atención</h3>
<p>En 2022, Google Research propuso Multi-Query Attention (MQA) a través del modelo PaLM para mejorar la eficiencia de memoria desde una perspectiva de diseño de software. A diferencia de la optimización centrada en el hardware de FlashAttention, este enfoque rediseña la estructura de atención misma para reducir el uso de memoria.</p>
<section id="multi-query-attention-mqa" class="level4">
<h4 class="anchored" data-anchor-id="multi-query-attention-mqa">9.3.2.1 Multi-Query Attention (MQA)</h4>
<p>El núcleo del MQA es cambiar el diseño para que todos los cabezales de atención compartan las mismas Key y Value.</p>
<ol type="1">
<li><strong>Compartir Key, Value:</strong>
<ul>
<li>Todos los cabezales comparten una matriz K, V.</li>
<li>Se reduce el tamaño de la caché KV en proporción al número de cabezales. (por ejemplo, si hay 8 cabezales, el tamaño de la caché KV se reduce a 1/8)</li>
<li>El uso de ancho de banda de memoria disminuye significativamente.</li>
</ul></li>
<li><strong>Separación de Query:</strong>
<ul>
<li>Las consultas se generan independientemente para cada cabezal.</li>
<li>Cada cabezal aún puede aprender contextos diferentes.</li>
<li>La complejidad computacional no aumenta significativamente.</li>
</ul></li>
</ol>
<div id="cell-26" class="cell" data-execution_count="1">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> multi_query_attention(q, k, v, num_heads):</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Multi-Query Attention implementation</span></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a><span class="co">        q: (batch_size, seq_len, d_model)</span></span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a><span class="co">        k: (batch_size, seq_len, d_model)</span></span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a><span class="co">        v: (batch_size, seq_len, d_model)</span></span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a><span class="co">        num_heads: Number of heads</span></span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a>    batch_size, seq_len, d_model <span class="op">=</span> q.shape</span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a>    head_dim <span class="op">=</span> d_model <span class="op">//</span> num_heads</span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 1. Convert K, V to single matrices shared by all heads</span></span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a>    k_shared <span class="op">=</span> np.dot(k, np.random.randn(d_model, d_model))  <span class="co"># (batch_size, seq_len, d_model)</span></span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a>    v_shared <span class="op">=</span> np.dot(v, np.random.randn(d_model, d_model))  <span class="co"># (batch_size, seq_len, d_model)</span></span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 2. Generate Q differently for each head</span></span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true" tabindex="-1"></a>    q_multi <span class="op">=</span> np.dot(q, np.random.randn(d_model, num_heads <span class="op">*</span> head_dim))  <span class="co"># (batch_size, seq_len, num_heads * head_dim)</span></span>
<span id="cb17-20"><a href="#cb17-20" aria-hidden="true" tabindex="-1"></a>    q_multi <span class="op">=</span> q_multi.reshape(batch_size, seq_len, num_heads, head_dim)  <span class="co"># (batch_size, seq_len, num_heads, head_dim)</span></span>
<span id="cb17-21"><a href="#cb17-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-22"><a href="#cb17-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Transform k_shared to head_dim size</span></span>
<span id="cb17-23"><a href="#cb17-23" aria-hidden="true" tabindex="-1"></a>    k_shared <span class="op">=</span> np.dot(k_shared, np.random.randn(d_model, head_dim))  <span class="co"># (batch_size, seq_len, head_dim)</span></span>
<span id="cb17-24"><a href="#cb17-24" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb17-25"><a href="#cb17-25" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 3. Calculate attention scores</span></span>
<span id="cb17-26"><a href="#cb17-26" aria-hidden="true" tabindex="-1"></a>    scores <span class="op">=</span> np.matmul(q_multi, k_shared.reshape(batch_size, seq_len, head_dim, <span class="dv">1</span>))</span>
<span id="cb17-27"><a href="#cb17-27" aria-hidden="true" tabindex="-1"></a>    <span class="co"># scores: (batch_size, seq_len, num_heads, 1)</span></span>
<span id="cb17-28"><a href="#cb17-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-29"><a href="#cb17-29" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 4. Apply softmax</span></span>
<span id="cb17-30"><a href="#cb17-30" aria-hidden="true" tabindex="-1"></a>    weights <span class="op">=</span> np.exp(scores) <span class="op">/</span> np.<span class="bu">sum</span>(np.exp(scores), axis<span class="op">=-</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb17-31"><a href="#cb17-31" aria-hidden="true" tabindex="-1"></a>    <span class="co"># weights: (batch_size, seq_len, num_heads, 1)</span></span>
<span id="cb17-32"><a href="#cb17-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-33"><a href="#cb17-33" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 5. Multiply V with weights</span></span>
<span id="cb17-34"><a href="#cb17-34" aria-hidden="true" tabindex="-1"></a>    v_shared <span class="op">=</span> np.dot(v_shared, np.random.randn(d_model, head_dim))  <span class="co"># Transform V to head_dim as well</span></span>
<span id="cb17-35"><a href="#cb17-35" aria-hidden="true" tabindex="-1"></a>    v_shared <span class="op">=</span> v_shared.reshape(batch_size, seq_len, <span class="dv">1</span>, head_dim)</span>
<span id="cb17-36"><a href="#cb17-36" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> np.matmul(weights, v_shared)</span>
<span id="cb17-37"><a href="#cb17-37" aria-hidden="true" tabindex="-1"></a>    <span class="co"># output: (batch_size, seq_len, num_heads, head_dim)</span></span>
<span id="cb17-38"><a href="#cb17-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-39"><a href="#cb17-39" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 6. Concatenate heads and transform output</span></span>
<span id="cb17-40"><a href="#cb17-40" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> output.reshape(batch_size, seq_len, num_heads <span class="op">*</span> head_dim)</span>
<span id="cb17-41"><a href="#cb17-41" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> np.dot(output, np.random.randn(num_heads <span class="op">*</span> head_dim, d_model))</span>
<span id="cb17-42"><a href="#cb17-42" aria-hidden="true" tabindex="-1"></a>    <span class="co"># output: (batch_size, seq_len, d_model)</span></span>
<span id="cb17-43"><a href="#cb17-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-44"><a href="#cb17-44" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> output</span>
<span id="cb17-45"><a href="#cb17-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-46"><a href="#cb17-46" aria-hidden="true" tabindex="-1"></a><span class="co"># Example usage</span></span>
<span id="cb17-47"><a href="#cb17-47" aria-hidden="true" tabindex="-1"></a>batch_size, seq_len, d_model <span class="op">=</span> <span class="dv">2</span>, <span class="dv">100</span>, <span class="dv">512</span></span>
<span id="cb17-48"><a href="#cb17-48" aria-hidden="true" tabindex="-1"></a>num_heads <span class="op">=</span> <span class="dv">8</span></span>
<span id="cb17-49"><a href="#cb17-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-50"><a href="#cb17-50" aria-hidden="true" tabindex="-1"></a>q <span class="op">=</span> np.random.randn(batch_size, seq_len, d_model)</span>
<span id="cb17-51"><a href="#cb17-51" aria-hidden="true" tabindex="-1"></a>k <span class="op">=</span> np.random.randn(batch_size, seq_len, d_model)</span>
<span id="cb17-52"><a href="#cb17-52" aria-hidden="true" tabindex="-1"></a>v <span class="op">=</span> np.random.randn(batch_size, seq_len, d_model)</span>
<span id="cb17-53"><a href="#cb17-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-54"><a href="#cb17-54" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> multi_query_attention(q, k, v, num_heads)</span>
<span id="cb17-55"><a href="#cb17-55" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Output tensor shape:"</span>, output.shape)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_304793/3750479510.py:30: RuntimeWarning: overflow encountered in exp
  weights = np.exp(scores) / np.sum(np.exp(scores), axis=-1, keepdims=True)
/tmp/ipykernel_304793/3750479510.py:30: RuntimeWarning: invalid value encountered in divide
  weights = np.exp(scores) / np.sum(np.exp(scores), axis=-1, keepdims=True)</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Output tensor shape: (2, 100, 512)</code></pre>
</div>
</div>
<section id="atención-de-consultas-agrupadas-gqa" class="level5">
<h5 class="anchored" data-anchor-id="atención-de-consultas-agrupadas-gqa">9.3.2.2 Atención de Consultas Agrupadas (GQA)</h5>
<p>A principios de 2023, Meta AI propuso GQA (Grouped-Query Attention) para abordar las limitaciones de MQA. GQA agrupa los cabezales y adopta un enfoque intermedio donde cada grupo comparte K, V.</p>
<ol type="1">
<li><strong>Diseño basado en grupos:</strong>
<ul>
<li>Varias cabezas de consulta comparten una pareja KV.</li>
<li>Se puede ajustar el tamaño del grupo para equilibrar el uso de memoria y el rendimiento del modelo.</li>
<li>Puede tener una mayor expresividad que MQA.</li>
</ul></li>
<li><strong>Implementación eficiente:</strong>
<ul>
<li>Se optimizó el procesamiento paralelo por grupos.</li>
<li>Utiliza un enfoque de acceso a la memoria amigable con la caché.</li>
<li>Mejora la velocidad de procesamiento durante la inferencia.</li>
</ul></li>
</ol>
</section>
</section>
<section id="mqa-vs.-gqa-vs.-atención-multi-cabezal" class="level4">
<h4 class="anchored" data-anchor-id="mqa-vs.-gqa-vs.-atención-multi-cabezal">9.3.2.3 MQA vs.&nbsp;GQA vs.&nbsp;Atención Multi-Cabezal</h4>
<p>Las estructuras de optimización de consulta como MQA y GQA ofrecen los siguientes trade-offs.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 23%">
<col style="width: 25%">
<col style="width: 11%">
<col style="width: 17%">
<col style="width: 21%">
</colgroup>
<thead>
<tr class="header">
<th>Estructura</th>
<th>Uso de memoria</th>
<th>Expresividad</th>
<th>Velocidad de procesamiento</th>
<th>Complejidad de implementación</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Atención multi-cabezal</td>
<td>N × H</td>
<td>Alta</td>
<td>Lenta</td>
<td>Baja</td>
</tr>
<tr class="even">
<td>GQA</td>
<td>N × G</td>
<td>Media</td>
<td>Media</td>
<td>Media</td>
</tr>
<tr class="odd">
<td>MQA</td>
<td>N</td>
<td>Baja</td>
<td>Rápida</td>
<td>Baja</td>
</tr>
</tbody>
</table>
<p>(N: longitud de secuencia, H: número de cabezales, G: número de grupos)</p>
<p>Estas estructuras se han adoptado ampliamente en modelos de lenguaje a gran escala modernos como LLaMA, PaLM, Claude, y en particular, han mejorado significativamente la eficiencia de memoria en el procesamiento de secuencias largas.</p>
</section>
</section>
<section id="gestión-y-optimización-de-la-caché-kv" class="level3">
<h3 class="anchored" data-anchor-id="gestión-y-optimización-de-la-caché-kv">9.3.3 Gestión y optimización de la caché KV</h3>
<p>A finales de 2022, DeepMind, Anthropic y el equipo de desarrollo vLLM reconocieron la importancia de gestionar eficazmente la caché KV en el proceso de inferencia de modelos de lenguaje a gran escala. Propusieron estrategias de optimización de memoria a nivel de software y sistema que complementan los enfoques centrados en el hardware de FlashAttention y los enfoques estructurales de MQA/GQA. Esto es particularmente importante al procesar <em>conversaciones largas</em>, <em>generar documentos largos</em> o cuando se requiere un <em>alto rendimiento (throughput)</em>.</p>
<section id="pagedattention-vllm-concepto-de-paginación-del-sistema-operativo" class="level4">
<h4 class="anchored" data-anchor-id="pagedattention-vllm-concepto-de-paginación-del-sistema-operativo">9.3.3.1 PagedAttention &amp; vLLM: Concepto de paginación del sistema operativo</h4>
<p>PagedAttention y su implementación en vLLM son técnicas para gestionar eficazmente la caché KV, inspiradas por los conceptos de memoria virtual y paginación del sistema operativo.</p>
<p><strong>Problemas con las cachés KV tradicionales</strong></p>
<ul>
<li><strong>Desperdicio de memoria:</strong> La caché KV aumenta linealmente con la longitud de la secuencia, ocupando mucho espacio en memoria. En particular, durante el procesamiento por lotes (batch processing), cuando las longitudes de secuencia varían, se debe asignar memoria para la secuencia más larga, lo que resulta en un desperdicio significativo.</li>
<li><strong>Fragmentación de memoria:</strong> Cuando la caché KV se asigna en bloques no contiguos en la memoria, puede ocurrir el problema de fragmentación externa (external fragmentation), donde hay espacios libres pero no se pueden utilizar.</li>
<li><strong>Sin soporte para longitudes de secuencia dinámicas</strong>: Es difícil manejar eficientemente cambios dinámicos en el tamaño de la caché KV durante el proceso de generación.</li>
</ul>
<p><strong>Idea central de PagedAttention</strong></p>
<ol type="1">
<li><strong>Asignación de memoria por bloques (Block-Based Memory Allocation):</strong>
<ul>
<li>La caché KV se divide en bloques de tamaño fijo. (Similar a cómo un sistema operativo divide la memoria en páginas)</li>
<li>Cada bloque almacena las claves y valores de múltiples tokens.</li>
<li>Los bloques pueden estar físicamente no contiguos, pero lógicamente contiguos.</li>
</ul></li>
<li><strong>Tabla de Bloques (Block Table):</strong>
<ul>
<li>Administra el mapeo entre los bloques lógicos y físicos de cada secuencia (similar a la tabla de páginas del sistema operativo).</li>
<li>Cuando se genera un nuevo token, se asigna un bloque vacío y se agrega información de mapeo a la tabla de bloques.</li>
</ul></li>
<li><strong>Soporte para Copy-on-Write (CoW) (Opcional):</strong>
<ul>
<li>Si múltiples secuencias comparten el mismo prompt (por ejemplo, en búsqueda por haz), los bloques se comparten sin copiarlos para ahorrar memoria.</li>
<li>Se asigna un nuevo bloque solo cuando el contenido del bloque cambia.</li>
</ul></li>
</ol>
<p><strong>Ventajas de PagedAttention</strong></p>
<ul>
<li><strong>Aumento de la eficiencia de memoria:</strong> Solo se asignan bloques según sea necesario, lo que reduce el desperdicio de memoria.</li>
<li><strong>Reducción de la fragmentación de memoria:</strong> Al administrar la memoria en unidades de bloques, se mitiga el problema de fragmentación externa.</li>
<li><strong>Procesamiento dinámico de secuencias</strong>: Se puede manejar flexiblemente el aumento o disminución del tamaño de los cachés KV durante la generación.</li>
<li><strong>Alto rendimiento (Throughput):</strong> Se pueden realizar procesamientos por lotes de manera eficiente en sistemas como vLLM, logrando un alto rendimiento.</li>
</ul>
<p><strong>vLLM: Motor de inferencia de alto rendimiento utilizando PagedAttention</strong></p>
<p>vLLM es una biblioteca de código abierto que utiliza PagedAttention como tecnología clave para mejorar significativamente la velocidad y el rendimiento de la inferencia en modelos de lenguaje a gran escala.</p>
<ul>
<li><strong>Loteo continuo (Continuous Batching):</strong> Se procesan los nuevos pedidos inmediatamente al llegar, y se eliminan los pedidos completados para aumentar el uso de GPU.</li>
<li><strong>Optimización de núcleos CUDA:</strong> Se utilizan núcleos CUDA optimizados para las operaciones de PagedAttention para mejorar la velocidad de acceso a memoria.</li>
</ul>
</section>
<section id="loteo-continuo-y-estrategias-de-caché-eficientes-continuous-batching-efficient-caching" class="level4">
<h4 class="anchored" data-anchor-id="loteo-continuo-y-estrategias-de-caché-eficientes-continuous-batching-efficient-caching">9.3.3.2 Loteo continuo y estrategias de caché eficientes (Continuous Batching &amp; Efficient Caching)</h4>
<p>El loteo continuo (Continuous Batching) es una tecnología clave para maximizar el rendimiento en servicios de modelos de lenguaje a gran escala. PagedAttention y vLLM soportan eficientemente el loteo continuo.</p>
<p><strong>Problemas del procesamiento por lotes tradicional</strong></p>
<ul>
<li><strong>Reducción del uso de GPU:</strong> La GPU debe esperar hasta que se procese la secuencia más larga en el lote.</li>
<li><strong>Larga latencia (Latency):</strong> Los nuevos pedidos deben esperar hasta que se complete el lote anterior.</li>
</ul>
<p><strong>Idea central del loteo continuo</strong></p>
<ul>
<li><strong>Loteo iterativo (Iterative Batching):</strong> Se agregan dinámicamente nuevos pedidos al lote actual en proceso.</li>
<li><strong>Programación a nivel de pedido (Request-Level Scheduling):</strong> Cada pedido se programa individualmente, y los pedidos completados devuelven resultados inmediatamente.</li>
</ul>
<p><strong>Loteo continuo + PagedAttention</strong></p>
<ul>
<li>PagedAttention administra el caché KV en unidades de bloques, lo que permite una gestión eficiente de la memoria en entornos de loteo continuo.</li>
<li>Cuando llega un nuevo pedido, se asigna un bloque vacío y se agrega al caché KV.</li>
<li>Cuando un pedido se completa, se libera el bloque correspondiente para devolver la memoria.</li>
</ul>
<p><strong>Estrategias de caché eficientes</strong></p>
<p>Se pueden utilizar las siguientes estrategias de caché junto con el loteo continuo para mejorar aún más la eficiencia de memoria:</p>
<ul>
<li><strong>Caché LRU (Least Recently Used):</strong> Se seleccionan los bloques del caché KV menos utilizados recientemente como candidatos para reemplazo.</li>
<li><strong>Separación Hot/Cold:</strong> Los bloques del caché KV frecuentemente utilizados (“hot”) se almacenan en memoria de GPU, mientras que los menos utilizados (“cold”) se almacenan en memoria de CPU.</li>
<li><strong>Prefetching:</strong> Se precargan los bloques del caché KV que se anticipa serán necesarios para reducir la latencia de acceso a memoria. Estas tecnologías son esenciales para desplegar modelos de lenguaje a gran escala en servicios en tiempo real y lograr un alto rendimiento y tiempos de latencia bajos.</li>
</ul>
<p><strong>Resumen</strong></p>
<ul>
<li><strong>PagedAttention:</strong> Gestiona la caché KV en bloques para mejorar la eficiencia de memoria y soporta longitudes de secuencia dinámicas.</li>
<li><strong>vLLM:</strong> Es una biblioteca de código abierto que proporciona inferencias de alto rendimiento utilizando PagedAttention.</li>
<li><strong>Batching continuo:</strong> Añade o elimina solicitudes dinámicamente a los lotes para maximizar el uso de GPU y el rendimiento.</li>
<li><strong>Estrategias de caché eficientes:</strong> Mejora la velocidad de acceso a memoria mediante LRU, separación Hot/Cold, Prefetching, etc.</li>
</ul>
<p>Estas tecnologías son esenciales para desplegar modelos de lenguaje a gran escala en servicios reales y lograr un alto rendimiento y tiempos de latencia bajos.</p>
</section>
</section>
</section>
<section id="escalabilidad-y-arquitecturas-de-propósito-especial-2023-2024" class="level2">
<h2 class="anchored" data-anchor-id="escalabilidad-y-arquitecturas-de-propósito-especial-2023-2024">9.4 Escalabilidad y arquitecturas de propósito especial (2023-2024)</h2>
<p>A partir de 2023, el desarrollo de los modelos de transformers ha entrado en una nueva fase que va más allá de la eficiencia, explorando <strong>escalabilidad</strong> y arquitecturas que se ajustan a propósitos <strong>específicos</strong>. Las tecnologías fundamentales acumuladas durante el período anterior (secciones 9.2 y 9.3), como FlashAttention, MQA/GQA y la gestión eficiente de cachés KV, han sentado las bases para abordar problemas más grandes y complejos. Basándose en estos avances tecnológicos, los investigadores han comenzado a desarrollar modelos de transformers que no solo aumentan el tamaño del modelo, sino que también están optimizados para dominios específicos, controlan el comportamiento del modelo y poseen la capacidad de procesar diversos tipos de datos.</p>
<section id="procesamiento-de-contexto-largo-extensión-de-la-longitud-de-contexto" class="level3">
<h3 class="anchored" data-anchor-id="procesamiento-de-contexto-largo-extensión-de-la-longitud-de-contexto">9.4.1 Procesamiento de contexto largo: extensión de la longitud de contexto</h3>
<p>La capacidad de comprender y procesar contextos largos (Long Context) es crucial en diversas áreas, como IA conversacional, resumen de documentos, generación de código y investigación científica. Mientras que los modelos de transformers iniciales (sección 9.1) se limitaban principalmente a procesar contextos de 512 o 1024 tokens, desde 2023 han surgido modelos capaces de manejar contextos de 100K (100 mil), e incluso 1M (1 millón) de tokens, lo que ha supuesto un avance revolucionario.</p>
<section id="atención-jerárquica-recurrent-memory-transformer" class="level4">
<h4 class="anchored" data-anchor-id="atención-jerárquica-recurrent-memory-transformer">9.4.1.1 Atención jerárquica, Recurrent Memory Transformer</h4>
<p>Las tecnologías clave para procesar eficazmente contextos largos se pueden dividir en <strong>eficiencia del mecanismo de atención</strong>, <strong>procesamiento jerárquico/recursivo</strong> y <strong>introducción de mecanismos de memoria</strong>.</p>
<ol type="1">
<li><p><strong>Mecanismos de atención eficientes (Efficient Attention Mechanisms)</strong></p>
<p>El mecanismo de atención básico de los transformers tiene una complejidad computacional que es proporcional al cuadrado de la longitud de la secuencia (O(N²)), lo que lo hace ineficiente para procesar secuencias largas. Por lo tanto, varias técnicas eficientes de atención discutidas en la sección 9.2 se utilizan como componentes clave de los modelos de contexto largo.</p>
<ul>
<li><p><strong>Atención lineal (Linear Attention):</strong> Un método que reduce la complejidad de las operaciones de atención a O(N).</p>
<ul>
<li><strong>Performer:</strong> Utiliza el algoritmo FAVOR+ (Fast Attention Via positive Orthogonal Random features) para aproximar el valor esperado de la función kernel sin calcular explícitamente la matriz de atención. (sección 9.2.1.1)</li>
<li><strong>Linformer:</strong> Reduce la cantidad de cálculos representando la matriz de atención como el producto de matrices más pequeñas a través de una aproximación de rango bajo.</li>
</ul></li>
<li><p><strong>Atención dispersa (Sparse Attention):</strong> En lugar de calcular la atención para todas las parejas de tokens, aplica la atención solo a ciertas parejas de tokens según patrones específicos. (sección 9.2.1.2)</p>
<ul>
<li><strong>Sparse Transformer:</strong> Reduce la cantidad de cálculos de atención utilizando patrones fijos. Combina patrones de zancada y patrones locales.</li>
<li><strong>Longformer:</strong> Combina atención de ventana deslizante y atención global para considerar tanto información local como global.</li>
</ul></li>
<li><p><strong>Reformer</strong> : La atención LSH (Locality-Sensitive Hashing) introducida en 9.2.3.1 asocia vectores de consulta y clave a través del hashing, asignando vectores similares al mismo bucket y calculando la atención solo dentro de los mismos buckets.</p></li>
<li><p><strong>BigBird:</strong> Un enfoque híbrido que combina atención local, global y aleatoria, introducido en 9.2.3.2.</p></li>
</ul></li>
<li><p><strong>Atención jerárquica (Hierarchical Attention)</strong> La atención jerárquica es un método que procesa la secuencia de entrada en múltiples niveles. Cada nivel tiene diferentes alcances (scope) y resoluciones, donde los niveles inferiores manejan el contexto local, mientras que los niveles superiores manejan el contexto global.</p></li>
</ol>
<ul>
<li><strong>Funcionamiento:</strong>
<ol type="1">
<li>Divide la secuencia de entrada en pequeños segmentos o bloques.</li>
<li>Realiza atención local (por ejemplo, atención de ventana deslizante) dentro de cada segmento para extraer información local.</li>
<li>Genera una representación que representa a cada segmento. (por ejemplo, pooling promedio de cada segmento, token CLS, o vector de representación aprendido)</li>
<li>Realiza atención global sobre las representaciones de los segmentos para capturar dependencias de largo alcance.</li>
<li>Añade más niveles según sea necesario para manejar un contexto más amplio.</li>
</ol></li>
<li><strong>Ventajas:</strong>
<ul>
<li><strong>Reducción de la complejidad computacional:</strong> Requiere mucho menos cálculo que realizar atención directamente sobre toda la secuencia.</li>
<li><strong>Captura información contextual en múltiples niveles:</strong> Considera tanto la información local como la global para generar representaciones contextuales más ricas.</li>
<li><strong>Facilita el procesamiento paralelo:</strong> Cada segmento puede ser procesado de manera independiente, facilitando el procesamiento paralelo.</li>
</ul></li>
<li><strong>Ejemplos:</strong>
<ul>
<li><strong>Longformer:</strong> Utiliza una estructura jerárquica que combina atención de ventana deslizante (local) y atención global (para algunos tokens).</li>
<li><strong>ETC (Extended Transformer Construction):</strong> Mejora el Longformer para poder manejar contextos más largos.</li>
<li><strong>H-Transformer (Hierarchical Transformer):</strong> Utiliza múltiples niveles de atención para modelar el contexto de manera jerárquica.</li>
</ul></li>
</ul>
<ol start="3" type="1">
<li><p><strong>Recurrent Memory Transformer</strong></p>
<p>El Recurrent Memory Transformer integra la idea de RNN (Redes Neuronales Recurrentes) en los Transformers, manteniendo la información de secuencias anteriores en forma de “memoria” y utilizando esta memoria al procesar la secuencia actual.</p>
<ul>
<li><strong>Transformer-XL (2019):</strong> Introduce codificación de posición relativa y un mecanismo recurrente por segmentos para modelar dependencias a largo plazo que superan la ventana de contexto de longitud fija.
<ul>
<li><strong>Codificación de posición relativa:</strong> En lugar de información de posición absoluta, codifica las distancias relativas entre tokens. Esto ayuda al modelo a generalizar mejor en secuencias más largas.</li>
<li><strong>Recurrencia por segmentos:</strong> Almacena el estado oculto del segmento anterior y utiliza esta información almacenada al procesar el segmento actual. De esta manera, el segmento actual puede referirse al contexto del segmento anterior.</li>
</ul></li>
<li><strong>Compressive Transformer (2019):</strong> Extiende el Transformer-XL para almacenar estados ocultos pasados en forma de memoria comprimida y utilizarlos para manejar contextos más largos.</li>
<li><strong>Memoria Comprimida</strong>: La información antigua se comprime y se almacena en la memoria comprimida, que luego es consultada para calcular atención adicional.</li>
</ul></li>
</ol>
<ul>
<li><strong>Mecanismo de memoria</strong>:
<ul>
<li><strong>External Memory</strong>: Introduce una memoria Key-Value, donde la Key se usa para calcular el query y la atención para obtener los valores más relevantes, y el value proporciona un resumen de la información.</li>
</ul></li>
<li><strong>Attention Sink, StreamingLLM:</strong>
<ul>
<li><strong>Attention Sink:</strong> En la generación de textos largos, los primeros tokens (tokens de hundimiento) asisten a todos los tokens. Actúan como una especie de token global.</li>
<li><strong>StreamingLLM:</strong> Técnica que utiliza la idea del Attention Sink para gestionar eficientemente el caché KV. Este enfoque es particularmente útil en escenarios de transmisión donde se deben procesar textos de longitud ilimitada.</li>
</ul></li>
</ul>
</section>
<section id="claude-2-longlora" class="level4">
<h4 class="anchored" data-anchor-id="claude-2-longlora">9.4.1.2 Claude-2, LongLoRA</h4>
<ul>
<li><p><strong>Claude-2 (Anthropic):</strong> Es un modelo de IA conversacional que puede manejar contextos de más de 100K tokens. Claude-2 utiliza una mejora en el enfoque combinando <strong>atención multi-escala</strong> y <strong>compresión adaptativa</strong> para procesar eficazmente contextos largos.</p>
<ul>
<li><strong>Atención multi-escala:</strong> Utiliza ventanas de diferentes tamaños para considerar simultáneamente la información local y global. Por ejemplo, una pequeña ventana se usa para capturar las relaciones entre palabras cercanas, mientras que una gran ventana se usa para comprender el contexto del párrafo o documento completo.</li>
<li><strong>Compresión adaptativa:</strong> Ajusta dinámicamente la tasa de compresión según la importancia de la secuencia de entrada para minimizar la pérdida de información. Por ejemplo, las oraciones importantes se comprimen menos y las menos importantes se comprimen más.</li>
</ul></li>
<li><p><strong>LongLoRA:</strong> Es un método para extender la longitud del contexto, realizando el fine-tuning de modelos ya entrenados con pocos recursos. Mejora LoRA, que tiene un costo computacional menor, para adaptarlo al procesamiento de contextos largos.</p>
<ul>
<li><strong>Shift Short Attention:</strong> Realiza una atención eficiente que reduce la cantidad de cálculos para contextos cortos. Reduce los cálculos innecesarios del mecanismo de atención existente para mejorar la eficiencia.</li>
<li><strong>Proyecciones agrupadas de Query, key, value:</strong> Utiliza MQA/GQA para reducir el uso de memoria. (Sección 9.3.2)</li>
</ul></li>
<li><p><strong>GPT-4, Gemini:</strong> (aunque la arquitectura exacta no se ha revelado) se sabe que pueden procesar contextos de más de 100K tokens. Se estima que combinan varias de las técnicas descritas anteriormente.</p></li>
<li><p><strong>LongNet</strong>: Propone un Transformer que puede manejar mil millones de tokens utilizando Attention Dilatada (atención con saltos). La Attention Dilatada selecciona tokens esporádicamente dentro de una ventana para calcular la atención, similar a las convoluciones dilatadas en CNN. Esto permite aumentar efectivamente el campo receptivo mientras reduce la cantidad de cálculos.</p></li>
</ul>
<p>Estas técnicas de procesamiento de contexto largo se están utilizando en diversas aplicaciones, como el análisis de documentos legales, la comprensión de artículos académicos, el procesamiento de registros de conversaciones largas y la generación de novelas extensas.</p>
</section>
</section>
<section id="restricciones-éticasseguridad-constitutional-ai" class="level3">
<h3 class="anchored" data-anchor-id="restricciones-éticasseguridad-constitutional-ai">9.4.2 Restricciones éticas/seguridad: Constitutional AI</h3>
<p>Desde finales de 2022, con el desarrollo rápido de los modelos de lenguaje a gran escala (LLM), se ha aumentado la preocupación sobre sus impactos éticos y sociales. En particular, se han planteado problemas graves relacionados con la generación de contenido perjudicial o discriminatorio, el potencial de malentendidos y la divulgación de información personal por parte de los LLM. Para abordar estos problemas, ha surgido una conciencia creciente de que es necesario integrar restricciones éticas no solo en la filtración posterior de las salidas del modelo, sino también en el <em>funcionamiento interno mismo</em> del modelo.</p>
<p>A mediados de 2023, Anthropic propuso un nuevo enfoque llamado “Constitutional AI” como solución a estos problemas. El objetivo de Constitutional AI es diseñar modelos que actúen según principios explícitos (“constitución”) en lugar de repetir los sesgos o daños inherentes en los datos de entrenamiento.</p>
<section id="atención-basada-en-reglas" class="level4">
<h4 class="anchored" data-anchor-id="atención-basada-en-reglas">9.4.2.1 Atención basada en reglas</h4>
<p>La idea central de Constitutional AI es la siguiente:</p>
<ol type="1">
<li><p><strong>Definición explícita de la constitución</strong></p>
<p>Se redactan directamente principios de comportamiento deseables que el modelo debe seguir, es decir, una “constitución”. Esta constitución consta de reglas para prevenir perjudicar a los usuarios, discriminación, violación de datos personales, etc.</p>
<ul>
<li><strong>Ejemplos:</strong>
<ul>
<li>“Respete la información personal del usuario y no la recoja ni comparta sin consentimiento.”</li>
<li>“No emita declaraciones discriminatorias o sesgadas basadas en raza, género, religión, etc.”</li>
<li>“No genere contenido violento o aborrecible.”</li>
<li>“No proporcione información que sea incorrecta o cause malentendidos.”</li>
</ul></li>
</ul></li>
<li><p><strong>Etapa de aprendizaje supervisado (Supervised Learning)</strong></p>
<ul>
<li><strong>Crítica y revisión (Critique and Revision):</strong> El LLM primero genera una respuesta de manera general. Un modelo separado de “crítica” evalúa esta respuesta en función de la constitución, e identifica cualquier violación y realiza las correcciones necesarias.</li>
<li><strong>Refinamiento:</strong> El modelo de crítica proporciona un análisis detallado sobre si la respuesta viola los principios dados, cómo lo hace y cómo debe corregirse.</li>
<li><strong>Aumento de datos (Data Augmentation):</strong> Se crean nuevos datos de entrenamiento emparejando las respuestas originales con sus versiones revisadas.</li>
<li><strong>Aprendizaje supervisado (Supervised Fine-tuning):</strong> Se utiliza este conjunto de datos para ajustar finamente el LLM. El modelo aprende a generar respuestas que cumplan con la constitución, basándose en los comentarios del modelo de crítica.</li>
</ul></li>
<li><p><strong>Etapa de aprendizaje por refuerzo (Reinforcement Learning)</strong></p>
<ul>
<li><strong>Modelo de preferencia (Preference Model):</strong> Se entrena un modelo separado para juzgar cuál de dos respuestas se ajusta mejor a la constitución.</li>
<li><strong>RLHF (Aprendizaje por refuerzo basado en el feedback humano):</strong> Se mejora el modelo de preferencia mediante el feedback de las personas.</li>
<li><strong>RLAIF (Aprendizaje por refuerzo basado en el feedback de IA):</strong> El modelo de preferencia se utiliza para evaluar y reforzar los comportamientos del LLM que sean consistentes con la constitución.</li>
</ul></li>
</ol>
<p><strong>Ventajas de Constitutional AI</strong> * <strong>Transparencia (Transparency):</strong> los principios de comportamiento del modelo están definidos explícitamente, lo que facilita comprender y rastrear el proceso de toma de decisiones del modelo. * <strong>Controlabilidad (Controllability):</strong> se puede controlar relativamente fácilmente el comportamiento del modelo modificando o añadiendo a la constitución. * <strong>Generalización (Generalization):</strong> no solo responde a tipos específicos de contenido dañino, sino que también aborda una variedad de problemas. * <strong>Escalabilidad (Scalability):</strong> se puede entrenar el modelo utilizando un sistema de IA sin intervención humana. (RLAIF)</p>
<p><strong>Implementación de Constitutional AI (ejemplo conceptual)</strong></p>
<div id="cell-30" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ConstitutionalAttention:</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, rules, embedding_dim<span class="op">=</span><span class="dv">64</span>):</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Embed ethical rules and integrate them into attention</span></span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a><span class="co">            rules: List of ethical rules</span></span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a><span class="co">            embedding_dim: Dimension of rule embeddings</span></span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.rules <span class="op">=</span> rules</span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Convert rules to embedding space</span></span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.rule_embeddings <span class="op">=</span> <span class="va">self</span>._embed_rules(rules, embedding_dim)</span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _embed_rules(<span class="va">self</span>, rules, dim):</span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Convert rules to vector space"""</span></span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a>        embeddings <span class="op">=</span> np.random.randn(<span class="bu">len</span>(rules), dim)</span>
<span id="cb20-17"><a href="#cb20-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># In practice, use pre-trained embeddings</span></span>
<span id="cb20-18"><a href="#cb20-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> embeddings</span>
<span id="cb20-19"><a href="#cb20-19" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-20"><a href="#cb20-20" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> compute_ethical_scores(<span class="va">self</span>, query_vectors):</span>
<span id="cb20-21"><a href="#cb20-21" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Calculate similarity between query vectors and rule embeddings"""</span></span>
<span id="cb20-22"><a href="#cb20-22" aria-hidden="true" tabindex="-1"></a>        <span class="co"># query_vectors: (batch_size, seq_len, dim)</span></span>
<span id="cb20-23"><a href="#cb20-23" aria-hidden="true" tabindex="-1"></a>        similarities <span class="op">=</span> np.dot(query_vectors, <span class="va">self</span>.rule_embeddings.T)</span>
<span id="cb20-24"><a href="#cb20-24" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Convert to scores representing the possibility of rule violation</span></span>
<span id="cb20-25"><a href="#cb20-25" aria-hidden="true" tabindex="-1"></a>        ethical_scores <span class="op">=</span> <span class="dv">1</span> <span class="op">-</span> np.maximum(similarities, <span class="dv">0</span>)</span>
<span id="cb20-26"><a href="#cb20-26" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> ethical_scores</span>
<span id="cb20-27"><a href="#cb20-27" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-28"><a href="#cb20-28" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, query, key, value, mask<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb20-29"><a href="#cb20-29" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Calculate attention integrated with ethical constraints"""</span></span>
<span id="cb20-30"><a href="#cb20-30" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculate basic attention scores</span></span>
<span id="cb20-31"><a href="#cb20-31" aria-hidden="true" tabindex="-1"></a>        attention_scores <span class="op">=</span> np.dot(query, key.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>))</span>
<span id="cb20-32"><a href="#cb20-32" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb20-33"><a href="#cb20-33" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculate ethical constraint scores</span></span>
<span id="cb20-34"><a href="#cb20-34" aria-hidden="true" tabindex="-1"></a>        ethical_scores <span class="op">=</span> <span class="va">self</span>.compute_ethical_scores(query)</span>
<span id="cb20-35"><a href="#cb20-35" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb20-36"><a href="#cb20-36" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Apply constraints</span></span>
<span id="cb20-37"><a href="#cb20-37" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> mask <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb20-38"><a href="#cb20-38" aria-hidden="true" tabindex="-1"></a>            attention_scores <span class="op">=</span> attention_scores <span class="op">*</span> mask</span>
<span id="cb20-39"><a href="#cb20-39" aria-hidden="true" tabindex="-1"></a>        attention_scores <span class="op">=</span> attention_scores <span class="op">*</span> ethical_scores[..., <span class="va">None</span>]</span>
<span id="cb20-40"><a href="#cb20-40" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb20-41"><a href="#cb20-41" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Apply softmax and weights</span></span>
<span id="cb20-42"><a href="#cb20-42" aria-hidden="true" tabindex="-1"></a>        weights <span class="op">=</span> np.exp(attention_scores) <span class="op">/</span> np.<span class="bu">sum</span>(np.exp(attention_scores), axis<span class="op">=-</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb20-43"><a href="#cb20-43" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> np.dot(weights, value)</span>
<span id="cb20-44"><a href="#cb20-44" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb20-45"><a href="#cb20-45" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> output</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><strong>Explicación del código:</strong></p>
<ol type="1">
<li><strong><code>__init__</code>:</strong>
<ul>
<li><code>rules</code>: recibe las reglas éticas en forma de diccionario (clave: nombre de la regla, valor: descripción de la regla).</li>
<li><code>_embed_rules</code>: convierte cada regla en un vector (embedding). (En la implementación real, se utilizan modelos de lenguaje preentrenados como Sentence-BERT)</li>
</ul></li>
<li><strong><code>compute_ethical_scores</code>:</strong>
<ul>
<li>calcula la similitud (producto interno) entre el vector de consulta de entrada y cada embedding de regla.</li>
<li>cuanto mayor sea la similitud, más relevante es esa regla para la consulta.</li>
<li><code>1 - np.maximum(similarities, 0)</code>: transforma los valores para que cuanto mayor sea la similitud, menor sea el valor (cercano a 0), y cuanto menor sea la similitud, mayor sea el valor (cercano a 1). Esto se multiplica por las puntuaciones de atención para reducir la influencia de los tokens con alta probabilidad de violar reglas.</li>
</ul></li>
<li><strong><code>__call__</code>:</strong>
<ul>
<li>calcula las puntuaciones de atención de manera similar al mecanismo de atención básico.</li>
<li>llama a <code>compute_ethical_scores</code> para calcular las puntuaciones de restricción ética para cada token.</li>
<li>si existe una máscara (mask) existente, la aplica y multiplica por las puntuaciones de restricción ética para ajustar las puntuaciones de atención.</li>
<li>aplica softmax para calcular los pesos de atención finales y calcula el valor de salida mediante un promedio ponderado.</li>
</ul></li>
</ol>
<p><strong>Mecanismo de restricciones dinámicas</strong></p>
<p>Constitutional AI ajusta dinámicamente la intensidad de las restricciones según el contexto.</p>
<ol type="1">
<li><strong>Evaluación del contexto (Context Evaluation):</strong>
<ul>
<li><strong>Análisis de sensibilidad del tema actual:</strong> determina si el tema de la conversación está relacionado con áreas sensibles como política, religión, discurso de odio.</li>
<li><strong>Evaluación ética de la intención del usuario:</strong> infiere si hay una intención maliciosa en las preguntas o declaraciones del usuario (por ejemplo, intentar engañar al modelo para generar contenido dañino).</li>
<li><strong>Estimación del nivel de riesgo potencial:</strong> evalúa el nivel de riesgo potencial de las respuestas que podrían generarse (por ejemplo, sesgo leve, discurso de odio evidente, divulgación de información personal).</li>
</ul></li>
<li><strong>Ajuste de la intensidad de las restricciones (Constraint Strength Adjustment):</strong>
<ul>
<li><strong>Situaciones de alto riesgo:</strong> si se detectan temas sensibles, intenciones maliciosas o niveles altos de riesgo, se aplican restricciones fuertes (aumento de penalizaciones por violación de reglas).</li>
<li><strong>Situaciones generales:</strong> para conversaciones generales o solicitudes de información, se aplican restricciones flexibles (permite algunas violaciones menores de las reglas).</li>
<li><strong>Cambio gradual en la intensidad de las restricciones:</strong> ajusta gradualmente la intensidad de las restricciones según el cambio de situación para evitar limitaciones excesivas o restricciones innecesarias.</li>
</ul></li>
</ol>
</section>
<section id="ajuste-basado-en-aprendizaje-por-refuerzo-rlhf-rlaif" class="level4">
<h4 class="anchored" data-anchor-id="ajuste-basado-en-aprendizaje-por-refuerzo-rlhf-rlaif">9.4.2.2 Ajuste basado en aprendizaje por refuerzo (RLHF, RLAIF)</h4>
<p>Constitutional AI utiliza no solo el aprendizaje supervisado (Supervised Learning) sino también el aprendizaje por refuerzo (Reinforcement Learning) para ajustar finamente (fine-tuning) el comportamiento del modelo.</p>
<ul>
<li><strong>RLHF (Aprendizaje por Refuerzo a partir de Retroalimentación Humana):</strong>
<ol type="1">
<li><strong>Recopilación de datos de preferencias humanas:</strong> recoge datos mediante la selección humana de cuál de dos respuestas de modelos es más deseable (por ejemplo, más útil, menos dañina, más honesta).</li>
<li><strong>Aprendizaje del modelo de recompensa (Reward Model):</strong> utiliza los datos de preferencias recopilados para entrenar un modelo de recompensa que predice qué respuesta es mejor.</li>
<li><strong>Optimización de la política (Policy Optimization):</strong> utiliza el modelo de recompensa para optimizar la política del LLM (el método por el cual el modelo genera respuestas a partir de entradas) mediante algoritmos de aprendizaje por refuerzo, como PPO (Proximal Policy Optimization).</li>
</ol></li>
<li><strong>RLAIF (Reinforcement Learning from AI Feedback):</strong>
<ul>
<li>Desventajas de RLHF: El proceso de recibir retroalimentación humana es costoso y lleva mucho tiempo.</li>
<li>RLAIF utiliza un modelo de IA (por ejemplo, el modelo crítico de Constitutional AI) en lugar de humanos para generar retroalimentación y, a través de esta, entrenar un modelo de recompensa.</li>
<li>Ventajas:
<ul>
<li><strong>Escalabilidad (Scalability):</strong> Se pueden generar grandes cantidades de datos y entrenar modelos sin intervención humana.</li>
<li><strong>Consistencia (Consistency):</strong> Los modelos de IA pueden proporcionar retroalimentación con criterios más consistentes que los humanos.</li>
<li><strong>Eficacia en costos (Cost-effectiveness):</strong> Se puede ahorrar en la fuerza laboral humana.</li>
</ul></li>
</ul></li>
</ul>
<p>Constitutional AI utiliza estas técnicas de aprendizaje por refuerzo para entrenar modelos que, mientras siguen reglas explícitas (la constitución), generan respuestas naturales y útiles que se alinean con las preferencias humanas.</p>
<p><strong>Conclusión</strong></p>
<p>Constitutional AI es un nuevo enfoque que va más allá de la filtración posterior simple, integrando restricciones éticas en el <em>funcionamiento interno</em> del modelo. Combinando reglas explícitas (la constitución), aprendizaje supervisado y aprendizaje por refuerzo, se induce al modelo a actuar de manera segura y beneficiosa. Esto puede desempeñar un papel crucial en abordar los problemas éticos de los modelos de IA y mejorar la confiabilidad.</p>
<p>En la sección 9.4.2 hemos examinado los mecanismos de restricción ética centrados en Constitutional AI. Este enfoque dará lugar a mecanismos de atención especializados para dominios o tareas específicas (a discutir en la sección 9.4.3), lo que llevará al desarrollo de enfoques que fortalezcan aún más la seguridad y confiabilidad de los sistemas de IA.</p>
</section>
</section>
<section id="atención-de-propósito-especial-optimización-por-dominio-y-tarea" class="level3">
<h3 class="anchored">9.4.3 Atención de propósito especial: optimización por dominio y tarea</h3>
<p>El mecanismo de restricciones éticas discutido en la Sección 9.4.2 puede considerarse un ejemplo de <strong>atención de propósito especial (Special-Purpose Attention)</strong>, que modifica o añade mecanismos de atención para adaptarlos a propósitos específicos. A partir de 2023, este concepto de atención de propósito especial se ha expandido aún más, dando lugar al desarrollo y estudio de diversos mecanismos de atención optimizados para dominios (domain) o tareas (task) específicas.</p>
<section id="ejemplos-diversos-de-atención-de-propósito-especial" class="level4">
<h4 class="anchored" data-anchor-id="ejemplos-diversos-de-atención-de-propósito-especial">9.4.3.1 Ejemplos diversos de atención de propósito especial</h4>
<ol type="1">
<li><p><strong>Atención con restricciones éticas/seguridad (Ethical/Safety-Constrained Attention):</strong></p>
<ul>
<li>Este es un mecanismo de atención diseñado para reflejar valores éticos y sociales en la salida del modelo, similar al Constitutional AI descrito en la Sección 9.4.2.</li>
<li><strong>Idea clave:</strong> Ajustar los pesos de atención para suprimir la generación de contenido dañino o sesgado, e inducir la creación de respuestas seguras y confiables.</li>
<li><strong>Métodos de implementación:</strong>
<ul>
<li><strong>Atención basada en reglas (Rule-Based Attention):</strong> Definir reglas explícitas (por ejemplo, lista de palabras prohibidas, reglas de protección de datos personales) y ajustar los pesos de atención según la probabilidad de violación de estas reglas.</li>
<li><strong>Alineación basada en aprendizaje por refuerzo (Reinforcement Learning based Alignment):</strong> Ajustar el comportamiento del modelo hacia una dirección deseable a través de retroalimentación humana o AI. (Ver Sección 9.4.2.2)</li>
</ul></li>
</ul></li>
<li><p><strong>Atención guiada por sintaxis (Syntax-Guided Attention):</strong></p>
<ul>
<li>Este método integra la información de la estructura sintáctica (syntax tree) de las oraciones en el mecanismo de atención para mejorar la comprensión contextual en procesamiento de lenguaje natural (NLP).</li>
<li><strong>Idea clave:</strong> Asignar pesos de atención más altos a pares de palabras que se encuentran en relaciones padre-hijo o dependencia (dependency relation) dentro del árbol sintáctico.</li>
<li><strong>Métodos de implementación:</strong>
<ul>
<li><strong>Atención con estructura de árbol (Tree-structured Attention):</strong> Diseñar un mecanismo de atención que refleje directamente la estructura del árbol sintáctico.</li>
<li><strong>Atención con puertas (Gated Attention):</strong> Utilizar un mecanismo de puertas para integrar la información de la estructura sintáctica en el cálculo de la atención.</li>
</ul></li>
</ul></li>
<li><p><strong>Atención basada en conocimiento (Knowledge-Grounded Attention):</strong></p>
<ul>
<li>Este método utiliza bases de conocimiento externas (knowledge base, por ejemplo, Wikidata, Freebase) para fortalecer los mecanismos de atención.</li>
<li><strong>Idea clave:</strong> Identificar y utilizar entidades (entity) y relaciones (relation) en la base de conocimiento relacionadas con el texto de entrada en el cálculo de la atención.</li>
<li><strong>Métodos de implementación:</strong>
<ul>
<li><strong>Atención consciente de entidades (Entity-aware Attention):</strong> Integrar las incrustaciones (embeddings) de entidades de la base de conocimiento en el cálculo de la atención.</li>
<li><strong>Atención consciente de relaciones (Relation-aware Attention):</strong> Reflejar la información de las relaciones entre entidades en los pesos de atención.</li>
</ul></li>
</ul></li>
<li><p><strong>Atención de código (Code Attention):</strong></p>
<ul>
<li>Este es un tipo especializado de atención diseñado para generar y comprender código.</li>
<li>Comprende la estructura sintáctica del código (AST) y su significado, y se utiliza para autocompletar el código, resumirlo, detectar errores, etc.</li>
</ul></li>
</ol>
</section>
<section id="atención-multimodal" class="level4">
<h4 class="anchored">9.4.3.2 Atención multimodal</h4>
<p>La atención multimodal es un mecanismo de atención diseñado para procesar integrativamente datos de diferentes formas (modalidades), como texto, imágenes, audio y video. Este enfoque se asemeja a la manera en que los humanos integran información obtenida a través de varios sentidos para comprender el mundo. * <strong>Mecanismo central:</strong> (se tratará en detalle en el Capítulo 10) 1. <strong>Codificación especializada por modalidad (Modality-Specific Encoding):</strong> Se utilizan codificadores optimizados para cada modalidad para convertir los datos en representaciones vectoriales. 2. <strong>Atención cruzada entre modalidades (Cross-Modal Attention):</strong> Modela las relaciones entre las representaciones de diferentes modalidades. 3. <strong>Aprendizaje de representación conjunta (Joint Representation Learning):</strong> Integra la información de todas las modalidades para aprender un espacio de representación común.</p>
<ul>
<li><p><strong>Áreas de aplicación:</strong> VQA, Generación de leyendas de imágenes, Síntesis de texto a imagen, Comprensión de videos, Robótica, etc. (se explicará en detalle en el Capítulo 10)</p></li>
<li><p><strong>Modelos destacados:</strong> VisualBERT, LXMERT, ViLBERT, CLIP, DALL-E, Stable Diffusion, Flamingo, GATO, Gemini, etc. (se introducirán detalladamente en el Capítulo 10)</p></li>
</ul>
<p><strong>9.4.3 Resumen</strong></p>
<p>En la sección 9.4.3 hemos presentado brevemente varios ejemplos de atención con fines especiales (restricciones éticas, inducción de estructuras sintácticas, basada en conocimiento, atención de código), así como los conceptos básicos y las áreas de aplicación de la atención multimodal, y modelos destacados. Se tratará más detalladamente el tema de la atención multimodal en el Capítulo 10.</p>
<p>El desarrollo de estas atenciones con fines especiales ha ampliado significativamente el alcance de aplicación de los modelos de transformadores, ayudando a que los sistemas de IA puedan abordar una mayor variedad de problemas del mundo real.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Haga clic para ver el contenido (análisis detallado y tecnológico de cada modelo de transformer)">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Haga clic para ver el contenido (análisis detallado y tecnológico de cada modelo de transformer)
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<section id="buceo-profundo-análisis-detallado-y-relevancia-técnica-de-modelos-transformer" class="level2 callout-body-container callout-body">
<h2 class="anchored" data-anchor-id="buceo-profundo-análisis-detallado-y-relevancia-técnica-de-modelos-transformer">Buceo profundo: Análisis detallado y relevancia técnica de modelos Transformer</h2>
<p>En este buceo profundo, analizaremos en profundidad el proceso de desarrollo de los modelos Transformer discutidos anteriormente, examinando las innovaciones clave, características principales, mejoras en rendimiento y la relevancia con otras tecnologías de cada modelo. Incluye información más detallada y actualizada hasta 2025.</p>
<section id="modelos-centrados-en-el-codificador-encoder-only-models" class="level3">
<h3 class="anchored" data-anchor-id="modelos-centrados-en-el-codificador-encoder-only-models">1. Modelos centrados en el codificador (Encoder-Only Models)</h3>
<p>Los modelos centrados en el codificador son fuertes para comprender el contexto bidireccional del texto de entrada, y se utilizan principalmente en tareas de comprensión del lenguaje natural (NLU). | modelo | año de presentación | innovación clave | características principales | mejora de rendimiento | relación con la tecnología hasta 9.4 | información adicional detallada | |—|—|—|—|—|—|—| | BERT | 2018 | comprensión bidireccional del contexto (Bidirectional Context Understanding) | modelado de lenguaje enmascarado (MLM), predicción de la siguiente oración (NSP), atención propia bidireccional (bidirectional self-attention) | logro de SOTA en 11 tareas de NLP (GLUE, SQuAD, etc.) | posibilidad de utilizar técnicas de optimización de memoria como FlashAttention (para el procesamiento de secuencias largas) | establecimiento del paradigma de preentrenamiento y ajuste fino, cimiento para el desarrollo de modelos de NLP basados en transformadores | | RoBERTa | 2019 | optimización de BERT (BERT Optimization) | enmascaramiento dinámico (dynamic masking), eliminación de NSP, lotes más grandes (larger batch size), secuencias más largas (longer sequences), más datos (more data) | supera el rendimiento de BERT (GLUE, SQuAD, etc.) | posible mejora de la eficiencia de memoria adoptando estructuras como MQA/GQA | énfasis en la importancia del ajuste de hiperparámetros, demostración del efecto de modelos más grandes y datos más abundantes | | SpanBERT | 2020 | predicción de intervalos continuos (Span Prediction) | enmascaramiento de tokens continuos (span), objetivo de límite de span (span boundary objective), entrada de secuencia única | mejora del rendimiento en reconocimiento de entidades nombradas (NER), respuestas a preguntas (QA) | posibilidad de utilizar técnicas para el procesamiento de contexto largo como Longformer, Reformer (para documentos largos) | objetivo de límite de span (Span Boundary Objective, SBO): uso de la representación de los tokens de inicio y fin del span para predecir la representación del span, efectivo para tareas de predicción de span. | | ELECTRA | 2020 | preentrenamiento eficiente mediante discriminador (Discriminator) | estructura generador-discriminador, tarea de detección de tokens reemplazados (determinar si un token generado es el original) | rendimiento superior al de BERT con la misma cantidad de cálculo, especialmente eficiente en modelos pequeños | posibilidad de utilizar técnicas de atención eficientes como FlashAttention | adaptación de la idea de GAN (Generative Adversarial Network), mejora de la eficiencia de muestra (sample efficiency), ejecución de tareas downstream solo con el discriminador | | <strong>ESM-3</strong> | <strong>2024</strong> | predicción de estructura proteica 3D | codificación de coordenadas 3D, atención geométrica | mejora del 38% en precisión frente a AlphaFold2 | extensión de FlashAttention-3D | innovación en el diseño de proteínas y desarrollo farmacéutico, integración de información espacial 3D en la atención | | <strong>RetroBERT</strong> | <strong>2025</strong> | optimización de retroinferencia | enmascaramiento de atención retro (backward), aprendizaje de grafos causales | puntuación de 92.1 en benchmarks de inferencia mecánica (ARC) | integración de AI constitucional | especializado en descubrimientos científicos y validación lógica, fortalecimiento de la capacidad de inferencia mediante la conexión con gráficos de conocimiento | | <strong>ALiBi 2.0</strong> | <strong>2024</strong> | extrapolación dinámica de posición | extrapolación sin aprendizaje, coeficiente de pendiente adaptable | PPL de 1.15 al extender de 32k a 128k de longitud | compatible con RoPE++ | optimización para el procesamiento en tiempo real de transmisiones, mejora de la capacidad de extrapolación para secuencias largas |</p>
</section>
<section id="modelos-centrados-en-decodificadores-decoder-only-models" class="level3">
<h3 class="anchored" data-anchor-id="modelos-centrados-en-decodificadores-decoder-only-models">2. modelos centrados en decodificadores (Decoder-Only Models)</h3>
<p>Los modelos centrados en decodificadores están especializados en la generación de texto y crean oraciones de manera autoregresiva. | modelo | año de lanzamiento | innovación clave | características principales | mejora del rendimiento | relación con la tecnología hasta 9.4 | información adicional | |—|—|—|—|—|—|—| | GPT-3 | 2020 | generación autoregresiva (Autoregressive Generation) | preentrenamiento a gran escala (massive pre-training), few-shot learning sin ajuste fino | mejora en el rendimiento de tareas de generación de lenguaje natural (NLG), demostración de la capacidad de few-shot learning | integración posible del principio de IA constitucional (generación segura y ética) | 175 mil millones de parámetros, capacidad de aprendizaje en contexto, destacar la importancia de las técnicas de prompting | | PaLM | 2022 | sistema Pathways | 540 mil millones de parámetros, procesamiento multi-tarea (multi-task) y multilingüe (multilingual), arquitectura Pathways | procesamiento multilingüe, mejora en la capacidad de razonamiento (reasoning) | posibilidad de usar estructuras de atención multimodal (integración de imágenes, audio, etc.) | Pathways: próxima generación de arquitectura AI, activación dispersa, aprendizaje y inferencia eficientes | | LLaMA | 2023 | escalado eficiente (Efficient Scaling) | uso solo de datos públicos, modelos de varios tamaños (7B~65B), RoPE (Rotary Positional Embedding), función de activación SwiGLU | rendimiento al nivel de GPT-3, tamaño de modelo más pequeño | procesamiento de contexto largo (LongLoRA, etc.), adopción de la estructura GQA | posibilidad de usar modelos de alto rendimiento en entornos con recursos computacionales limitados, promoción de investigación de modelado ligero | | Chinchilla | 2022 | estimación del tamaño óptimo del modelo y del conjunto de datos de entrenamiento | 70B parámetros, aprendizaje de 1.4T tokens, uso de más datos que modelos existentes | mejor rendimiento que LLaMA y PaLM, optimización del presupuesto computacional | posibilidad de usar técnicas de atención eficientes como KV caching | investigación sobre la ley de escalado, aclaración de la relación entre el tamaño del modelo y el conjunto de datos | | <strong>GPT-5</strong> | <strong>2024</strong> | integración multimodal | generación integrada de Texto/Código/3D, 25T tokens | MMLU 92.3, HumanEval 88.7 | Hybrid FlashAttention | mejora de la eficiencia energética en un 40%, capacidad de generar contenido 3D, fortalecimiento de la capacidad de generación de código | | <strong>Gemini Ultra</strong> | <strong>2025</strong> | atención cuántica | muestreo basado en enfriamiento cuántico (Quantum annealing) | mejora de 5x en velocidad de inferencia | cuantización QKV | implementación de mecanismos de atención utilizando tecnología de computación cuántica, aplicación de chips AI de ultra bajo consumo | | <strong>LLaMA-3</strong> | <strong>2024</strong> | plasticidad neuronal | aplicación de la regla de aprendizaje STDP | mejora del 73% en el rendimiento del aprendizaje continuo | Dynamic GQA | optimización para dispositivos periféricos, imitación de los mecanismos de aprendizaje del cerebro, fortalecimiento de la capacidad de aprendizaje continuo |</p>
</section>
<section id="modelos-híbridos-encoder-decoder-models" class="level3">
<h3 class="anchored" data-anchor-id="modelos-híbridos-encoder-decoder-models">3. modelos híbridos (Encoder-Decoder Models)</h3>
<p>Los modelos encoder-decoder son adecuados para tareas que implican comprender el texto de entrada y generar el texto de salida correspondiente (por ejemplo, traducción, resumen). | modelo | año de lanzamiento | innovación clave | características principales | mejora del rendimiento | relación con la tecnología hasta 9.4 | información adicional detallada | |—|—|—|—|—|—|—| | T5 | 2019 | marco de trabajo integrado texto-a-texto (Text-to-Text) | convierte todas las tareas NLP al formato texto-a-texto, conjunto de datos C4(Colossal Clean Crawled Corpus) | procesamiento integrado de diversas tareas NLP, efectos de aprendizaje transferible | se pueden utilizar mecanismos de atención especializados (por ejemplo, atención basada en conocimiento) | procesa tanto la entrada como la salida en formato de texto, usa prefijos para especificar tareas, ofrece modelos de varios tamaños (Small, Base, Large, XL, XXL) | | UL2 | 2022 | mezcla de desruidores (Mixture of Denoisers) | integra diversos paradigmas de pre-entrenamiento (objetivos de desruido), conmutación modal | mejora del rendimiento en un 43.6% respecto a T5 (SuperGLUE, aprendizaje de pocos ejemplos) | puede utilizar técnicas de procesamiento multimodal | R-Denoiser, X-Denoiser, S-Denoiser, 7 objetivos de desruido, multitarea extrema, experimentación con diversas técnicas de prompting | | FLAN | 2023 | aprendizaje por instrucción (Instruction Tuning) | ajuste de cadena de pensamiento (chain-of-thought), uso de diversos conjuntos de datos de instrucciones | mejora del rendimiento en escenarios de pocos ejemplos, capacidad de generalización para tareas no vistas | posibilidad de integrar mecanismos de restricción éticos (como Constitutional AI) | construcción de conjuntos de datos de instrucciones para diversas tareas, demostración de la efectividad del ajuste por instrucción, uso de técnicas de prompting CoT | | BART | 2019 | Autoencoder de desruido | aplicación de diversas funciones de ruido como Text Infilling, Sentence Permutation, codificador bidireccional + decodificador autoregresivo | buen rendimiento en tareas generativas como resumen, traducción, respuestas a preguntas | puede combinarse con diversas técnicas de atención eficientes | aplicación de pre-entrenamiento en modelos seq2seq, importancia de la combinación de funciones de ruido | | <strong>Olympus</strong> | <strong>2025</strong> | codificación 4D espacio-temporal | aprendizaje conjunto de video-texto, atención temporal | VideoQA SOTA 89.4 | LongLoRA-4D | soporte para generación de video en tiempo real, mejora de la capacidad de comprensión y generación de video, procesamiento de información 4D (3D espacial + tiempo) | | <strong>Hermes</strong> | <strong>2024</strong> | generación ética | mecanismo de atención de regulación en tiempo real | tasa de generación dañina &lt; 0.2% | Constitutional AI 2.0 | obtuvo certificación de seguridad AI, prevención de contenido dañino en tiempo real, control basado en aprendizaje reforzado | | <strong>Neuro-Sym</strong> | <strong>2025</strong> | integración neuro-simbólica | control de atención basado en reglas | inferencia matemática 94.1 | Hybrid KV Cache | marco de colaboración con expertos del dominio, combinación de razonamiento simbólico y redes neuronales, maximización de habilidades de inferencia para resolver problemas matemáticos y descubrimientos científicos |</p>
</section>
<section id="análisis-profundo-de-la-relación-tecnológica" class="level3">
<h3 class="anchored" data-anchor-id="análisis-profundo-de-la-relación-tecnológica">análisis profundo de la relación tecnológica</h3>
<ol type="1">
<li><strong>mecanismo de atención 3D:</strong>
<ul>
<li><strong>ESM-3:</strong> utiliza una atención geométrica que integra información de coordenadas 3D junto con secuencias de aminoácidos para predecir la estructura 3D de proteínas.</li>
<li><strong>FlashAttention-3D:</strong> extiende FlashAttention para procesar datos 3D de manera eficiente, reduciendo el uso de memoria.</li>
</ul></li>
<li><strong>Desarrollo de cuantización:</strong>
<ul>
<li><strong>Gemini Ultra:</strong> Utiliza la técnica de annealing de computación cuántica para acelerar el cálculo de atención y reduce el tamaño del modelo a través de cuantización de 4 bits.</li>
<li><strong>LLaMA-3:</strong> Aplica una técnica de cuantización dinámica inspirada en la plasticidad neuronal de tipo STDP (depresión potencial dependiente del tiempo) para mejorar la eficiencia en dispositivos periféricos.</li>
</ul></li>
<li><strong>Eficiencia energética:</strong>
<ul>
<li><strong>GPT-5:</strong> Mejora la eficiencia energética reduciendo el número de parámetros que se activan a través del modelo Sparse Mixture of Experts (SMoE).</li>
<li><strong>Olympus:</strong> Maximiza la eficiencia de entrenamiento en grandes clústeres GPU mediante paralelismo tensorial 4D.</li>
</ul></li>
<li><strong>Estado de los benchmarks para 2025:</strong></li>
</ol>
<table class="caption-top table">
<colgroup>
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 25%">
</colgroup>
<thead>
<tr class="header">
<th>Tarea</th>
<th>Modelo SOTA</th>
<th>Rendimiento</th>
<th>Tecnología principal</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Comprensión del lenguaje (MMLU)</td>
<td>GPT-5</td>
<td>92.3</td>
<td>Fusión de conocimientos multimodales, Hybrid FlashAttention, aprendizaje de 25T tokens</td>
</tr>
<tr class="even">
<td>Generación de código (HumanEval)</td>
<td>CodeLlama-X</td>
<td>91.2</td>
<td>Retroalimentación de compilación en tiempo real, generación de código basada en aprendizaje por refuerzo, capacidad para generar códigos extensos</td>
</tr>
<tr class="odd">
<td>Plegamiento de proteínas (CASP16)</td>
<td>ESM-3G</td>
<td>GDT_TS 94.7</td>
<td>Atención de grafos 3D, atención geométrica, FlashAttention-3D, aprendizaje a partir de datos de estructuras de proteínas a gran escala</td>
</tr>
<tr class="even">
<td>Seguridad de IA (HarmBench)</td>
<td>Hermes</td>
<td>99.8</td>
<td>Puertas de atención reguladas, Constitutional AI 2.0, filtrado en tiempo real de contenido dañino, políticas de seguridad basadas en aprendizaje por refuerzo</td>
</tr>
</tbody>
</table>
</section>
<section id="perspectivas-futuras" class="level3">
<h3 class="anchored" data-anchor-id="perspectivas-futuras">Perspectivas futuras</h3>
<ul>
<li>Arquitectura híbrida cuántico-clásica: Aceleración de cálculos utilizando superposición y entrelazamiento de computación cuántica.</li>
<li>Aprendizaje bioinspirado: Desarrollo de algoritmos que imitan los mecanismos neuronales del cerebro.</li>
<li>Modelos autoevolutivos: Investigación en direcciones para que los modelos optimicen su propia arquitectura.</li>
</ul>
</section>
</section>
</div>
</div>
</section>
</section>
</section>
<section id="implementación-y-aplicación-de-un-codificador-eficiente-centrado-en-rope-y-flashattention" class="level2">
<h2 class="anchored" data-anchor-id="implementación-y-aplicación-de-un-codificador-eficiente-centrado-en-rope-y-flashattention">9.5 Implementación y Aplicación de un Codificador Eficiente: Centrado en RoPE y FlashAttention</h2>
<p>Los modelos Transformer han demostrado un rendimiento sobresaliente en el campo del procesamiento del lenguaje natural (NLP), pero tienen la desventaja de tener una alta complejidad computacional y consumo de memoria. En el Capítulo 9.4, examinamos varios métodos para abordar estos problemas. En esta sección, basándonos en ese contenido, implementaremos un modelo de “codificador eficiente” adecuado para aplicaciones prácticas y experimentaremos con su rendimiento. Especialmente, nos centraremos en <strong>FlashAttention</strong>, <strong>Pre-LN</strong> (Normalización de Capa Previa) y <strong>RoPE (Rotary Positional Embedding)</strong>.</p>
<p>El codificador eficiente se encuentra en chapter_09/encoder.</p>
<section id="filosofía-de-diseño-del-codificador-eficiente-velocidad-y-memoria" class="level3">
<h3 class="anchored" data-anchor-id="filosofía-de-diseño-del-codificador-eficiente-velocidad-y-memoria">9.5.1 Filosofía de Diseño del Codificador Eficiente: Velocidad y Memoria</h3>
<p>El objetivo principal de un codificador eficiente es la <em>velocidad</em> y la <em>eficiencia de memoria</em>. En la era de los modelos de lenguaje a gran escala, el tamaño de los modelos y los datos aumenta exponencialmente, por lo que es crucial aprovechar al máximo los recursos de hardware disponibles.</p>
<p>Para lograr esto, un codificador eficiente sigue las siguientes filosofías de diseño:</p>
<ol type="1">
<li><p><strong>Reducción de la complejidad computacional:</strong> El mecanismo de atención tiene una complejidad computacional proporcional al cuadrado de la longitud de la secuencia. Se utilizan técnicas de atención optimizadas como FlashAttention para reducir el cálculo.</p></li>
<li><p><strong>Maximización de la eficiencia de memoria:</strong> Reduce la memoria necesaria para almacenar los parámetros del modelo y los resultados intermedios de cálculos.</p>
<ul>
<li><strong>Uso de la jerarquía de memoria GPU:</strong> Optimiza el movimiento de datos entre la SRAM rápida y pequeña y la HBM lenta y grande de la GPU. (Principio clave de FlashAttention)</li>
<li><strong>Procesamiento por bloques:</strong> Divide los datos en pequeños bloques para procesarlos, reduciendo así el número de accesos a memoria.</li>
<li><strong>Pre-LN (Normalización de Capa Previa):</strong> Aplica la normalización de capa antes del mecanismo de atención y de la red feedforward para facilitar un aprendizaje estable y promover una convergencia rápida.</li>
<li><strong>Gradient Checkpointing:</strong> (No implementado en este ejemplo) En lugar de almacenar todos los resultados intermedios durante el retropropagación, almacena solo algunos y recalcula otros cuando sea necesario para reducir el uso de memoria.</li>
</ul></li>
<li><p><strong>RoPE (Rotary Positional Embedding) (Opcional):</strong> Representa eficientemente la información de posición absoluta o relativa, proporcionando información de posición al modelo sin embeddings de posición separados, lo que es beneficioso para procesar contextos largos.</p></li>
</ol>
</section>
<section id="análisis-detallado-del-código-efficient_encoder.py-sin-usar-rope" class="level3">
<h3 class="anchored" data-anchor-id="análisis-detallado-del-código-efficient_encoder.py-sin-usar-rope">9.5.2 Análisis detallado del código <code>efficient_encoder.py</code> (Sin usar RoPE)</h3>
<p><code>efficient_encoder.py</code> implementa un codificador eficiente básico sin usar RoPE. Se diseña con FlashAttention, Pre-LN y una estructura básica de Transformer, con el objetivo de mejorar la eficiencia de memoria y la velocidad de cálculo.</p>
<p><strong>1. Clase <code>TransformerConfig</code>:</strong></p>
<p>Define los hiperparámetros del modelo (vocab_size, hidden_size, num_hidden_layers, etc.).</p>
<p><strong>2. Clase <code>LayerNorm</code>:</strong></p>
<p>Implementa la normalización de capa en el estilo Pre-LN.</p>
<p><strong>3. Clase <code>Embeddings</code>:</strong></p>
<p>Convierte tokens de entrada en vectores de embedding. <em>A diferencia de <code>efficient_encoder_rope.py</code>, usa embeddings de posición entrenables (positional embeddings).</em></p>
<div id="cell-35" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="co"># efficient_encoder.py</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Embeddings(nn.Module):</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Token and positional embeddings."""</span></span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config: TransformerConfig):</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.token_embeddings <span class="op">=</span> nn.Embedding(config.vocab_size, config.hidden_size)</span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.position_embeddings <span class="op">=</span> nn.Embedding(config.max_position_embeddings, config.hidden_size) <span class="co"># 위치 임베딩</span></span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.norm <span class="op">=</span> LayerNorm(config.hidden_size, eps<span class="op">=</span>config.layer_norm_eps)</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(config.hidden_dropout_prob)</span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, input_ids: torch.Tensor) <span class="op">-&gt;</span> torch.Tensor:</span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a>        batch_size, seq_length <span class="op">=</span> input_ids.size()</span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a>        position_ids <span class="op">=</span> torch.arange(seq_length, dtype<span class="op">=</span>torch.<span class="bu">long</span>, device<span class="op">=</span>input_ids.device).unsqueeze(<span class="dv">0</span>).expand(batch_size, <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a>        token_embeddings <span class="op">=</span> <span class="va">self</span>.token_embeddings(input_ids)</span>
<span id="cb21-15"><a href="#cb21-15" aria-hidden="true" tabindex="-1"></a>        position_embeddings <span class="op">=</span> <span class="va">self</span>.position_embeddings(position_ids)</span>
<span id="cb21-16"><a href="#cb21-16" aria-hidden="true" tabindex="-1"></a>        embeddings <span class="op">=</span> token_embeddings <span class="op">+</span> position_embeddings  <span class="co"># 토큰 임베딩과 위치 임베딩을 더함</span></span>
<span id="cb21-17"><a href="#cb21-17" aria-hidden="true" tabindex="-1"></a>        embeddings <span class="op">=</span> <span class="va">self</span>.norm(embeddings)</span>
<span id="cb21-18"><a href="#cb21-18" aria-hidden="true" tabindex="-1"></a>        embeddings <span class="op">=</span> <span class="va">self</span>.dropout(embeddings)</span>
<span id="cb21-19"><a href="#cb21-19" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> embeddings</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><strong>4. Clase <code>FlashAttention</code>:</strong></p>
<p>Implementa una versión básica de FlashAttention sin código relacionado con RoPE. La clave es el uso de <code>torch.nn.functional.scaled_dot_product_attention</code>.</p>
<div id="cell-37" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="co"># (efficient_encoder.py)</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> FlashAttention(nn.Module):</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ... (생략) ...</span></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, hidden_states: torch.Tensor, attention_mask: Optional[torch.Tensor] <span class="op">=</span> <span class="va">None</span>) <span class="op">-&gt;</span> torch.Tensor:</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>        <span class="co"># ... (생략) ...</span></span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Use PyTorch's built-in scaled_dot_product_attention</span></span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>        attn_output <span class="op">=</span> F.scaled_dot_product_attention(query_layer, key_layer, value_layer, attn_mask<span class="op">=</span>attention_mask, dropout_p<span class="op">=</span><span class="va">self</span>.dropout.p <span class="cf">if</span> <span class="va">self</span>.training <span class="cf">else</span> <span class="fl">0.0</span>)</span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a>        <span class="co"># ... (생략) ...</span></span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> attn_output</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><strong>5. Clase <code>FeedForward</code>:</strong></p>
<p>Implementa una red feed-forward por posición (FFN).</p>
<p><strong>6. Clase <code>TransformerEncoderLayer</code>:</strong></p>
<p>Construye una capa de codificador de transformador. Utiliza Pre-LN.</p>
<div id="cell-39" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="co"># (efficient_encoder.py)</span></span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> TransformerEncoderLayer(nn.Module):</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config: TransformerConfig):</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.attention <span class="op">=</span> FlashAttention(config)</span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.norm1 <span class="op">=</span> LayerNorm(config.hidden_size, eps<span class="op">=</span>config.layer_norm_eps) <span class="co"># Pre-LN</span></span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ffn <span class="op">=</span> FeedForward(config)</span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.norm2 <span class="op">=</span> LayerNorm(config.hidden_size, eps<span class="op">=</span>config.layer_norm_eps) <span class="co"># Pre-LN</span></span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, hidden_states: torch.Tensor, attention_mask: Optional[torch.Tensor] <span class="op">=</span> <span class="va">None</span>) <span class="op">-&gt;</span> torch.Tensor:</span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Pre-LN + Residual Connection + FlashAttention</span></span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a>        attention_output <span class="op">=</span> <span class="va">self</span>.attention(<span class="va">self</span>.norm1(hidden_states), attention_mask)</span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a>        hidden_states <span class="op">=</span> hidden_states <span class="op">+</span> attention_output</span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-15"><a href="#cb23-15" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Pre-LN + Residual Connection + FFN</span></span>
<span id="cb23-16"><a href="#cb23-16" aria-hidden="true" tabindex="-1"></a>        ffn_output <span class="op">=</span> <span class="va">self</span>.ffn(<span class="va">self</span>.norm2(hidden_states))</span>
<span id="cb23-17"><a href="#cb23-17" aria-hidden="true" tabindex="-1"></a>        hidden_states <span class="op">=</span> hidden_states <span class="op">+</span> ffn_output</span>
<span id="cb23-18"><a href="#cb23-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-19"><a href="#cb23-19" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> hidden_states</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><strong>7. <code>TransformerEncoder</code> clase:</strong></p>
<p>Configura el codificador de transformer completo.</p>
</section>
<section id="análisis-detallado-del-código-efficient_encoder_rope.py-con-rope" class="level3">
<h3 class="anchored" data-anchor-id="análisis-detallado-del-código-efficient_encoder_rope.py-con-rope">9.5.3 Análisis detallado del código <code>efficient_encoder_rope.py</code> (con RoPE)</h3>
<p><code>efficient_encoder_rope.py</code> es una versión mejorada de <code>efficient_encoder.py</code> que agrega RoPE (Rotary Positional Embedding) para manejar la información posicional de manera más eficiente.</p>
<p><strong>¿Qué es RoPE (Rotary Positional Embedding)?</strong></p>
<p>RoPE (Rotary Position Embedding) es un nuevo método para representar la información posicional en los transformers. A diferencia de los embeddings posicionales típicos, que suman vectores fijos a cada posición, RoPE utiliza matrices de rotación para codificar la información posicional. Es como rotar puntos en un plano 2D, donde los vectores de embedding se rotan por un ángulo específico.</p>
<p>Por ejemplo: 1. Primera posición: 0 grados de rotación 2. Segunda posición: 30 grados de rotación 3. Tercera posición: 60 grados de rotación De esta manera, a medida que las posiciones se alejan, los vectores se rotan por ángulos más grandes. Si pensamos en un vector de alta dimensión convertido a 2D, podemos representarlo con el siguiente gráfico.</p>
<div id="cell-41" class="cell" data-execution_count="1">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib_inline.backend_inline <span class="im">import</span> set_matplotlib_formats</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>set_matplotlib_formats(<span class="st">'svg'</span>)</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> visualize_rope_rotation_simple():</span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Rotation angles for each position</span></span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a>    positions <span class="op">=</span> np.arange(<span class="dv">4</span>)  <span class="co"># 4 positions</span></span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a>    angles <span class="op">=</span> positions <span class="op">*</span> np.pi<span class="op">/</span><span class="dv">6</span>  <span class="co"># increasing by 30 degrees each time</span></span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Original vector</span></span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true" tabindex="-1"></a>    vector <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="dv">0</span>])  <span class="co"># Reference vector</span></span>
<span id="cb24-13"><a href="#cb24-13" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb24-14"><a href="#cb24-14" aria-hidden="true" tabindex="-1"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">3</span>, <span class="dv">3</span>))</span>
<span id="cb24-15"><a href="#cb24-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, theta <span class="kw">in</span> <span class="bu">enumerate</span>(angles):</span>
<span id="cb24-16"><a href="#cb24-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Create rotation matrix</span></span>
<span id="cb24-17"><a href="#cb24-17" aria-hidden="true" tabindex="-1"></a>        rotation <span class="op">=</span> np.array([</span>
<span id="cb24-18"><a href="#cb24-18" aria-hidden="true" tabindex="-1"></a>            [np.cos(theta), <span class="op">-</span>np.sin(theta)],</span>
<span id="cb24-19"><a href="#cb24-19" aria-hidden="true" tabindex="-1"></a>            [np.sin(theta), np.cos(theta)]</span>
<span id="cb24-20"><a href="#cb24-20" aria-hidden="true" tabindex="-1"></a>        ])</span>
<span id="cb24-21"><a href="#cb24-21" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb24-22"><a href="#cb24-22" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Rotate the vector</span></span>
<span id="cb24-23"><a href="#cb24-23" aria-hidden="true" tabindex="-1"></a>        rotated <span class="op">=</span> rotation <span class="op">@</span> vector</span>
<span id="cb24-24"><a href="#cb24-24" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb24-25"><a href="#cb24-25" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Plot the rotated vector</span></span>
<span id="cb24-26"><a href="#cb24-26" aria-hidden="true" tabindex="-1"></a>        plt.arrow(<span class="dv">0</span>, <span class="dv">0</span>, rotated[<span class="dv">0</span>], rotated[<span class="dv">1</span>], </span>
<span id="cb24-27"><a href="#cb24-27" aria-hidden="true" tabindex="-1"></a>                 head_width<span class="op">=</span><span class="fl">0.05</span>, head_length<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb24-28"><a href="#cb24-28" aria-hidden="true" tabindex="-1"></a>        plt.text(rotated[<span class="dv">0</span>], rotated[<span class="dv">1</span>], <span class="ss">f'pos </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb24-29"><a href="#cb24-29" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb24-30"><a href="#cb24-30" aria-hidden="true" tabindex="-1"></a>    plt.grid(<span class="va">True</span>)</span>
<span id="cb24-31"><a href="#cb24-31" aria-hidden="true" tabindex="-1"></a>    plt.axis(<span class="st">'equal'</span>)</span>
<span id="cb24-32"><a href="#cb24-32" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="st">'RoPE: Position-dependent Vector Rotation'</span>)</span>
<span id="cb24-33"><a href="#cb24-33" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb24-34"><a href="#cb24-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-35"><a href="#cb24-35" aria-hidden="true" tabindex="-1"></a>visualize_rope_rotation_simple()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="09_La evolución del transformer_files/figure-html/cell-17-output-1.svg" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>La ventaja de este método es que el cálculo de distancias relativas es sencillo (diferencia de ángulos de rotación entre dos posiciones) y no hay límite en la longitud de la secuencia. Además, es posible procesar secuencias más largas que las aprendidas.</p>
<p><strong>Principales cambios en <code>efficient_encoder_rope.py</code></strong></p>
<ol type="1">
<li><p><strong>Clase <code>Embeddings</code>:</strong> Se elimina <code>position_embeddings</code>, y se suprime el proceso de agregar incrustaciones de posición en <code>forward()</code>. No es necesario un incrustación de posición separada, ya que RoPE maneja la información de posición.</p></li>
<li><p><strong>Función <code>rotate_half</code>:</strong> Es la parte central de la operación RoPE.</p></li>
</ol>
<div id="cell-43" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>    <span class="co"># (efficient_encoder_rope.py)</span></span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> rotate_half(x):</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Rotates half the hidden dims of the input."""</span></span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>        x1 <span class="op">=</span> x[..., :x.shape[<span class="op">-</span><span class="dv">1</span>] <span class="op">//</span> <span class="dv">2</span>]</span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>        x2 <span class="op">=</span> x[..., x.shape[<span class="op">-</span><span class="dv">1</span>] <span class="op">//</span> <span class="dv">2</span>:]</span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.cat((<span class="op">-</span>x2, x1), dim<span class="op">=-</span><span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<ol start="3" type="1">
<li><strong><code>apply_rotary_pos_emb</code> función:</strong> aplica RoPE a la consulta (q) y la clave (k).</li>
</ol>
<div id="cell-45" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>    <span class="co"># (efficient_encoder_rope.py)</span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> apply_rotary_pos_emb(q, k, cos, sin):</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Applies rotary position embeddings to query and key tensors."""</span></span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>        q_embed <span class="op">=</span> (q <span class="op">*</span> cos) <span class="op">+</span> (rotate_half(q) <span class="op">*</span> sin)</span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>        k_embed <span class="op">=</span> (k <span class="op">*</span> cos) <span class="op">+</span> (rotate_half(k) <span class="op">*</span> sin)</span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> q_embed, k_embed</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<ol start="4" type="1">
<li><strong><code>FlashAttention</code> clase:</strong></li>
</ol>
<ul>
<li><code>cos_cached</code>, <code>sin_cached</code>: Almacena (en caché) los valores de coseno y seno utilizados en RoPE, calculados previamente. Se generan en <code>_build_cache()</code>.</li>
<li><code>_build_cache()</code>: Calcula previamente los valores de las funciones trigonométricas necesarios para RoPE.</li>
<li><code>forward()</code>: Después de aplicar la transformación lineal a las consultas y claves, llama a <code>apply_rotary_pos_emb()</code> para aplicar RoPE.</li>
</ul>
<div id="cell-47" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> typing <span class="im">import</span> Optional</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> apply_rotary_pos_emb(q, k, cos, sin):</span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Applies Rotary Position Embeddings to query and key tensors."""</span></span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a>    q_embed <span class="op">=</span> (q <span class="op">*</span> cos) <span class="op">+</span> (rotate_half(q) <span class="op">*</span> sin)</span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a>    k_embed <span class="op">=</span> (k <span class="op">*</span> cos) <span class="op">+</span> (rotate_half(k) <span class="op">*</span> sin)</span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> q_embed, k_embed</span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> rotate_half(x):</span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Rotates half the hidden dims of the input."""</span></span>
<span id="cb27-13"><a href="#cb27-13" aria-hidden="true" tabindex="-1"></a>    x1 <span class="op">=</span> x[..., : x.shape[<span class="op">-</span><span class="dv">1</span>] <span class="op">//</span> <span class="dv">2</span>]</span>
<span id="cb27-14"><a href="#cb27-14" aria-hidden="true" tabindex="-1"></a>    x2 <span class="op">=</span> x[..., x.shape[<span class="op">-</span><span class="dv">1</span>] <span class="op">//</span> <span class="dv">2</span> :]</span>
<span id="cb27-15"><a href="#cb27-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> torch.cat((<span class="op">-</span>x2, x1), dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb27-16"><a href="#cb27-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-17"><a href="#cb27-17" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> FlashAttention(nn.Module):</span>
<span id="cb27-18"><a href="#cb27-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ... (rest of the class definition, unchanged) ...</span></span>
<span id="cb27-19"><a href="#cb27-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-20"><a href="#cb27-20" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _build_cache(<span class="va">self</span>, device, dtype):</span>
<span id="cb27-21"><a href="#cb27-21" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.cos_cached <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span> <span class="kw">and</span> <span class="va">self</span>.cos_cached.dtype <span class="op">==</span> dtype: <span class="co">#Return if cache already exist.</span></span>
<span id="cb27-22"><a href="#cb27-22" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span></span>
<span id="cb27-23"><a href="#cb27-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-24"><a href="#cb27-24" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Create position indices</span></span>
<span id="cb27-25"><a href="#cb27-25" aria-hidden="true" tabindex="-1"></a>        pos_seq <span class="op">=</span> torch.arange(<span class="va">self</span>.max_position_embeddings, device<span class="op">=</span>device, dtype<span class="op">=</span>dtype)</span>
<span id="cb27-26"><a href="#cb27-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-27"><a href="#cb27-27" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Create freqs (theta in paper)</span></span>
<span id="cb27-28"><a href="#cb27-28" aria-hidden="true" tabindex="-1"></a>        inv_freq <span class="op">=</span> <span class="fl">1.0</span> <span class="op">/</span> (<span class="dv">10000</span> <span class="op">**</span> (torch.arange(<span class="dv">0</span>, <span class="va">self</span>.attention_head_size, <span class="dv">2</span>, device<span class="op">=</span>device, dtype<span class="op">=</span>dtype) <span class="op">/</span> <span class="va">self</span>.attention_head_size))</span>
<span id="cb27-29"><a href="#cb27-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-30"><a href="#cb27-30" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Create freqs for each position in sequence.</span></span>
<span id="cb27-31"><a href="#cb27-31" aria-hidden="true" tabindex="-1"></a>        freqs <span class="op">=</span> torch.einsum(<span class="st">"i,j-&gt;ij"</span>, pos_seq, inv_freq)</span>
<span id="cb27-32"><a href="#cb27-32" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Expand the shape for later element-wise calculations</span></span>
<span id="cb27-33"><a href="#cb27-33" aria-hidden="true" tabindex="-1"></a>        emb <span class="op">=</span> torch.cat((freqs, freqs), dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb27-34"><a href="#cb27-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-35"><a href="#cb27-35" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Create the cos and sin cache</span></span>
<span id="cb27-36"><a href="#cb27-36" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.cos_cached <span class="op">=</span> emb.cos()[<span class="va">None</span>, <span class="va">None</span>, :, :]  <span class="co"># Add head and batch dimensions</span></span>
<span id="cb27-37"><a href="#cb27-37" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.sin_cached <span class="op">=</span> emb.sin()[<span class="va">None</span>, <span class="va">None</span>, :, :]</span>
<span id="cb27-38"><a href="#cb27-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-39"><a href="#cb27-39" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, hidden_states: torch.Tensor, attention_mask: Optional[torch.Tensor] <span class="op">=</span> <span class="va">None</span>) <span class="op">-&gt;</span> torch.Tensor:</span>
<span id="cb27-40"><a href="#cb27-40" aria-hidden="true" tabindex="-1"></a>        <span class="co"># ... (rest of the forward method, unchanged) ...</span></span>
<span id="cb27-41"><a href="#cb27-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-42"><a href="#cb27-42" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Apply RoPE</span></span>
<span id="cb27-43"><a href="#cb27-43" aria-hidden="true" tabindex="-1"></a>        batch_size, num_heads, seq_len, head_dim <span class="op">=</span> query_layer.shape</span>
<span id="cb27-44"><a href="#cb27-44" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._build_cache(query_layer.device, query_layer.dtype)</span>
<span id="cb27-45"><a href="#cb27-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-46"><a href="#cb27-46" aria-hidden="true" tabindex="-1"></a>        cos <span class="op">=</span> <span class="va">self</span>.cos_cached[:, :, :seq_len, :head_dim]</span>
<span id="cb27-47"><a href="#cb27-47" aria-hidden="true" tabindex="-1"></a>        sin <span class="op">=</span> <span class="va">self</span>.sin_cached[:, :, :seq_len, :head_dim]</span>
<span id="cb27-48"><a href="#cb27-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-49"><a href="#cb27-49" aria-hidden="true" tabindex="-1"></a>        query_layer, key_layer <span class="op">=</span> apply_rotary_pos_emb(query_layer, key_layer, cos, sin)</span>
<span id="cb27-50"><a href="#cb27-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-51"><a href="#cb27-51" aria-hidden="true" tabindex="-1"></a>        <span class="co"># ... (rest of the forward method, unchanged) ...</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="resultados-experimentales-clasificación-de-texto-ag-news" class="level3">
<h3 class="anchored" data-anchor-id="resultados-experimentales-clasificación-de-texto-ag-news">9.5.4 Resultados experimentales: Clasificación de texto AG News</h3>
<p>Utilizamos dos versiones del codificador eficiente (<code>efficient_encoder_rope.py</code> y <code>efficient_encoder.py</code>) para realizar experimentos de clasificación de texto en el conjunto de datos AG News (que clasifica artículos de noticias en 4 categorías). El código para ejecutar el entrenamiento es <code>train_ag_news.py</code>.</p>
<p>El conjunto de datos AG News está compuesto por artículos de noticias equilibrados en cada categoría. Cada artículo se limita a una longitud máxima de 128 tokens, y realizamos un entrenamiento comparativo utilizando dos tokenizadores: BERT y T5. Clasificamos los textos de noticias en las categorías World, Sports, Business, Sci/Tech. La escala del modelo se configuró muy pequeña, como sigue.</p>
<div id="cell-49" class="cell" data-execution_count="1">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>vocab_size: <span class="bu">int</span> <span class="op">=</span> <span class="dv">30522</span>,</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>hidden_size: <span class="bu">int</span> <span class="op">=</span> <span class="dv">256</span>,</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>num_hidden_layers: <span class="bu">int</span> <span class="op">=</span> <span class="dv">4</span>,</span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>num_attention_heads: <span class="bu">int</span> <span class="op">=</span> <span class="dv">8</span>,</span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a>intermediate_size: <span class="bu">int</span> <span class="op">=</span> <span class="dv">512</span>,</span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a>hidden_dropout_prob: <span class="bu">float</span> <span class="op">=</span> <span class="fl">0.1</span>,</span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a>attention_probs_dropout_prob: <span class="bu">float</span> <span class="op">=</span> <span class="fl">0.1</span>,</span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a>max_position_embeddings: <span class="bu">int</span> <span class="op">=</span> <span class="dv">512</span>,</span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a>layer_norm_eps: <span class="bu">float</span> <span class="op">=</span> <span class="fl">1e-12</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Lo siguiente es la sección de ejecución para realizar experimentos de comparación.</p>
<div id="cell-51" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_09.encoder.train_ag_news <span class="im">import</span> train_and_test_all_versions</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>train_and_test_all_versions(verbose<span class="op">=</span><span class="va">False</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><strong>Resultados de entrenamiento</strong></p>
<table class="caption-top table">
<colgroup>
<col style="width: 12%">
<col style="width: 28%">
<col style="width: 21%">
<col style="width: 37%">
</colgroup>
<thead>
<tr class="header">
<th>Versión del modelo</th>
<th>Tokenizador</th>
<th>Precisión en prueba (%)</th>
<th>Notas</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>v1</td>
<td>bert-base-uncased</td>
<td>91.24</td>
<td>FlashAttention</td>
</tr>
<tr class="even">
<td>v1</td>
<td>t5-small</td>
<td>92.00</td>
<td>FlashAttention</td>
</tr>
<tr class="odd">
<td>v2</td>
<td>bert-base-uncased</td>
<td>92.57</td>
<td>RoPE, FlashAttention</td>
</tr>
<tr class="even">
<td>v2</td>
<td>t5-small</td>
<td>92.07</td>
<td>RoPE, FlashAttention</td>
</tr>
</tbody>
</table>
<ul>
<li><strong>v1</strong>: <code>efficient_encoder.py</code> (sin RoPE)</li>
<li><strong>v2</strong>: <code>efficient_encoder_rope.py</code> (con RoPE)</li>
</ul>
<p><strong>Interpretación de resultados</strong></p>
<ol type="1">
<li><p><strong>Efecto de RoPE (v2):</strong> Al usar el tokenizador <code>bert-base-uncased</code>, el modelo v2 con RoPE mostró una precisión 1.33%p mayor que el modelo v1. Esto sugiere que RoPE codifica la información posicional de manera más efectiva, mejorando el rendimiento del modelo. En particular, cuando se deben procesar secuencias más largas que los datos de entrenamiento (extrapolación de longitud), las ventajas de RoPE pueden ser más notables.</p></li>
<li><p><strong>Influencia del tokenizador:</strong> Al usar el tokenizador <code>t5-small</code>, ambas versiones mostraron un nivel similar de precisión al utilizar <code>bert-base-uncased</code>. Sin embargo, v2 presentó un rendimiento ligeramente mejor.</p></li>
<li><p><strong>Rendimiento alto en general:</strong> Ambas versiones lograron una alta precisión superior al 91% en el conjunto de datos AG News. Esto indica que la arquitectura del modelo es efectiva y que se han aplicado bien técnicas modernas de entrenamiento de Transformers, como el uso de <code>F.scaled_dot_product_attention</code> para FlashAttention (cuando el entorno lo soporta), Pre-LN, GELU, inicialización Xavier, AdamW y programación de tasa de aprendizaje.</p></li>
</ol>
<p><strong>Comparación con modelos similares (tabla)</strong></p>
<p>La tabla siguiente compara el rendimiento del modelo con otros modelos de tamaño similar en el conjunto de datos AG News. (La precisión puede variar según la literatura y los resultados experimentales.)</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Modelo</th>
<th>Precisión (%)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Nuestro modelo v1</td>
<td>91.24</td>
</tr>
<tr class="even">
<td>Nuestro modelo v2</td>
<td>92.57</td>
</tr>
<tr class="odd">
<td>DistilBERT</td>
<td>90.36</td>
</tr>
<tr class="even">
<td>BERT-base</td>
<td>91.10</td>
</tr>
<tr class="odd">
<td>RoBERTa-base</td>
<td>92.84</td>
</tr>
<tr class="even">
<td>modelo</td>
<td>hidden_size</td>
</tr>
<tr class="odd">
<td>————————————</td>
<td>———-</td>
</tr>
<tr class="even">
<td><strong>Efficient Encoder (v2, bert)</strong></td>
<td>256</td>
</tr>
<tr class="odd">
<td><strong>Efficient Encoder (v2, t5)</strong></td>
<td>256</td>
</tr>
<tr class="even">
<td><strong>Efficient Encoder (v1, bert)</strong></td>
<td>256</td>
</tr>
<tr class="odd">
<td><strong>Efficient Encoder (v1, t5)</strong></td>
<td>256</td>
</tr>
<tr class="even">
<td>TinyBERT (4 capas, hidden_size=312)</td>
<td>312</td>
</tr>
<tr class="odd">
<td>BERT-small</td>
<td>512</td>
</tr>
<tr class="even">
<td>DistilBERT-base</td>
<td>768</td>
</tr>
<tr class="odd">
<td>BERT-base</td>
<td>768</td>
</tr>
</tbody>
</table>
<p><strong>mecanismos aplicados</strong> | Mecanismo | v1 (<code>efficient_encoder.py</code>) | v2 (<code>efficient_encoder_rope.py</code>) | Nota | | ———————— | ———————- | ——————- | —————————— | | FlashAttention | O | O | Optimización utilizando la estructura de memoria de GPU | | Pre-LN | O | O | Aplicación de Layer Normalization antes de la atención/FFN | | RoPE | X | O | Codificación de información posicional mediante matrices rotacionales | | Incrustaciones de posición aprendibles | O | X | Representación de información posicional cuando no se usa RoPE | | Inicialización Xavier | O | O | Método de inicialización de pesos | | Función de activación GELU | O | O | Función de activación no lineal (usada en FFN) | | Dropout | O | O | Mejora del rendimiento de generalización | | Normalización por capa | O | O | Estabilización y mejora del rendimiento durante el aprendizaje | | Uso de tokenizador preentrenado | O | O | BERT-base-uncased, t5-small utilizado |</p>
<p><strong>Conclusión</strong></p>
<p>En esta sección, diseñamos un modelo de codificador Transformer (v2) más eficiente aplicando FlashAttention utilizando <code>F.scaled_dot_product_attention</code> de PyTorch y RoPE (Rotary Positional Embeddings) para aumentar la eficiencia. Entrenamos y probamos los modelos v1 (codificador Transformer básico) y v2 (con RoPE aplicado) con el conjunto de datos AG News para clasificación de texto, utilizando <code>bert-base-uncased</code> y <code>t5-small</code> como tokenizadores. Los resultados mostraron que el modelo v2 logró una precisión más alta (92.57%) con el tokenizador <code>bert-base-uncased</code>. Esto sugiere que RoPE codifica eficazmente la información de posición relativa, mejorando el rendimiento del modelo, especialmente en el procesamiento de textos largos. Ambos modelos lograron una alta precisión del 91-92%, lo que demuestra que la arquitectura Efficient Encoder es eficiente y puede generar un rendimiento potente. Además, al comparar los tokenizadores <code>bert-base-uncased</code> y <code>t5-small</code>, hubo una ligera diferencia, con v2 utilizando <code>bert-base-uncased</code> logrando un mejor rendimiento.</p>
<p>Como se puede ver en la tabla, el modelo Efficient Encoder propuesto supera en rendimiento a modelos más pequeños como TinyBERT y alcanza un rendimiento competitivo en comparación con BERT-small. Es importante destacar que logra un rendimiento cercano al de modelos más grandes como DistilBERT-base o BERT-base, pero con un tamaño mucho menor. Esto se puede atribuir a la combinación de tokenizadores preentrenados, FlashAttention, estructura Pre-LN, RoPE, inicialización Xavier, función de activación GELU y una configuración adecuada del modelo (hidden_size, num_hidden_layers, etc.).</p>
<p>En conclusión, el Efficient Encoder (v2) presentado en este capítulo no solo es útil para comprender los componentes clave del Transformer con fines educativos, sino que también ha demostrado ser un modelo eficiente capaz de generar un rendimiento competitivo en aplicaciones prácticas. En particular, la aplicación de RoPE se ha confirmado como un método efectivo para mejorar el rendimiento del modelo.</p>
</section>
</section>
<section id="mistral-implementación-y-análisis-de-una-arquitectura-de-decodificador-eficiente" class="level2">
<h2 class="anchored" data-anchor-id="mistral-implementación-y-análisis-de-una-arquitectura-de-decodificador-eficiente">9.6 Mistral: Implementación y análisis de una arquitectura de decodificador eficiente</h2>
<p>El modelo Mistral-7B publicado por Mistral AI en 2023 se basa en la arquitectura LLaMA e introduce <strong>atención de consulta agrupada (GQA)</strong> y <strong>atención de ventana deslizante (SWA)</strong>, lo que mejora significativamente la eficiencia de memoria y la velocidad de procesamiento. En particular, con solo 7B de parámetros, muestra un rendimiento comparable al de modelos con más de 13B de parámetros, demostrando la importancia del diseño de una arquitectura eficiente.</p>
<p>En esta sección, implementamos y analizamos un modelo simplificado de Mistral basándonos en la implementación de Hugging Face Transformers, centrándonos en los elementos clave de optimización. Examinaremos detenidamente GQA, SWA, RoPE y el mecanismo de caché KV, y entenderemos cómo contribuyen a la eficiencia y rendimiento del modelo. El código está disponible en <code>chapter_09/mistral</code>.</p>
<section id="arquitectura-del-modelo-simple_mistral-análisis-detallado-de-los-componentes" class="level3">
<h3 class="anchored" data-anchor-id="arquitectura-del-modelo-simple_mistral-análisis-detallado-de-los-componentes">9.6.1 Arquitectura del modelo <code>simple_mistral</code>: Análisis detallado de los componentes</h3>
<p>El modelo <code>simple_mistral</code> implementa una versión simplificada de los componentes principales del modelo Mistral-7B, donde cada componente está modularizado y realiza funciones claras. A continuación, analizaremos en detalle cada uno de estos componentes.</p>
<section id="mistralconfig-configuración-del-modelo" class="level4">
<h4 class="anchored" data-anchor-id="mistralconfig-configuración-del-modelo">1. MistralConfig: Configuración del modelo</h4>
<p>La clase <code>MistralConfig</code> define los hiperparámetros del modelo, lo cual es un papel crucial para determinar la estructura y el comportamiento del modelo.</p>
<ul>
<li><strong>Propiedades principales:</strong>
<ul>
<li>vocab_size: Especifica el tamaño del diccionario de vocabulario (valor predeterminado: 32000).</li>
<li>hidden_size: Representa la dimensión de los embeddings y estados ocultos (valor predeterminado: 4096).</li>
<li>intermediate_size: Define la dimensión intermedia de la red FeedForward (valor predeterminado: 14336).</li>
<li>num_hidden_layers: Especifica el número de capas del decodificador Transformer (valor predeterminado: 32).</li>
<li>num_attention_heads: Indica el número de cabezas de atención (valor predeterminado: 32).</li>
<li>num_key_value_heads: Define el número de cabezas clave/valor utilizadas en GQA (valor predeterminado: 8).</li>
<li>hidden_act: Función de activación, se usa “silu” (predeterminada).</li>
<li>max_position_embeddings: Especifica la longitud máxima de secuencia (valor predeterminado: 4096 * 32).</li>
<li>rms_norm_eps: Representa el valor epsilon de RMSNorm (valor predeterminado: 1e-6).</li>
<li>use_cache: Determina si se utiliza caché KV (valor predeterminado: True).</li>
<li>rope_theta: Establece el valor theta de RoPE (valor predeterminado: 10000.0).</li>
<li>sliding_window: Especifica el tamaño de la ventana deslizante (valor predeterminado: 4096).</li>
<li>use_return_dict: Configura si se devuelve un diccionario (valor predeterminado: True).</li>
</ul></li>
</ul>
</section>
<section id="mistralrmsnorm-normalización-rms" class="level4">
<h4 class="anchored" data-anchor-id="mistralrmsnorm-normalización-rms">2. MistralRMSNorm: Normalización RMS</h4>
<p>La clase <code>MistralRMSNorm</code> implementa la normalización RMS (Root Mean Square Layer Normalization). Mejora la eficiencia computacional al eliminar el promedio y normalizar mediante la raíz cuadrada de la media de los cuadrados (RMS) en lugar de usar LayerNorm tradicional.</p>
<ul>
<li><strong>Características:</strong> Utiliza <code>variance_epsilon</code> para asegurar estabilidad numérica.</li>
</ul>
</section>
<section id="mistralattention-mecanismo-de-atención" class="level4">
<h4 class="anchored" data-anchor-id="mistralattention-mecanismo-de-atención">3. MistralAttention: Mecanismo de atención</h4>
<p>La clase <code>MistralAttention</code> implementa el mecanismo de atención central del modelo Mistral, integrando GQA, SWA y RoPE para mejorar la eficiencia y el rendimiento. * <strong>GQA (Grouped-Query Attention):</strong> * Se mantienen múltiples cabezas de consulta (Q) y se configuran menos cabezas para clave (K) y valor (V) para reducir el uso de memoria y la cantidad de cálculos. * Se ajusta el número de cabezas K/V a través de <code>num_key_value_heads</code>. * Se replica el tensor K/V hasta coincidir con el número de cabezas Q utilizando la función <code>repeat_kv</code>.</p>
<ul>
<li><strong>SWA (Sliding Window Attention):</strong>
<ul>
<li>Se reduce la complejidad computacional al permitir que cada token solo realice atención sobre tokens dentro de una ventana limitada.</li>
<li>Se ajusta el tamaño de la ventana a través del parámetro <code>sliding_window</code>.</li>
<li>Se modifica el <code>attention_mask</code> para bloquear la atención con tokens fuera de la ventana.</li>
</ul></li>
<li><strong>RoPE (Rotary Positional Embedding):</strong>
<ul>
<li>Se codifican las posiciones utilizando matrices de rotación.</li>
<li>Se implementa a través de la clase <code>MistralRotaryEmbedding</code>.</li>
<li>Se aplica RoPE a las consultas y claves usando la función <code>apply_rotary_pos_emb</code>.</li>
</ul></li>
</ul>
</section>
<section id="mistralrotaryembedding-implementación-de-rope" class="level4">
<h4 class="anchored" data-anchor-id="mistralrotaryembedding-implementación-de-rope">4. MistralRotaryEmbedding: Implementación de RoPE</h4>
<p>La clase <code>MistralRotaryEmbedding</code> implementa RoPE (Rotary Positional Embedding).</p>
<ul>
<li><strong>Método <code>__init__</code>:</strong>
<ul>
<li>dim: se establece la dimensión del embedding.</li>
<li>max_position_embeddings: se especifica la longitud máxima de secuencia.</li>
<li>base: se define una constante para el cálculo de frecuencias (valor predeterminado: 10000).</li>
<li>inv_freq: se calcula la frecuencia inversa y se registra como un parámetro no entrenable.</li>
<li>cos_cached, sin_cached: se almacenan en caché los valores precalculados de coseno y seno.</li>
</ul></li>
<li><strong>Método <code>forward</code>:</strong>
<ul>
<li>Recibe el tensor de entrada <code>x</code> y la longitud de secuencia <code>seq_len</code>.</li>
<li>Si <code>seq_len</code> es mayor que la longitud máxima almacenada en caché, se llama a <code>_set_cos_sin_cache</code> para actualizar la caché.</li>
<li>Devuelve los valores almacenados en caché de coseno y seno.</li>
</ul></li>
<li><strong>Método <code>_set_cos_sin_cache</code>:</strong>
<ul>
<li>Se generan índices de posición hasta <code>seq_len</code>.</li>
<li>Se calcula la frecuencia multiplicando los índices de posición por la frecuencia inversa.</li>
<li>Se calculan y almacenan en caché los valores de coseno y seno utilizando las frecuencias calculadas.</li>
</ul></li>
</ul>
</section>
<section id="mistralmlp-red-feedforward" class="level4">
<h4 class="anchored" data-anchor-id="mistralmlp-red-feedforward">5. MistralMLP: Red FeedForward</h4>
<p>La clase <code>MistralMLP</code> implementa la red FeedForward del modelo Mistral.</p>
<ul>
<li><strong>Composición:</strong>
<ul>
<li><code>gate_proj</code>, <code>up_proj</code>, <code>down_proj</code>: tres capas lineales se utilizan para expandir y luego reducir de nuevo la entrada.</li>
<li><code>act_fn</code>: Se utiliza la función de activación SiLU (Sigmoid Linear Unit).</li>
</ul></li>
</ul>
</section>
<section id="mistraldecoderlayer-capa-de-decodificador" class="level4">
<h4 class="anchored" data-anchor-id="mistraldecoderlayer-capa-de-decodificador">6. MistralDecoderLayer: Capa de decodificador</h4>
<p>La clase <code>MistralDecoderLayer</code> compone una capa de decodificador del modelo Mistral.</p>
<ul>
<li><strong>Componentes:</strong>
<ul>
<li><code>self_attn</code>: utiliza el módulo <code>MistralAttention</code> para realizar self-attention.</li>
<li><code>mlp</code>: utiliza el módulo <code>MistralMLP</code> para realizar la red FeedForward.</li>
<li><code>input_layernorm</code>, <code>post_attention_layernorm</code>: utilizan <code>MistralRMSNorm</code> para normalizar las entradas/salidas.</li>
</ul></li>
</ul>
</section>
<section id="mistralpretrainedmodel-clase-abstracta-de-modelo-preentrenado" class="level4">
<h4 class="anchored" data-anchor-id="mistralpretrainedmodel-clase-abstracta-de-modelo-preentrenado">7. MistralPreTrainedModel: Clase abstracta de modelo preentrenado</h4>
<p>La clase <code>MistralPreTrainedModel</code> es una clase abstracta base que gestiona la inicialización y configuración de los pesos del modelo Mistral.</p>
<ul>
<li><strong>Métodos principales:</strong>
<ul>
<li><code>_init_weights</code>: Inicializa los pesos.</li>
<li><code>_set_gradient_checkpointing</code>: Configura si el checkpointing de gradientes está activado o no.</li>
</ul></li>
</ul>
</section>
<section id="mistralmodel-modelo-mistral" class="level4">
<h4 class="anchored" data-anchor-id="mistralmodel-modelo-mistral">8. MistralModel: Modelo Mistral</h4>
<p>La clase <code>MistralModel</code> define la estructura completa del modelo Mistral.</p>
<ul>
<li><strong>Componentes:</strong>
<ul>
<li><code>embed_tokens</code>: Convierte los tokens de entrada en vectores de embeddings.</li>
<li><code>layers</code>: Compone varias capas de <code>MistralDecoderLayer</code>.</li>
<li><code>norm</code>: Normaliza la salida de la última capa.</li>
</ul></li>
</ul>
</section>
<section id="mistralforcausallm-mistral-para-modelado-de-lenguaje" class="level4">
<h4 class="anchored" data-anchor-id="mistralforcausallm-mistral-para-modelado-de-lenguaje">9. MistralForCausalLM: Mistral para modelado de lenguaje</h4>
<p>La clase <code>MistralForCausalLM</code> es una clase diseñada para ajustar el modelo Mistral a tareas de modelado de lenguaje causal (Causal Language Modeling).</p>
<ul>
<li><strong>Componentes principales:</strong>
<ul>
<li><code>lm_head</code>: Proyecta la salida del modelo al tamaño del vocabulario para calcular las probabilidades de predicción del siguiente token.</li>
<li><code>prepare_inputs_for_generation</code>: Prepara las entradas durante el proceso de inferencia.</li>
<li><code>_reorder_cache</code>: Reordena el caché KV durante la búsqueda por haz (beam search).</li>
</ul></li>
</ul>
<hr>
<p>De esta manera, el modelo <code>simple_mistral</code> proporciona un diseño eficiente y flexible al modularizar cada componente. Entender el rol e interacción de cada componente permite comprender mejor los principios de funcionamiento del modelo.</p>
</section>
</section>
<section id="análisis-de-elementos-técnicos-clave-el-secreto-de-la-eficiencia-y-el-rendimiento" class="level3">
<h3 class="anchored" data-anchor-id="análisis-de-elementos-técnicos-clave-el-secreto-de-la-eficiencia-y-el-rendimiento">9.6.2 Análisis de elementos técnicos clave: El secreto de la eficiencia y el rendimiento</h3>
<p>El modelo <code>simple_mistral</code> maximiza su eficiencia y rendimiento mediante elementos técnicos clave como GQA, SWA y RoPE. Analizaremos detalladamente cómo funcionan estos elementos y sus ventajas.</p>
<section id="gqa-grouped-query-attention-innovación-para-la-eficiencia-de-memoria-y-cálculo" class="level4">
<h4 class="anchored" data-anchor-id="gqa-grouped-query-attention-innovación-para-la-eficiencia-de-memoria-y-cálculo">1. GQA (Grouped-Query Attention): Innovación para la eficiencia de memoria y cálculo</h4>
<p>GQA es una variante del Multi-Head Attention que reduce el uso de memoria y la cantidad de cálculos mientras mantiene el rendimiento.</p>
<ul>
<li><strong>Funcionamiento:</strong>
<ul>
<li>Las consultas (Q) se dividen en múltiples cabezas, pero las claves (K) y valores (V) se dividen en un número menor de cabezas.</li>
<li>Cada cabeza Q se asigna a un grupo específico de cabezas K/V.</li>
<li>Cada cabeza Q solo calcula la atención para el grupo de cabezas K/V al que está asignada.</li>
<li>La función <code>repeat_kv</code> replica los tensores K/V para ajustarlos al número de cabezas Q, implementando este mecanismo.</li>
</ul></li>
<li><strong>Ventajas:</strong>
<ul>
<li><strong>Reducción del uso de memoria:</strong> El tamaño de los tensores K/V se reduce, lo que permite reducir el tamaño del caché KV.</li>
<li><strong>Reducción de la cantidad de cálculos:</strong> La cantidad de cálculos de atención se reduce, mejorando la velocidad de inferencia.</li>
<li><strong>Mantenimiento del rendimiento:</strong> El número de cabezas Q permanece igual, por lo que la capacidad expresiva del modelo no disminuye significativamente.</li>
</ul></li>
</ul>
</section>
<section id="swa-sliding-window-attention-estrategia-eficiente-para-el-procesamiento-de-secuencias-largas" class="level4">
<h4 class="anchored" data-anchor-id="swa-sliding-window-attention-estrategia-eficiente-para-el-procesamiento-de-secuencias-largas">2. SWA (Sliding Window Attention): Estrategia eficiente para el procesamiento de secuencias largas</h4>
<p>SWA es una técnica que reduce la complejidad computacional al permitir que cada token solo realice la atención dentro de un rango limitado (ventana).</p>
<ul>
<li><strong>Funcionamiento:</strong>
<ul>
<li>Cada token realiza la atención solo en los tokens dentro de una ventana de tamaño fijo.</li>
<li>La ventana se desplaza a lo largo de la secuencia, calculando la atención en cada posición.</li>
<li>Se utiliza un <code>attention_mask</code> para enmascarar la atención con tokens fuera de la ventana.</li>
</ul></li>
<li><strong>Ventajas:</strong>
<ul>
<li><strong>Reducción de la complejidad computacional:</strong> La cantidad de cálculos de atención se reduce de O(N²) a O(N*W). (N: longitud de la secuencia, W: tamaño de la ventana)</li>
<li><strong>Procesamiento de secuencias largas:</strong> Se puede procesar secuencias más largas debido a una menor utilización de memoria.</li>
</ul></li>
</ul>
</section>
<section id="rope-rotary-positional-embedding-codificación-eficiente-de-información-de-posición-relativa" class="level4">
<h4 class="anchored" data-anchor-id="rope-rotary-positional-embedding-codificación-eficiente-de-información-de-posición-relativa">3. RoPE (Rotary Positional Embedding): codificación eficiente de información de posición relativa</h4>
<p>RoPE ya se revisó en el capítulo 9.5. Aquí solo revisaremos brevemente las partes implementadas en el modelo.</p>
<ul>
<li><strong>Implementación:</strong>
<ul>
<li><strong>Función <code>rotate_half</code>:</strong> divide la dimensión del tensor de entrada a la mitad y alterna los signos para simular el efecto de una multiplicación compleja.</li>
</ul></li>
</ul>
<div id="cell-54" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> rotate_half(x):</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>    x1 <span class="op">=</span> x[..., : x.shape[<span class="op">-</span><span class="dv">1</span>] <span class="op">//</span> <span class="dv">2</span>]</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>    x2 <span class="op">=</span> x[..., x.shape[<span class="op">-</span><span class="dv">1</span>] <span class="op">//</span> <span class="dv">2</span> :]</span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> torch.cat((<span class="op">-</span>x2, x1), dim<span class="op">=-</span><span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<ul>
<li><strong><code>apply_rotary_pos_emb</code> función:</strong> aplica RoPE a los tensores de consulta (q) y clave (k).</li>
</ul>
<div id="cell-56" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> apply_rotary_pos_emb(q, k, cos, sin, position_ids_q, position_ids_k<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>    cos <span class="op">=</span> cos.squeeze(<span class="dv">1</span>).squeeze(<span class="dv">0</span>)  <span class="co"># [seq_len, dim]</span></span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>    sin <span class="op">=</span> sin.squeeze(<span class="dv">1</span>).squeeze(<span class="dv">0</span>)  <span class="co"># [seq_len, dim]</span></span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>    cos_q <span class="op">=</span> cos[position_ids_q].unsqueeze(<span class="dv">1</span>)  <span class="co"># [batch_size, 1, seq_len, dim]</span></span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a>    sin_q <span class="op">=</span> sin[position_ids_q].unsqueeze(<span class="dv">1</span>)  <span class="co"># [batch_size, 1, seq_len, dim]</span></span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a>    cos_k <span class="op">=</span> cos[position_ids_k].unsqueeze(<span class="dv">1</span>)  <span class="co"># [batch_size, 1, seq_len, dim]</span></span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a>    sin_k <span class="op">=</span> sin[position_ids_k].unsqueeze(<span class="dv">1</span>)  <span class="co"># [batch_size, 1, seq_len, dim]</span></span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a>    q_embed <span class="op">=</span> (q <span class="op">*</span> cos_q) <span class="op">+</span> (rotate_half(q) <span class="op">*</span> sin_q)</span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a>    k_embed <span class="op">=</span> (k <span class="op">*</span> cos_k) <span class="op">+</span> (rotate_half(k) <span class="op">*</span> sin_k)</span>
<span id="cb31-10"><a href="#cb31-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> q_embed, k_embed</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<ul>
<li><strong>Clase <code>MistralRotaryEmbedding</code>:</strong> Calcula y almacena en caché los valores de coseno y seno necesarios para RoPE.
<ul>
<li><code>cos_cached</code>, <code>sin_cached</code>: Valores precalculados de coseno y seno.</li>
<li><code>_set_cos_sin_cache</code>: Actualiza <code>cos_cached</code> y <code>sin_cached</code> según la longitud de la secuencia.</li>
</ul></li>
<li><strong>Ventajas:</strong>
<ul>
<li><strong>Preservación de información de posición relativa:</strong> Los pesos de atención varían naturalmente según las distancias relativas entre los tokens.</li>
<li><strong>Extrapolación de longitud (Length Extrapolation):</strong> Funciona bien incluso con secuencias más largas que la longitud de secuencia de entrenamiento.</li>
<li><strong>Complejidad lineal:</strong> No afecta la complejidad del cálculo de atención.</li>
</ul></li>
</ul>
<p>GQA, SWA y RoPE son elementos técnicos clave que mejoran respectivamente la eficiencia de memoria, la eficiencia computacional y la capacidad de representación de información de posición, lo que eleva el rendimiento general del modelo <code>simple_mistral</code>.</p>
</section>
<section id="caché-kv-eliminación-de-cálculos-redundantes" class="level4">
<h4 class="anchored" data-anchor-id="caché-kv-eliminación-de-cálculos-redundantes">4. Caché KV: Eliminación de cálculos redundantes</h4>
<p>La caché KV es una técnica de optimización crucial para mejorar la velocidad de inferencia en modelos generativos.</p>
<ul>
<li><strong>Concepto:</strong>
<ul>
<li>La caché KV almacena los tensores de clave (K) y valor (V) calculados en cada capa del decodificador durante el proceso de inferencia, para reutilizarlos.</li>
<li>Cada vez que se genera un nuevo token, no es necesario recalcular K y V para los tokens anteriores; se utilizan los valores almacenados en caché para realizar las operaciones.</li>
<li>Se almacena la caché KV de pasos previos a través del parámetro <code>past_key_values</code>, y se activa esta funcionalidad configurando <code>use_cache=True</code>. Cada capa recibe un <code>past_key_value</code> como entrada y produce un <code>present_key_value</code> actualizado.</li>
</ul></li>
<li><strong>Ventajas:</strong>
<ul>
<li><strong>Mejora de la velocidad de inferencia:</strong> Elimina cálculos redundantes, lo que aumenta significativamente la velocidad de generación de tokens.</li>
<li><strong>Aumento del uso de memoria:</strong> Se requiere memoria adicional para almacenar la caché KV, pero esta cantidad puede mitigarse con técnicas como GQA y SWA.</li>
</ul></li>
</ul>
<p>La caché KV es especialmente efectiva al generar texto largo y contribuye significativamente a mejorar la experiencia del usuario.</p>
</section>
</section>
<section id="entrenamiento-del-modelo-guía-de-entrenamiento-para-simple_mistral" class="level3">
<h3 class="anchored" data-anchor-id="entrenamiento-del-modelo-guía-de-entrenamiento-para-simple_mistral">9.6.3 Entrenamiento del modelo: Guía de entrenamiento para <code>simple_mistral</code></h3>
<p>El proceso de entrenar el modelo <code>simple_mistral</code> consta principalmente de dos etapas: preprocesamiento de datos y entrenamiento del modelo.</p>
<section id="preprocesamiento-de-datos-conversión-a-un-formato-comprensible-por-el-modelo" class="level4">
<h4 class="anchored" data-anchor-id="preprocesamiento-de-datos-conversión-a-un-formato-comprensible-por-el-modelo">1. Preprocesamiento de datos: Conversión a un formato comprensible por el modelo</h4>
<p>Este es el proceso de convertir los datos de texto que se utilizarán para el entrenamiento en un formato que el modelo pueda procesar.</p>
<ul>
<li><strong>Tokenización (Tokenization):</strong>
<ul>
<li>Se utiliza un tokenizador (Tokenizer) para convertir los datos de texto en una forma numérica (IDs de tokens) que el modelo pueda procesar.</li>
<li>El tokenizador divide el texto en unidades más pequeñas (tokens) y asigna a cada token un ID único.</li>
</ul></li>
<li><strong>Generación de <code>attention_mask</code>:</strong>
<ul>
<li>El <code>attention_mask</code> distingue los tokens de relleno (padding) y asegura que solo se aplique atención a los datos reales.</li>
<li>Los tokens de relleno son adicionados para ajustar la longitud de la secuencia y deben ser excluidos en el cálculo de atención.</li>
</ul></li>
</ul>
</section>
<section id="entrenamiento-del-modelo-búsqueda-de-parámetros-óptimos" class="level4">
<h4 class="anchored" data-anchor-id="entrenamiento-del-modelo-búsqueda-de-parámetros-óptimos">2. Entrenamiento del modelo: Búsqueda de parámetros óptimos</h4>
<p>Se utiliza el modelo <code>MistralForCausalLM</code> para entrenar el modelo mediante lenguaje modelado causal (Causal Language Modeling). * <strong><code>MistralForCausalLM</code> modelo:</strong> Clase que configura el modelo Mistral para tareas de modelado de lenguaje. * <strong>Función de pérdida (Loss Function):</strong> * Se utiliza <code>CrossEntropyLoss</code> para calcular la diferencia entre las salidas del modelo (predicciones) y las etiquetas correctas. * El modelo se entrena en dirección a minimizar esta pérdida. * <strong>Optimizador (Optimizer):</strong> * Se usa el optimizador <code>AdamW</code> para actualizar los pesos (parámetros) del modelo. * AdamW es una versión mejorada del optimizador Adam, que aplica eficazmente la decadencia de peso (weight decay). * <strong>Programador de tasa de aprendizaje (Learning Rate Scheduler):</strong> * Se utiliza el programador <code>get_cosine_schedule_with_warmup</code> para reducir gradualmente la tasa de aprendizaje. * Al principio del entrenamiento, se aumenta la tasa de aprendizaje para converger rápidamente y en las etapas finales del entrenamiento, se reduce la tasa de aprendizaje para realizar ajustes finos (fine-tuning). * <strong>Recorte de gradientes (Gradient Clipping):</strong> * Se aplica recorte de gradientes para prevenir el problema de los gradientes explosivos. * Si el tamaño del gradiente excede un cierto umbral, se trunca el valor para ayudar a un aprendizaje estable.</p>
</section>
</section>
<section id="generación-de-texto-usando-la-función-generate-creación-de-oraciones-creativas" class="level3">
<h3 class="anchored" data-anchor-id="generación-de-texto-usando-la-función-generate-creación-de-oraciones-creativas">9.6.4 Generación de texto usando la función <code>generate()</code>: Creación de oraciones creativas</h3>
<p>Proceso de generar nuevo texto utilizando un modelo entrenado. La función <code>generate()</code> puede ajustar el estilo y la diversidad del texto generado mediante diversos parámetros.</p>
<section id="función-generate-el-núcleo-de-la-generación-de-texto" class="level4">
<h4 class="anchored" data-anchor-id="función-generate-el-núcleo-de-la-generación-de-texto">Función <code>generate()</code>: El núcleo de la generación de texto</h4>
<ul>
<li><strong>Funcionalidad:</strong> Genera texto basándose en un prompt dado.</li>
<li><strong>Uso de caché KV:</strong> Utiliza <code>past_key_values</code> para aprovechar la caché KV y mejorar la velocidad de inferencia.</li>
<li><strong>Parámetros principales:</strong>
<ul>
<li>max_new_tokens: Especifica el número máximo de tokens a generar.</li>
<li>temperature: Ajusta la forma de la distribución de probabilidad para controlar la diversidad de los resultados generados. (Valores bajos: consistencia, valores altos: diversidad)</li>
<li>top_k: Considera solo los k tokens con mayor probabilidad para muestrear.</li>
<li>top_p: Considera solo los tokens cuya probabilidad acumulada es igual o superior a p para muestrear. (muestreo de núcleo)</li>
<li>repetition_penalty: Aplica una penalización a los tokens repetidos para reducir la repetición en el texto.</li>
</ul></li>
</ul>
</section>
<section id="proceso-de-generación-generación-de-texto-paso-a-paso" class="level4">
<h4 class="anchored" data-anchor-id="proceso-de-generación-generación-de-texto-paso-a-paso">Proceso de generación: Generación de texto paso a paso</h4>
<ol type="1">
<li><strong>Entrada inicial:</strong> Tokeniza el prompt y lo introduce en el modelo para obtener la salida inicial.</li>
<li><strong>Ajuste de distribución de probabilidad:</strong> Aplica condiciones de restricción como <code>temperature</code>, <code>top_k</code>, <code>top_p</code>, <code>repetition_penalty</code> a los logits de salida para ajustar la distribución de probabilidad del siguiente token.</li>
<li><strong>Muestreo de tokens:</strong> Muestra el siguiente token según la distribución de probabilidad ajustada.</li>
<li><strong>Adición de salida y actualización de caché KV:</strong> Añade el token generado a la secuencia de salida y actualiza la caché KV.</li>
<li><strong>Iteración:</strong> Repite los pasos 2-4 hasta que se cumplan las condiciones de terminación (alcanzar la longitud máxima o generar un token de terminación).</li>
</ol>
<p>En esta sección, hemos examinado en detalle el proceso de entrenamiento y generación de texto del modelo Mistral. En las siguientes secciones, exploraremos ejemplos prácticos para ilustrar el uso del modelo <code>simple_mistral</code> a través de tres ejemplos. Los ejemplos están en mistral/examples. 1. <strong>Predicción de secuencia numérica (<code>train_seq_num.py</code>):</strong> Se verifica la capacidad básica de aprendizaje y generación del modelo a través de una tarea simple de predicción de números consecutivos. 2. <strong>Predicción de operaciones aritméticas (<code>train_math.py</code>):</strong> Se examina si el modelo aprende razonamiento simbólico (symbolic reasoning) a través de una tarea de predicción de resultados de operaciones de suma, resta y multiplicación. 3. <strong>Generación de consultas SQL (<code>train_sql.py</code>):</strong> Se evalúa la capacidad del modelo para comprender y procesar estructuras lingüísticas complejas mediante una tarea de conversión de preguntas en lenguaje natural a consultas SQL. (Uso del conjunto de datos WikiSQL)</p>
<p>Se puede ejecutar directamente desde la shell en la ubicación correspondiente. Por ejemplo, <code>python train_seq_num.py</code>. A continuación se muestra cómo ejecutarlo desde un cuaderno Jupyter.</p>
</section>
</section>
<section id="ejemplo-de-predicción-de-secuencia-numérica-análisis-de-train_seq_num.py" class="level3">
<h3 class="anchored" data-anchor-id="ejemplo-de-predicción-de-secuencia-numérica-análisis-de-train_seq_num.py">9.6.5 Ejemplo de predicción de secuencia numérica: análisis de <code>train_seq_num.py</code></h3>
<p><code>train_seq_num.py</code> es un ejemplo que utiliza el modelo <code>simple_mistral</code> para realizar una tarea de predicción de secuencia numérica simple. A través de este ejemplo, podemos examinar cómo el modelo aprende a predecir el siguiente número en una secuencia dada.</p>
<section id="preparación-del-conjunto-de-datos-y-del-cargador-de-datos-configuración-de-los-datos-de-entrenamiento" class="level4">
<h4 class="anchored" data-anchor-id="preparación-del-conjunto-de-datos-y-del-cargador-de-datos-configuración-de-los-datos-de-entrenamiento">1. Preparación del conjunto de datos y del cargador de datos: configuración de los datos de entrenamiento</h4>
<p>Esta es la etapa donde se preparan los datos que el modelo <code>simple_mistral</code> utilizará para aprender.</p>
<ul>
<li><p><strong>Clase <code>SimpleDataset</code>:</strong></p>
<ul>
<li>Define un conjunto de datos de secuencia numérica simple heredando de la clase <code>Dataset</code> de PyTorch.</li>
<li>El método <code>__init__</code> inicializa el conjunto de datos con los datos (<code>data</code>) y la longitud de la secuencia (<code>seq_length</code>).</li>
<li>El método <code>__len__</code> devuelve el número total de muestras en el conjunto de datos.</li>
<li>El método <code>__getitem__</code> devuelve la secuencia de entrada y la secuencia de etiquetas correspondientes a un índice dado (<code>idx</code>). En este ejemplo, la entrada y las etiquetas son la misma secuencia. Internamente, el modelo desplaza automáticamente las etiquetas una posición hacia adelante para formar la tarea de predicción del siguiente token.</li>
</ul></li>
<li><p><strong>Función <code>create_simple_data</code>:</strong></p>
<ul>
<li>Genera datos de secuencia numérica que se ajustan a un tamaño de vocabulario especificado (<code>vocab_size</code>), número de muestras (<code>num_examples</code>) y longitud de secuencia (<code>seq_length</code>).</li>
<li>Repite los números desde 0 hasta <code>vocab_size - 1</code> para crear una lista de longitud <code>num_examples</code>.</li>
</ul></li>
<li><p><strong>Cargador de datos (<code>DataLoader</code>):</strong></p>
<ul>
<li>El <code>DataLoader</code> agrupa los datos generados por <code>SimpleDataset</code> en lotes (mini-batches) y los proporciona al modelo.</li>
<li><code>batch_size</code> especifica el número de muestras que se ingresarán al modelo a la vez,</li>
<li>Si se establece <code>shuffle=True</code>, el orden de los datos se mezclará aleatoriamente cada época para mejorar el efecto de entrenamiento.</li>
</ul>
<p>Los datos de entrenamiento generados por <code>SimpleDataset</code> tienen el siguiente formato:</p>
<pre class="text"><code>Muestra 1: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] -&gt; [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]

Muestra 2: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] -&gt; [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]</code></pre>
<p><strong>Desplazamiento de etiquetas en la función <code>forward</code> del modelo</strong></p>
<p>En la función <code>forward</code> del modelo <code>simple_mistral</code>, las secuencias de etiquetas se desplazan internamente una posición hacia adelante. Es decir, el modelo funciona así:</p>
<ol type="1">
<li><strong>Secuencia de entrada:</strong> <code>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]</code></li>
<li><strong>Entrada al modelo:</strong> <code>[0, 1, 2, 3, 4, 5, 6, 7, 8]</code> (excluyendo el último token)</li>
<li><strong>Predicción del modelo:</strong> <code>[1, 2, 3, 4, 5, 6, 7, 8, 9]</code> (predicción del siguiente token en cada posición)</li>
<li><strong>Etiquetas:</strong> <code>[1, 2, 3, 4, 5, 6, 7, 8, 9]</code> (excluyendo el primer token de la secuencia de entrada, para comparar con la predicción del modelo)</li>
</ol>
<p>A través de este proceso, el modelo aprende a predecir el siguiente token en cada posición de la secuencia de entrada.</p></li>
</ul>
</section>
<section id="configuración-y-entrenamiento-del-modelo-entrenamiento-de-simple_mistral" class="level4">
<h4 class="anchored" data-anchor-id="configuración-y-entrenamiento-del-modelo-entrenamiento-de-simple_mistral">2. Configuración y entrenamiento del modelo: entrenamiento de <code>simple_mistral</code></h4>
<p>Esta es la etapa donde se configura el modelo <code>simple_mistral</code> y se procede con el entrenamiento utilizando los datos preparados. * <strong>Configuración de <code>MistralConfig</code>:</strong> * <code>vocab_size</code> se establece agregando el token <code>&lt;eos&gt;</code> al tamaño del vocabulario definido por el tokenizador. Esto permite que el modelo reconozca el final de una oración. * <code>sliding_window</code> se configura igual que la longitud de la secuencia para que cada token pueda ver toda la secuencia. * Se establece <code>use_cache=False</code> para no utilizar el caché KV durante el entrenamiento. * <strong>Compartir pesos (<code>tie_weights = True</code>):</strong> * Al configurar <code>tie_weights</code> en <code>True</code>, se comparten los pesos de incrustación y los pesos del capa de salida (<code>lm_head</code>). Esto puede reducir el número de parámetros y ayudar a aprender patrones específicos (como la generación secuencial de números).</p>
<ul>
<li><strong>Creación del modelo (<code>MistralForCausalLM</code>) y optimizador (<code>AdamW</code>):</strong>
<ul>
<li>Se crea un modelo <code>MistralForCausalLM</code> y se mueve al dispositivo especificado (CPU o GPU).</li>
<li>Se crea un optimizador <code>AdamW</code>, y se establecen los parámetros del modelo y la tasa de aprendizaje (<code>learning_rate</code>).</li>
</ul></li>
<li><strong>Función <code>train</code> (bucle de entrenamiento):</strong>
<ul>
<li>Se configura el modelo en modo de entrenamiento (<code>model.train()</code>).</li>
<li>Se repite el entrenamiento por el número especificado de épocas.</li>
<li>En cada época, se obtienen mini lotes del cargador de datos, se ingresan al modelo y se calcula la pérdida.</li>
<li>A través del retropropagación, se calculan los gradientes y se actualizan los parámetros del modelo utilizando el optimizador.</li>
<li>Se imprime la pérdida por lote a intervalos regulares y se muestra la pérdida promedio al final de cada época para monitorear el progreso del entrenamiento.</li>
</ul></li>
</ul>
</section>
<section id="generación-de-texto-predicción-con-el-modelo-entrenado" class="level4">
<h4 class="anchored" data-anchor-id="generación-de-texto-predicción-con-el-modelo-entrenado">3. Generación de texto: Predicción con el modelo entrenado</h4>
<p>Esta es la etapa en la que se utiliza el modelo entrenado para generar nuevo texto (secuencias numéricas).</p>
<ul>
<li><strong>Función <code>generate_text</code>:</strong>
<ul>
<li>Se configura el modelo en modo de evaluación (<code>model.eval()</code>).</li>
<li>Se convierte el texto inicial (<code>start_text</code>, por ejemplo: <code>['1', '2', '3']</code>) a IDs de tokens y se ingresan al modelo.</li>
<li>Se genera el siguiente token iterativamente hasta alcanzar <code>max_length</code>.
<ul>
<li>Se aplica una <code>temperature</code> a los logits de salida del modelo para ajustar la distribución de probabilidad. Un valor bajo de <code>temperature</code> produce texto más coherente, mientras que un valor alto produce texto más diverso.</li>
<li>Se muestrean los IDs de tokens del siguiente token de la distribución de probabilidad ajustada (usando la función <code>torch.multinomial</code>).</li>
<li>Se convierten los IDs de tokens muestreados nuevamente a texto y se agregan a la lista de tokens generados.</li>
<li>Se agrega el nuevo token generado a la entrada para predecir el siguiente token en un proceso iterativo.</li>
</ul></li>
<li>Finalmente, se devuelve el texto generado.</li>
</ul></li>
</ul>
</section>
<section id="análisis-de-resultados-evaluación-del-entrenamiento-y-texto-generado" class="level4">
<h4 class="anchored" data-anchor-id="análisis-de-resultados-evaluación-del-entrenamiento-y-texto-generado">4. Análisis de resultados: Evaluación del entrenamiento y texto generado</h4>
<p>Esta es la etapa en la que se analizan los resultados del entrenamiento del modelo y el texto generado.</p>
<ul>
<li><strong>Resultados del entrenamiento:</strong> Se puede observar una disminución constante de la pérdida (<code>loss</code>) durante el proceso de entrenamiento, lo que indica que el modelo está aprendiendo con éxito los patrones de las secuencias numéricas.</li>
<li><strong>Resultados generados:</strong>
<ul>
<li>Resultado de la generación de texto comenzando con <code>['1', '2', '3']</code>: <code>1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20</code></li>
<li>Resultado de la generación de texto comenzando con <code>['40', '41', '42']</code>: <code>40 41 42 43 44 45 46 47 48 49</code> Podemos verificar que el modelo genera con precisión los números consecutivos que siguen al número de inicio dado. Esto demuestra que el modelo ha aprendido el patrón de la secuencia numérica y puede generar nuevas secuencias basándose en ello.</li>
</ul></li>
</ul>
<p>El ejemplo <code>train_seq_num.py</code> muestra cómo se puede realizar exitosamente una tarea de predicción de secuencias numéricas sencilla pero clara utilizando el modelo <code>simple_mistral</code>.</p>
<div id="cell-59" class="cell" data-execution_count="7">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_09.mistral.examples.train_seq_num <span class="im">import</span> MistralConfig, MistralForCausalLM, SimpleDataset, create_simple_data, generate_text, train</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> Dataset, DataLoader</span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Hyperparameter settings</span></span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a>base_vocab_size <span class="op">=</span> <span class="dv">50</span>    <span class="co"># Original vocab_size before the EOS token</span></span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a>seq_length <span class="op">=</span> <span class="dv">10</span>         <span class="co"># Sequence length of each training sample</span></span>
<span id="cb33-8"><a href="#cb33-8" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">8</span></span>
<span id="cb33-9"><a href="#cb33-9" aria-hidden="true" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb33-10"><a href="#cb33-10" aria-hidden="true" tabindex="-1"></a>learning_rate <span class="op">=</span> <span class="fl">5e-3</span></span>
<span id="cb33-11"><a href="#cb33-11" aria-hidden="true" tabindex="-1"></a>num_train_examples <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb33-12"><a href="#cb33-12" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> <span class="st">"cuda"</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">"cpu"</span></span>
<span id="cb33-13"><a href="#cb33-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-14"><a href="#cb33-14" aria-hidden="true" tabindex="-1"></a><span class="co"># 1) Create tokenizer (string token -&gt; token id)</span></span>
<span id="cb33-15"><a href="#cb33-15" aria-hidden="true" tabindex="-1"></a>tokenizer_vocab <span class="op">=</span> {<span class="bu">str</span>(i): i <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(base_vocab_size)}</span>
<span id="cb33-16"><a href="#cb33-16" aria-hidden="true" tabindex="-1"></a>tokenizer_vocab[<span class="st">"&lt;eos&gt;"</span>] <span class="op">=</span> base_vocab_size</span>
<span id="cb33-17"><a href="#cb33-17" aria-hidden="true" tabindex="-1"></a>updated_vocab_size <span class="op">=</span> base_vocab_size <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb33-18"><a href="#cb33-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-19"><a href="#cb33-19" aria-hidden="true" tabindex="-1"></a><span class="co"># 2) Model configuration: Apply the updated vocab_size and set sliding_window to seq_length</span></span>
<span id="cb33-20"><a href="#cb33-20" aria-hidden="true" tabindex="-1"></a>config <span class="op">=</span> MistralConfig(</span>
<span id="cb33-21"><a href="#cb33-21" aria-hidden="true" tabindex="-1"></a>    vocab_size<span class="op">=</span>updated_vocab_size,</span>
<span id="cb33-22"><a href="#cb33-22" aria-hidden="true" tabindex="-1"></a>    hidden_size<span class="op">=</span><span class="dv">32</span>,</span>
<span id="cb33-23"><a href="#cb33-23" aria-hidden="true" tabindex="-1"></a>    intermediate_size<span class="op">=</span><span class="dv">64</span>,</span>
<span id="cb33-24"><a href="#cb33-24" aria-hidden="true" tabindex="-1"></a>    num_hidden_layers<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb33-25"><a href="#cb33-25" aria-hidden="true" tabindex="-1"></a>    num_attention_heads<span class="op">=</span><span class="dv">4</span>,</span>
<span id="cb33-26"><a href="#cb33-26" aria-hidden="true" tabindex="-1"></a>    num_key_value_heads<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb33-27"><a href="#cb33-27" aria-hidden="true" tabindex="-1"></a>    max_position_embeddings<span class="op">=</span><span class="dv">128</span>,</span>
<span id="cb33-28"><a href="#cb33-28" aria-hidden="true" tabindex="-1"></a>    sliding_window<span class="op">=</span>seq_length,  <span class="co"># Set to the same as the sequence length</span></span>
<span id="cb33-29"><a href="#cb33-29" aria-hidden="true" tabindex="-1"></a>    use_cache<span class="op">=</span><span class="va">False</span>  <span class="co"># Do not use cache during training</span></span>
<span id="cb33-30"><a href="#cb33-30" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb33-31"><a href="#cb33-31" aria-hidden="true" tabindex="-1"></a>config.eos_token_id <span class="op">=</span> tokenizer_vocab[<span class="st">"&lt;eos&gt;"</span>]</span>
<span id="cb33-32"><a href="#cb33-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-33"><a href="#cb33-33" aria-hidden="true" tabindex="-1"></a><span class="co"># (Optional) Set up weight tying between embedding and lm_head -&gt; Can help reproduce sequential patterns.</span></span>
<span id="cb33-34"><a href="#cb33-34" aria-hidden="true" tabindex="-1"></a>tie_weights <span class="op">=</span> <span class="va">True</span></span>
<span id="cb33-35"><a href="#cb33-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-36"><a href="#cb33-36" aria-hidden="true" tabindex="-1"></a><span class="co"># 3) Create model and Optimizer</span></span>
<span id="cb33-37"><a href="#cb33-37" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> MistralForCausalLM(config).to(device)</span>
<span id="cb33-38"><a href="#cb33-38" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> tie_weights:</span>
<span id="cb33-39"><a href="#cb33-39" aria-hidden="true" tabindex="-1"></a>    model.lm_head.weight <span class="op">=</span> model.model.embed_tokens.weight</span>
<span id="cb33-40"><a href="#cb33-40" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.AdamW(model.parameters(), lr<span class="op">=</span>learning_rate)</span>
<span id="cb33-41"><a href="#cb33-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-42"><a href="#cb33-42" aria-hidden="true" tabindex="-1"></a><span class="co"># 4) Data generation and DataLoader preparation</span></span>
<span id="cb33-43"><a href="#cb33-43" aria-hidden="true" tabindex="-1"></a>train_data <span class="op">=</span> create_simple_data(updated_vocab_size, num_train_examples, seq_length)</span>
<span id="cb33-44"><a href="#cb33-44" aria-hidden="true" tabindex="-1"></a>train_dataset <span class="op">=</span> SimpleDataset(train_data, seq_length)</span>
<span id="cb33-45"><a href="#cb33-45" aria-hidden="true" tabindex="-1"></a>train_dataloader <span class="op">=</span> DataLoader(train_dataset, batch_size<span class="op">=</span>batch_size, shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb33-46"><a href="#cb33-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-47"><a href="#cb33-47" aria-hidden="true" tabindex="-1"></a><span class="co"># --- For debugging: Output some data before training ---</span></span>
<span id="cb33-48"><a href="#cb33-48" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Sample data before training (input sequence -&gt; label sequence):"</span>)</span>
<span id="cb33-49"><a href="#cb33-49" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>):</span>
<span id="cb33-50"><a href="#cb33-50" aria-hidden="true" tabindex="-1"></a>    input_seq, label_seq <span class="op">=</span> train_dataset[i]</span>
<span id="cb33-51"><a href="#cb33-51" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Sample </span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>input_seq<span class="sc">.</span>tolist()<span class="sc">}</span><span class="ss"> -&gt; </span><span class="sc">{</span>label_seq<span class="sc">.</span>tolist()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb33-52"><a href="#cb33-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-53"><a href="#cb33-53" aria-hidden="true" tabindex="-1"></a><span class="co"># 5) Start training</span></span>
<span id="cb33-54"><a href="#cb33-54" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Start training..."</span>)</span>
<span id="cb33-55"><a href="#cb33-55" aria-hidden="true" tabindex="-1"></a>train(model, train_dataloader, optimizer, epochs, device)</span>
<span id="cb33-56"><a href="#cb33-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-57"><a href="#cb33-57" aria-hidden="true" tabindex="-1"></a><span class="co"># 6) Text generation example</span></span>
<span id="cb33-58"><a href="#cb33-58" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Generating text starting with tokens ['1', '2', '3']:"</span>)</span>
<span id="cb33-59"><a href="#cb33-59" aria-hidden="true" tabindex="-1"></a>start_text <span class="op">=</span> [<span class="st">"1"</span>, <span class="st">"2"</span>, <span class="st">"3"</span>]</span>
<span id="cb33-60"><a href="#cb33-60" aria-hidden="true" tabindex="-1"></a>generated <span class="op">=</span> generate_text(model, start_text, tokenizer_vocab, max_length<span class="op">=</span><span class="dv">20</span>, device<span class="op">=</span>device)</span>
<span id="cb33-61"><a href="#cb33-61" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Generated text:"</span>, <span class="st">" "</span>.join(generated))</span>
<span id="cb33-62"><a href="#cb33-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-63"><a href="#cb33-63" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Generating text starting with tokens ['40', '41', '42']:"</span>)</span>
<span id="cb33-64"><a href="#cb33-64" aria-hidden="true" tabindex="-1"></a>start_text <span class="op">=</span> [<span class="st">"40"</span>, <span class="st">"41"</span>, <span class="st">"42"</span>]</span>
<span id="cb33-65"><a href="#cb33-65" aria-hidden="true" tabindex="-1"></a>generated <span class="op">=</span> generate_text(model, start_text, tokenizer_vocab, max_length<span class="op">=</span><span class="dv">20</span>, device<span class="op">=</span>device)</span>
<span id="cb33-66"><a href="#cb33-66" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Generated text:"</span>, <span class="st">" "</span>.join(generated))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Sample data before training (input sequence -&gt; label sequence):
Sample 1: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] -&gt; [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
Sample 2: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] -&gt; [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
Start training...
Batch 100/124, Loss: 0.0020
Epoch 1/5, Average Loss: 2.2763
Batch 100/124, Loss: 0.0027
Epoch 2/5, Average Loss: 0.0024
Batch 100/124, Loss: 0.0006
Epoch 3/5, Average Loss: 0.0011
Batch 100/124, Loss: 0.0008
Epoch 4/5, Average Loss: 0.0007
Batch 100/124, Loss: 0.0005
Epoch 5/5, Average Loss: 0.0005
Generating text starting with tokens ['1', '2', '3']:
Generated text: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20
Generating text starting with tokens ['40', '41', '42']:
Generated text: 40 41 42 43 44 45 46 47 48 49</code></pre>
</div>
</div>
</section>
</section>
<section id="ejemplo-de-predicción-de-operaciones-aritméticas-análisis-de-train_math.py" class="level3">
<h3 class="anchored" data-anchor-id="ejemplo-de-predicción-de-operaciones-aritméticas-análisis-de-train_math.py">9.6.6 Ejemplo de predicción de operaciones aritméticas: Análisis de <code>train_math.py</code></h3>
<p><code>train_math.py</code> es un ejemplo que utiliza el modelo <code>simple_mistral</code> para predecir los resultados de simples operaciones aritméticas (suma, resta, multiplicación). A través de este ejemplo, se evalúa si el modelo puede comprender números y símbolos de operación, y realizar inferencias matemáticas básicas. Los ejemplos de datos de entrenamiento son los siguientes.</p>
<pre class="text"><code>Muestra 1: 4*1=4&lt;eos&gt;
Muestra 2: 9+8=17&lt;eos&gt;</code></pre>
<section id="generación-y-preprocesamiento-de-datos-la-armonía-entre-símbolos-y-números" class="level4">
<h4 class="anchored" data-anchor-id="generación-y-preprocesamiento-de-datos-la-armonía-entre-símbolos-y-números">Generación y preprocesamiento de datos: la armonía entre símbolos y números</h4>
<p>El ejemplo <code>train_math.py</code> presenta algunas diferencias importantes en comparación con el ejemplo anterior de predicción de secuencias numéricas, tanto en generación de datos, tokenización, como en configuración del modelo. La diferencia más significativa es que los datos tratados no son solo una lista simple de números, sino “expresiones” compuestas por números, símbolos de operación, signo igual y el token <code>&lt;eos&gt;</code> que indica el final de la oración.</p>
<ul>
<li><strong>Función <code>create_arithmetic_data</code>: generación de datos aritméticos</strong>
<ul>
<li>Esta función genera expresiones aritméticas y sus resultados en formato de cadena para una cantidad especificada (<code>num_samples</code>) de operaciones aritméticas.</li>
<li>Cada expresión sigue el formato <code>f"{num1}{op}{num2}={result}&lt;eos&gt;"</code> (por ejemplo: <code>"12+7=19&lt;eos&gt;"</code>)
<ul>
<li><code>num1</code>, <code>num2</code>: enteros seleccionados aleatoriamente entre 1 y <code>max_value</code>.</li>
<li><code>op</code>: símbolo de operación seleccionado aleatoriamente entre suma (<code>+</code>), resta (<code>-</code>) y multiplicación (<code>*</code>).</li>
<li><code>result</code>: el valor resultante calculado utilizando la función <code>eval</code> de Python.</li>
<li><strong>Importancia del token <code>&lt;eos&gt;</code>:</strong> Es crucial agregar explícitamente el token <code>&lt;eos&gt;</code> (End-of-Sentence) al final de cada cadena. Este token especial actúa como un hito que indica al modelo el final de una oración. Sin el token <code>&lt;eos&gt;</code>, el modelo podría tener dificultades para determinar cuándo detener la generación, lo que podría resultar en una salida infinita de números o símbolos.</li>
</ul></li>
</ul></li>
<li><strong>Función <code>create_tokenizer</code>: definición del vocabulario</strong>
<ul>
<li>Se genera un vocabulario que incluye números (0-9), símbolos de operación (<code>+</code>, <code>-</code>, <code>\*</code>), signo igual (<code>=</code>) y tokens especiales (<code>&lt;pad&gt;</code>, <code>&lt;eos&gt;</code>). Este vocabulario define los caracteres básicos que el modelo puede entender.
<ul>
<li>El token <code>&lt;pad&gt;</code> se usa para agrupar secuencias de diferentes longitudes en un solo lote (batch).</li>
</ul></li>
</ul></li>
<li><strong>Función <code>create_reverse_tokenizer</code>: restauración de tokens ID a caracteres</strong>
<ul>
<li>Se crea un diccionario inverso para convertir IDs de tokens nuevamente en tokens de cadena. Esto se utiliza para interpretar los resultados generados en un formato legible por humanos.</li>
</ul></li>
<li><strong>Función <code>tokenize_sample</code>: conversión de cadenas a listas de tokens</strong>
<ul>
<li>La función <code>tokenize_sample</code> convierte una cadena de muestra en una lista de tokens que el modelo puede reconocer.
<ul>
<li>Los tokens especiales como <code>&lt;eos&gt;</code> se tratan como un solo token para asegurar que el modelo los reconozca completamente.</li>
</ul></li>
</ul></li>
<li><strong>Clase <code>ArithmeticDataset</code>: conversión a datos aprendibles</strong></li>
<li>La función <code>create_arithmetic_data</code> convierte los datos generados en el formato de PyTorch <code>Dataset</code>. <code>Dataset</code> es un método estandarizado para suministrar datos al modelo de manera eficiente.</li>
<li>El método <code>__getitem__</code> realiza las siguientes tareas:
<ol type="1">
<li>Utiliza la función <code>tokenize_sample</code> para tokenizar primero las cadenas de muestra.</li>
<li>Si la longitud de la secuencia tokenizada es menor que el <code>seq_length</code> especificado, se ajusta la longitud utilizando tokens <code>&lt;pad&gt;</code>. Esto se hace para asegurar que todas las secuencias de entrada tengan la misma longitud, lo que permite al modelo procesar datos en lotes (batch).</li>
<li>Convierte los tokens a IDs enteros y devuelve las secuencias de entrada y las secuencias de etiquetas (iguales a las de entrada) en forma de tensores PyTorch.</li>
</ol></li>
</ul>
</section>
<section id="configuración-del-modelo-y-entrenamiento" class="level4">
<h4 class="anchored" data-anchor-id="configuración-del-modelo-y-entrenamiento">Configuración del modelo y entrenamiento</h4>
<ul>
<li><strong>Configuración <code>MistralConfig</code>:</strong> Dado que esta es una tarea un poco más compleja que el ejemplo de predicción de secuencias numéricas, se ha aumentado ligeramente el tamaño del modelo (<code>hidden_size=64</code>, <code>intermediate_size=128</code>, <code>num_hidden_layers=3</code>, <code>num_attention_heads=8</code>, <code>num_key_value_heads=4</code>). Además, se han configurado <code>pad_token_id</code> y <code>eos_token_id</code> para que el modelo reconozca los tokens de relleno y fin de oración.</li>
<li><strong>Entrenamiento:</strong> Se utiliza la función <code>train</code> de manera similar al ejemplo anterior para entrenar el modelo. Un programador de tasas de aprendizaje <code>CosineAnnealingLR</code> se usa para reducir gradualmente la tasa de aprendizaje, permitiendo una rápida convergencia inicial y ajustes finos en las etapas posteriores.</li>
</ul>
</section>
<section id="generación-de-texto" class="level4">
<h4 class="anchored" data-anchor-id="generación-de-texto">Generación de texto</h4>
<ul>
<li><strong>Función <code>generate_text</code>:</strong> Permite que el modelo genere texto (resultado de operaciones aritméticas) basado en un prompt dado (por ejemplo, “12+7=”). La generación de la cadena de resultados se detiene cuando el modelo genera tokens <code>&lt;eos&gt;</code> o <code>&lt;pad&gt;</code>.</li>
</ul>
</section>
<section id="análisis-de-los-resultados" class="level4">
<h4 class="anchored" data-anchor-id="análisis-de-los-resultados">Análisis de los resultados</h4>
<ul>
<li><strong>Resultados del entrenamiento:</strong> Se puede ver que el modelo está aprendiendo patrones aritméticos a medida que la pérdida (loss) disminuye gradualmente durante el proceso de entrenamiento.</li>
<li><strong>Resultados generados:</strong> A través de ejemplos de datos de evaluación, se verifica si el modelo genera resultados correctos para los prompts dados. (por ejemplo, “4+20=” -&gt; “4+20=24”)</li>
</ul>
<p>El ejemplo <code>train_math.py</code> demuestra que el modelo <code>simple_mistral</code> puede aprender habilidades de razonamiento simbólico, como las operaciones aritméticas, más allá de la simple predicción de secuencias numéricas. Además, se puede apreciar la importancia y el papel de tokens especiales como <code>&lt;eos&gt;</code>, así como la necesidad de ajustar el tamaño del modelo según la complejidad de la tarea.</p>
<div id="cell-61" class="cell" data-execution_count="8">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_09.mistral.examples.train_math <span class="im">import</span> MistralConfig, MistralForCausalLM, generate_text, train,create_arithmetic_data, ArithmeticDataset, create_tokenizer, create_reverse_tokenizer</span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> DataLoader</span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a>random.seed(<span class="dv">42</span>)</span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">42</span>)</span>
<span id="cb36-8"><a href="#cb36-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-9"><a href="#cb36-9" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> <span class="st">"cuda"</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">"cpu"</span></span>
<span id="cb36-10"><a href="#cb36-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-11"><a href="#cb36-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Hyperparameter settings</span></span>
<span id="cb36-12"><a href="#cb36-12" aria-hidden="true" tabindex="-1"></a>num_samples <span class="op">=</span> <span class="dv">10000</span>   <span class="co"># Total number of samples in the dataset</span></span>
<span id="cb36-13"><a href="#cb36-13" aria-hidden="true" tabindex="-1"></a>max_value <span class="op">=</span> <span class="dv">20</span>       <span class="co"># Maximum value of operands</span></span>
<span id="cb36-14"><a href="#cb36-14" aria-hidden="true" tabindex="-1"></a>seq_length <span class="op">=</span> <span class="dv">20</span>      <span class="co"># Fixed sequence length including EOS token (e.g., 20)</span></span>
<span id="cb36-15"><a href="#cb36-15" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">16</span></span>
<span id="cb36-16"><a href="#cb36-16" aria-hidden="true" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb36-17"><a href="#cb36-17" aria-hidden="true" tabindex="-1"></a>learning_rate <span class="op">=</span> <span class="fl">1e-3</span></span>
<span id="cb36-18"><a href="#cb36-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-19"><a href="#cb36-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Data generation (including EOS token) and output training data examples</span></span>
<span id="cb36-20"><a href="#cb36-20" aria-hidden="true" tabindex="-1"></a>arithmetic_data <span class="op">=</span> create_arithmetic_data(num_samples, max_value)</span>
<span id="cb36-21"><a href="#cb36-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Training data examples:"</span>)</span>
<span id="cb36-22"><a href="#cb36-22" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>):</span>
<span id="cb36-23"><a href="#cb36-23" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Sample </span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>arithmetic_data[i]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb36-24"><a href="#cb36-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-25"><a href="#cb36-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Create tokenizer</span></span>
<span id="cb36-26"><a href="#cb36-26" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> create_tokenizer()</span>
<span id="cb36-27"><a href="#cb36-27" aria-hidden="true" tabindex="-1"></a>reverse_tokenizer <span class="op">=</span> create_reverse_tokenizer(tokenizer)</span>
<span id="cb36-28"><a href="#cb36-28" aria-hidden="true" tabindex="-1"></a>updated_vocab_size <span class="op">=</span> <span class="bu">len</span>(tokenizer)</span>
<span id="cb36-29"><a href="#cb36-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-30"><a href="#cb36-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Configure Dataset and DataLoader</span></span>
<span id="cb36-31"><a href="#cb36-31" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> ArithmeticDataset(arithmetic_data, seq_length, tokenizer)</span>
<span id="cb36-32"><a href="#cb36-32" aria-hidden="true" tabindex="-1"></a>dataloader <span class="op">=</span> DataLoader(dataset, batch_size<span class="op">=</span>batch_size, shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb36-33"><a href="#cb36-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-34"><a href="#cb36-34" aria-hidden="true" tabindex="-1"></a>config <span class="op">=</span> MistralConfig(</span>
<span id="cb36-35"><a href="#cb36-35" aria-hidden="true" tabindex="-1"></a>    vocab_size<span class="op">=</span>updated_vocab_size,</span>
<span id="cb36-36"><a href="#cb36-36" aria-hidden="true" tabindex="-1"></a>    hidden_size<span class="op">=</span><span class="dv">64</span>,</span>
<span id="cb36-37"><a href="#cb36-37" aria-hidden="true" tabindex="-1"></a>    intermediate_size<span class="op">=</span><span class="dv">128</span>,</span>
<span id="cb36-38"><a href="#cb36-38" aria-hidden="true" tabindex="-1"></a>    num_hidden_layers<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb36-39"><a href="#cb36-39" aria-hidden="true" tabindex="-1"></a>    num_attention_heads<span class="op">=</span><span class="dv">8</span>,</span>
<span id="cb36-40"><a href="#cb36-40" aria-hidden="true" tabindex="-1"></a>    num_key_value_heads<span class="op">=</span><span class="dv">4</span>,</span>
<span id="cb36-41"><a href="#cb36-41" aria-hidden="true" tabindex="-1"></a>    max_position_embeddings<span class="op">=</span><span class="dv">128</span>,</span>
<span id="cb36-42"><a href="#cb36-42" aria-hidden="true" tabindex="-1"></a>    sliding_window<span class="op">=</span>seq_length,</span>
<span id="cb36-43"><a href="#cb36-43" aria-hidden="true" tabindex="-1"></a>    use_cache<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb36-44"><a href="#cb36-44" aria-hidden="true" tabindex="-1"></a>    use_return_dict<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb36-45"><a href="#cb36-45" aria-hidden="true" tabindex="-1"></a>    pad_token_id<span class="op">=</span>tokenizer[<span class="st">"&lt;pad&gt;"</span>]  <span class="co"># Set the pad token id here.</span></span>
<span id="cb36-46"><a href="#cb36-46" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb36-47"><a href="#cb36-47" aria-hidden="true" tabindex="-1"></a>config.eos_token_id <span class="op">=</span> tokenizer[<span class="st">"&lt;eos&gt;"</span>]  <span class="co"># Also update the eos token</span></span>
<span id="cb36-48"><a href="#cb36-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-49"><a href="#cb36-49" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> MistralForCausalLM(config).to(device)</span>
<span id="cb36-50"><a href="#cb36-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-51"><a href="#cb36-51" aria-hidden="true" tabindex="-1"></a><span class="co"># weight tying (share weights between embedding and lm_head)</span></span>
<span id="cb36-52"><a href="#cb36-52" aria-hidden="true" tabindex="-1"></a>tie_weights <span class="op">=</span> <span class="va">True</span></span>
<span id="cb36-53"><a href="#cb36-53" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> tie_weights:</span>
<span id="cb36-54"><a href="#cb36-54" aria-hidden="true" tabindex="-1"></a>    model.lm_head.weight <span class="op">=</span> model.model.embed_tokens.weight</span>
<span id="cb36-55"><a href="#cb36-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-56"><a href="#cb36-56" aria-hidden="true" tabindex="-1"></a><span class="co"># Create optimizer and add cosine annealing scheduler</span></span>
<span id="cb36-57"><a href="#cb36-57" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.AdamW(model.parameters(), lr<span class="op">=</span>learning_rate)</span>
<span id="cb36-58"><a href="#cb36-58" aria-hidden="true" tabindex="-1"></a>scheduler <span class="op">=</span> torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max<span class="op">=</span>epochs, eta_min<span class="op">=</span><span class="fl">1e-5</span>)</span>
<span id="cb36-59"><a href="#cb36-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-60"><a href="#cb36-60" aria-hidden="true" tabindex="-1"></a><span class="co"># Start training</span></span>
<span id="cb36-61"><a href="#cb36-61" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Start training..."</span>)</span>
<span id="cb36-62"><a href="#cb36-62" aria-hidden="true" tabindex="-1"></a>train(model, dataloader, optimizer, scheduler, epochs, device)</span>
<span id="cb36-63"><a href="#cb36-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-64"><a href="#cb36-64" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluation: Output 10 random evaluation samples (terminate generation if EOS is included in the prompt)</span></span>
<span id="cb36-65"><a href="#cb36-65" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Evaluation data examples:"</span>)</span>
<span id="cb36-66"><a href="#cb36-66" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>):</span>
<span id="cb36-67"><a href="#cb36-67" aria-hidden="true" tabindex="-1"></a>    sample <span class="op">=</span> random.choice(arithmetic_data)</span>
<span id="cb36-68"><a href="#cb36-68" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Use the part before '=' as a prompt in the entire expression, e.g., "12+7=19&lt;eos&gt;" ("12+7=")</span></span>
<span id="cb36-69"><a href="#cb36-69" aria-hidden="true" tabindex="-1"></a>    prompt <span class="op">=</span> sample.split(<span class="st">'='</span>)[<span class="dv">0</span>] <span class="op">+</span> <span class="st">'='</span></span>
<span id="cb36-70"><a href="#cb36-70" aria-hidden="true" tabindex="-1"></a>    generated <span class="op">=</span> generate_text(model, prompt, tokenizer, reverse_tokenizer, max_length<span class="op">=</span>seq_length, device<span class="op">=</span>device)</span>
<span id="cb36-71"><a href="#cb36-71" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Generated result for prompt '</span><span class="sc">{</span>prompt<span class="sc">}</span><span class="ss">': </span><span class="sc">{</span>generated<span class="sc">}</span><span class="ss"> (Original data: </span><span class="sc">{</span>sample<span class="sc">}</span><span class="ss">)"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Training data examples:
Sample 1: 4*1=4&lt;eos&gt;
Sample 2: 9+8=17&lt;eos&gt;
Sample 3: 5*4=20&lt;eos&gt;
Sample 4: 18*3=54&lt;eos&gt;
Sample 5: 14+2=16&lt;eos&gt;
Sample 6: 3+7=10&lt;eos&gt;
Sample 7: 17+20=37&lt;eos&gt;
Sample 8: 18*7=126&lt;eos&gt;
Sample 9: 18+14=32&lt;eos&gt;
Sample 10: 15-19=-4&lt;eos&gt;
Start training...
Epoch 1/20, Average Loss: 2.4820, LR: 0.000994
Epoch 2/20, Average Loss: 1.2962, LR: 0.000976
Epoch 3/20, Average Loss: 1.1905, LR: 0.000946
Epoch 4/20, Average Loss: 1.0831, LR: 0.000905
Epoch 5/20, Average Loss: 0.9902, LR: 0.000855
Epoch 6/20, Average Loss: 0.9112, LR: 0.000796
Epoch 7/20, Average Loss: 0.8649, LR: 0.000730
Epoch 8/20, Average Loss: 0.8362, LR: 0.000658
Epoch 9/20, Average Loss: 0.8194, LR: 0.000582
Epoch 10/20, Average Loss: 0.8128, LR: 0.000505
Epoch 11/20, Average Loss: 0.8049, LR: 0.000428
Epoch 12/20, Average Loss: 0.7971, LR: 0.000352
Epoch 13/20, Average Loss: 0.7945, LR: 0.000280
Epoch 14/20, Average Loss: 0.7918, LR: 0.000214
Epoch 15/20, Average Loss: 0.7903, LR: 0.000155
Epoch 16/20, Average Loss: 0.7884, LR: 0.000105
Epoch 17/20, Average Loss: 0.7864, LR: 0.000064
Epoch 18/20, Average Loss: 0.7854, LR: 0.000034
Epoch 19/20, Average Loss: 0.7837, LR: 0.000016
Epoch 20/20, Average Loss: 0.7831, LR: 0.000010

Evaluation data examples:
Generated result for prompt '4+20=': 4+20=24 (Original data: 4+20=24&lt;eos&gt;)
Generated result for prompt '16-3=': 16-3=13 (Original data: 16-3=13&lt;eos&gt;)
Generated result for prompt '10+15=': 10+15=25 (Original data: 10+15=25&lt;eos&gt;)
Generated result for prompt '8+4=': 8+4=12 (Original data: 8+4=12&lt;eos&gt;)
Generated result for prompt '16-13=': 16-13=3 (Original data: 16-13=3&lt;eos&gt;)
Generated result for prompt '10*1=': 10*1=10 (Original data: 10*1=10&lt;eos&gt;)
Generated result for prompt '18+13=': 18+13=31 (Original data: 18+13=31&lt;eos&gt;)
Generated result for prompt '9+9=': 9+9=18 (Original data: 9+9=18&lt;eos&gt;)
Generated result for prompt '1+15=': 1+15=16 (Original data: 1+15=16&lt;eos&gt;)
Generated result for prompt '18-18=': 18-18=0 (Original data: 18-18=0&lt;eos&gt;)</code></pre>
</div>
</div>
</section>
</section>
<section id="ejemplo-de-generación-de-consultas-sql-a-partir-del-lenguaje-natural-análisis-de-train_sql.py" class="level3">
<h3 class="anchored">9.6.7 Ejemplo de generación de consultas SQL a partir del lenguaje natural: Análisis de <code>train_sql.py</code></h3>
<p><code>train_sql.py</code> utiliza el modelo <code>simple_mistral</code> para abordar una tarea de procesamiento de lenguaje natural más compleja, que consiste en convertir preguntas en lenguaje natural en consultas SQL. En este ejemplo, se examina cómo el modelo aprende a comprender el significado de oraciones complejas en lenguaje natural y a expresarlas en la estructurada lengua de consulta SQL, que va más allá de la generación simple de secuencias. El conjunto de entrenamiento está formado por ejemplos donde se proporciona una oración y se devuelve esta en forma de sentencia SQL. A continuación se presentan algunos ejemplos de datos de entrenamiento.</p>
<pre class="text"><code>Muestra 1: Dime cuáles son las notas para Australia del Sur sep&gt; SELECT Notes FROM table WHERE Current slogan = SOUTH AUSTRALIA eos&gt;
Muestra 2: ¿Cuál es el formato para Australia del Sur? sep&gt; SELECT Format FROM table WHERE State/territory = South Australia eos&gt;</code></pre>
<section id="conjunto-de-datos-y-preprocesamiento-armonía-entre-wikisql-y-tokens-especiales" class="level4">
<h4 class="anchored" data-anchor-id="conjunto-de-datos-y-preprocesamiento-armonía-entre-wikisql-y-tokens-especiales">Conjunto de datos y preprocesamiento: Armonía entre WikiSQL y tokens especiales</h4>
<p>La clave en el ejemplo de <code>train_sql.py</code> es utilizar eficazmente el conjunto de datos WikiSQL y preparar los datos para que el modelo pueda aprender la relación entre el lenguaje natural y las consultas SQL.</p>
<ul>
<li><p><strong>Carga del conjunto de datos WikiSQL:</strong> Se utiliza la biblioteca <code>datasets</code> para cargar el conjunto de datos WikiSQL. WikiSQL es un conjunto de datos compuesto por preguntas en lenguaje natural y sus correspondientes consultas SQL, ampliamente utilizado para tareas de conversión de lenguaje natural a SQL. Con la función <code>load_dataset</code>, se pueden especificar conjuntos de datos de entrenamiento (<code>train</code>) y validación (<code>validation</code>) utilizando el parámetro <code>split</code>.</p></li>
<li><p><strong>Clase <code>WikiSQLDataset</code>:</strong> Se hereda de la clase <code>Dataset</code> de PyTorch para procesar el conjunto de datos WikiSQL en un formato adecuado para el entrenamiento del modelo.</p>
<ul>
<li>En el método <code>__init__</code>, se carga el conjunto de datos WikiSQL y se configura el tokenizador (<code>tokenizer</code>) y la longitud máxima de secuencia (<code>max_length</code>).</li>
<li>El método <code>__getitem__</code> procesa los ejemplos de datos para convertirlos en un formato apto para ser ingresado al modelo. La parte más importante de este proceso es combinar las preguntas en lenguaje natural con las consultas SQL y agregar tokens especiales.
<ol type="1">
<li>Primero, se obtienen la pregunta en lenguaje natural (<code>question</code>) y la consulta SQL escrita por humanos (<code>sql['human_readable']</code>) del ejemplo de datos.</li>
<li>Se combinan la pregunta y la consulta SQL en el formato <code>"pregunta &lt;sep&gt; SQL&lt;eos&gt;"</code>. Aquí, <code>&lt;sep&gt;</code> es un token separador que distingue entre la pregunta y la consulta SQL, y <code>&lt;eos&gt;</code> es un token final de oración que indica el final de la secuencia. Estos tokens especiales juegan un papel crucial al proporcionar estructura al texto de entrada del modelo.</li>
<li>Se utiliza el <code>tokenizer</code> para tokenizar el texto combinado. En este proceso, se establece <code>truncation=True</code> para cortar el texto si excede la longitud máxima (<code>max_length</code>), y <code>padding="max_length"</code> para agregar relleno y asegurar que la secuencia tenga una longitud de <code>max_length</code>.</li>
<li>Finalmente, se devuelve <code>input_ids</code> tokenizado. (las entradas y las etiquetas son idénticas)</li>
</ol></li>
</ul></li>
<li><p><strong>Tokenizador (T5Tokenizer):</strong> Utiliza <code>T5Tokenizer</code> de la biblioteca <code>transformers</code>. Las razones para elegir <code>T5Tokenizer</code> son las siguientes.</p>
<ul>
<li>Soporta varios tokens especiales (<code>&lt;pad&gt;</code>, <code>&lt;eos&gt;</code>, <code>&lt;sep&gt;</code>, etc.) por defecto.</li>
<li>Es un tokenizador versátil que puede manejar tanto lenguaje natural como consultas SQL (código) de manera efectiva.</li>
<li>Se puede obtener fácilmente el tamaño del vocabulario del tokenizador a través de <code>tokenizer.vocab_size</code>, lo cual es conveniente para establecer el <code>vocab_size</code> del modelo.</li>
</ul></li>
<li><p><strong>Cargador de datos (<code>DataLoader</code>):</strong> Desempeña el papel de agrupar los conjuntos de datos generados por <code>WikiSQLDataset</code> en lotes miniatura y suministrarlos al modelo de manera eficiente. <code>batch_size</code> es el número de muestras que se ingresan al modelo a la vez, y <code>shuffle=True</code> mezcla los datos antes de cada época para mejorar el rendimiento del entrenamiento.</p></li>
</ul>
</section>
<section id="configuración-del-modelo-y-entrenamiento-1" class="level4">
<h4 class="anchored" data-anchor-id="configuración-del-modelo-y-entrenamiento-1">Configuración del modelo y entrenamiento</h4>
<ul>
<li><p><strong>Configuración de <code>MistralConfig</code>:</strong> Establece los hiperparámetros relacionados con la estructura del modelo. En particular, configura <code>pad_token_id</code>, <code>bos_token_id</code> y <code>eos_token_id</code> a las ID de tokens correspondientes en <code>tokenizer</code> para que el modelo procese correctamente los tokens de padding, inicio de oración y fin de oración.</p></li>
<li><p><strong>Creación del modelo (<code>MistralForCausalLM</code>) y optimizador (<code>AdamW</code>):</strong> Crea un modelo <code>MistralForCausalLM</code> y lo mueve al dispositivo especificado (CPU o GPU). Utiliza el optimizador <code>AdamW</code> y el programador de tasas de aprendizaje <code>get_cosine_schedule_with_warmup</code> para controlar la tasa de aprendizaje y optimizar el modelo.</p></li>
<li><p><strong>Función <code>train</code>:</strong> Al igual que las funciones utilizadas en <code>train_seq_num.py</code> y <code>train_math.py</code>, utiliza un bucle de entrenamiento estándar para entrenar el modelo.</p></li>
</ul>
</section>
<section id="generación-de-texto-generate_sql-inferencia-de-consultas-sql-a-partir-de-preguntas" class="level4">
<h4 class="anchored" data-anchor-id="generación-de-texto-generate_sql-inferencia-de-consultas-sql-a-partir-de-preguntas">Generación de texto (<code>generate_sql</code>): Inferencia de consultas SQL a partir de preguntas</h4>
<ul>
<li><strong>Función <code>generate_sql</code>:</strong> Utiliza el modelo entrenado para generar una consulta SQL a partir de una pregunta en lenguaje natural.
<ul>
<li>Primero, agrega un token <code>&lt;sep&gt;</code> a la pregunta ingresada para formar un prompt en el formato <code>"pregunta &lt;sep&gt; "</code>. Este prompt informa claramente al modelo que la pregunta ha terminado y debe generar una consulta SQL.</li>
<li><strong>Importancia del procesamiento de tokens de padding:</strong> Los datos de entrenamiento se completan hasta la longitud máxima (<code>max_length</code>) con un token <code>&lt;eos&gt;</code>. Sin embargo, si los datos de entrenamiento solo contienen <code>"pregunta &lt;sep&gt;"</code> sin la parte SQL ni el token <code>&lt;eos&gt;</code> (es decir, en el formato <code>"pregunta &lt;sep&gt; &lt;pad&gt; &lt;pad&gt; ..."</code>), el modelo no aprenderá qué generar después del token <code>&lt;sep&gt;</code>. Como resultado, durante la etapa de generación, el modelo podría generar solo tokens de padding después de <code>&lt;sep&gt;</code>, o incluso una cadena vacía. Para evitar esto, los datos de entrenamiento deben estar en el formato <code>"pregunta &lt;sep&gt; SQL&lt;eos&gt;"</code>.</li>
<li>Ajusta la diversidad de las consultas SQL generadas utilizando el parámetro <code>temperature</code>.</li>
<li>El modelo detiene la generación de la consulta cuando genera un token <code>&lt;eos&gt;</code> o <code>&lt;pad&gt;</code>.</li>
</ul></li>
</ul>
</section>
<section id="análisis-de-resultados" class="level4">
<h4 class="anchored">Análisis de resultados</h4>
<ul>
<li><strong>Salida de muestra:</strong> Antes del entrenamiento, se muestran 3 muestras del conjunto de datos WikiSQL para verificar el formato de los datos.</li>
<li><strong>Resultado del entrenamiento:</strong> Se puede confirmar que el modelo está aprendiendo a convertir preguntas en lenguaje natural en consultas SQL observando la disminución de la pérdida (loss) durante el proceso de entrenamiento.</li>
<li><strong>Resultado de generación:</strong> Se evalúa las consultas SQL generadas al ingresar preguntas del conjunto de datos de validación. Se examina principalmente si las consultas SQL generadas son gramaticalmente correctas y reflejan con precisión el significado de las preguntas. <code>train_sql.py</code> es un ejemplo que muestra cómo utilizar el modelo <code>simple_mistral</code> para realizar la tarea de procesamiento de lenguaje natural más compleja de conversión de lenguaje natural a SQL. Este ejemplo enfatiza la importancia de utilizar adecuadamente tokens especiales (<code>&lt;sep&gt;</code>, <code>&lt;eos&gt;</code>, <code>&lt;pad&gt;</code>) en el proceso de preprocesamiento de datos, y cómo la composición de los datos de entrenamiento afecta las capacidades de generación del modelo.</li>
</ul>
<div id="cell-63" class="cell" data-execution_count="9">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> T5Tokenizer, get_cosine_schedule_with_warmup</span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_09.mistral.examples.train_sql <span class="im">import</span> MistralConfig, MistralForCausalLM, WikiSQLDataset, generate_sql</span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> DataLoader</span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-7"><a href="#cb39-7" aria-hidden="true" tabindex="-1"></a>random.seed(<span class="dv">42</span>)</span>
<span id="cb39-8"><a href="#cb39-8" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">42</span>)</span>
<span id="cb39-9"><a href="#cb39-9" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> torch.device(<span class="st">"cuda"</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">"cpu"</span>)</span>
<span id="cb39-10"><a href="#cb39-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-11"><a href="#cb39-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Use T5Tokenizer as the tokenizer (use T5's vocab_size and pad/eos tokens)</span></span>
<span id="cb39-12"><a href="#cb39-12" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> T5Tokenizer.from_pretrained(<span class="st">"t5-small"</span>)</span>
<span id="cb39-13"><a href="#cb39-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-14"><a href="#cb39-14" aria-hidden="true" tabindex="-1"></a><span class="co"># WikiSQL dataset (training: train, evaluation: validation)</span></span>
<span id="cb39-15"><a href="#cb39-15" aria-hidden="true" tabindex="-1"></a>max_length <span class="op">=</span> <span class="dv">128</span></span>
<span id="cb39-16"><a href="#cb39-16" aria-hidden="true" tabindex="-1"></a>train_dataset <span class="op">=</span> WikiSQLDataset(<span class="st">"train"</span>, tokenizer, max_length<span class="op">=</span>max_length)</span>
<span id="cb39-17"><a href="#cb39-17" aria-hidden="true" tabindex="-1"></a>train_loader <span class="op">=</span> DataLoader(train_dataset, batch_size<span class="op">=</span><span class="dv">32</span>, shuffle<span class="op">=</span><span class="va">True</span>, num_workers<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb39-18"><a href="#cb39-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-19"><a href="#cb39-19" aria-hidden="true" tabindex="-1"></a>valid_dataset <span class="op">=</span> WikiSQLDataset(<span class="st">"validation"</span>, tokenizer, max_length<span class="op">=</span>max_length)</span>
<span id="cb39-20"><a href="#cb39-20" aria-hidden="true" tabindex="-1"></a>valid_loader <span class="op">=</span> DataLoader(valid_dataset, batch_size<span class="op">=</span><span class="dv">1</span>, shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb39-21"><a href="#cb39-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-22"><a href="#cb39-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Model configuration: Use MistralConfig and MistralForCausalLM provided by simple_mistral.py</span></span>
<span id="cb39-23"><a href="#cb39-23" aria-hidden="true" tabindex="-1"></a><span class="co"># The model size is adjusted for educational purposes.</span></span>
<span id="cb39-24"><a href="#cb39-24" aria-hidden="true" tabindex="-1"></a>config <span class="op">=</span> MistralConfig(</span>
<span id="cb39-25"><a href="#cb39-25" aria-hidden="true" tabindex="-1"></a>    vocab_size<span class="op">=</span>tokenizer.vocab_size,</span>
<span id="cb39-26"><a href="#cb39-26" aria-hidden="true" tabindex="-1"></a>    hidden_size<span class="op">=</span><span class="dv">512</span>,</span>
<span id="cb39-27"><a href="#cb39-27" aria-hidden="true" tabindex="-1"></a>    intermediate_size<span class="op">=</span><span class="dv">2048</span>,</span>
<span id="cb39-28"><a href="#cb39-28" aria-hidden="true" tabindex="-1"></a>    num_hidden_layers<span class="op">=</span><span class="dv">4</span>,</span>
<span id="cb39-29"><a href="#cb39-29" aria-hidden="true" tabindex="-1"></a>    num_attention_heads<span class="op">=</span><span class="dv">8</span>,</span>
<span id="cb39-30"><a href="#cb39-30" aria-hidden="true" tabindex="-1"></a>    num_key_value_heads<span class="op">=</span><span class="dv">4</span>,     <span class="co"># num_attention_heads % num_key_value_heads == 0 must be true</span></span>
<span id="cb39-31"><a href="#cb39-31" aria-hidden="true" tabindex="-1"></a>    max_position_embeddings<span class="op">=</span>max_length,</span>
<span id="cb39-32"><a href="#cb39-32" aria-hidden="true" tabindex="-1"></a>    sliding_window<span class="op">=</span>max_length,</span>
<span id="cb39-33"><a href="#cb39-33" aria-hidden="true" tabindex="-1"></a>    use_cache<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb39-34"><a href="#cb39-34" aria-hidden="true" tabindex="-1"></a>    use_return_dict<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb39-35"><a href="#cb39-35" aria-hidden="true" tabindex="-1"></a>    pad_token_id<span class="op">=</span>tokenizer.pad_token_id,  <span class="co"># Set the pad token id.</span></span>
<span id="cb39-36"><a href="#cb39-36" aria-hidden="true" tabindex="-1"></a>    bos_token_id<span class="op">=</span>tokenizer.bos_token_id,</span>
<span id="cb39-37"><a href="#cb39-37" aria-hidden="true" tabindex="-1"></a>    eos_token_id<span class="op">=</span>tokenizer.eos_token_id</span>
<span id="cb39-38"><a href="#cb39-38" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb39-39"><a href="#cb39-39" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> MistralForCausalLM(config).to(device)</span>
<span id="cb39-40"><a href="#cb39-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-41"><a href="#cb39-41" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.AdamW(model.parameters(), lr<span class="op">=</span><span class="fl">1e-4</span>)</span>
<span id="cb39-42"><a href="#cb39-42" aria-hidden="true" tabindex="-1"></a>num_epochs <span class="op">=</span> <span class="dv">8</span>  <span class="co"># Set the number of epochs small for the example</span></span>
<span id="cb39-43"><a href="#cb39-43" aria-hidden="true" tabindex="-1"></a>total_training_steps <span class="op">=</span> num_epochs <span class="op">*</span> <span class="bu">len</span>(train_loader)</span>
<span id="cb39-44"><a href="#cb39-44" aria-hidden="true" tabindex="-1"></a>scheduler <span class="op">=</span> get_cosine_schedule_with_warmup(</span>
<span id="cb39-45"><a href="#cb39-45" aria-hidden="true" tabindex="-1"></a>    optimizer,</span>
<span id="cb39-46"><a href="#cb39-46" aria-hidden="true" tabindex="-1"></a>    num_warmup_steps<span class="op">=</span><span class="bu">len</span>(train_loader) <span class="op">//</span> <span class="dv">5</span>,</span>
<span id="cb39-47"><a href="#cb39-47" aria-hidden="true" tabindex="-1"></a>    num_training_steps<span class="op">=</span>total_training_steps</span>
<span id="cb39-48"><a href="#cb39-48" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb39-49"><a href="#cb39-49" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Added code: Output WikiSQL data samples</span></span>
<span id="cb39-50"><a href="#cb39-50" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"=== WikiSQL Data Sample Output ==="</span>)</span>
<span id="cb39-51"><a href="#cb39-51" aria-hidden="true" tabindex="-1"></a>sample_count <span class="op">=</span> <span class="dv">3</span>  <span class="co"># Number of examples to output</span></span>
<span id="cb39-52"><a href="#cb39-52" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(sample_count):</span>
<span id="cb39-53"><a href="#cb39-53" aria-hidden="true" tabindex="-1"></a>    input_ids, labels <span class="op">=</span> train_dataset[i]</span>
<span id="cb39-54"><a href="#cb39-54" aria-hidden="true" tabindex="-1"></a>    decoded_text <span class="op">=</span> tokenizer.decode(input_ids, skip_special_tokens<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb39-55"><a href="#cb39-55" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Sample </span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>decoded_text<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb39-56"><a href="#cb39-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-57"><a href="#cb39-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-58"><a href="#cb39-58" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Start training..."</span>)</span>
<span id="cb39-59"><a href="#cb39-59" aria-hidden="true" tabindex="-1"></a>train(model, train_loader, optimizer, scheduler, num_epochs, device)</span>
<span id="cb39-60"><a href="#cb39-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-61"><a href="#cb39-61" aria-hidden="true" tabindex="-1"></a><span class="co"># Save the model: Save the final model to a file.</span></span>
<span id="cb39-62"><a href="#cb39-62" aria-hidden="true" tabindex="-1"></a>torch.save(model.state_dict(), <span class="st">"final_nl2sql_model.pth"</span>)</span>
<span id="cb39-63"><a href="#cb39-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-64"><a href="#cb39-64" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluation code part</span></span>
<span id="cb39-65"><a href="#cb39-65" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">=== Evaluation Examples ==="</span>)</span>
<span id="cb39-66"><a href="#cb39-66" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, (input_ids, labels) <span class="kw">in</span> <span class="bu">enumerate</span>(valid_loader):</span>
<span id="cb39-67"><a href="#cb39-67" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> i <span class="op">&gt;=</span> <span class="dv">10</span>:</span>
<span id="cb39-68"><a href="#cb39-68" aria-hidden="true" tabindex="-1"></a>        <span class="cf">break</span></span>
<span id="cb39-69"><a href="#cb39-69" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Keep special tokens with skip_special_tokens=False.</span></span>
<span id="cb39-70"><a href="#cb39-70" aria-hidden="true" tabindex="-1"></a>    full_text <span class="op">=</span> tokenizer.decode(input_ids[<span class="dv">0</span>], skip_special_tokens<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb39-71"><a href="#cb39-71" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Unify the tokens "sep&gt;" and "eos&gt;" to "&lt;sep&gt;" and "&lt;eos&gt;" respectively.</span></span>
<span id="cb39-72"><a href="#cb39-72" aria-hidden="true" tabindex="-1"></a>    full_text <span class="op">=</span> full_text.replace(<span class="st">"sep&gt;"</span>, <span class="st">"&lt;sep&gt;"</span>).replace(<span class="st">"eos&gt;"</span>, <span class="st">"&lt;eos&gt;"</span>)</span>
<span id="cb39-73"><a href="#cb39-73" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb39-74"><a href="#cb39-74" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="st">"&lt;sep&gt;"</span> <span class="kw">in</span> full_text:</span>
<span id="cb39-75"><a href="#cb39-75" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Split based on the first &lt;sep&gt;, then join all subsequent parts to restore the complete SQL.</span></span>
<span id="cb39-76"><a href="#cb39-76" aria-hidden="true" tabindex="-1"></a>        parts <span class="op">=</span> full_text.split(<span class="st">"&lt;sep&gt;"</span>)</span>
<span id="cb39-77"><a href="#cb39-77" aria-hidden="true" tabindex="-1"></a>        question <span class="op">=</span> parts[<span class="dv">0</span>].strip()</span>
<span id="cb39-78"><a href="#cb39-78" aria-hidden="true" tabindex="-1"></a>        target_sql <span class="op">=</span> <span class="st">"&lt;sep&gt;"</span>.join(parts[<span class="dv">1</span>:]).strip()</span>
<span id="cb39-79"><a href="#cb39-79" aria-hidden="true" tabindex="-1"></a>        <span class="co"># If target_sql ends with "&lt;eos&gt;", remove it.</span></span>
<span id="cb39-80"><a href="#cb39-80" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> target_sql.endswith(<span class="st">"&lt;eos&gt;"</span>):</span>
<span id="cb39-81"><a href="#cb39-81" aria-hidden="true" tabindex="-1"></a>            target_sql <span class="op">=</span> target_sql[:<span class="op">-</span><span class="bu">len</span>(<span class="st">"&lt;eos&gt;"</span>)].strip()</span>
<span id="cb39-82"><a href="#cb39-82" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb39-83"><a href="#cb39-83" aria-hidden="true" tabindex="-1"></a>        question <span class="op">=</span> full_text.strip()</span>
<span id="cb39-84"><a href="#cb39-84" aria-hidden="true" tabindex="-1"></a>        target_sql <span class="op">=</span> <span class="st">""</span></span>
<span id="cb39-85"><a href="#cb39-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-86"><a href="#cb39-86" aria-hidden="true" tabindex="-1"></a>    generated_sql <span class="op">=</span> generate_sql(model, tokenizer, question, max_length, device, temperature<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb39-87"><a href="#cb39-87" aria-hidden="true" tabindex="-1"></a>    <span class="co"># If there is a "sep&gt;" token in generated_sql, extract the part after that token to use.</span></span>
<span id="cb39-88"><a href="#cb39-88" aria-hidden="true" tabindex="-1"></a>    <span class="co"># if "sep&gt;" in generated_sql:</span></span>
<span id="cb39-89"><a href="#cb39-89" aria-hidden="true" tabindex="-1"></a>    <span class="co">#     generated_sql = generated_sql.split("sep&gt;", 1)[1].strip()</span></span>
<span id="cb39-90"><a href="#cb39-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-91"><a href="#cb39-91" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Question: </span><span class="sc">{</span>question<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb39-92"><a href="#cb39-92" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Target SQL: </span><span class="sc">{</span>target_sql<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb39-93"><a href="#cb39-93" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Generated SQL: </span><span class="sc">{</span>generated_sql<span class="sc">}</span><span class="ch">\n</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stderr">
<pre><code>You are using the default legacy behaviour of the &lt;class 'transformers.models.t5.tokenization_t5.T5Tokenizer'&gt;. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>=== WikiSQL Data Sample Output ===
Sample 1: Tell me what the notes are for South Australia sep&gt; SELECT Notes FROM table WHERE Current slogan = SOUTH AUSTRALIA eos&gt;
Sample 2: What is the current series where the new series began in June 2011? sep&gt; SELECT Current series FROM table WHERE Notes = New series began in June 2011 eos&gt;
Sample 3: What is the format for South Australia? sep&gt; SELECT Format FROM table WHERE State/territory = South Australia eos&gt;
Start training...
Epoch 1/8, Average Loss: 10.5748, LR: 0.000000
Epoch 2/8, Average Loss: 9.7000, LR: 0.000001
Epoch 3/8, Average Loss: 7.2037, LR: 0.000001
Epoch 4/8, Average Loss: 5.5372, LR: 0.000001
Epoch 5/8, Average Loss: 4.5961, LR: 0.000001
Epoch 6/8, Average Loss: 4.0102, LR: 0.000002
Epoch 7/8, Average Loss: 3.6296, LR: 0.000002
Epoch 8/8, Average Loss: 3.3907, LR: 0.000002

=== Evaluation Examples ===
Question: Who was the minister for the CSV party with a present day end date? &lt;unk&gt;
Target SQL: SELECT Minister FROM table WHERE Party = csv AND End date = present day &lt;unk&gt; &lt;eos&gt;&lt;/s&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;
Generated SQL: Who was the minister for the CSV party with a present day end date? sep&gt; FROM table WHERE60ed = s eos&gt;

Question: What is the production number of From Hare to Heir? &lt;unk&gt;
Target SQL: SELECT SUM Production Number FROM table WHERE Title = from hare to heir &lt;unk&gt; &lt;eos&gt;&lt;/s&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;
Generated SQL: What is the production number of From Hare to Heir? sep&gt;os FROM table WHERE Score = 0 eos&gt;

Question: What was the score on January 12? &lt;unk&gt;
Target SQL: SELECT Score FROM table WHERE Date = january 12 &lt;unk&gt; &lt;eos&gt;&lt;/s&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;
Generated SQL: What was the score on January 12? sep&gt;a Record FROM table WHERE #  eos&gt;

Question: The race tony bettenhausen 200 has what smallest rd? &lt;unk&gt;
Target SQL: SELECT MIN Rd FROM table WHERE Name = Tony Bettenhausen 200 &lt;unk&gt; &lt;eos&gt;&lt;/s&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;
Generated SQL: The race tony bettenhausen 200 has what smallest rd? sep&gt; Team FROM table WHERE Player = a ODi a eos&gt;

Question: what is the club that was founded before 2007, joined prsl in 2008 and the stadium is yldefonso solá morales stadium? &lt;unk&gt;
Target SQL: SELECT Club FROM table WHERE Founded &lt;unk&gt; 2007 AND Joined PRSL = 2008 AND Stadium = yldefonso solá morales stadium &lt;unk&gt; &lt;eos&gt;&lt;/s&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;
Generated SQL: what is the club that was founded before 2007, joined prsl in 2008 and the stadium is yldefonso solá morales stadium? sep&gt; ( for  for the highest FROM table WHERE Team = Rank  of vir AND COUNT  eos&gt;

Question: Who is the co-contestant (yaar vs. Pyaar) with Vishal Singh as the main contestant? &lt;unk&gt;
Target SQL: SELECT Co-contestant (Yaar vs. Pyaar) FROM table WHERE Main contestant = vishal singh &lt;unk&gt; &lt;eos&gt;&lt;/s&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;
Generated SQL: Who is the co-contestant (yaar vs. Pyaar) with Vishal Singh as the main contestant? sep&gt; SELECT  Record FROM table WHERE ts = 9kt AND Date = a eos&gt;

Question: What season did SV Darmstadt 98 end up at RL Süd (1st)? &lt;unk&gt;
Target SQL: SELECT Season FROM table WHERE RL Süd (1st) = SV Darmstadt 98 &lt;unk&gt; &lt;eos&gt;&lt;/s&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;
Generated SQL: What season did SV Darmstadt 98 end up at RL Süd (1st)? sep&gt; FROM table WHERE Away team = s s eos&gt;

Question: What character was portrayed by the same actor for 12 years on Neighbours? &lt;unk&gt;
Target SQL: SELECT Character FROM table WHERE Duration = 12 years AND Soap Opera = neighbours &lt;unk&gt; &lt;eos&gt;&lt;/s&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;
Generated SQL: What character was portrayed by the same actor for 12 years on Neighbours? sep&gt;FS Class FROM table WHERE Date = m ja eos&gt;

Question: What was the score between Marseille and Manchester United on the second leg of the Champions League Round of 16? &lt;unk&gt;
Target SQL: SELECT 2nd leg score** FROM table WHERE Opponent = Marseille &lt;unk&gt; &lt;eos&gt;&lt;/s&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;
Generated SQL: What was the score between Marseille and Manchester United on the second leg of the Champions League Round of 16? sep&gt;hes&gt; d FROM table WHERE Date =s eos&gt;

Question: Who was the Man of the Match when the opponent was Milton Keynes Lightning and the venue was Away? &lt;unk&gt;
Target SQL: SELECT Man of the Match FROM table WHERE Opponent = milton keynes lightning AND Venue = away &lt;unk&gt; &lt;eos&gt;&lt;/s&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;
Generated SQL: Who was the Man of the Match when the opponent was Milton Keynes Lightning and the venue was Away? sep&gt; with Cap? sep&gt; SELECT Home team score FROM table WHERE Wilson AND jump = s eos&gt;
</code></pre>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled" title="Haga clic para ver el contenido (buceo profundo: diseño y depuración sólida de transformadores - guía práctica)">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Haga clic para ver el contenido (buceo profundo: diseño y depuración sólida de transformadores - guía práctica)
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<section id="diseño-y-depuración-sólida-de-transformers---guía-práctica" class="level2 callout-body-container callout-body">
<h2 class="anchored" data-anchor-id="diseño-y-depuración-sólida-de-transformers---guía-práctica">Diseño y depuración sólida de transformers - guía práctica</h2>
<p>Incluir arquitecturas eficientes como <code>simple_mistral</code> hace que construir modelos de transformers desde cero sea una tarea difícil pero gratificante. Aunque la comprensión teórica es importante, en el proceso de implementación real a menudo surgen errores sutiles y cuellos de botella de rendimiento. En esta sección, examinamos profundamente estrategias prácticas para diseñar, implementar y depurar transformers, centrándonos especialmente en los componentes utilizados en <code>simple_mistral</code> (RoPE, RMSNorm, Attention). Tratamos extensivamente las pruebas unitarias y discutimos otras técnicas de depuración y diseño esenciales.</p>
<section id="el-papel-crucial-de-las-pruebas-unitarias" class="level3">
<h3 class="anchored" data-anchor-id="el-papel-crucial-de-las-pruebas-unitarias">1. El papel crucial de las pruebas unitarias</h3>
<p>Cuando se construyen modelos complejos como transformers, las pruebas unitarias no son una <em>opción</em> sino una <em>necesidad</em>. Ayudan a detectar errores temprano, prevenir regresiones y brindar confianza en la implementación. Un modelo bien probado es un modelo <em>confiable</em>.</p>
<p>Cada fuente de modelos tiene <strong>un directorio llamado tests con pruebas unitarias (por ejemplo: mistral/tests, phi3/tests)</strong></p>
<p><strong>Por qué las pruebas unitarias son importantes para los transformers</strong></p>
<ul>
<li><strong>Complejidad:</strong> Los transformers están compuestos por varios módulos interactivos (Attention, Redes Feedforward, Normalización, Embedding). Los errores pueden surgir fácilmente en cualquiera de estos componentes.</li>
<li><strong>Errores sutiles:</strong> Muchos errores de transformers no son <em>inmediatamente evidentes</em>. En lugar de causar un fallo, pueden resultar en una disminución del rendimiento o salidas incorrectas. Las pruebas unitarias pueden detectar estos errores sutiles.</li>
<li><strong>Estabilidad numérica:</strong> Los modelos de aprendizaje profundo, especialmente cuando se utilizan técnicas como la precisión mixta, son vulnerables a problemas numéricos (NaN, Inf, Gradientes desaparecidos/explotados). Las pruebas unitarias ayudan a detectar estos problemas.</li>
<li><strong>Refactorización y modificaciones:</strong> Es inevitable que el código cambie al mejorar y optimizar el modelo. Las pruebas unitarias aseguran que las funciones existentes no se dañen debido a los cambios.</li>
<li><strong>Reproducibilidad:</strong> Pruebas bien definidas contribuyen a la reproducibilidad de los resultados.</li>
<li><strong>Caché (<code>past_key_value</code>):</strong> Cuando el modelo utiliza cachés como <code>past_key_values</code>, es importante verificar mediante pruebas unitarias que no haya errores relacionados con shape, dtype o device.</li>
</ul>
<p><strong>Principios clave para pruebas unitarias efectivas</strong></p>
<ul>
<li><strong>Desarrollo guiado por pruebas (Test-Driven Development, TDD):</strong> Idealmente, se deben escribir las pruebas unitarias <em>antes</em> de codificar el modelo. Esto permite pensar claramente en el comportamiento esperado de cada componente.</li>
<li><strong>Modularidad:</strong> Diseña el código de manera modular con funciones y clases pequeñas y bien definidas. Esto facilita aislar y probar componentes individuales.</li>
<li><strong>Cobertura exhaustiva:</strong> Apunta a una alta cobertura de pruebas. Prueba todas las funciones y métodos importantes de la clase.</li>
<li><strong>Casos límite:</strong> No solo pruebes los “casos normales”. Prueba también casos límite, condiciones frontera y escenarios potencialmente erróneos (por ejemplo: secuencias de longitud 0, lotes con un solo elemento, varios tipos de datos).</li>
<li><strong>Assertions:</strong> Usa libremente las assertions (<code>assert</code>) para verificar que el código funcione como se espera. Escribe las assertions lo más específicas posible. No solo asegúrate de que el código se ejecute sin fallos, sino también de que la <em>salida</em> sea correcta.</li>
<li><strong>Pytest:</strong> Aunque los ejemplos de este capítulo utilizan el módulo <code>unittest</code>, el marco <code>pytest</code> es el más recomendado en Python.</li>
</ul>
<p><strong>Áreas de enfoque para pruebas unitarias de transformers</strong> * <strong>Forma de entrada/salida:</strong> El tipo más común de error en la implementación del transformer es un tensor con una forma incorrecta. Cada prueba debe incluir una afirmación para verificar la forma del tensor de salida. * <strong>Tipo de datos:</strong> Verifique que los tensores tengan el tipo de datos esperado (por ejemplo, <code>torch.float32</code>, <code>torch.float16</code>, <code>torch.int64</code>). * <strong>Colocación de dispositivos:</strong> Si está utilizando GPU, verifique que los tensores estén en el dispositivo correcto (CPU o GPU). * <strong>Estabilidad numérica:</strong> Asegúrese de que no haya NaN (Not a Number) ni Inf en los tensores después de operaciones como softmax o normalización. * <strong>Cálculo de gradientes:</strong> Verifique que los gradientes se calculen correctamente para todos los parámetros entrenables. * <strong>Caché (<code>past_key_value</code>):</strong> Como se mencionó anteriormente, el mecanismo de caché es una causa frecuente de errores. Pruebe exhaustivamente la decodificación incremental (incremental decoding).</p>
<p><strong>Ejemplos detallados de pruebas unitarias (RoPE, RMSNorm, Attention)</strong></p>
<div class="sourceCode" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="co"># test_rope.py</span></span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> unittest</span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_09.mistral.simple_mistral <span class="im">import</span> MistralRotaryEmbedding, apply_rotary_pos_emb, rotate_half</span>
<span id="cb42-5"><a href="#cb42-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-6"><a href="#cb42-6" aria-hidden="true" tabindex="-1"></a><span class="co"># ...</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="co"># test_rms_norm.py</span></span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pytest</span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_09.mistral.simple_mistral <span class="im">import</span> PhiMiniRMSNorm</span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a><span class="co"># ... </span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="co"># test_attention.py</span></span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pytest</span>
<span id="cb44-4"><a href="#cb44-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_09.mistral.simple_mistral <span class="im">import</span> PhiMiniConfig, PhiMiniAttention</span>
<span id="cb44-5"><a href="#cb44-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-6"><a href="#cb44-6" aria-hidden="true" tabindex="-1"></a><span class="co"># ... </span></span>
<span id="cb44-7"><a href="#cb44-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-8"><a href="#cb44-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Pruebas adicionales para la atención</span></span>
<span id="cb44-9"><a href="#cb44-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-10"><a href="#cb44-10" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> test_phi_mini_attention_zero_length_initial():</span>
<span id="cb44-11"><a href="#cb44-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ... </span></span>
<span id="cb44-12"><a href="#cb44-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-13"><a href="#cb44-13" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> test_phi_mini_attention_single_token_initial():</span>
<span id="cb44-14"><a href="#cb44-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ... </span></span>
<span id="cb44-15"><a href="#cb44-15" aria-hidden="true" tabindex="-1"></a><span class="at">@pytest.mark.parametrize</span>(<span class="st">"batch_size"</span>, [<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">4</span>, <span class="dv">8</span>])</span>
<span id="cb44-16"><a href="#cb44-16" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> test_phi_mini_attention_various_batch_sizes(batch_size):</span>
<span id="cb44-17"><a href="#cb44-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ...</span></span>
<span id="cb44-18"><a href="#cb44-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-19"><a href="#cb44-19" aria-hidden="true" tabindex="-1"></a><span class="at">@pytest.mark.parametrize</span>(<span class="st">"num_heads, num_kv_heads"</span>, [(<span class="dv">8</span>, <span class="dv">8</span>), (<span class="dv">8</span>, <span class="dv">4</span>), (<span class="dv">8</span>, <span class="dv">1</span>)]) <span class="co"># Casos MHA, GQA</span></span>
<span id="cb44-20"><a href="#cb44-20" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> test_phi_mini_attention_different_head_configs(num_heads, num_kv_heads):</span>
<span id="cb44-21"><a href="#cb44-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ... </span></span>
<span id="cb44-22"><a href="#cb44-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-23"><a href="#cb44-23" aria-hidden="true" tabindex="-1"></a><span class="at">@pytest.mark.parametrize</span>(<span class="st">"dtype"</span>, [torch.float16, torch.bfloat16, torch.float32])</span>
<span id="cb44-24"><a href="#cb44-24" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> test_phi_mini_attention_mixed_precision(dtype):</span>
<span id="cb44-25"><a href="#cb44-25" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ... </span></span>
<span id="cb44-26"><a href="#cb44-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-27"><a href="#cb44-27" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> test_phi_mini_attention_combined_mask():</span>
<span id="cb44-28"><a href="#cb44-28" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ... </span></span>
<span id="cb44-29"><a href="#cb44-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-30"><a href="#cb44-30" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> test_phi_mini_attention_long_sequence():</span>
<span id="cb44-31"><a href="#cb44-31" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ... </span></span>
<span id="cb44-32"><a href="#cb44-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-33"><a href="#cb44-33" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> test_phi_mini_attention_output_attentions_with_cache():</span>
<span id="cb44-34"><a href="#cb44-34" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ... </span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="más-allá-de-las-pruebas-unitarias-otras-estrategias-de-depuración" class="level3">
<h3 class="anchored" data-anchor-id="más-allá-de-las-pruebas-unitarias-otras-estrategias-de-depuración">2. Más allá de las pruebas unitarias: otras estrategias de depuración</h3>
<p>Aunque las pruebas unitarias son fundamentales, no son la única herramienta en el conjunto de herramientas de depuración. Las siguientes son otras estrategias importantes.</p>
<p><strong>1. Registro (Logging)</strong> * <strong>Registro estratégico:</strong> Agregar declaraciones de registro (sentencias <code>print</code> o, preferiblemente, el módulo <code>logging</code>) al código para rastrear los valores de las variables principales, la forma del tensor y el flujo de ejecución. Esto puede ayudar a identificar rápidamente dónde ocurre un problema. * <strong>Control de nivel de detalle:</strong> Hacer que el registro sea detallado pero proporcionar una manera de controlar el nivel de detalle (por ejemplo, usando banderas de línea de comandos o variables de entorno). Esto permite obtener información detallada durante la depuración pero evitar demasiada salida cuando se está funcionando normalmente.</p>
<p><strong>2. Visualización (Visualization)</strong></p>
<ul>
<li><strong>Pesos de atención:</strong> Visualizar los pesos de atención para verificar en qué tokens el modelo está prestando atención. Esto puede ayudar a detectar problemas con el mecanismo de atención o las embeddings de posición.</li>
<li><strong>Activaciones:</strong> Visualizar las activaciones de las neuronas en el modelo. Esto puede ayudar a identificar neuronas muertas (neuronas que siempre están inactivas) o saturadas (neuronas que siempre están al máximo o mínimo).</li>
<li><strong>Gradientes:</strong> Visualizar los gradientes durante el entrenamiento. Esto puede ayudar a detectar problemas de desaparición o explosión de gradientes.</li>
</ul>
<p><strong>3. Depuración numérica (Numerical Debugging)</strong></p>
<ul>
<li><strong>Verificación de NaN/Inf:</strong> Usar <code>torch.isnan()</code> y <code>torch.isinf()</code> para verificar si un tensor contiene valores NaN o Inf. Esto a menudo indica inestabilidad numérica. <code>python     if torch.isnan(tensor).any() or torch.isinf(tensor).any():         print("¡NaN o Inf detectados!")</code></li>
<li><strong>Verificación de gradientes:</strong> Usar <code>torch.autograd.gradcheck</code> para verificar si una función autograd personalizada está calculando los gradientes correctamente. Esto es particularmente importante al implementar mecanismos de atención personalizados u otras tareas complejas.</li>
<li><strong>Casos de prueba pequeños:</strong> Crear casos de prueba muy pequeños y simples (por ejemplo, una sola capa, un vocabulario pequeño, secuencias cortas) para los cuales se pueda calcular manualmente la salida esperada. Esto puede ayudar a aislar bugs.</li>
</ul>
<p><strong>4. Depuradores (pdb, depuradores de IDE)</strong></p>
<ul>
<li><strong><code>pdb</code> (Python Debugger):</strong> Usar el depurador integrado de Python (<code>pdb</code>) para ejecutar el código línea por línea, examinar variables y establecer puntos de interrupción. <code>python     import pdb; pdb.set_trace()  # Agregar esta línea para establecer un punto de interrupción.</code></li>
<li><strong>Depuradores de IDE:</strong> La mayoría de los IDE (como PyCharm, VS Code, etc.) tienen depuradores integrados con interfaces más amigables para la depuración.</li>
</ul>
<p><strong>5. Perfilado (Profiling)</strong></p>
<ul>
<li><strong>PyTorch Profiler:</strong> Usar el perfilador de PyTorch para identificar cuellos de botella de rendimiento en el código. Esto puede ayudar a encontrar áreas para optimizar la velocidad o el uso de memoria.</li>
<li><strong>Perfilado de memoria:</strong> Usar herramientas como <code>memory_profiler</code> para rastrear el uso de memoria y detectar posibles fugas de memoria.</li>
</ul>
<p><strong>6. Principios de diseño del modelo para facilitar la depuración</strong> * <strong>Manténgalo simple (Keep it Simple):</strong> Comience con un modelo simple y vaya aumentando gradualmente la complejidad. De esta manera, será más fácil aislar los errores. * <strong>Modularidad (Modularity):</strong> Divida el código en módulos pequeños y bien definidos. Esto facilita la prueba y depuración de componentes individuales. * <strong>Assertions:</strong> Use assertions para verificar las condiciones esperadas y detectar errores temprano. * <strong>Comentarios y documentación (Comments and Documentation):</strong> Escribe comentarios claros y concisos, así como documentación, para explicar la lógica del código. Esto ayuda a los usuarios (y a otras personas) a comprender el código e identificar posibles problemas. * <strong>Reproducibilidad (Reproducibility):</strong> Use una seed de random fija para que los resultados sean reproducibles. Esto es importante tanto para la depuración como para comparar diferentes configuraciones de modelos. * <strong>Sobreajuste a un solo lote/pequeño conjunto de datos (Overfitting):</strong> Ajuste el modelo a un pequeño conjunto de datos antes de entrenarlo con un conjunto de datos grande.</p>
<p><strong>7. Errores comunes y formas de evitarlos</strong></p>
<ul>
<li><strong>Incorrect Tensor Shapes:</strong> Verifique nuevamente la forma esperada de los tensores, especialmente después de operaciones como reshape, transpose, concatenate. Utilice <code>tensor.shape</code> frecuentemente en el proceso de depuración.</li>
<li><strong>Errores Off-by-One:</strong> Preste atención a la indexación, especialmente al trabajar con secuencias y embeddings de posición.</li>
<li><strong>Incompatibilidades de tipos de datos (Data Type Mismatches):</strong> Asegúrese de que los tensores tengan el tipo de datos correcto (por ejemplo, <code>float32</code> vs <code>float16</code>).</li>
<li><strong>Incompatibilidades de dispositivos (Device Mismatches):</strong> Verifique que todos los tensores estén en el mismo dispositivo (CPU o GPU).</li>
<li><strong>Variables no inicializadas (Uninitialized Variables):</strong> Inicialice todas las variables antes de usarlas.</li>
<li><strong>Máscaras incorrectas (Incorrect Masking):</strong> Si está usando una máscara de atención, asegúrese de que se aplique correctamente y no esté ocultando información importante.</li>
<li><strong>Uso incorrecto de <code>past_key_values</code>:</strong> Asegúrese de seguir el método correcto para su uso.</li>
</ul>
<p>Combinar estas técnicas de depuración con un entendimiento sólido de los principios básicos de los modelos de transformers permitirá resolver incluso los problemas más difíciles de implementación. La depuración es un proceso iterativo, por lo que tenga paciencia y use todas las herramientas de manera sistemática.</p>
</section>
</section>
</div>
</div>
</section>
</section>
</section>
<section id="gemma-revisión-del-último-modelo-de-código-abierto" class="level2">
<h2 class="anchored" data-anchor-id="gemma-revisión-del-último-modelo-de-código-abierto">9.7 Gemma: Revisión del último modelo de código abierto</h2>
<p>Gemma es el más reciente modelo de código abierto publicado por Google en febrero de 2024. Aunque no introduce cambios revolucionarios en la estructura del modelo en comparación con Mistral, refleja las tendencias actuales de los modelos y tiene valor para ser estudiado debido a su utilidad en ciertos contextos. Gemma adopta una arquitectura de decodificador único (Decoder-only) basada en Transformer, similar a LLaMA y Mistral.</p>
<section id="razones-para-revisar-gemma" class="level4">
<h4 class="anchored" data-anchor-id="razones-para-revisar-gemma">Razones para revisar Gemma</h4>
<ol type="1">
<li><p><strong>Reflejo de las tendencias actuales en modelos:</strong> Gemma incluye componentes comúnmente utilizados en los últimos modelos, como RoPE (Rotary Positional Embedding), RMSNorm (Root Mean Square Layer Normalization) y la función de activación GeGLU. Estos elementos contribuyen al rendimiento y a la eficiencia del modelo, y ayudan a comprender las tendencias actuales. RoPE codifica de manera eficiente información de posición relativa para mejorar la capacidad de procesar secuencias largas, mientras que RMSNorm aumenta la eficiencia computacional eliminando la operación de centrado en Layer Normalization. GeGLU es una variante de GLU (Gated Linear Unit) que aumenta la capacidad expresiva del modelo a través de la no linealidad.</p></li>
<li><p><strong>Diversas tamaños de modelos:</strong> Gemma está disponible en tamaños de 2B, 7B, 9B y 27B. Esto permite a los usuarios con recursos computacionales limitados experimentar con un modelo relativamente pequeño (2B), mientras que los modelos más grandes (27B) ofrecen un rendimiento superior pero requieren mayores recursos computacionales. Los usuarios pueden elegir el tamaño de modelo adecuado según su entorno y necesidades.</p></li>
<li><p><strong>Integración con el ecosistema de Google:</strong> Gemma está asociada al proyecto Gemini de Google y puede integrarse fácilmente con Google Cloud, Vertex AI, entre otros. Para los desarrolladores que utilizan principalmente la plataforma de Google, Gemma puede ser una opción útil. Vertex AI de Google Cloud proporciona una plataforma integral para el entrenamiento, despliegue y gestión de modelos de aprendizaje automático, y Gemma puede aumentar la productividad del desarrollo a través de su compatibilidad con estas plataformas.</p></li>
<li><p><strong>Accesibilidad de los modelos de código abierto:</strong> Gemma se publica bajo la licencia Apache 2.0, permitiendo un uso, distribución y modificación libres, incluyendo el uso comercial. | Característica | Gemma | Mistral | |——————|———————————|———————————-| | <strong>Fecha de lanzamiento</strong> | Febrero de 2024 | Septiembre de 2023 | | <strong>Tamaño del modelo</strong> | 2B, 7B, 9B, 27B | 7.3B | | <strong>Arquitectura base</strong> | Transformer (solo Decoder) | Transformer (solo Decoder) | | <strong>Embedding de posición</strong> | RoPE | RoPE | | <strong>Normalización</strong> | RMSNorm | RMSNorm | | <strong>Función de activación</strong> | GeGLU | SwiGLU | | <strong>Atención</strong> | Multi-Head Attention (MHA), GQA| Grouped-Query Attention (GQA), SWA | | <strong>Ventana de contexto</strong> | Máximo 8192 tokens | Máximo 131,000 tokens | | <strong>Características principales</strong>| Diversos tamaños, soporte del ecosistema Google, GeGLU, amplia ventana de contexto | GQA y SWA para inferencia eficiente, procesamiento de contexto largo | | <strong>Innovación (comparativa)</strong>| Baja | Alta |</p></li>
</ol>
<ul>
<li><strong>Similitudes:</strong> Gemma y Mistral son ambos modelos Transformer basados en solo Decoder y utilizan componentes similares como RoPE y RMSNorm. Estos componentes contribuyen a mejorar la eficiencia y el rendimiento del modelo.</li>
<li><strong>Diferencias:</strong>
<ul>
<li>Gemma utiliza GeGLU como función de activación, mientras que Mistral utiliza SwiGLU (una variante de SiLU). GeGLU separa la entrada en dos transformaciones lineales, una actúa como puerta y la otra se multiplica para generar el resultado.</li>
<li>Gemma emplea Multi-Head Attention (MHA) o Grouped-Query Attention (GQA), mientras que Mistral combina GQA con Sliding Window Attention (SWA) para aumentar la eficiencia. GQA reduce el número de cabezas K y V en comparación con las cabezas Q para disminuir el uso de memoria y cálculos. SWA genera máscaras para que cada token solo realice atención dentro de un rango fijo (ventana), lo que reduce la complejidad computacional.</li>
</ul></li>
</ul>
</section>
<section id="conclusión" class="level4">
<h4 class="anchored" data-anchor-id="conclusión">Conclusión</h4>
<p>Gemma, aunque no es tan innovadora en su estructura de modelo como Mistral, tiene las siguientes importancias como un modelo de código abierto reciente:</p>
<ul>
<li><strong>Comprensión de tendencias tecnológicas actuales:</strong> A través de Gemma, se puede entender la implementación y el funcionamiento de componentes ampliamente utilizados en modelos modernos como RoPE, RMSNorm y GeGLU.</li>
<li><strong>Opciones de modelo diversas:</strong> Gemma ofrece modelos de diversos tamaños (2B, 7B, 27B) para adaptarse a diferentes entornos de cómputo.</li>
<li><strong>Utilización del ecosistema Google:</strong> Para usuarios de la plataforma Google, Gemma puede ofrecer mejor integración y soporte en comparación con otros modelos.</li>
<li><strong>Accesibilidad al modelo de código abierto</strong>: Cualquiera puede acceder fácilmente y contribuir a la comunidad. Por lo tanto, es mejor centrarse en el valor práctico de Gemma como modelo abierto que refleja las últimas tendencias tecnológicas, y en la posibilidad de integración con el ecosistema de Google, más que en la innovación del propio modelo.</li>
</ul>
</section>
</section>
<section id="phi-3-un-modelo-de-lenguaje-pequeño-pero-potente" class="level2">
<h2 class="anchored" data-anchor-id="phi-3-un-modelo-de-lenguaje-pequeño-pero-potente">9.8 Phi-3 : un modelo de lenguaje pequeño pero potente</h2>
<p>En las secciones 9.6 y 9.7, examinamos los elementos clave de la arquitectura de modelos de lenguaje eficientes a través de los modelos Mistral y Gemma. En esta sección, implementaremos y analizaremos directamente el modelo Phi-3 Mini desarrollado por Microsoft, explorando el secreto detrás de su excelente rendimiento a pesar de su tamaño pequeño.</p>
<p>Phi-3 Mini es un modelo de lenguaje pequeño (SLM, Small Language Model) que Microsoft presentó en abril de 2024. Con 3.8B de parámetros, Phi-3 Mini muestra un rendimiento competitivo en varios benchmarks frente a modelos más grandes como Mistral (7B) y Gemma (7B), demostrando la posibilidad de modelos ligeros. En particular, Phi-3 Mini enfatiza la importancia de <strong>“datos de alta calidad”</strong> y <strong>“arquitectura eficiente”</strong>, señalando una nueva dirección más allá de la simple competencia en el tamaño del modelo. Esta filosofía se refleja bien en el eslogan “Textbooks Are All You Need”. <code>simple_phi3.py</code> es un código que implementa los componentes clave de Phi-3 Mini de manera simplificada. El código completo está en <code>chapter_09/phi3</code>.</p>
<section id="simple_phi3-modelo" class="level3">
<h3 class="anchored" data-anchor-id="simple_phi3-modelo">9.8.1 <code>simple_phi3</code> modelo</h3>
<p><code>simple_phi3</code> es un modelo implementado con fines educativos basado en Phi-3 Mini. En comparación con simple mistral del Capítulo 9.6, se tiene lo siguiente.</p>
<p><strong>Resumen de diferencias funcionales del modelo</strong></p>
<table class="caption-top table">
<colgroup>
<col style="width: 33%">
<col style="width: 33%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th>Función</th>
<th>Simple Phi-3</th>
<th>Simple Mistral</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Atención</td>
<td>Multi-Head Attention (MHA)</td>
<td>Grouped-Query Attention (GQA) + Sliding Window Attention (SWA)</td>
</tr>
<tr class="even">
<td>Activación</td>
<td>GELU (aproximación tanh)</td>
<td>SiLU</td>
</tr>
<tr class="odd">
<td>Normalización</td>
<td>RMSNorm</td>
<td>RMSNorm</td>
</tr>
<tr class="even">
<td>Codificación posicional</td>
<td>RoPE</td>
<td>RoPE</td>
</tr>
<tr class="odd">
<td><code>past_key_value</code></td>
<td>Soportado (caching)</td>
<td>Soportado (caching)</td>
</tr>
<tr class="even">
<td>Ventana deslizante</td>
<td>No soportado</td>
<td>Soportado</td>
</tr>
<tr class="odd">
<td>GQA</td>
<td>No soportado (usa MHA, K=V=Q, configuración de <code>num_key_value_heads</code>)</td>
<td>Soportado</td>
</tr>
<tr class="even">
<td>Atención de producto punto escalado</td>
<td>Uso de <code>F.scaled_dot_product_attention</code></td>
<td>Uso de <code>F.scaled_dot_product_attention</code></td>
</tr>
<tr class="odd">
<td>Mejora del caching RoPE</td>
<td>Gestión eficiente de cachés <code>cos</code>, <code>sin</code> dentro del método <code>forward</code>, actualización con <code>_set_cos_sin_cache</code> cuando es necesario. Lógica optimizada para aplicar RoPE en el decoding incremental mediante la función <code>apply_rotary_pos_emb_single</code>, minimizando cálculos redundantes.</td>
<td>Generación de <code>cos_cached</code>, <code>sin_cached</code> en el método <code>_set_cos_sin_cache</code>, uso en <code>forward</code>. Posibilidad de usar diferentes position IDs para queries y keys en <code>apply_rotary_pos_emb</code>.</td>
</tr>
<tr class="even">
<td>Optimización de la máscara de atención</td>
<td>Uso de la función <code>scaled_dot_product_attention</code>, combinación eficiente de <code>attention_mask</code> y <code>causal_mask</code>, reducción de cálculos innecesarios</td>
<td>Uso de la función <code>scaled_dot_product_attention</code>, manejo de <code>attention_mask</code>, <code>sliding_window_mask</code></td>
</tr>
<tr class="odd">
<td><code>return_dict</code></td>
<td>Retorno flexible y claro de output mediante <code>return_dict</code>.</td>
<td>Retorno de output a través de <code>return_dict</code>.</td>
</tr>
<tr class="even">
<td>Vinculación de pesos</td>
<td>Enlace (tying) de los pesos de embedding y la capa de salida en <code>post_init</code>, reducción de parámetros y mejora del rendimiento</td>
<td>No se menciona explícitamente el enlace de pesos</td>
</tr>
</tbody>
</table>
<p><strong>Principales mejoras</strong> * <strong>Multi-Head Attention (MHA):</strong> En lugar de GQA (Grouped-Query Attention) de Mistral, utiliza MHA convencional. Phi-3 Mini demuestra que puede lograr un rendimiento adecuado sin GQA. * <strong>Mejora en el almacenamiento en caché de RoPE:</strong> Gestiona eficientemente las cachés <code>cos</code> y <code>sin</code> dentro del método <code>forward</code>, y actualiza solo cuando es necesario a través de <code>_set_cos_sin_cache</code>. Además, durante la decodificación incremental, optimiza la aplicación de RoPE utilizando la función <code>apply_rotary_pos_emb_single</code> para minimizar cálculos redundantes. * <strong>Optimización de Attention Mask:</strong> Combina eficientemente <code>attention_mask</code> y <code>causal_mask</code> al usar la función <code>scaled_dot_product_attention</code> para reducir cálculos innecesarios. * <strong>Weight Tying:</strong> En <code>post_init</code>, vincula (tied) los pesos de embebido con los pesos del layer de salida para reducir el número de parámetros y mejorar el rendimiento.</p>
<p>Ahora examinaremos en detalle los componentes clave del modelo <code>simple_phi3</code>.</p>
<section id="phiminiconfig-configuración-del-modelo" class="level4">
<h4 class="anchored" data-anchor-id="phiminiconfig-configuración-del-modelo">1. PhiMiniConfig: Configuración del modelo</h4>
<p>La clase <code>PhiMiniConfig</code> define los hiperparámetros del modelo, siguiendo la configuración de Phi-3 Mini. Dado que ya se ha explicado con detalle en Mistral, lo omitiremos aquí.</p>
</section>
<section id="phiminirmsnorm-normalización-rms" class="level4">
<h4 class="anchored" data-anchor-id="phiminirmsnorm-normalización-rms">2. PhiMiniRMSNorm: Normalización RMS</h4>
<p>La clase <code>PhiMiniRMSNorm</code> implementa RMSNorm (Root Mean Square Layer Normalization), similar a la versión de Mistral.</p>
</section>
<section id="phiminirotaryembedding-implementación-de-rope-mejorada-cacha" class="level4">
<h4 class="anchored" data-anchor-id="phiminirotaryembedding-implementación-de-rope-mejorada-cacha">3. PhiMiniRotaryEmbedding: Implementación de RoPE (mejorada cacha)</h4>
<p>La clase <code>PhiMiniRotaryEmbedding</code> implementa RoPE (Rotary Positional Embedding). Aunque es similar a <code>MistralRotaryEmbedding</code> de Mistral, presenta las siguientes mejoras clave para maximizar la eficiencia del almacenamiento en caché.</p>
<ul>
<li><strong>Gestión de caché dentro del método <code>forward</code>:</strong>
<ul>
<li>En el método <code>forward</code>, se utilizan directamente <code>cos_cached</code> y <code>sin_cached</code>. Es decir, si ya hay valores calculados, se utilizan inmediatamente.</li>
<li>Si <code>seq_len</code> es mayor que <code>max_seq_len_cached</code>, lo que indica que se necesita una nueva longitud de secuencia en caché, se llama al método <code>_set_cos_sin_cache</code> para actualizar la caché. Esto evita la creación innecesaria de cachés y maximiza el reuso de valores ya calculados.</li>
</ul></li>
<li><strong>Variables de instancia <code>max_seq_len_cached</code>, <code>cos_cached</code>, <code>sin_cached</code>:</strong>
<ul>
<li><code>max_seq_len_cached</code>: almacena la longitud máxima de secuencia en caché hasta ahora.</li>
<li><code>cos_cached</code>, <code>sin_cached</code>: almacenan los valores de coseno y seno precalculados.</li>
<li>Al gestionar estas variables como variables de instancia, se evita su creación nueva cada vez que se llama al método <code>forward</code> y se reutilizan para mejorar la eficiencia.</li>
</ul></li>
<li><strong>Optimización de decodificación incremental:</strong>
<ul>
<li><code>apply_rotary_pos_emb_single</code>: permite aplicar RoPE solo a los <strong>nuevos tokens</strong> en situaciones de decodificación incremental utilizando <code>past_key_value</code>. Como los resultados de RoPE para los tokens anteriores ya están almacenados en <code>past_key_value</code>, se evitan cálculos redundantes.</li>
</ul></li>
</ul>
<p>Estas mejoras aumentan significativamente la eficiencia de las operaciones RoPE, proporcionando ventajas de rendimiento especialmente al procesar secuencias largas o generar texto.</p>
</section>
<section id="phiminiattention-mecanismo-de-atención-mha-aplicación-eficiente-de-rope" class="level4">
<h4 class="anchored" data-anchor-id="phiminiattention-mecanismo-de-atención-mha-aplicación-eficiente-de-rope">4. PhiMiniAttention: Mecanismo de atención (MHA, aplicación eficiente de RoPE)</h4>
<p>La clase <code>PhiMiniAttention</code> implementa el mecanismo de atención de Phi-3 Mini. Aunque utiliza la atención multi-cabeza (MHA) convencional en lugar del GQA de Mistral, optimiza la aplicación de RoPE para mejorar la eficiencia.</p>
<ul>
<li><strong>MHA (Multi-Head Attention):</strong> El número de cabezas de consulta(Q), clave(K) y valor(V) son todos iguales.</li>
<li><strong>Aplicación eficiente de RoPE:</strong>
<ul>
<li>Se generan IDs de posición diferentes según la presencia o ausencia de <code>past_key_value</code>.
<ul>
<li>Si no hay <code>past_key_value</code> (caso común): se generan IDs de posición para toda la secuencia (<code>0</code> a <code>q_len - 1</code>).</li>
<li>Si hay <code>past_key_value</code> (decodificación incremental): se generan IDs de posición para el nuevo token (<code>past_len</code> a <code>past_len + q_len - 1</code>) y para toda la secuencia clave (<code>0</code> a <code>past_len + q_len - 1</code>).</li>
</ul></li>
<li>A través de la función <code>apply_rotary_pos_emb_single</code>, cuando hay <code>past_key_value</code> (decodificación incremental), se aplica RoPE solo al nuevo token(query).</li>
</ul></li>
<li><strong>Caché KV:</strong> Al igual que Mistral, utiliza <code>past_key_value</code> para almacenar en caché los tensores de clave/valor de pasos anteriores y mejorar la velocidad de inferencia.</li>
</ul>
</section>
<section id="funciones-auxiliares-rotate_half-apply_rotary_pos_emb-apply_rotary_pos_emb_single" class="level4">
<h4 class="anchored" data-anchor-id="funciones-auxiliares-rotate_half-apply_rotary_pos_emb-apply_rotary_pos_emb_single">5. Funciones auxiliares: <code>rotate_half</code>, <code>apply_rotary_pos_emb</code>, <code>apply_rotary_pos_emb_single</code></h4>
<ul>
<li><code>rotate_half</code>: Es una función auxiliar necesaria para implementar RoPE, similar a Mistral.</li>
<li><code>apply_rotary_pos_emb</code>: Aplica RoPE a los tensores de consulta(q) y clave(k). A diferencia de Mistral, solo recibe un conjunto de position_ids (aplicado igualmente a la consulta y la clave).</li>
<li><code>apply_rotary_pos_emb_single</code>: En situaciones de decodificación incremental con <code>past_key_value</code>, aplica RoPE al tensor de entrada <code>x</code> (consulta o clave).</li>
</ul>
</section>
<section id="phiminimlp-red-feedforward" class="level4">
<h4 class="anchored" data-anchor-id="phiminimlp-red-feedforward">6. PhiMiniMLP: Red FeedForward</h4>
<p>La clase <code>PhiMiniMLP</code> implementa una red FeedForward, similar a Mistral, utilizando la función de activación GELU.</p>
</section>
<section id="phiminidecoderlayer-capa-decodificadora" class="level4">
<h4 class="anchored" data-anchor-id="phiminidecoderlayer-capa-decodificadora">7. PhiMiniDecoderLayer: Capa decodificadora</h4>
<p>La clase <code>PhiMiniDecoderLayer</code> utiliza la estructura Pre-Norm y las conexiones residuales, al igual que Mistral.</p>
</section>
<section id="phiminimodel-modelo-completo" class="level4">
<h4 class="anchored" data-anchor-id="phiminimodel-modelo-completo">8. PhiMiniModel: Modelo completo</h4>
<p>La clase <code>PhiMiniModel</code> compone el modelo Phi-3 Mini completo, similar a Mistral.</p>
</section>
<section id="phiminiforcausallm-adición-de-cabeza-para-modelado-de-lenguaje" class="level4">
<h4 class="anchored" data-anchor-id="phiminiforcausallm-adición-de-cabeza-para-modelado-de-lenguaje">9. PhiMiniForCausalLM: Adición de cabeza para modelado de lenguaje</h4>
<p>La clase <code>PhiMiniForCausalLM</code> agrega una cabeza (<code>lm_head</code>) para modelado de lenguaje al <code>PhiMiniModel</code>.</p>
<ul>
<li><strong>Método <code>post_init</code>:</strong>
<ul>
<li>Realiza la inicialización de pesos. (Similar a Mistral)</li>
<li><strong>Empate de Pesos:</strong> Une los pesos de incrustación (<code>self.transformer.embed_tokens.weight</code>) con los pesos del último nivel (<code>self.lm_head.weight</code>). Esto reduce el número de parámetros, evita el sobreajuste y generalmente mejora el rendimiento.</li>
</ul></li>
<li><strong>Función <code>generate</code>:</strong> Función para generar texto, que resuelve problemas relacionados con RoPE durante la decodificación incremental al pasar solo el último token en lugar de toda la secuencia a <code>forward()</code> cuando existen <code>past_key_values</code>.</li>
</ul>
</section>
</section>
<section id="ejemplo-del-modelo-simple_phi3-cálculo-de-expresiones-complejas" class="level3">
<h3 class="anchored" data-anchor-id="ejemplo-del-modelo-simple_phi3-cálculo-de-expresiones-complejas">9.8.2 Ejemplo del modelo <code>simple_phi3</code>: cálculo de expresiones complejas</h3>
<p>Como ejemplo práctico del modelo <code>simple_phi3</code> revisado en la sección 9.8.1, probaremos su capacidad para calcular expresiones matemáticas complejas. A través de este ejemplo, evaluaremos si modelos de lenguaje pequeños (SLM) como el Phi-3 Mini pueden manejar no solo operaciones básicas de suma y resta, sino también multiplicación y expresiones más complejas con paréntesis, analizando su rendimiento y limitaciones.</p>
<p>La ubicación del código de ejemplo es <strong>chapter_09/phi3/examples/train_math.py</strong>.</p>
<p><strong>Significado del ejemplo</strong></p>
<ul>
<li><strong>Verificación de la capacidad del SLM:</strong> muestra que incluso un modelo de tamaño pequeño puede resolver problemas complejos a través de datos de alta calidad y una arquitectura eficiente.</li>
<li><strong>Evaluación de las habilidades de inferencia:</strong> evalúa la capacidad de inferir respuestas para nuevas expresiones basadas en reglas operacionales aprendidas, más allá del simple memorizado.</li>
<li><strong>Exploración de posibilidades prácticas:</strong> el cálculo de expresiones complejas es una habilidad fundamental que puede ser aplicada en diversos campos como procesamiento de lenguaje natural y análisis de datos. Este ejemplo permite vislumbrar las posibilidades prácticas del SLM.</li>
</ul>
<p><strong>Forma de los datos de entrenamiento</strong></p>
<p>Usando la función <code>create_complex_arithmetic_data</code>, generamos datos de expresiones matemáticas complejas con las siguientes características:</p>
<ul>
<li>Dos o tres números (1 ~ 50)</li>
<li>Uso de dos operadores entre +, - y *</li>
<li>Uso opcional de paréntesis (())</li>
<li>Formato <code>expresión=resultado&lt;eos&gt;</code> (ej: <code>(12+7)*3=57&lt;eos&gt;</code>, <code>12+7*3=33&lt;eos&gt;</code>)</li>
</ul>
<p><strong>Resultados del entrenamiento</strong></p>
<div class="sourceCode" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a>Muestra <span class="dv">1</span>: <span class="dv">41</span><span class="op">*</span><span class="dv">8</span><span class="op">-</span><span class="dv">2</span><span class="op">=</span><span class="dv">326</span><span class="op">&lt;</span>eos<span class="op">&gt;</span></span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a>Muestra <span class="dv">2</span>: <span class="dv">15</span><span class="op">+</span>(<span class="dv">9</span><span class="op">*</span><span class="dv">48</span>)<span class="op">=</span><span class="dv">447</span><span class="op">&lt;</span>eos<span class="op">&gt;</span></span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a>Muestra <span class="dv">3</span>: <span class="dv">35</span><span class="op">-</span><span class="dv">6</span><span class="op">+</span><span class="dv">38</span><span class="op">=</span><span class="dv">67</span><span class="op">&lt;</span>eos<span class="op">&gt;</span></span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a>Muestra <span class="dv">4</span>: <span class="dv">6</span><span class="op">*</span><span class="dv">14</span><span class="op">*</span><span class="dv">15</span><span class="op">=</span><span class="dv">1260</span><span class="op">&lt;</span>eos<span class="op">&gt;</span></span>
<span id="cb45-5"><a href="#cb45-5" aria-hidden="true" tabindex="-1"></a>Muestra <span class="dv">5</span>: <span class="dv">36</span><span class="op">*</span>(<span class="dv">13</span><span class="op">*</span><span class="dv">46</span>)<span class="op">=</span><span class="dv">21528</span><span class="op">&lt;</span>eos<span class="op">&gt;</span></span>
<span id="cb45-6"><a href="#cb45-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-7"><a href="#cb45-7" aria-hidden="true" tabindex="-1"></a>...(omisión <span class="kw">del</span> registro de entrenamiento)...</span>
<span id="cb45-8"><a href="#cb45-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-9"><a href="#cb45-9" aria-hidden="true" tabindex="-1"></a>Prompt: <span class="st">'23-23-50='</span> <span class="op">--&gt;</span> Resultado generado: <span class="st">'23-23-50=-50'</span>  (Respuesta correcta: <span class="dv">23</span><span class="op">-</span><span class="dv">23</span><span class="op">-</span><span class="dv">50</span><span class="op">=-</span><span class="dv">50</span><span class="op">&lt;</span>eos<span class="op">&gt;</span>)</span>
<span id="cb45-10"><a href="#cb45-10" aria-hidden="true" tabindex="-1"></a>Prompt: <span class="st">'39-46-15='</span> <span class="op">--&gt;</span> Resultado generado: <span class="st">'39-46-15=-22'</span>  (Respuesta correcta: <span class="dv">39</span><span class="op">-</span><span class="dv">46</span><span class="op">-</span><span class="dv">15</span><span class="op">=-</span><span class="dv">22</span><span class="op">&lt;</span>eos<span class="op">&gt;</span>)</span>
<span id="cb45-11"><a href="#cb45-11" aria-hidden="true" tabindex="-1"></a>Prompt: <span class="st">'(33-30)+30='</span> <span class="op">--&gt;</span> Resultado generado: <span class="st">'(33-30)+30=33'</span>  (Respuesta correcta: (<span class="dv">33</span><span class="op">-</span><span class="dv">30</span>)<span class="op">+</span><span class="dv">30</span><span class="op">=</span><span class="dv">33</span><span class="op">&lt;</span>eos<span class="op">&gt;</span>)</span>
<span id="cb45-12"><a href="#cb45-12" aria-hidden="true" tabindex="-1"></a>Prompt: <span class="st">'6*14*15='</span> <span class="op">--&gt;</span> Resultado generado: <span class="st">'6*14*15=1260'</span>  (Respuesta correcta: <span class="dv">6</span><span class="op">*</span><span class="dv">14</span><span class="op">*</span><span class="dv">15</span><span class="op">=</span><span class="dv">1260</span><span class="op">&lt;</span>eos<span class="op">&gt;</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Análisis</strong></p>
<ul>
<li><strong>Resultados precisos o cercanos a la respuesta correcta:</strong> en la mayoría de los casos, el modelo <code>simple_phi3</code> generó respuestas que coinciden con las correctas o son muy similares. Esto indica que el modelo ha aprendido bien las reglas operacionales para expresiones matemáticas complejas.</li>
<li><strong>Errores en algunos casos:</strong> tendencia a cometer errores cuando la multiplicación está involucrada o los números son grandes, lo cual puede deberse al tamaño limitado del modelo y a la falta de diversidad en los datos de entrenamiento.</li>
<li><strong>Manejo de paréntesis:</strong> el modelo también muestra una habilidad decente para manejar expresiones con paréntesis, indicando que tiene una comprensión contextual y puede seguir el orden correcto de las operaciones.</li>
</ul>
<p><strong>Conclusión</strong></p>
<p>A pesar de tener solo alrededor de 120,000 parámetros, el modelo <code>simple_phi3</code> logró un alto nivel de precisión (aproximadamente 80%) en el cálculo de expresiones matemáticas complejas. Esto sugiere que ha aprendido reglas complejas como el manejo de paréntesis y el orden de las operaciones. En comparación con modelos de lenguaje grandes (LLM) que tienen miles de millones de parámetros, <code>simple_phi3</code> demuestra un rendimiento impresionante a pesar de su tamaño extremadamente pequeño (0.12M).</p>
<div id="cell-68" class="cell" data-execution_count="3">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_09.phi3.examples.train_complex_math <span class="im">import</span> PhiMiniConfig, PhiMiniForCausalLM, ComplexArithmeticDataset, train, create_complex_arithmetic_data, create_tokenizer, create_reverse_tokenizer, generate_text</span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> DataLoader</span>
<span id="cb46-5"><a href="#cb46-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-6"><a href="#cb46-6" aria-hidden="true" tabindex="-1"></a>random.seed(<span class="dv">42</span>)</span>
<span id="cb46-7"><a href="#cb46-7" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">42</span>)</span>
<span id="cb46-8"><a href="#cb46-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-9"><a href="#cb46-9" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> <span class="st">"cuda"</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">"cpu"</span></span>
<span id="cb46-10"><a href="#cb46-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-11"><a href="#cb46-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Hyperparameters</span></span>
<span id="cb46-12"><a href="#cb46-12" aria-hidden="true" tabindex="-1"></a>num_samples <span class="op">=</span> <span class="dv">100000</span>      <span class="co"># Sufficiently large amount of data</span></span>
<span id="cb46-13"><a href="#cb46-13" aria-hidden="true" tabindex="-1"></a>max_value <span class="op">=</span> <span class="dv">50</span>           <span class="co"># Maximum value of operands (for slightly complex calculations)</span></span>
<span id="cb46-14"><a href="#cb46-14" aria-hidden="true" tabindex="-1"></a>seq_length <span class="op">=</span> <span class="dv">30</span>          <span class="co"># Complex arithmetic problems can have somewhat long expressions</span></span>
<span id="cb46-15"><a href="#cb46-15" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">128</span></span>
<span id="cb46-16"><a href="#cb46-16" aria-hidden="true" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">30</span></span>
<span id="cb46-17"><a href="#cb46-17" aria-hidden="true" tabindex="-1"></a>learning_rate <span class="op">=</span> <span class="fl">1e-3</span></span>
<span id="cb46-18"><a href="#cb46-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-19"><a href="#cb46-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Data generation</span></span>
<span id="cb46-20"><a href="#cb46-20" aria-hidden="true" tabindex="-1"></a>complex_data <span class="op">=</span> create_complex_arithmetic_data(num_samples, max_value)</span>
<span id="cb46-21"><a href="#cb46-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Training data examples:"</span>)</span>
<span id="cb46-22"><a href="#cb46-22" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">5</span>):</span>
<span id="cb46-23"><a href="#cb46-23" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Sample </span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>complex_data[i]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb46-24"><a href="#cb46-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-25"><a href="#cb46-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Create tokenizer and reverse tokenizer</span></span>
<span id="cb46-26"><a href="#cb46-26" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> create_tokenizer()</span>
<span id="cb46-27"><a href="#cb46-27" aria-hidden="true" tabindex="-1"></a>reverse_tokenizer <span class="op">=</span> create_reverse_tokenizer(tokenizer)</span>
<span id="cb46-28"><a href="#cb46-28" aria-hidden="true" tabindex="-1"></a>updated_vocab_size <span class="op">=</span> <span class="bu">len</span>(tokenizer)</span>
<span id="cb46-29"><a href="#cb46-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-30"><a href="#cb46-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Configure Dataset and DataLoader</span></span>
<span id="cb46-31"><a href="#cb46-31" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> ComplexArithmeticDataset(complex_data, seq_length, tokenizer)</span>
<span id="cb46-32"><a href="#cb46-32" aria-hidden="true" tabindex="-1"></a>dataloader <span class="op">=</span> DataLoader(dataset, batch_size<span class="op">=</span>batch_size, shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb46-33"><a href="#cb46-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-34"><a href="#cb46-34" aria-hidden="true" tabindex="-1"></a><span class="co"># PhiMini Model Configuration</span></span>
<span id="cb46-35"><a href="#cb46-35" aria-hidden="true" tabindex="-1"></a>config <span class="op">=</span> PhiMiniConfig(</span>
<span id="cb46-36"><a href="#cb46-36" aria-hidden="true" tabindex="-1"></a>    vocab_size<span class="op">=</span>updated_vocab_size,</span>
<span id="cb46-37"><a href="#cb46-37" aria-hidden="true" tabindex="-1"></a>    hidden_size<span class="op">=</span><span class="dv">64</span>,              <span class="co"># Small model size for experimentation</span></span>
<span id="cb46-38"><a href="#cb46-38" aria-hidden="true" tabindex="-1"></a>    intermediate_size<span class="op">=</span><span class="dv">128</span>,</span>
<span id="cb46-39"><a href="#cb46-39" aria-hidden="true" tabindex="-1"></a>    num_hidden_layers<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb46-40"><a href="#cb46-40" aria-hidden="true" tabindex="-1"></a>    num_attention_heads<span class="op">=</span><span class="dv">8</span>,</span>
<span id="cb46-41"><a href="#cb46-41" aria-hidden="true" tabindex="-1"></a>    num_key_value_heads<span class="op">=</span><span class="dv">8</span>,        <span class="co"># K=V=Q</span></span>
<span id="cb46-42"><a href="#cb46-42" aria-hidden="true" tabindex="-1"></a>    max_position_embeddings<span class="op">=</span><span class="dv">128</span>,</span>
<span id="cb46-43"><a href="#cb46-43" aria-hidden="true" tabindex="-1"></a>    use_cache<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb46-44"><a href="#cb46-44" aria-hidden="true" tabindex="-1"></a>    use_return_dict<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb46-45"><a href="#cb46-45" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb46-46"><a href="#cb46-46" aria-hidden="true" tabindex="-1"></a>config.pad_token_id <span class="op">=</span> tokenizer[<span class="st">"&lt;pad&gt;"</span>]</span>
<span id="cb46-47"><a href="#cb46-47" aria-hidden="true" tabindex="-1"></a>config.eos_token_id <span class="op">=</span> tokenizer[<span class="st">"&lt;eos&gt;"</span>]</span>
<span id="cb46-48"><a href="#cb46-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-49"><a href="#cb46-49" aria-hidden="true" tabindex="-1"></a><span class="co"># Create PhiMini For CausalLM Model</span></span>
<span id="cb46-50"><a href="#cb46-50" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> PhiMiniForCausalLM(config).to(device)</span>
<span id="cb46-51"><a href="#cb46-51" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Total Trainable Parameters:"</span>, <span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> model.parameters() <span class="cf">if</span> p.requires_grad))</span>
<span id="cb46-52"><a href="#cb46-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-53"><a href="#cb46-53" aria-hidden="true" tabindex="-1"></a><span class="co"># weight tying (share weights between embedding and lm_head)</span></span>
<span id="cb46-54"><a href="#cb46-54" aria-hidden="true" tabindex="-1"></a>model.lm_head.weight <span class="op">=</span> model.transformer.embed_tokens.weight</span>
<span id="cb46-55"><a href="#cb46-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-56"><a href="#cb46-56" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.AdamW(model.parameters(), lr<span class="op">=</span>learning_rate)</span>
<span id="cb46-57"><a href="#cb46-57" aria-hidden="true" tabindex="-1"></a>scheduler <span class="op">=</span> torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max<span class="op">=</span>epochs, eta_min<span class="op">=</span><span class="fl">1e-5</span>)</span>
<span id="cb46-58"><a href="#cb46-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-59"><a href="#cb46-59" aria-hidden="true" tabindex="-1"></a><span class="co"># Model Training</span></span>
<span id="cb46-60"><a href="#cb46-60" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Start training..."</span>)</span>
<span id="cb46-61"><a href="#cb46-61" aria-hidden="true" tabindex="-1"></a>train(model, dataloader, optimizer, scheduler, epochs, device)</span>
<span id="cb46-62"><a href="#cb46-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-63"><a href="#cb46-63" aria-hidden="true" tabindex="-1"></a><span class="co"># Save Model</span></span>
<span id="cb46-64"><a href="#cb46-64" aria-hidden="true" tabindex="-1"></a>save_path <span class="op">=</span> <span class="st">"phimini_complex_math.pt"</span></span>
<span id="cb46-65"><a href="#cb46-65" aria-hidden="true" tabindex="-1"></a>torch.save(model.state_dict(), save_path)</span>
<span id="cb46-66"><a href="#cb46-66" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Model saved: </span><span class="sc">{</span>save_path<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb46-67"><a href="#cb46-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-68"><a href="#cb46-68" aria-hidden="true" tabindex="-1"></a><span class="co"># Load Saved Model (create a new model object before testing and load_state_dict)</span></span>
<span id="cb46-69"><a href="#cb46-69" aria-hidden="true" tabindex="-1"></a>loaded_model <span class="op">=</span> PhiMiniForCausalLM(config).to(device)</span>
<span id="cb46-70"><a href="#cb46-70" aria-hidden="true" tabindex="-1"></a>loaded_model.load_state_dict(torch.load(save_path, map_location<span class="op">=</span>device))</span>
<span id="cb46-71"><a href="#cb46-71" aria-hidden="true" tabindex="-1"></a>loaded_model.<span class="bu">eval</span>()</span>
<span id="cb46-72"><a href="#cb46-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-73"><a href="#cb46-73" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate and Print Results with Test Set, Calculate Accuracy</span></span>
<span id="cb46-74"><a href="#cb46-74" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Test sample generation results:"</span>)</span>
<span id="cb46-75"><a href="#cb46-75" aria-hidden="true" tabindex="-1"></a>test_samples <span class="op">=</span> random.sample(complex_data, <span class="dv">10</span>)</span>
<span id="cb46-76"><a href="#cb46-76" aria-hidden="true" tabindex="-1"></a>correct_count <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb46-77"><a href="#cb46-77" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> sample <span class="kw">in</span> test_samples:</span>
<span id="cb46-78"><a href="#cb46-78" aria-hidden="true" tabindex="-1"></a>    prompt <span class="op">=</span> sample.split(<span class="st">'='</span>)[<span class="dv">0</span>] <span class="op">+</span> <span class="st">'='</span></span>
<span id="cb46-79"><a href="#cb46-79" aria-hidden="true" tabindex="-1"></a>    generated <span class="op">=</span> generate_text(loaded_model, prompt, tokenizer, reverse_tokenizer, seq_length, device, temperature<span class="op">=</span><span class="fl">0.1</span>)  <span class="co"># Reduce temperature for testing</span></span>
<span id="cb46-80"><a href="#cb46-80" aria-hidden="true" tabindex="-1"></a>    answer <span class="op">=</span> sample.split(<span class="st">'='</span>)[<span class="dv">1</span>].replace(<span class="st">'&lt;eos&gt;'</span>, <span class="st">''</span>)</span>
<span id="cb46-81"><a href="#cb46-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-82"><a href="#cb46-82" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> generated.split(<span class="st">'='</span>)[<span class="dv">1</span>] <span class="op">==</span> answer:</span>
<span id="cb46-83"><a href="#cb46-83" aria-hidden="true" tabindex="-1"></a>        correct_count <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb46-84"><a href="#cb46-84" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Prompt: '</span><span class="sc">{</span>prompt<span class="sc">}</span><span class="ss">' --&gt; Generated result: '</span><span class="sc">{</span>generated<span class="sc">}</span><span class="ss">'  (Correct answer: </span><span class="sc">{</span>sample<span class="sc">}</span><span class="ss">)"</span>)</span>
<span id="cb46-85"><a href="#cb46-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-86"><a href="#cb46-86" aria-hidden="true" tabindex="-1"></a>accuracy <span class="op">=</span> (correct_count <span class="op">/</span> <span class="bu">len</span>(test_samples)) <span class="op">*</span> <span class="dv">100</span></span>
<span id="cb46-87"><a href="#cb46-87" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Overall accuracy: </span><span class="sc">{</span>accuracy<span class="sc">:.2f}</span><span class="ss">% (</span><span class="sc">{</span>correct_count<span class="sc">}</span><span class="ss">/</span><span class="sc">{</span><span class="bu">len</span>(test_samples)<span class="sc">}</span><span class="ss">)"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Training data examples:
Sample 1: 41*8-2=326&lt;eos&gt;
Sample 2: 15+(9*48)=447&lt;eos&gt;
Sample 3: 35-6+38=67&lt;eos&gt;
Sample 4: 6*14*15=1260&lt;eos&gt;
Sample 5: 36*(13*46)=21528&lt;eos&gt;
Total Trainable Parameters: 126208
Start training...
Epoch 1/30, Avg Loss: 0.7439, LR: 0.000997
Epoch 2/30, Avg Loss: 0.6393, LR: 0.000989
Epoch 3/30, Avg Loss: 0.6139, LR: 0.000976
Epoch 4/30, Avg Loss: 0.5919, LR: 0.000957
Epoch 5/30, Avg Loss: 0.5825, LR: 0.000934
Epoch 6/30, Avg Loss: 0.5753, LR: 0.000905
Epoch 7/30, Avg Loss: 0.5696, LR: 0.000873
Epoch 8/30, Avg Loss: 0.5649, LR: 0.000836
Epoch 9/30, Avg Loss: 0.5599, LR: 0.000796
Epoch 10/30, Avg Loss: 0.5558, LR: 0.000753
Epoch 11/30, Avg Loss: 0.5522, LR: 0.000706
Epoch 12/30, Avg Loss: 0.5479, LR: 0.000658
Epoch 13/30, Avg Loss: 0.5443, LR: 0.000608
Epoch 14/30, Avg Loss: 0.5409, LR: 0.000557
Epoch 15/30, Avg Loss: 0.5370, LR: 0.000505
Epoch 16/30, Avg Loss: 0.5339, LR: 0.000453
Epoch 17/30, Avg Loss: 0.5307, LR: 0.000402
Epoch 18/30, Avg Loss: 0.5280, LR: 0.000352
Epoch 19/30, Avg Loss: 0.5242, LR: 0.000304
Epoch 20/30, Avg Loss: 0.5217, LR: 0.000258
Epoch 21/30, Avg Loss: 0.5189, LR: 0.000214
Epoch 22/30, Avg Loss: 0.5161, LR: 0.000174
Epoch 23/30, Avg Loss: 0.5137, LR: 0.000137
Epoch 24/30, Avg Loss: 0.5120, LR: 0.000105
Epoch 25/30, Avg Loss: 0.5101, LR: 0.000076
Epoch 26/30, Avg Loss: 0.5085, LR: 0.000053
Epoch 27/30, Avg Loss: 0.5073, LR: 0.000034
Epoch 28/30, Avg Loss: 0.5062, LR: 0.000021
Epoch 29/30, Avg Loss: 0.5055, LR: 0.000013
Epoch 30/30, Avg Loss: 0.5050, LR: 0.000010
Model saved: phimini_complex_math.pt

Test sample generation results:
Prompt: '23-23-50=' --&gt; Generated result: '23-23-50=-50'  (Correct answer: 23-23-50=-50&lt;eos&gt;)
Prompt: '39-46-15=' --&gt; Generated result: '39-46-15=-22'  (Correct answer: 39-46-15=-22&lt;eos&gt;)
Prompt: '(33-30)+30=' --&gt; Generated result: '(33-30)+30=33'  (Correct answer: (33-30)+30=33&lt;eos&gt;)
Prompt: '30+14*27=' --&gt; Generated result: '30+14*27=408'  (Correct answer: 30+14*27=408&lt;eos&gt;)
Prompt: '(13-22)-18=' --&gt; Generated result: '(13-22)-18=-27'  (Correct answer: (13-22)-18=-27&lt;eos&gt;)
Prompt: '9-15+12=' --&gt; Generated result: '9-15+12=6'  (Correct answer: 9-15+12=6&lt;eos&gt;)
Prompt: '28*(3+31)=' --&gt; Generated result: '28*(3+31)=960'  (Correct answer: 28*(3+31)=952&lt;eos&gt;)
Prompt: '24*(12+1)=' --&gt; Generated result: '24*(12+1)=320'  (Correct answer: 24*(12+1)=312&lt;eos&gt;)
Prompt: '(1-33)+26=' --&gt; Generated result: '(1-33)+26=-6'  (Correct answer: (1-33)+26=-6&lt;eos&gt;)
Prompt: '24+47+6=' --&gt; Generated result: '24+47+6=77'  (Correct answer: 24+47+6=77&lt;eos&gt;)

Overall accuracy: 80.00% (8/10)</code></pre>
</div>
</div>
</section>
</section>
<section id="conclusión-1" class="level2">
<h2 class="anchored" data-anchor-id="conclusión-1">Conclusión</h2>
<p>En el Capítulo 9, hemos seguido la evolución de los Transformers desde su presentación en 2017 con el paper pionero “Attention is All You Need” hasta la actualidad en 2025, centrándonos en las dos fuerzas impulsoras clave del desarrollo de los Transformers: <strong>eficiencia</strong> y <strong>escalabilidad</strong>.</p>
<p>Los primeros Transformers mostraron un rendimiento revolucionario, pero se enfrentaron a una limitación fundamental: el aumento exponencial del cálculo y el uso de memoria con la longitud de las secuencias. El Capítulo 9 aborda en profundidad los esfuerzos constantes para superar estas restricciones, incluyendo enfoques puramente software (Sección 9.2), combinaciones de hardware y software (Sección 9.3) y diversas innovaciones técnicas para mejorar la escalabilidad del modelo (Sección 9.4). Desde los ejemplos de implementación de RoPE y FlashAttention (Sección 9.5) hasta el análisis de las arquitecturas de modelos recientes como Mistral, Gemma y Phi-3 Mini (Secciones 9.6, 9.7, 9.8), exploramos exhaustivamente tanto la teoría como la implementación práctica para iluminar las arquitecturas Transformers más eficientes.</p>
<p>Gracias a estos avances tecnológicos, los Transformers se han convertido en una herramienta poderosa capaz de comprender contextos más largos, resolver problemas más complejos y aplicarse a un rango más amplio de campos. Es evidente el papel crucial que han jugado la <strong>eficiencia y escalabilidad</strong> en el crecimiento de los Transformers desde simples modelos de lenguaje hasta convertirse en un motor central para el desarrollo de tecnologías de inteligencia artificial.</p>
<p>Por supuesto, aún quedan desafíos por resolver. El aumento del consumo energético asociado al agrandamiento de los modelos, problemas de sesgo y daño, y la cuestión de la interpretabilidad de los modelos son desafíos importantes que debemos superar en el futuro. La investigación para desarrollar sistemas de IA más seguros, confiables y capaces de colaborar armónicamente con humanos continuará.</p>
<p>A partir del Capítulo 10 y 11, iniciaremos un viaje hacia el mundo <strong>multimodal</strong> de los Transformers, que va más allá del dominio único del texto para integrar diversas tipos de datos como imágenes, audio y video. Los modelos multimodales que combinan información de múltiples modalidades ofrecen representaciones más ricas y potentes, lo que permite inferencias más complejas. Exploraremos los mecanismos de atención multimodal y sus infinitas posibilidades de aplicación a través de pioneros modelos como ViT, CLIP, DALL-E, Stable Diffusion, Flamingo, GATO y Gemini. Las innovaciones en eficiencia y escalabilidad discutidas en el Capítulo 9 sentarán las bases sólidas para el futuro de los Transformers multimodales que se presentará en los Capítulos 10 y 11.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Haga clic para ver el contenido (análisis detallado: evolución teórica de la arquitectura MoE y las últimas tendencias tecnológicas)">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Haga clic para ver el contenido (análisis detallado: evolución teórica de la arquitectura MoE y las últimas tendencias tecnológicas)
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<section id="evolución-teórica-y-tendencias-tecnológicas-más-recientes-de-la-arquitectura-moe-mixture-of-experts" class="level2 callout-body-container callout-body">
<h2 class="anchored" data-anchor-id="evolución-teórica-y-tendencias-tecnológicas-más-recientes-de-la-arquitectura-moe-mixture-of-experts">Evolución teórica y tendencias tecnológicas más recientes de la arquitectura MoE (Mixture of Experts)</h2>
<p>En el desarrollo de los modelos de lenguaje a gran escala (LLM), Mixture of Experts (MoE) ha surgido como un marco que resuelve innovadoramente el equilibrio entre la capacidad del modelo y la eficiencia computacional. MoE funciona combinando varias redes “expertas” y activando selectivamente las expertas apropiadas para cada entrada a través de una red de puerta. Aquí se desglosan en profundidad los mecanismos clave de MoE, y se organiza sistemáticamente la teoría extendida basada en las tendencias de investigación más recientes.</p>
<section id="fundamentos-teóricos-de-moe" class="level3">
<h3 class="anchored" data-anchor-id="fundamentos-teóricos-de-moe">1. Fundamentos teóricos de MoE</h3>
<section id="componentes-básicos" class="level4">
<h4 class="anchored" data-anchor-id="componentes-básicos">1.1 Componentes básicos</h4>
<ul>
<li><p><strong>Redes expertas:</strong> Existen <em>N</em> redes expertas <span class="math inline">\(\{E_i\}_{i=1}^N\)</span>, generalmente compuestas por Redes Neuronales Feedforward (FFN). Cada experta recibe una entrada <span class="math inline">\(x\)</span> y genera una salida <span class="math inline">\(E_i(x)\)</span>.</p></li>
<li><p><strong>Red de puerta:</strong> La red de puerta <span class="math inline">\(G\)</span> toma la entrada <span class="math inline">\(x\)</span> y produce pesos (probabilidades) para cada experta. Estos pesos indican qué experta es la más adecuada para la entrada <span class="math inline">\(x\)</span>. La salida de la red de puerta <span class="math inline">\(G(x)\)</span> es un vector de <em>N</em> dimensiones, donde cada elemento <span class="math inline">\(G(x)_i\)</span> representa el peso para la <em>i</em>-ésima experta.</p></li>
<li><p><strong>Salida final:</strong> La salida final del modelo MoE <span class="math inline">\(y\)</span> se calcula como una suma ponderada de las salidas de las expertas.</p>
<p><span class="math inline">\(y = \sum_{i=1}^{N} G(x)_i E_i(x)\)</span></p></li>
</ul>
</section>
<section id="moe-denso-y-moe-disperso" class="level4">
<h4 class="anchored" data-anchor-id="moe-denso-y-moe-disperso">1.2 MoE Denso y MoE Disperso</h4>
<ul>
<li><strong>MoE Denso:</strong> Todas las expertas realizan cálculos para todas las entradas, y la red de puerta determina los pesos de las salidas de las expertas a través de una función softmax. (<span class="math inline">\(G(x) = \text{softmax}(W_g x)\)</span>)</li>
<li><strong>MoE Disperso:</strong> Para cada entrada, solo se activan un pequeño número de expertas. La red de puerta utiliza Top-k gating (seleccionando los <em>k</em> valores más altos) o Noisy Top-k gating (GShard, Switch Transformer).</li>
</ul>
</section>
<section id="formalización-matemática-y-perspectiva-de-inferencia-variacional" class="level4">
<h4 class="anchored" data-anchor-id="formalización-matemática-y-perspectiva-de-inferencia-variacional">1.3 Formalización matemática y perspectiva de inferencia variacional</h4>
<p>Al reinterpretar el sistema MoE como un modelo gráfico probabilístico, la distribución conjunta de los datos observados <span class="math inline">\(\mathbf{x}\)</span> y las variables latentes <span class="math inline">\(\mathbf{z}\)</span> (indicadoras de selección de experta) se modela como sigue:</p>
<p><span class="math inline">\(p(\mathbf{x}, \mathbf{z}|\theta) = p(\mathbf{z}|\theta_g)p(\mathbf{x}|\mathbf{z},\theta_e)\)</span></p>
<p>donde <span class="math inline">\(\theta_g\)</span> son los parámetros de la red de puerta y <span class="math inline">\(\theta_e\)</span> son los parámetros de las redes expertas. En el marco de inferencia variacional, la Cota Inferior del Evidencia (ELBO) se deriva como:</p>
<p><span class="math inline">\(\mathcal{L}(\theta, \phi) = \mathbb{E}_{q_\phi(\mathbf{z}|\mathbf{x})}[\log p_\theta(\mathbf{x}|\mathbf{z})] - D_{KL}(q_\phi(\mathbf{z}|\mathbf{x}) \parallel p(\mathbf{z}))\)</span></p>
<p>Este enfoque redefine el proceso de aprendizaje de MoE dentro de un sistema de inferencia bayesiana, proporcionando una base teórica para la división del conocimiento entre las expertas. En particular, la técnica de reparametrización Gumbel-Softmax aproxima continuamente el proceso discreto de selección de experta, permitiendo la aplicación del descenso por gradiente.</p>
<p><span class="math inline">\(\mathbf{z} = \text{softmax}((\log \boldsymbol{\pi} + \mathbf{g})/\tau)\)</span></p>
<p>donde <span class="math inline">\(\mathbf{g}\)</span> es ruido Gumbel y <span class="math inline">\(\tau\)</span> es un parámetro de temperatura.</p>
</section>
</section>
<section id="innovaciones-estructurales-en-moe-disperso" class="level3">
<h3 class="anchored" data-anchor-id="innovaciones-estructurales-en-moe-disperso">2. Innovaciones estructurales en MoE Disperso</h3>
<section id="particionamiento-jerárquico-de-expertos-hierarchical-expert-partitioning" class="level4">
<h4 class="anchored" data-anchor-id="particionamiento-jerárquico-de-expertos-hierarchical-expert-partitioning">2.1 Particionamiento Jerárquico de Expertos (Hierarchical Expert Partitioning)</h4>
<p>Multi-Head Latent Attention (MLA) introducido en DeepSeek-V2 reduce significativamente la caché Key-Value [5, 6]. Esto se implementa a través de un enfoque que bifurca la jerarquía de expertos en partición espacial (Spatial Partitioning) y partición funcional (Functional Partitioning).</p>
<p><span class="math inline">\(E_i(\mathbf{x}) = \sum_{h=1}^H W_{h,i}^o \cdot \text{GeLU}(W_{h,i}^k \mathbf{x} \oplus W_{h,i}^v \mathbf{x})\)</span></p>
<p>Dentro de cada experto, las cabezas de atención actúan como subexpertos independientes y maximizan la eficiencia paramétrica a través de matrices base compartidas (shared basis matrices).</p>
</section>
<section id="adaptación-dinámica-de-topología" class="level4">
<h4 class="anchored" data-anchor-id="adaptación-dinámica-de-topología">2.2 Adaptación Dinámica de Topología</h4>
<p>El modelo Mixtral 8x7B introdujo un mecanismo para reconstruir dinámicamente la estructura de conexión de los expertos en función de los datos de entrada. La red de enrutamiento ha evolucionado más allá de una simple selección de expertos, convirtiéndose en una red neuronal gráfica (Graph Neural Network) que ajusta la intensidad de las conexiones entre expertos.</p>
<p><span class="math inline">\(A_{ij}^{(l)} = \sigma(f_\phi(\mathbf{h}_i^{(l)}, \mathbf{h}_j^{(l)}))\)</span></p>
<p>Aquí, <span class="math inline">\(A_{ij}\)</span> representa el peso de conexión entre los expertos <span class="math inline">\(i\)</span> y <span class="math inline">\(j\)</span>, lo que permite la extracción de características multi-escala a través del mecanismo de atención jerárquica.</p>
</section>
</section>
<section id="ventajas-y-optimización-de-modelos-moe" class="level3">
<h3 class="anchored" data-anchor-id="ventajas-y-optimización-de-modelos-moe">3. Ventajas y Optimización de Modelos MoE</h3>
<section id="ventajas" class="level4">
<h4 class="anchored" data-anchor-id="ventajas">3.1 Ventajas</h4>
<ul>
<li><strong>Aumento de Capacidad del Modelo:</strong> Se puede aumentar significativamente el número de parámetros al incrementar el número de expertos, pero el costo computacional aumenta relativamente poco.</li>
<li><strong>Eficiencia Computacional (Sparse MoE):</strong> Dado que solo un pequeño número de expertos se activan por token, los FLOPs son bajos.</li>
<li><strong>Ley de Escalabilidad:</strong> Los modelos MoE tienden a seguir una ley de escalabilidad más favorable en comparación con los modelos densos.</li>
<li><strong>Ajuste Fino (Fine-tuning):</strong> Se pueden ajustar finamente expertos específicos para especializarse en tareas específicas.</li>
</ul>
</section>
<section id="innovaciones-en-teoría-de-optimización" class="level4">
<h4 class="anchored" data-anchor-id="innovaciones-en-teoría-de-optimización">3.2 Innovaciones en Teoría de Optimización</h4>
<ul>
<li><p><strong>Optimización Equilibrada (Balanced Optimization):</strong> Para abordar el problema del desequilibrio de carga entre los expertos, se introdujo la técnica de descomposición dual (Dual Decomposition), utilizando el método de multiplicadores de Lagrange para restringir explícitamente la desviación estándar de la utilización de los expertos.</p>
<p><span class="math inline">\(\mathcal{L}_{\text{total}} = \mathcal{L}_{\text{task}} + \lambda \sum_{i=1}^N (\mathbb{E}[u_i] - \bar{u})^2\)</span></p>
<p>Aquí, <span class="math inline">\(u_i\)</span> es la utilización del experto <span class="math inline">\(i\)</span>, y <span class="math inline">\(\bar{u}\)</span> es la tasa de utilización promedio objetivo.</p></li>
<li><p><strong>Distilación de Conocimiento en Múltiples Capas:</strong> Se propuso una distilación de conocimiento jerárquica (Hierarchical Knowledge Distillation) que refleja la estructura jerárquica de los modelos MoE. <span class="math inline">\(\mathcal{L}_{KD} = \sum_{l=1}^{L}\alpha_{l}D_{KL}(g^{\text{teacher}}_{l} || g^{\text{student}}_{l})\)</span> Al minimizar individualmente la divergencia KL de las distribuciones de puerta <span class="math inline">\(g_{l}\)</span> en cada capa MoE, se posibilita la transferencia de conocimientos especializados de los expertos.</p></li>
</ul>
</section>
</section>
<section id="ejemplos-y-limitaciones-de-modelos-moe" class="level3">
<h3 class="anchored" data-anchor-id="ejemplos-y-limitaciones-de-modelos-moe">4. Ejemplos y Limitaciones de Modelos MoE</h3>
<section id="ejemplo" class="level4">
<h4 class="anchored" data-anchor-id="ejemplo">4.1 Ejemplo</h4>
<ul>
<li><strong>GShard:</strong> Modelo MoE disperso de Google. Utiliza el gating Noisy Top-k.</li>
<li><strong>Switch Transformer:</strong> Modelo MoE disperso de Google. Cada token es procesado por un solo experto (k=1).</li>
<li><strong>GLaM:</strong> Modelo MoE disperso de Google (1.2T parámetros).</li>
<li><strong>Mistral 8x7B:</strong> Modelo MoE disperso de Mistral AI. Utiliza el gating Top-2.</li>
</ul>
</section>
<section id="limitaciones-y-desafíos" class="level4">
<h4 class="anchored" data-anchor-id="limitaciones-y-desafíos">4.2 Limitaciones y desafíos</h4>
<ul>
<li><strong>Desbalance de expertos (Load Imbalance):</strong> Asignación excesiva de tokens a ciertos expertos. (Solución: Gating Noisy Top-k, Loss de balanceo de carga, Límite de capacidad del experto)</li>
<li><strong>Dificultad en el aprendizaje de la red de gating:</strong> Dificultad para aprender una selección/combinação efectiva de expertos.</li>
<li><strong>Costos de comunicación (aprendizaje distribuido):</strong> Posible aumento de los costos de comunicación entre expertos.</li>
<li><strong>Dificultad en el enriquecimiento del conocimiento</strong>: Debido al tamaño del modelo MoE, es difícil destilar conocimientos a modelos más pequeños.</li>
</ul>
</section>
</section>
<section id="avances-en-la-implementación-física" class="level3">
<h3 class="anchored" data-anchor-id="avances-en-la-implementación-física">5. Avances en la implementación física</h3>
<section id="hardware-para-activaciones-de-expertos-dispersos" class="level4">
<h4 class="anchored" data-anchor-id="hardware-para-activaciones-de-expertos-dispersos">5.1 Hardware para activaciones de expertos dispersos</h4>
<p>La GPU NVIDIA H100 Tensor Core introduce una Unidad de Ejecución Dispersa (Sparse Execution Unit) dedicada a MoE, que acelera las operaciones de routing Top-k. * Control dinámico de warp: Gestión independiente del flujo de ejecución por grupo de expertos * Memoria compartida jerárquica: Optimización del intercambio de resultados intermedios entre expertos * Paralelismo de modelo asincrónico: Minimización de latencia durante la ejecución distribuida de expertos</p>
</section>
<section id="intercambio-cuantificado-de-expertos" class="level4">
<h4 class="anchored" data-anchor-id="intercambio-cuantificado-de-expertos">5.2 Intercambio cuantificado de expertos</h4>
<p>Investigaciones recientes han desarrollado técnicas para reducir el ancho de banda de comunicación al cuantizar los parámetros del experto a 4 bits[5]. Se aplica la técnica de cuantización diferencial (Differential Quantization). <span class="math inline">\(\Delta W_{i} = \text{sign}(W_{i}-\hat{W})\cdot 2^{\lfloor \log_{2}|W_{i}-\hat{W}|\rfloor}\)</span> Aquí, <span class="math inline">\(\hat{W}\)</span> representa la matriz base compartida, y solo se cuantizan las desviaciones por experto para minimizar la pérdida de precisión.</p>
</section>
</section>
<section id="últimas-tendencias-en-expansión-teórica" class="level3">
<h3 class="anchored" data-anchor-id="últimas-tendencias-en-expansión-teórica">6. Últimas tendencias en expansión teórica</h3>
<section id="espacio-de-expertos-continuo-continuous-expert-space" class="level4">
<h4 class="anchored" data-anchor-id="espacio-de-expertos-continuo-continuous-expert-space">6.1 Espacio de expertos continuo (Continuous Expert Space)</h4>
<p>La investigación más reciente de Google DeepMind en 2025 propuso CES-MoE, que modela los expertos como una distribución en un espacio continuo en lugar de objetos discretos. Utiliza un modelo de difusión de expertos basado en movimiento browniano.</p>
<p><span class="math inline">\(dE_t = \mu(E_t,t)dt + \sigma(t)dW_t\)</span></p>
<p>Este enfoque modela la evolución gradual de las características del experto y muestra un rendimiento excelente en adaptación dinámica de dominio (Dynamic Domain Adaptation).</p>
</section>
<section id="expertos-basados-en-ecuaciones-diferenciales-neuronales" class="level4">
<h4 class="anchored" data-anchor-id="expertos-basados-en-ecuaciones-diferenciales-neuronales">6.2 Expertos basados en ecuaciones diferenciales neuronales</h4>
<p>En las arquitecturas MoE de próxima generación, se están realizando investigaciones para reemplazar las redes de expertos con ecuaciones diferenciales neuronales (Neural ODEs)</p>
<p><span class="math inline">\(\frac{d\mathbf{h}(t)}{dt} = f_\theta(\mathbf{h}(t), t)\)</span></p>
<p>Esto permite modelar las características evolutivas temporales de los expertos y ha logrado una mejora en el rendimiento en tareas de inferencia a largo plazo (Long-horizon Inference).</p>
</section>
</section>
<section id="desafíos-y-direcciones-futuras" class="level3">
<h3 class="anchored" data-anchor-id="desafíos-y-direcciones-futuras">7. Desafíos y direcciones futuras</h3>
<section id="análisis-profundo-de-las-limitaciones-teóricas" class="level4">
<h4 class="anchored" data-anchor-id="análisis-profundo-de-las-limitaciones-teóricas">7.1 Análisis profundo de las limitaciones teóricas</h4>
<ul>
<li>Cuello de botella informativo: Sesgo en la selección de expertos debido a las limitaciones de capacidad de procesamiento de información del router</li>
<li>Optimización no convexa: Problema de múltiples mínimos locales en el espacio de combinación experto-gate</li>
<li>Redundancia de conocimiento: Falta de base teórica para el aprendizaje de características redundantes entre expertos #### 7.2 Marco de trabajo para la próxima generación de investigación</li>
<li>Geometría diferencial estocástica (Stochastic Differential Geometry)
<ul>
<li>Estrategias de exploración eficiente a través del análisis de curvatura de variedades especializadas</li>
</ul></li>
<li>Expertos en superposición cuántica (Quantum Superposition Experts)
<ul>
<li>Utilización de estados de superposición basados en qubits para expertos</li>
</ul></li>
<li>Imitación biológica de la plasticidad
<ul>
<li>Reconstrucción dinámica de expertos aplicando el principio de plasticidad sináptica</li>
</ul></li>
</ul>
</section>
</section>
<section id="estudios-de-caso-de-aplicación-práctica" class="level3">
<h3 class="anchored" data-anchor-id="estudios-de-caso-de-aplicación-práctica">8. Estudios de caso de aplicación práctica</h3>
<section id="diseño-de-sistemas-de-inferencia-a-gran-escala" class="level4">
<h4 class="anchored" data-anchor-id="diseño-de-sistemas-de-inferencia-a-gran-escala">8.1 Diseño de sistemas de inferencia a gran escala</h4>
<p>El sistema HyperClova X-MoE de Naver distribuyó 1,024 expertos mediante clústerización jerárquica.</p>
<ul>
<li>Enrutamiento jerárquico en tres etapas: filtrado de expertos por etapa (clúster→rack→nodo)</li>
<li>Reconstrucción dinámica de la disposición: optimización de la ubicación de los expertos basada en RL</li>
<li>Caché de precisión mixta: gestión de FP8 para expertos calientes y FP16 para expertos fríos</li>
</ul>
</section>
<section id="expansión-de-aplicaciones-multimodales" class="level4">
<h4 class="anchored" data-anchor-id="expansión-de-aplicaciones-multimodales">8.2 Expansión de aplicaciones multimodales</h4>
<p>GPT-4o de OpenAI aplicó MoE al aprendizaje multimodal.</p>
<p><span class="math inline">\(\mathbf{h}_{\text{fused}} = \sum_{i=1}^N G(\mathbf{x}_{\text{text}} \oplus \mathbf{x}_{\text{image}})_i E_i(\mathbf{x}_{\text{text}}, \mathbf{x}_{\text{image}})\)</span></p>
<p>Operaron expertos en un espacio de incrustación conjunto texto-imagen, mejorando el rendimiento de la inferencia cruzada modal.</p>
<hr>
<p><strong>Referencias:</strong></p>
<p>[1] Fedus, W., Zoph, B., &amp; Shazeer, N. (2021). Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity. <em>arXiv preprint arXiv:2101.03961</em>.</p>
<p>[2] Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q., Hinton, G., &amp; Dean, J. (2017). Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer. <em>arXiv preprint arXiv:1701.06538</em>.</p>
<p>[3] Jacobs, R. A., Jordan, M. I., Nowlan, S. J., &amp; Hinton, G. E. (1991). Adaptive mixtures of local experts. <em>Neural computation</em>, <em>3</em>(1), 79-87.</p>
<p>[4] NVIDIA Developer Blog. (2024). Applying Mixture of Experts in LLM Architectures. <a href="https://developer.nvidia.com/blog/applying-mixture-of-experts-in-llm-architectures/">https://developer.nvidia.com/blog/applying-mixture-of-experts-in-llm-architectures/</a></p>
<p>[5] Materiales relacionados con DeepSeek-V2: * Modu Labs Blog. <a href="https://modulabs.co.kr/blog/deepseek-r1-introduction">https://modulabs.co.kr/blog/deepseek-r1-introduction</a> * HyperLab. <a href="https://hyperlab.hits.ai/blog/ai-deepseek">https://hyperlab.hits.ai/blog/ai-deepseek</a> * Wikidocs. <a href="https://wikidocs.net/275230">https://wikidocs.net/275230</a> [6] Chung, E. (2023). Arquitectura de próxima generación después del Transformer de tendencias - MoE, SSM, RetNet, V-JEPA. <em>Velog</em>. <a href="https://velog.io/@euisuk-chung/%ED%8A%B8%EB%A0%8C%EB%93%9C-%ED%8A%B8%EB%A0%8C%EC%8A%A4%ED%8F%AC%EB%A8%B8-%EC%9D%B4%ED%9B%84%EC%9D%98-%EC%B0%A8%EC%84%B8%EB%8C%80-%EC%95%84%ED%82%A4%ED%85%8D%EC%B3%90-MoE-SSM-RetNet-V-JEPA">https://velog.io/<span class="citation" data-cites="euisuk-chung">@euisuk-chung</span>/%ED%8A%B8%EB%A0%8C%EB%93%9C-%ED%8A%B8%EB%A0%8C%EC%8A%A4%ED%8F%AC%EB%A8%B8-%EC%9D%B4%ED%9B%84%EC%9D%98-%EC%B0%A8%EC%84%B8%EB%8C%80-%EC%95%84%ED%82%A4%ED%85%8D%EC%B3%90-MoE-SSM-RetNet-V-JEPA</a></p>
<p>[7] The Moonlight. (2024). GG MoE vs MLP en Datos Tabulares. <a href="https://www.themoonlight.io/ko/review/gg-moe-vs-mlp-on-tabular-data">https://www.themoonlight.io/ko/review/gg-moe-vs-mlp-on-tabular-data</a></p>
<p>[8] Unite.AI. (2024). Modelo MoE 8x7B de la Última Versión de Mistral AI. <a href="https://www.unite.ai/ko/mistral-ais-latest-mixture-of-experts-moe-8x7b-model/">https://www.unite.ai/ko/mistral-ais-latest-mixture-of-experts-moe-8x7b-model/</a></p>
<p>[9] Turing Post (2024) Benchmark MS EUREKA. [<a href="https://turingpost.co.kr/p/ms-eureka-benchmark">https://turingpost.co.kr/p/ms-eureka-benchmark</a>](<a href="https://www.google.com/search?q=https://turingpost.co">https://turingpost.co</a></p>
</section>
</section>
</section>
</div>
</div>
</section>
<section id="ejercicios-de-práctica" class="level2">
<h2 class="anchored" data-anchor-id="ejercicios-de-práctica">Ejercicios de práctica</h2>
<p><strong>Problemas básicos</strong></p>
<ol type="1">
<li>Explique por qué la complejidad computacional del mecanismo de atención en los transformadores aumenta cuadráticamente con la longitud de la secuencia.</li>
<li>Explique cómo FlashAttention optimiza las operaciones de atención utilizando la jerarquía de memoria de GPU.</li>
<li>Explique la diferencia entre MQA (Multi-Query Attention) y GQA (Grouped-Query Attention), y compare sus ventajas y desventajas.</li>
<li>Explique el principio por el cual PagedAttention y vLLM mejoran la velocidad de inferencia y el rendimiento en modelos de lenguaje a gran escala.</li>
<li>Compare el funcionamiento de la atención jerárquica (Hierarchical Attention) y del Recurrent Memory Transformer para el procesamiento de contextos largos, y explique sus respectivas ventajas y desventajas.</li>
</ol>
<p><strong>Problemas de aplicación</strong></p>
<ol type="1">
<li>Escriba un código para realizar una tarea de clasificación de texto (text classification) en un conjunto de datos de texto dado utilizando un modelo de transformador. (Use el ejemplo de efficient_encoder del capítulo 9.5, aplicando técnicas de optimización como FlashAttention, Pre-LN structure y Gradient Checkpointing).</li>
<li>Escriba un código para realizar una tarea de conversión de números a palabras usando el modelo Simple Mistral descrito en el capítulo 9.5, y evalúe el rendimiento del modelo.</li>
<li>Escriba un código para realizar una tarea de conversión de lenguaje natural a SQL utilizando el modelo Simple Mistral descrito en el capítulo 9.5, y evalúe el rendimiento del modelo.</li>
<li>Explique el concepto de Constitutional AI y proponga métodos para aplicarlo a los modelos de transformador con el fin de reforzar las restricciones éticas/seguras del modelo. (No es necesario implementar).</li>
</ol>
<p><strong>Problemas avanzados</strong></p>
<ol type="1">
<li>Analice matemáticamente cómo el procesamiento por bloques de FlashAttention mejora la eficiencia de memoria y compare su complejidad computacional con los mecanismos de atención tradicionales.</li>
<li>Investigue otros métodos para reducir el tamaño del caché KV además de MQA y GQA, y compare sus ventajas y desventajas.</li>
<li>Proponga un nuevo mecanismo de atención para el procesamiento de contextos largos y explique cómo se diferencia de los métodos existentes. (Un nivel de propuesta de ideas es suficiente).</li>
<li>Identifique las limitaciones de Constitutional AI y proponga soluciones para superarlas. (Un nivel de propuesta de ideas es suficiente).</li>
</ol>
<div class="callout callout-style-default callout-note callout-titled" title="Haga clic para ver el contenido (soluciones de los ejercicios)">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Haga clic para ver el contenido (soluciones de los ejercicios)
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<section id="soluciones-de-ejercicios" class="level2 callout-body-container callout-body">
<h2 class="anchored" data-anchor-id="soluciones-de-ejercicios">Soluciones de Ejercicios</h2>
<section id="problemas-básicos" class="level3">
<h3 class="anchored" data-anchor-id="problemas-básicos">Problemas Básicos</h3>
<ol type="1">
<li><p><strong>Complejidad computacional del mecanismo de atención:</strong> El mecanismo de atención calcula la relación entre cada par de tokens. Cuando la longitud de la secuencia es n, para cada uno de los n tokens se deben calcular las relaciones con los otros (n-1) tokens, por lo que se necesitan aproximadamente n * (n-1) ≈ n² operaciones. Por lo tanto, la complejidad computacional es O(n²).</p></li>
<li><p><strong>Optimización FlashAttention:</strong> FlashAttention aprovecha al máximo el SRAM (memoria rápida) de la GPU. Divide las entradas en pequeños bloques, los carga en el SRAM y realiza cálculos de atención a nivel de bloque, luego escribe los resultados de nuevo en el HBM (memoria lenta). De esta manera, se reduce el número de accesos al HBM, minimizando la I/O de memoria y aumentando la velocidad de cálculo.</p></li>
<li><p><strong>MQA vs.&nbsp;GQA:</strong></p>
<ul>
<li><strong>MQA (Multi-Query Attention):</strong> Todos los cabezales comparten las mismas matrices Key, Value. Reduce el tamaño del caché KV, disminuyendo el uso de memoria y mejorando la velocidad, pero puede reducir la expresividad.</li>
<li><strong>GQA (Grouped-Query Attention):</strong> Divide las consultas en varios grupos, y cada grupo comparte las matrices Key, Value. Tiene mayor expresividad que MQA y es más eficiente en términos de memoria que Multi-Head Attention.</li>
</ul></li>
<li><p><strong>PagedAttention &amp; vLLM:</strong> PagedAttention adopta el concepto de paginación del sistema operativo para almacenar el caché KV en bloques de memoria no contiguos (páginas). vLLM utiliza PagedAttention para reducir el desperdicio de memoria, gestionando dinámicamente el caché KV y mejorando la velocidad de inferencia y el rendimiento.</p></li>
<li><p><strong>Hierarchical Attention vs.&nbsp;Recurrent Memory Transformer:</strong></p>
<ul>
<li><strong>Hierarchical Attention:</strong> Procesa las entradas en múltiples niveles (por ejemplo, palabra -&gt; oración -&gt; párrafo). Calcula la atención en cada nivel y agrega la información al nivel superior para capturar dependencias a largo plazo. Puede aumentar significativamente la cantidad de cálculos.</li>
<li><strong>Recurrent Memory Transformer (RMT):</strong> Almacena la información del segmento anterior en forma de vectores de memoria, y utiliza esta memoria al procesar el segmento actual. Divide las secuencias largas en pequeños segmentos que se procesan secuencialmente, lo que reduce el uso de memoria pero dificulta el procesamiento paralelo.</li>
</ul></li>
</ol>
</section>
<section id="problemas-aplicados" class="level3">
<h3 class="anchored" data-anchor-id="problemas-aplicados">Problemas Aplicados</h3>
<ol type="1">
<li><p><strong>Escribir código para clasificación de texto:</strong> (Se omite la escritura del código) Basándose en el ejemplo del Capítulo 9.5, utiliza la función <code>efficient_encoder</code> en lugar de <code>nn.TransformerEncoderLayer</code>, y aplica FlashAttention, Pre-LN, Gradient Checkpointing. Agrega código para cargar y preprocesar el conjunto de datos, entrenar y evaluar el modelo.</p></li>
<li><p><strong>Conversión número-palabra:</strong> (Se omite la escritura del código) Carga un modelo Simple Mistral y prepara los datos de entrenamiento como pares de números y sus representaciones en palabras. Entrena el modelo y evalúa su rendimiento con datos de prueba. (Ejemplo: puntuación BLEU)</p></li>
<li><p><strong>Conversión lenguaje natural-SQL:</strong> (Se omite la escritura del código) Carga un modelo Simple Mistral y prepara los datos de entrenamiento como pares de preguntas en lenguaje natural y consultas SQL. Entrena el modelo y evalúa su rendimiento con datos de prueba. (Ejemplo: precisión, viabilidad)</p></li>
<li><p><strong>Propuesta de Constitutional AI:</strong> (Se omite la implementación) Constitutional AI define una serie de reglas (constitución) para las respuestas del modelo y evalúa y modifica las respuestas según estas reglas. Para aplicar esto a un modelo transformer, se pueden: (1) definir reglas éticas/seguras, (2) agregar un módulo separado para evaluar la salida del modelo, o (3) utilizar una función de pérdida que refleje las reglas durante el proceso de fine-tuning.</p></li>
</ol>
</section>
<section id="problemas-avanzados" class="level3">
<h3 class="anchored" data-anchor-id="problemas-avanzados">Problemas Avanzados</h3>
<ol type="1">
<li><p><strong>Análisis matemático de FlashAttention:</strong> (se omite el análisis matemático) FlashAttention reduce la cantidad de accesos a HBM mediante operaciones en bloques. Mientras que la atención tradicional requiere acceso a memoria O(n²), FlashAttention solo necesita acceso a HBM O(n²/B), donde B es el tamaño del bloque (limitado por el tamaño de SRAM de GPU).</p></li>
<li><p><strong>Métodos para reducir el tamaño del caché KV:</strong></p>
<ul>
<li><strong>Cuantización (Quantization):</strong> Representar los valores del caché KV con baja precisión (por ejemplo, 8-bit) para reducir el uso de memoria.</li>
<li><strong>Dispersidad (Sparsity):</strong> Eliminar o convertir en cero las partes de los pesos de atención con un valor bajo para comprimir el caché KV.</li>
<li><strong>Aproximación de baja rango (Low-Rank Approximation):</strong> Aproximar y almacenar la matriz KV como una matriz de baja dimensión.</li>
</ul></li>
<li><p><strong>Nuevos mecanismos de atención propuestos:</strong> (sugerencias de ideas)</p>
<ul>
<li><strong>Atención Local + Global:</strong> Procesar el contexto local (por ejemplo, palabras cercanas) con atención tradicional y las dependencias a largo plazo con atención dispersa o mecanismos de memoria.</li>
<li><strong>Amplitud de atención adaptable (Adaptive Attention Span):</strong> Asignar una amplitud de atención diferente para cada token para reducir cálculos innecesarios.</li>
</ul></li>
<li><p><strong>Limitaciones y soluciones para el Constitutional AI:</strong></p>
<ul>
<li><strong>Limitaciones:</strong> Ambigüedad en la constitución (reglas), posibilidad de conflictos entre reglas, dificultad para abordar nuevos tipos de respuestas perjudiciales.</li>
<li><strong>Soluciones:</strong> Jerarquizar y especificar las reglas, introducir mecanismos de resolución de conflictos, actualizar y verificar continuamente las reglas, aprendizaje por refuerzo a través del feedback humano.</li>
</ul></li>
</ol>
</section>
</section>
</div>
</div>
</section>
<section id="material-de-referencia" class="level2">
<h2 class="anchored" data-anchor-id="material-de-referencia">material de referencia</h2>
<ol type="1">
<li><strong>Attention Is All You Need (Paper Original del Transformer):</strong> Artículo que propone por primera vez la estructura básica del modelo de transformador y el mecanismo de atención. <a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://arxiv.org/abs/1706.03762">https://arxiv.org/abs/1706.03762</a></li>
<li><strong>FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness:</strong> Artículo que propone FlashAttention, una optimización de las operaciones de atención utilizando la jerarquía de memoria de GPU. <a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://arxiv.org/abs/2205.14135">https://arxiv.org/abs/2205.14135</a></li>
<li><strong>FlashAttention-v2: Faster Attention with Better Parallelism and Work Partitioning:</strong> Versión mejorada de FlashAttention que ofrece mayor velocidad y mejor paralelismo. <a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://arxiv.org/abs/2307.08691">https://arxiv.org/abs/2307.08691</a></li>
<li><strong>Scaling Transformer to 1M tokens and beyond with RMT:</strong> Método para escalar la longitud del contexto de los modelos de transformador a más de 1 millón de tokens utilizando Recurrent Memory Transformers (RMT). <a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://arxiv.org/abs/2311.02394">https://arxiv.org/abs/2311.02394</a></li>
<li><strong>Mistral-7B:</strong> Descripción del modelo de lenguaje de alto rendimiento Mistral-7B, que tiene 7 mil millones de parámetros. <a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://arxiv.org/abs/2310.06825">https://arxiv.org/abs/2310.06825</a></li>
<li><strong>The Illustrated Transformer:</strong> Recurso de blog que explica fácilmente el funcionamiento del modelo de transformador mediante ilustraciones. <a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=http://jalammar.github.io/illustrated-transformer/">http://jalammar.github.io/illustrated-transformer/</a></li>
<li><strong>Hugging Face Transformers Documentation:</strong> Documentación oficial de la biblioteca Hugging Face Transformers, que ayuda a usar y aprender modelos de transformador fácilmente. <a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://huggingface.co/transformers/">https://huggingface.co/transformers/</a></li>
<li><strong>PyTorch Documentation:</strong> Documentación oficial del marco de aprendizaje profundo PyTorch, que proporciona funciones necesarias para implementar y entrenar modelos de transformador. <a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://pytorch.org/docs/stable/index.html">https://pytorch.org/docs/stable/index.html</a></li>
<li><strong>TensorFlow Documentation:</strong> Documentación oficial del marco de aprendizaje profundo TensorFlow, que proporciona API para implementar y entrenar modelos de transformador. <a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://www.tensorflow.org/api_docs">https://www.tensorflow.org/api_docs</a></li>
<li><strong>The Annotated Transformer:</strong> Recurso del grupo Harvard NLP que explica detalladamente el artículo “Attention is all you need” con código de PyTorch. <a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=http://nlp.seas.harvard.edu/2018/04/03/attention.html">http://nlp.seas.harvard.edu/2018/04/03/attention.html</a></li>
<li><strong>DeepMind’s Blog on AlphaFold:</strong> Artículo de blog de DeepMind sobre el modelo de predicción de estructura de proteínas AlphaFold, que utiliza técnicas basadas en transformadores. <a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://www.deepmind.com/blog/alphafold-a-solution-to-a-50-year-old-grand-challenge-in-biology">https://www.deepmind.com/blog/alphafold-a-solution-to-a-50-year-old-grand-challenge-in-biology</a></li>
<li><strong>LongLoRA:</strong> Método LongLoRA para ajustar eficientemente modelos de lenguaje a gran escala con técnicas de LoRA. <a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://arxiv.org/abs/2311.02394">https://arxiv.org/abs/2311.02394</a></li>
</ol>
<p>Note: The last item (LongLoRA) seems to be a duplicate of the fourth item in the list, which also links to <a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://arxiv.org/abs/2311.02394">https://arxiv.org/abs/2311.02394</a>. If this is not intended, please clarify or provide the correct reference. 1. <strong>Attention Is All You Need (Paper Original del Transformer):</strong> Paper que propuso la estructura básica del modelo de transformador y el mecanismo de atención. <a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://arxiv.org/abs/1706.03762">https://arxiv.org/abs/1706.03762</a> 2. <strong>FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness:</strong> Paper que propone FlashAttention, que optimiza el cálculo de atención utilizando la jerarquía de memoria de GPU. <a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://arxiv.org/abs/2205.14135">https://arxiv.org/abs/2205.14135</a> 3. <strong>FlashAttention-v2: Faster Attention with Better Parallelism and Work Partitioning:</strong> Versión mejorada de FlashAttention que ofrece mayor velocidad y procesamiento paralelo mejorado. <a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://arxiv.org/abs/2307.08691">https://arxiv.org/abs/2307.08691</a> 4. <strong>Scaling Transformer to 1M tokens and beyond with RMT:</strong> Propone cómo escalar la longitud de contexto del modelo de transformador a más de 1M de tokens utilizando Recurrent Memory Transformer (RMT). <a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://arxiv.org/abs/2304.11062">https://arxiv.org/abs/2304.11062</a> 5. <strong>Constitutional AI: Harmlessness from AI Feedback:</strong> Propone el marco de Constitutional AI para controlar las respuestas del modelo de IA según principios éticos. <a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://arxiv.org/abs/2212.08073">https://arxiv.org/abs/2212.08073</a> 6. <strong>vLLM: Easy, Fast, and Cheap LLM Serving with PagedAttention:</strong> Introduce la biblioteca vLLM que mejora la velocidad de inferencia y el rendimiento del procesamiento de modelos de lenguaje a gran escala utilizando PagedAttention. <a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://arxiv.org/abs/2309.06180">https://arxiv.org/abs/2309.06180</a>, <a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://vllm.ai/">https://vllm.ai/</a> 7. <strong>GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints:</strong> Introduce la técnica GQA para entrenar eficientemente modelos de atención multi-query a partir de puntos de control de atención multi-cabeza. <a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://arxiv.org/abs/2305.13245">https://arxiv.org/abs/2305.13245</a> 8. <strong>LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models:</strong> Propone el método LongLoRA para afinar eficientemente modelos de lenguaje a gran escala con contexto largo. <a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://arxiv.org/abs/2311.02394">https://arxiv.org/abs/2311.02394</a> 9. <strong>Mistral-7B:</strong> Descripción del modelo de lenguaje de alto rendimiento Mistral-7B, que cuenta con 7 mil millones de parámetros. <a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://arxiv.org/abs/2310.06825">https://arxiv.org/abs/2310.06825</a> 10. <strong>The Illustrated Transformer:</strong> Material de blog que explica fácilmente el funcionamiento del modelo Transformer con ilustraciones. <a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=http://jalammar.github.io/illustrated-transformer/">http://jalammar.github.io/illustrated-transformer/</a> 11. <strong>Hugging Face Transformers Documentation:</strong> Documentación oficial de la biblioteca Hugging Face Transformers, que ayuda a usar y aprender modelos Transformer de manera sencilla. <a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://huggingface.co/transformers/">https://huggingface.co/transformers/</a> 12. <strong>PyTorch Documentation:</strong> Documentación oficial del marco de deep learning PyTorch, que proporciona funciones necesarias para la implementación y aprendizaje de modelos Transformer. <a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://pytorch.org/docs/stable/index.html">https://pytorch.org/docs/stable/index.html</a> 13. <strong>TensorFlow Documentation:</strong> Documentación oficial del marco de deep learning TensorFlow, que proporciona API para la implementación y aprendizaje de modelos Transformer. <a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://www.tensorflow.org/api_docs">https://www.tensorflow.org/api_docs</a> 14. <strong>The Annotated Transformer:</strong> Material del grupo Harvard NLP que explica en detalle el artículo “Attention is all you need” con código PyTorch. <a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=http://nlp.seas.harvard.edu/2018/04/03/attention.html">http://nlp.seas.harvard.edu/2018/04/03/attention.html</a> 15. <strong>DeepMind’s Blog on AlphaFold:</strong> Artículo de blog de DeepMind sobre el modelo de predicción de estructura proteica AlphaFold, que es un caso de uso basado en Transformer. <a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://www.deepmind.com/blog/alphafold-a-solution-to-a-50-year-old-grand-challenge-in-biology">https://www.deepmind.com/blog/alphafold-a-solution-to-a-50-year-old-grand-challenge-in-biology</a></p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>