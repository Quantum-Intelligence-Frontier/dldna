<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>multimodal-deep-learning-el-inicio-de-la-fusión-multisensorial – Deep Learning DNA: Surviving Architectures and Essential Principles</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap-f507c7d0488cb7630e20aad62ad8c2aa.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script>window.MathJax = {loader: {load: ['[tex]/boldsymbol']},tex: {packages: {'[+]': ['boldsymbol']}}};</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../../notebooks/es/part_1/01_El inicio del aprendizaje profundo.html">part_1</a></li><li class="breadcrumb-item"><a href="../../../notebooks/es/part_1/10_Multimodal deep learning: el inicio de la fusión multisensorial.html">10. Multimodal deep learning: el inicio de la fusión multisensorial</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../../../">Español</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Language</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index_de.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Deutsch</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index_en.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">English</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index_es.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Español</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">한국어</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index_zh.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">中文</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/es/part_1/00_Introducción.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introducción</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">part_1</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/es/part_1/01_El inicio del aprendizaje profundo.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">1. El inicio del aprendizaje profundo</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/es/part_1/02_Matemáticas de deep learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">2. Matemáticas de deep learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/es/part_1/03_marco de aprendizaje profundo.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">3. marco de aprendizaje profundo</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/es/part_1/04_función de activación.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">4. función de activación</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/es/part_1/05_Optimización y visualización.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">5. Optimización y visualización</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/es/part_1/06_Sobreajuste y desarrollo de técnicas de solución.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">6. Sobreajuste y desarrollo de técnicas de solución</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/es/part_1/07_Evolución de las redes neuronales convolucionales.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">7. Evolución de las redes neuronales convolucionales</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/es/part_1/08_El nacimiento del transformer.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">8. El nacimiento del transformer</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/es/part_1/09_La evolución del transformer.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">9. La evolución del transformer</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/es/part_1/10_Multimodal deep learning: el inicio de la fusión multisensorial.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">10. Multimodal deep learning: el inicio de la fusión multisensorial</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/es/part_1/11_Multimodal deep learning: inteligencia más allá de los límites.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">11. Multimodal deep learning: inteligencia más allá de los límites</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">la vanguardia del deep learning</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/es/la vanguardia del deep learning/01_SLM: pequeño pero poderoso modelo de lenguaje.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">1. SLM: pequeño pero poderoso modelo de lenguaje</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/es/la vanguardia del deep learning/02_conducción autónoma.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">2. conducción autónoma</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#capítulo-aprendizaje-profundo-multimodal-el-comienzo-de-la-fusión-multisensorial" id="toc-capítulo-aprendizaje-profundo-multimodal-el-comienzo-de-la-fusión-multisensorial" class="nav-link active" data-scroll-target="#capítulo-aprendizaje-profundo-multimodal-el-comienzo-de-la-fusión-multisensorial">10. Capítulo Aprendizaje profundo multimodal: el comienzo de la fusión multisensorial</a>
  <ul class="collapse">
  <li><a href="#multimodalidad-y-aprendizaje-profundo" id="toc-multimodalidad-y-aprendizaje-profundo" class="nav-link" data-scroll-target="#multimodalidad-y-aprendizaje-profundo">10.1 Multimodalidad y aprendizaje profundo</a>
  <ul class="collapse">
  <li><a href="#el-encuentro-entre-datos-multimodales-y-aprendizaje-profundo" id="toc-el-encuentro-entre-datos-multimodales-y-aprendizaje-profundo" class="nav-link" data-scroll-target="#el-encuentro-entre-datos-multimodales-y-aprendizaje-profundo">10.1.1 El encuentro entre datos multimodales y aprendizaje profundo</a></li>
  <li><a href="#importancia-y-aplicaciones-del-aprendizaje-profundo-multimodal" id="toc-importancia-y-aplicaciones-del-aprendizaje-profundo-multimodal" class="nav-link" data-scroll-target="#importancia-y-aplicaciones-del-aprendizaje-profundo-multimodal">10.1.2 Importancia y aplicaciones del aprendizaje profundo multimodal</a></li>
  <li><a href="#historia-y-proceso-de-desarrollo-del-aprendizaje-profundo-multimodal" id="toc-historia-y-proceso-de-desarrollo-del-aprendizaje-profundo-multimodal" class="nav-link" data-scroll-target="#historia-y-proceso-de-desarrollo-del-aprendizaje-profundo-multimodal">10.1.3 Historia y proceso de desarrollo del aprendizaje profundo multimodal</a></li>
  </ul></li>
  <li><a href="#enfoques-iniciales-multimodales" id="toc-enfoques-iniciales-multimodales" class="nav-link" data-scroll-target="#enfoques-iniciales-multimodales">10.2 Enfoques iniciales multimodales</a>
  <ul class="collapse">
  <li><a href="#generación-de-descripciones-de-imágenes-primer-paso-hacia-la-fusión-multimodal" id="toc-generación-de-descripciones-de-imágenes-primer-paso-hacia-la-fusión-multimodal" class="nav-link" data-scroll-target="#generación-de-descripciones-de-imágenes-primer-paso-hacia-la-fusión-multimodal">10.2.1 Generación de descripciones de imágenes: primer paso hacia la fusión multimodal</a></li>
  <li><a href="#pregunta-y-respuesta-visual-vqa-comprensión-e-inferencia-de-imágenes" id="toc-pregunta-y-respuesta-visual-vqa-comprensión-e-inferencia-de-imágenes" class="nav-link" data-scroll-target="#pregunta-y-respuesta-visual-vqa-comprensión-e-inferencia-de-imágenes">10.2.2 Pregunta y Respuesta Visual (VQA): Comprensión e Inferencia de Imágenes</a></li>
  </ul></li>
  <li><a href="#teoría-de-fusión-multimodal-clasificación-basada-en-la-conferencia-de-cmu" id="toc-teoría-de-fusión-multimodal-clasificación-basada-en-la-conferencia-de-cmu" class="nav-link" data-scroll-target="#teoría-de-fusión-multimodal-clasificación-basada-en-la-conferencia-de-cmu">10.3 Teoría de Fusión Multimodal: Clasificación basada en la conferencia de CMU</a>
  <ul class="collapse">
  <li><a href="#representaciones-conjuntas" id="toc-representaciones-conjuntas" class="nav-link" data-scroll-target="#representaciones-conjuntas">10.3.1 Representaciones Conjuntas</a></li>
  <li><a href="#representaciones-coordinadas" id="toc-representaciones-coordinadas" class="nav-link" data-scroll-target="#representaciones-coordinadas">10.3.2 Representaciones Coordinadas</a></li>
  <li><a href="#codificador-decodificador" id="toc-codificador-decodificador" class="nav-link" data-scroll-target="#codificador-decodificador">10.3.3 Codificador-Decodificador</a></li>
  <li><a href="#estrategias-de-integración-modal" id="toc-estrategias-de-integración-modal" class="nav-link" data-scroll-target="#estrategias-de-integración-modal">10.3.4 Estrategias de Integración Modal</a></li>
  </ul></li>
  <li><a href="#técnicas-de-aprendizaje-de-representaciones-multimodales" id="toc-técnicas-de-aprendizaje-de-representaciones-multimodales" class="nav-link" data-scroll-target="#técnicas-de-aprendizaje-de-representaciones-multimodales">10.4 Técnicas de aprendizaje de representaciones multimodales</a>
  <ul class="collapse">
  <li><a href="#aprendizaje-de-representaciones-entre-modalidades" id="toc-aprendizaje-de-representaciones-entre-modalidades" class="nav-link" data-scroll-target="#aprendizaje-de-representaciones-entre-modalidades">10.4.1 Aprendizaje de representaciones entre modalidades</a></li>
  <li><a href="#estructura-de-atención-cruzada-modal" id="toc-estructura-de-atención-cruzada-modal" class="nav-link" data-scroll-target="#estructura-de-atención-cruzada-modal">10.4.2 Estructura de atención cruzada modal</a></li>
  <li><a href="#arquitectura-perceiver" id="toc-arquitectura-perceiver" class="nav-link" data-scroll-target="#arquitectura-perceiver">10.4.3 Arquitectura Perceiver</a></li>
  <li><a href="#implementación-de-atención-cruzada-y-estabilidad-del-entrenamiento" id="toc-implementación-de-atención-cruzada-y-estabilidad-del-entrenamiento" class="nav-link" data-scroll-target="#implementación-de-atención-cruzada-y-estabilidad-del-entrenamiento">10.4.4 Implementación de atención cruzada y estabilidad del entrenamiento</a></li>
  </ul></li>
  <li><a href="#transformer-de-visión-vit" id="toc-transformer-de-visión-vit" class="nav-link" data-scroll-target="#transformer-de-visión-vit">10.5 Transformer de visión (ViT)</a>
  <ul class="collapse">
  <li><a href="#cambio-de-paradigma-de-cnn-a-vit" id="toc-cambio-de-paradigma-de-cnn-a-vit" class="nav-link" data-scroll-target="#cambio-de-paradigma-de-cnn-a-vit">10.5.1 Cambio de paradigma de CNN a ViT</a></li>
  <li><a href="#principio-del-embedding-de-parches-de-imagen" id="toc-principio-del-embedding-de-parches-de-imagen" class="nav-link" data-scroll-target="#principio-del-embedding-de-parches-de-imagen">10.5.2 Principio del embedding de parches de imagen</a></li>
  <li><a href="#mecanismo-de-codificación-posicional" id="toc-mecanismo-de-codificación-posicional" class="nav-link" data-scroll-target="#mecanismo-de-codificación-posicional">10.5.3 Mecanismo de codificación posicional</a></li>
  <li><a href="#estructura-y-componentes-principales-de-vit" id="toc-estructura-y-componentes-principales-de-vit" class="nav-link" data-scroll-target="#estructura-y-componentes-principales-de-vit">10.5.4 Estructura y componentes principales de ViT</a></li>
  <li><a href="#ejemplo-de-entrenamiento-de-vit" id="toc-ejemplo-de-entrenamiento-de-vit" class="nav-link" data-scroll-target="#ejemplo-de-entrenamiento-de-vit">10.5.5 Ejemplo de Entrenamiento de ViT</a></li>
  <li><a href="#vit-22b-extremos-de-escala" id="toc-vit-22b-extremos-de-escala" class="nav-link" data-scroll-target="#vit-22b-extremos-de-escala">10.5.6 ViT-22B: Extremos de Escala</a></li>
  <li><a href="#mae-v3-aprendizaje-autónomo" id="toc-mae-v3-aprendizaje-autónomo" class="nav-link" data-scroll-target="#mae-v3-aprendizaje-autónomo">10.5.7 MAE v3: Aprendizaje Autónomo</a></li>
  </ul></li>
  <li><a href="#clip-hitos-en-el-aprendizaje-multimodal" id="toc-clip-hitos-en-el-aprendizaje-multimodal" class="nav-link" data-scroll-target="#clip-hitos-en-el-aprendizaje-multimodal">10.6 CLIP: Hitos en el aprendizaje multimodal</a>
  <ul class="collapse">
  <li><a href="#estructura-básica-de-clip-codificador-dual-dual-encoder" id="toc-estructura-básica-de-clip-codificador-dual-dual-encoder" class="nav-link" data-scroll-target="#estructura-básica-de-clip-codificador-dual-dual-encoder">10.6.1 Estructura básica de CLIP: Codificador dual (Dual Encoder)</a></li>
  <li><a href="#codificador-de-imagen" id="toc-codificador-de-imagen" class="nav-link" data-scroll-target="#codificador-de-imagen">10.6.2 Codificador de Imagen</a></li>
  <li><a href="#codificador-de-texto" id="toc-codificador-de-texto" class="nav-link" data-scroll-target="#codificador-de-texto">10.6.3 Codificador de Texto</a></li>
  <li><a href="#mecanismo-de-transferencia-cero-disparo" id="toc-mecanismo-de-transferencia-cero-disparo" class="nav-link" data-scroll-target="#mecanismo-de-transferencia-cero-disparo">10.6.4 Mecanismo de Transferencia Cero-disparo</a></li>
  </ul></li>
  <li><a href="#ejercicios-de-práctica" id="toc-ejercicios-de-práctica" class="nav-link" data-scroll-target="#ejercicios-de-práctica">Ejercicios de Práctica</a></li>
  <li><a href="#referencias" id="toc-referencias" class="nav-link" data-scroll-target="#referencias">Referencias</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../../notebooks/es/part_1/01_El inicio del aprendizaje profundo.html">part_1</a></li><li class="breadcrumb-item"><a href="../../../notebooks/es/part_1/10_Multimodal deep learning: el inicio de la fusión multisensorial.html">10. Multimodal deep learning: el inicio de la fusión multisensorial</a></li></ol></nav></header>




<p><a href="https://colab.research.google.com/github/Quantum-Intelligence-Frontier/dldna/blob/main/notebooks/es/part_1/10_multimodal_deep_learning%3A_el_comienzo_de_la_fusión_multi-sensorial.ipynb" target="_parent"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Abrir en Colab"> </a></p>
<section id="capítulo-aprendizaje-profundo-multimodal-el-comienzo-de-la-fusión-multisensorial" class="level1">
<h1>10. Capítulo Aprendizaje profundo multimodal: el comienzo de la fusión multisensorial</h1>
<blockquote class="blockquote">
<p>“La percepción no es un fragmento de un solo sentido, sino una sinfonía en la que todos los sentidos se fusionan armónicamente.”, James Gibson, fundador de la ecopsicología.</p>
</blockquote>
<p>Durante mucho tiempo en la historia de la inteligencia artificial, hubo un desafío difícil de resolver: el “multimodalidad”. Los humanos utilizamos simultáneamente varios sentidos (modalidades) como vista, audición y tacto para percibir el mundo, e integramos estos sentidos de manera orgánica. Por ejemplo, cuando bebemos café en un café, recibimos información diversa simultáneamente: el calor de la taza de café (tacto), el aroma del café (olfato), las conversaciones de las personas a nuestro alrededor (audición) y el paisaje interior del café (vista). A través de esta información, formamos una experiencia holística de estar en un café.</p>
<p>Sin embargo, los primeros modelos de inteligencia artificial tuvieron dificultades para procesar esta información multimodal. Las investigaciones en IA, que comenzaron a mediados de la década de 1950, se centraron principalmente en el procesamiento de una sola modalidad (texto, imagen, voz). Aunque hubo logros notables en cada campo, como traducción y reconocimiento de voz, integrar estos para comprender como lo hace un humano era un problema de otra dimensión.</p>
<p>En este capítulo, exploraremos a fondo las teorías fundamentales del aprendizaje profundo multimodal y las arquitecturas que han sobrevivido. Examinaremos cómo cada arquitectura ha extendido y evolucionado el ADN del aprendizaje profundo, y cómo contribuyen a resolver problemas complejos del mundo real.</p>
<section id="multimodalidad-y-aprendizaje-profundo" class="level2">
<h2 class="anchored" data-anchor-id="multimodalidad-y-aprendizaje-profundo">10.1 Multimodalidad y aprendizaje profundo</h2>
<blockquote class="blockquote">
<p><strong>Desafío:</strong> ¿Cómo integrar y procesar datos de diferentes formas, como texto, imagen y audio, en un solo modelo? Estos datos tienen formas de representación, dimensiones y características estadísticas diferentes. ¿Cómo fusionar información heterogénea para aprender una representación significativa?</p>
<p><strong>Agonía del investigador:</strong> Los investigadores necesitaban encontrar nuevos métodos que pudieran modelar eficazmente la interacción entre las modalidades mientras mantenían sus características únicas, es decir, un nuevo ADN del aprendizaje profundo. Se requería una verdadera fusión más allá de una simple concatenación, donde cada modalidad comprendiera el contexto de las otras y proporcionara información complementaria.</p>
</blockquote>
<section id="el-encuentro-entre-datos-multimodales-y-aprendizaje-profundo" class="level3">
<h3 class="anchored" data-anchor-id="el-encuentro-entre-datos-multimodales-y-aprendizaje-profundo">10.1.1 El encuentro entre datos multimodales y aprendizaje profundo</h3>
<p>Los datos multimodales se refieren a la combinación de dos o más formas diferentes de datos, como texto, imagen, audio y video. Por ejemplo, un artículo de noticias puede estar compuesto por texto e imágenes, mientras que una película está formada por video y audio. Los humanos integran naturalmente esta información multimodal; entender el entorno a través del tacto, oler y escuchar es algo completamente normal para nosotros.</p>
<p><strong>¿Por qué el aprendizaje profundo multimodal fue un problema difícil?</strong></p>
<ol type="1">
<li><p><strong>Representación de datos heterogéneos:</strong> El texto, las imágenes y los audios tienen formas de representación, dimensiones y características estadísticas diferentes. Integrar y procesar eficazmente estos datos heterogéneos en un solo modelo ha sido un desafío.</p></li>
<li><p><strong>Complejidad de la fusión de información:</strong> La simple concatenación de la información de cada modalidad no es una verdadera fusión. Se requiere modelar interacciones complejas, donde cada modalidad comprende el contexto de las otras, proporciona información complementaria y a veces armoniza información contradictoria.</p></li>
<li><p><strong>Escasez e imbalance de datos:</strong> Los datos multimodales son relativamente escasos en comparación con los datos de una sola modalidad, y también existe un problema de desequilibrio entre las diferentes modalidades. Por ejemplo, hay más datos compuestos por imágenes y texto que datos que incluyen imágenes, texto y audio.</p></li>
</ol>
<p>A pesar de estos desafíos, el aprendizaje profundo ha ofrecido nuevas posibilidades para el procesamiento de datos multimodales. Después de la década de 2010, el desarrollo de las tecnologías de aprendizaje profundo, especialmente la arquitectura del Transformer, jugó un papel decisivo en el avance del aprendizaje profundo multimodal. Esto fue un punto de inflexión crucial en el ADN del aprendizaje profundo. El mecanismo de autoatención (self-attention) del Transformer permitió modelar eficazmente no solo las relaciones entre los elementos dentro de cada modalidad, sino también las interacciones complejas entre diferentes modalidades. Hasta entonces, las CNN se habían especializado en el procesamiento de imágenes y las RNN en el procesamiento de datos secuenciales, mientras que el Transformer proporcionó una arquitectura versátil (universal) con la flexibilidad de aplicarse a diversas modalidades.</p>
</section>
<section id="importancia-y-aplicaciones-del-aprendizaje-profundo-multimodal" class="level3">
<h3 class="anchored" data-anchor-id="importancia-y-aplicaciones-del-aprendizaje-profundo-multimodal">10.1.2 Importancia y aplicaciones del aprendizaje profundo multimodal</h3>
<p>El aprendizaje profundo multimodal es una tecnología crucial para que la inteligencia artificial pueda entender e interactuar con el mundo como lo hace un ser humano. Va más allá de simplemente procesar diferentes formas de datos, conectando orgánicamente los significados contenidos en cada dato para permitir inferencias más ricas y precisas. Al igual que varias áreas del cerebro colaboran para realizar funciones cognitivas complejas, el aprendizaje profundo multimodal es una fuerza impulsora clave para elevar la inteligencia artificial a un nivel superior.</p>
<p><strong>Áreas de aplicación principales</strong></p>
<ul>
<li><p><strong>Respuesta a consultas visuales (Visual Question Answering, VQA):</strong> Genera respuestas a preguntas basadas en una imagen y una pregunta (texto). Va más allá del simple reconocimiento de objetos en la imagen, requiriendo una comprensión integral del significado de la imagen y la pregunta. Por ejemplo, para responder a “¿De qué color es el sombrero que lleva el hombre en la imagen?”, se necesita un proceso complejo que incluye encontrar al hombre, reconocer el sombrero y determinar su color.</p></li>
<li><p><strong>Generación de descripciones de imágenes (Image Captioning):</strong> Genera automáticamente texto para describir una imagen. Requiere comprender con precisión el contenido de la imagen y expresarlo en oraciones naturales.</p></li>
<li><p><strong>Análisis de sentimientos multimodal (Multimodal Sentiment Analysis):</strong> Determina los sentimientos del usuario combinando información como texto, voz y expresiones faciales. Puede detectar tonos irónicos o sutiles cambios emocionales a través de variaciones en el tono de voz o expresiones faciales que pueden ser difíciles de captar solo con texto.</p></li>
<li><p><strong>Conducción autónoma:</strong> Integra datos de varios sensores, como cámaras (imágenes), LiDAR (sensores 3D), GPS (información de ubicación) y radares para reconocer el entorno circundante y tomar decisiones de conducción. Cada sensor proporciona información diferente, y su análisis integrado es necesario para una conducción precisa.</p></li>
<li><p><strong>Manejo robótico:</strong> Para manipular objetos, un robot debe comprender la posición y forma del objeto visualmente y ajustar la fuerza adecuada basándose en la información táctil obtenida al agarrar el objeto.</p></li>
<li><p><strong>Diagnóstico médico:</strong> Combina datos de diversas fuentes, como radiografías, MRI (imágenes), registros médicos (texto), señales biológicas (datos de series temporales) y información genética para diagnosticar y predecir enfermedades. Cada conjunto de datos proporciona pistas diferentes sobre la enfermedad, y su análisis integrado es esencial para un diagnóstico preciso.</p></li>
</ul>
</section>
<section id="historia-y-proceso-de-desarrollo-del-aprendizaje-profundo-multimodal" class="level3">
<h3 class="anchored" data-anchor-id="historia-y-proceso-de-desarrollo-del-aprendizaje-profundo-multimodal">10.1.3 Historia y proceso de desarrollo del aprendizaje profundo multimodal</h3>
<p>La investigación en aprendizaje profundo multimodal es un viaje fascinante que muestra la evolución del ADN del aprendizaje profundo. Este viaje se puede dividir en las siguientes etapas principales.</p>
<section id="etapa-inicial-principios-de-2010" class="level4">
<h4 class="anchored" data-anchor-id="etapa-inicial-principios-de-2010">Etapa inicial (principios de 2010)</h4>
<p>A principios de la década de 2010, las investigaciones iniciales en aprendizaje profundo multimodal se centraron principalmente en la generación de descripciones de imágenes (image captioning) y VQA (Visual Question Answering). En esta época, los modelos basados en CNN-RNN predominaban, utilizando CNNs (Redes Neuronales Convolucionales) para extraer características de las imágenes y RNNs (Redes Neuronales Recurrentes) para procesar el texto. Las CNNs eran efectivas para capturar características espaciales en las imágenes, mientras que las RNNs eran fuertes en el procesamiento de información secuencial del texto. Sin embargo, los modelos iniciales solían utilizar principalmente el enfoque de <em>fusión tardía</em> (late fusion), que procesa cada modalidad de forma independiente y luego combina los resultados en la última etapa. Aunque este método tenía la ventaja de preservar las características únicas de cada modalidad, también tenía la limitación de no poder reflejar adecuadamente la interacción entre modalidades en las fases iniciales.</p>
<p>Entre los modelos representativos de esta época están <strong>DeViSE (Frome et al., 2013)</strong>, que proyecta imágenes y embeddings de palabras en el mismo espacio para calcular la similitud entre imagen y texto, y <strong>m-RNN (Mao et al., 2014)</strong>, que combina CNN y RNN para generar descripciones de imágenes y añade una capa multimodal para integrar información de diferentes modalidades.</p>
</section>
<section id="introducción-del-mecanismo-de-atención-mediados-de-la-década-de-2010" class="level4">
<h4 class="anchored" data-anchor-id="introducción-del-mecanismo-de-atención-mediados-de-la-década-de-2010">Introducción del mecanismo de atención (mediados de la década de 2010)</h4>
<p>A mediados de la década de 2010, la aparición del mecanismo de atención (attention mechanism) marcó un punto de inflexión importante en la investigación de deep learning multimodal. Este mecanismo permitió modelar más precisamente la relevancia entre imágenes y texto. Por ejemplo, en el caso de la generación de descripciones de imágenes, la atención permitía aprender qué regiones de la imagen “atender” al generar una palabra específica, y en VQA (Preguntas y Respuestas Visuales), ayudaba a determinar qué partes de la imagen observar para responder a una pregunta.</p>
<p>La introducción del mecanismo de atención mejoró significativamente el rendimiento de los modelos de generación de descripciones de imágenes y VQA. Algunos modelos representativos incluyen <strong>Show, Attend and Tell (Xu et al., 2015)</strong>, que introduce la atención en la generación de descripciones de imágenes para centrarse en las regiones de la imagen relevantes a las palabras generadas, y <strong>Stacked Attention Networks (Yang et al., 2016)</strong>, que aplica múltiples capas de atención a la imagen para generar respuestas a preguntas en VQA.</p>
</section>
<section id="aparición-del-transformer-y-revolución-multimodal-desde-2017" class="level4">
<h4 class="anchored" data-anchor-id="aparición-del-transformer-y-revolución-multimodal-desde-2017">Aparición del Transformer y revolución multimodal (desde 2017)</h4>
<p>En 2017, con la introducción de la arquitectura del Transformer en el artículo “Attention is All You Need”, el deep learning multimodal entró en una nueva fase. El Transformer tiene la ventaja de poder modelar directamente las relaciones entre todos los elementos de una secuencia de entrada basándose en el mecanismo de autoatención (self-attention).</p>
<ul>
<li><p><strong>ViT (Vision Transformer, 2020):</strong> ViT divide las imágenes en parches (patches) y los introduce en un Transformer. ViT se convirtió en una alternativa potente a las CNN en el procesamiento de imágenes, mostrando un rendimiento excepcional en tareas como la clasificación de imágenes al modelar eficazmente las dependencias de largo alcance dentro de las imágenes.</p></li>
<li><p><strong>CLIP (Contrastive Language-Image Pre-training, 2021):</strong> CLIP aprende a embeber imágenes y texto en el mismo espacio utilizando grandes conjuntos de datos de pares imagen-texto. Esto permitió lograr resultados revolucionarios en diversas tareas downstream, como la clasificación de imágenes y la detección de objetos, con un rendimiento sobresaliente sin necesidad de fine-tuning adicional.</p></li>
<li><p><strong>DALL-E (2021), Imagen (2022), Stable Diffusion (2022):</strong> Los modelos que generan imágenes de alta calidad a partir de descripciones de texto demostraron las sorprendentes capacidades de los modelos generativos basados en Transformers. Estos modelos aprenden relaciones complejas entre texto e imágenes, produciendo resultados de generación de imágenes a un nivel que era difícil de imaginar anteriormente.</p></li>
<li><p><strong>GPT-4V (2023), Gemini (2023):</strong> La aparición de modelos multimodales a gran escala (LMM, Large Multimodal Model) capaces de comprender y procesar texto e imágenes simultáneamente ha abierto nuevas posibilidades en el aprendizaje profundo multimodal. Estos enormes modelos con miles de millones de parámetros han logrado un rendimiento a nivel humano en diversas tareas multimodales, ubicándose en la vanguardia de la investigación de inteligencia artificial.</p></li>
</ul>
</section>
<section id="tendencias-recientes-expansión-y-fusión-de-la-inteligencia" class="level4">
<h4 class="anchored" data-anchor-id="tendencias-recientes-expansión-y-fusión-de-la-inteligencia">Tendencias recientes: expansión y fusión de la inteligencia</h4>
<p>La investigación reciente en aprendizaje profundo multimodal ha evolucionado más allá de la simple fusión de información, hacia el desarrollo de la capacidad de generar e inferir nuevo conocimiento basándose en las diferentes modalidades.</p>
<ul>
<li><p><strong>Desarrollo del LMM (Large Multimodal Model):</strong> Están surgiendo continuamente LMMs que integran más modalidades (audio, video, datos de sensores 3D, etc.) y poseen habilidades de inferencia más complejas.</p></li>
<li><p><strong>Investigación sobre técnicas eficientes de fusión:</strong> Por otro lado, también se está realizando una investigación activa en técnicas de fusión eficiente que maximicen el efecto de la integración de información mientras reducen los costos computacionales para utilizar eficazmente los modelos multimodales con recursos informáticos limitados.</p></li>
<li><p><strong>Explicabilidad (XAI) y problemas éticos:</strong> A medida que aumenta la complejidad de los modelos multimodales, también crece la importancia de la investigación dirigida a comprender el proceso de toma de decisiones del modelo y abordar cuestiones éticas como sesgos.</p></li>
</ul>
<p>En la siguiente sección examinaremos con más detalle los enfoques iniciales en el aprendizaje profundo multimodal, así como las principales arquitecturas que han “sobrevivido” durante este proceso.</p>
</section>
</section>
</section>
<section id="enfoques-iniciales-multimodales" class="level2">
<h2 class="anchored" data-anchor-id="enfoques-iniciales-multimodales">10.2 Enfoques iniciales multimodales</h2>
<p>Como se vio en la Sección 10.1.3, las transformer y CLIP han traído una innovación a la aprendizaje profundo multimodal. Sin embargo, este progreso no se logró de un día para otro. Antes de esto, ya existían numerosos intentos de combinar imágenes y texto, e incluso distintas modalidades, y estos estudios iniciales sentaron las bases sólidas del aprendizaje profundo multimodal moderno. En esta sección, examinaremos los enfoques clave y su significado que lideraron el amanecer de la investigación multimodal basada en aprendizaje profundo a principios y mediados de la década de 2010.</p>
<section id="generación-de-descripciones-de-imágenes-primer-paso-hacia-la-fusión-multimodal" class="level3">
<h3 class="anchored" data-anchor-id="generación-de-descripciones-de-imágenes-primer-paso-hacia-la-fusión-multimodal">10.2.1 Generación de descripciones de imágenes: primer paso hacia la fusión multimodal</h3>
<p>La generación de descripciones de imágenes (Image Captioning) es una tarea que consiste en generar automáticamente oraciones de lenguaje natural (descripciones) para describir una imagen dada. Esta es una problemática multimodal típica que convierte información visual (imágenes) en información lingüística (texto), y fue el principal objeto de estudio en los inicios de la investigación multimodal basada en aprendizaje profundo. La generación de descripciones de imágenes se asemeja a un niño que mira un libro de cuentos y dice: “¡Aquí hay un perro, y allá hay una pelota!”.</p>
<section id="estructura-inicial-cnn-rnn-antes-de-2014" class="level4">
<h4 class="anchored" data-anchor-id="estructura-inicial-cnn-rnn-antes-de-2014">Estructura inicial CNN-RNN (antes de 2014)</h4>
<p>En los inicios de la investigación en generación de descripciones de imágenes, los modelos combinados de CNN y RNN predominaron. Era similar a conectar dos hemisferios cerebrales dentro del cerebro del aprendizaje profundo: un hemisferio visual gestionado por CNN y otro lingüístico gestionado por RNN. La CNN actuaba como codificador de imagen, utilizando redes como VGGNet o AlexNet para extraer vectores de características de la imagen, mientras que el RNN funcionaba como decodificador de texto, usando modelos como LSTM para generar oraciones de descripción basadas en los vectores de características de la imagen.</p>
<p>Un modelo representativo es Show and Tell (Vinyals et al., 2015), que propuso un método end-to-end que introduce las características de la imagen extraídas por CNN como el estado inicial oculto del LSTM para generar descripciones. Sin embargo, esta estructura CNN-RNN capturaba bien el contenido general de la imagen, pero tenía limitaciones en modelar claramente la relación entre áreas específicas de la imagen y palabras particulares del texto.</p>
</section>
<section id="introducción-del-mecanismo-de-atención-después-de-2015" class="level4">
<h4 class="anchored" data-anchor-id="introducción-del-mecanismo-de-atención-después-de-2015">Introducción del mecanismo de atención (después de 2015)</h4>
<p>El mecanismo de atención, que “presta atención” a regiones específicas de la imagen, mejoró significativamente el rendimiento de los modelos de generación de descripciones de imágenes. El mecanismo de atención funciona similar a cómo nuestra vista naturalmente se dirige hacia las partes más importantes al apreciar una pintura.</p>
<p>Existen dos tipos principales de mecanismos de atención: Soft Attention y Hard Attention. Show, Attend and Tell (Xu et al., 2015) fue el primer modelo en introducir el mecanismo de Soft Attention a la generación de descripciones de imágenes, aprendiendo a qué región de la imagen prestar atención para generar cada palabra de la descripción, lo que permitió crear descripciones más precisas y detalladas.</p>
</section>
<section id="bottom-up-and-top-down-attention-después-de-2017" class="level4">
<h4 class="anchored" data-anchor-id="bottom-up-and-top-down-attention-después-de-2017">Bottom-Up and Top-Down Attention (después de 2017)</h4>
<p>A partir de 2017, surgió el enfoque Bottom-Up and Top-Down Attention, que combina información contextual general (top-down) con información sobre objetos individuales (bottom-up). El enfoque bottom-up utiliza modelos de detección de objetos como Faster R-CNN para identificar los objetos principales dentro de una imagen, mientras que el enfoque top-down calcula pesos de atención basados en estas características de objeto durante el proceso de generación de descripciones.</p>
<p>El modelo Bottom-Up and Top-Down Attention (Anderson et al., 2018) combinó ambos enfoques para mejorar significativamente el rendimiento en la generación de descripciones de imágenes. Esto es similar a considerar el flujo general de una historia mientras se describe detalladamente los objetos presentes en cada escena.</p>
</section>
<section id="evolución-de-la-generación-de-descripciones-de-imágenes-desde-la-perspectiva-del-adn-del-aprendizaje-profundo" class="level4">
<h4 class="anchored" data-anchor-id="evolución-de-la-generación-de-descripciones-de-imágenes-desde-la-perspectiva-del-adn-del-aprendizaje-profundo">Evolución de la generación de descripciones de imágenes desde la perspectiva del ADN del aprendizaje profundo</h4>
<p>La investigación en captioning de imágenes ha añadido elementos importantes al ADN del aprendizaje profundo. La combinación CNN-RNN presentó un marco fundamental para combinar eficazmente diferentes modalidades, y el mecanismo de atención se consolidó como una tecnología clave en el aprendizaje multimodal profundo. Además, la Atención Bottom-Up and Top-Down elevó un paso más la capacidad de comprensión de imágenes de los modelos de aprendizaje profundo.</p>
<p>Estos avances no solo han impulsado el captioning de imágenes, sino que también sentaron las bases para su posterior expansión a diversas tareas multimodales, como VQA y traducción multimodal. Recientemente, han surgido modelos basados en transformadores, como BLIP, que están demostrando un buen rendimiento no solo en el captioning de imágenes, sino también en una variedad de tareas multimodales.</p>
</section>
<section id="ejemplo-de-modelo-de-captioning-de-imágenes-blip" class="level4">
<h4 class="anchored" data-anchor-id="ejemplo-de-modelo-de-captioning-de-imágenes-blip">Ejemplo de modelo de captioning de imágenes (BLIP)</h4>
<p>BLIP (Bootstrapping Language-Image Pre-training) es un modelo basado en transformadores para el captioning de imágenes. BLIP se pre-entrena conjuntamente con imágenes y texto, lo que permite un buen rendimiento no solo en el captioning de imágenes sino también en tareas multimodales como VQA y búsqueda de imagen-texto.</p>
<p>A continuación se muestra un ejemplo de código para generar captions de imágenes utilizando el modelo BLIP a través de la biblioteca Hugging Face Transformers.</p>
<div id="cell-3" class="cell" data-execution_count="4">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># !pip install dldna[colab] # in Colab</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="co"># !pip install dldna[all] # in your local</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>load_ext autoreload</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>autoreload <span class="dv">2</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="cell-4" class="cell" data-execution_count="5">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> BlipProcessor, BlipForConditionalGeneration</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> Image</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> requests</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the model and processor</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>processor <span class="op">=</span> BlipProcessor.from_pretrained(<span class="st">"Salesforce/blip-image-captioning-base"</span>)</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> BlipForConditionalGeneration.from_pretrained(<span class="st">"Salesforce/blip-image-captioning-base"</span>)</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Download the image</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>url <span class="op">=</span> <span class="st">"http://images.cocodataset.org/val2017/000000000632.jpg"</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>image <span class="op">=</span> Image.<span class="bu">open</span>(requests.get(url, stream<span class="op">=</span><span class="va">True</span>).raw)</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the image</span></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>plt.imshow(image)</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>plt.axis(<span class="st">'off'</span>)</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Preprocess the input</span></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>inputs <span class="op">=</span> processor(image, return_tensors<span class="op">=</span><span class="st">"pt"</span>)</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate the caption</span></span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>outputs <span class="op">=</span> model.generate(<span class="op">**</span>inputs)</span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Decode and print the caption</span></span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>caption <span class="op">=</span> processor.decode(outputs[<span class="dv">0</span>], skip_special_tokens<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Generated caption:"</span>, caption)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="10_Multimodal deep learning: el inicio de la fusión multisensorial_files/figure-html/cell-3-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Generated caption: a bedroom with a bed and a window</code></pre>
</div>
</div>
</section>
</section>
<section id="pregunta-y-respuesta-visual-vqa-comprensión-e-inferencia-de-imágenes" class="level3">
<h3 class="anchored" data-anchor-id="pregunta-y-respuesta-visual-vqa-comprensión-e-inferencia-de-imágenes">10.2.2 Pregunta y Respuesta Visual (VQA): Comprensión e Inferencia de Imágenes</h3>
<p>Pregunta y Respuesta Visual (Visual Question Answering, VQA) es una tarea en la que se proporciona una imagen y una pregunta formulada en lenguaje natural, y se genera una respuesta a dicha pregunta basándose en el contenido de la imagen. Si bien la generación de descripciones de imágenes consiste en “describir” el contenido de las imágenes, VQA implica hacer “preguntas y respuestas” sobre la imagen. Por ejemplo, responder a preguntas como “¿Qué está comiendo el gato?”. VQA requiere una comprensión más compleja y de mayor nivel de las imágenes en comparación con la generación de descripciones de imágenes, especialmente la capacidad de captar y razonar sobre la relación entre la imagen y la pregunta (texto).</p>
<section id="modelos-iniciales-de-vqa-cnn-rnn-antes-de-2015" class="level4">
<h4 class="anchored" data-anchor-id="modelos-iniciales-de-vqa-cnn-rnn-antes-de-2015"><strong>Modelos Iniciales de VQA (CNN + RNN) (Antes de 2015)</strong></h4>
<p>Al igual que en la generación de descripciones de imágenes, los primeros modelos de VQA utilizaban una estructura combinada de CNN y RNN. Se extraían características de la imagen utilizando un CNN, se codificaba la pregunta con un RNN y luego se combinaban estas dos características para generar una respuesta. Sin embargo, simplemente combinar las características de la imagen y la pregunta no era suficiente para responder preguntas complejas.</p>
</section>
<section id="mecanismos-de-atención-multimodal-desde-2016" class="level4">
<h4 class="anchored" data-anchor-id="mecanismos-de-atención-multimodal-desde-2016"><strong>Mecanismos de Atención Multimodal (Desde 2016)</strong></h4>
<p>Tras el éxito de los mecanismos de atención en la generación de descripciones de imágenes, estos fueron incorporados a VQA. El Co-Attention aplica atención tanto a la imagen como a la pregunta, calculando la relevancia entre cada palabra de la pregunta y cada región de la imagen. Esto permite encontrar de manera más precisa las regiones de la imagen relacionadas con la pregunta.</p>
<p>El Stacked Attention repite el proceso de atención varias veces para capturar gradualmente las relaciones complejas entre la imagen y la pregunta. Es como si un detective revisara una foto varias veces, profundizando gradualmente en su comprensión de la relación con la pregunta.</p>
<p>Entre los modelos representativos se encuentran Stacked Attention Networks (SAN) (Yang et al., 2016) y Dual Attention Networks (DAN) (Nam et al., 2017). SAN es un modelo que aplica múltiples capas de atención a la imagen para generar una respuesta a la pregunta, mientras que DAN calcula la atención por separado para la imagen y la pregunta, y luego combina estos resultados para generar la respuesta.</p>
</section>
<section id="integración-de-conocimiento-externo-desde-2018" class="level4">
<h4 class="anchored" data-anchor-id="integración-de-conocimiento-externo-desde-2018">Integración de Conocimiento Externo (Desde 2018)</h4>
<p>La mayor diferencia entre la generación de descripciones de imágenes y VQA es la integración de conocimiento externo. Para mejorar aún más las capacidades de inferencia de los modelos de VQA, se han realizado investigaciones para utilizar el conocimiento externo (conocimientos comunes, enciclopedias, etc.). El Knowledge Base (KB) utiliza bases de datos estructuradas como Wikipedia y ConceptNet para proporcionar la información necesaria para encontrar respuestas a las preguntas.</p>
<p>Las Redes de Memoria integran el conocimiento externo en forma de memoria, buscando y utilizando la información relevante de la memoria según la pregunta. Sin embargo, utilizar eficazmente el conocimiento externo sigue siendo un desafío importante debido a la incompletitud del conocimiento, la evaluación de la relevancia con respecto a la pregunta y la complejidad del proceso de inferencia.</p>
</section>
<section id="evolución-de-vqa-desde-la-perspectiva-del-adn-de-aprendizaje-profundo" class="level4">
<h4 class="anchored" data-anchor-id="evolución-de-vqa-desde-la-perspectiva-del-adn-de-aprendizaje-profundo">Evolución de VQA desde la Perspectiva del ADN de Aprendizaje Profundo</h4>
<p>La investigación en VQA ha añadido genes importantes al ADN del aprendizaje profundo. La combinación CNN-RNN proporciona una estructura básica para combinar imágenes y texto, compartida con la generación de descripciones de imágenes. El multimodal attention otorga a los modelos de aprendizaje profundo la capacidad de modelar relaciones complejas entre imagen y pregunta, lo que significa que estos modelos no solo combinan información, sino que también comprenden las interacciones entre diferentes tipos de información y pueden realizar inferencias.</p>
<p>La integración de conocimiento externo ha abierto la posibilidad de que los modelos de aprendizaje profundo realicen inferencias de mayor nivel utilizando el conocimiento y la experiencia humanos, en lugar de depender únicamente de los datos. 10.2.1 y 10.2.2 analizaron el captioning de imágenes y VQA, que fueron dos pilares fundamentales en las primeras investigaciones de aprendizaje profundo multimodal. Estos estudios contribuyeron significativamente a aplicar y desarrollar tecnologías clave del aprendizaje profundo como CNN, RNN y mecanismos de atención a problemas multimodales, y sentaron las bases para el surgimiento de modelos multimodales más potentes basados en transformadores (como CLIP, DALL-E, GPT-4V, Gemini, etc.).</p>
<p>Recientemente, han surgido modelos VQA basados en transformadores como ViLT (Vision-and-Language Transformer), que están demostrando un buen rendimiento. ViLT introduce parches de imagen y tokens de texto en el mismo modelo de transformador para modelar eficazmente las complejas interacciones entre imágenes y texto.</p>
</section>
<section id="ejemplo-de-modelo-vqa-vilt" class="level4">
<h4 class="anchored" data-anchor-id="ejemplo-de-modelo-vqa-vilt">Ejemplo de modelo VQA (ViLT)</h4>
<p>ViLT (Vision-and-Language Transformer) es uno de los modelos VQA basados en transformadores más destacados. ViLT introduce parches de imagen y tokens de texto en el mismo modelo de transformador para modelar eficazmente las complejas interacciones entre imágenes y texto.</p>
<p>A continuación se presenta un ejemplo de código para realizar VQA utilizando el modelo ViLT con la biblioteca Transformers de Hugging Face.</p>
<div id="cell-6" class="cell" data-execution_count="6">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> ViltProcessor, ViltForQuestionAnswering</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> Image</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> requests</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="co"># 모델과 프로세서 로드</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>processor <span class="op">=</span> ViltProcessor.from_pretrained(<span class="st">"dandelin/vilt-b32-finetuned-vqa"</span>)</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> ViltForQuestionAnswering.from_pretrained(<span class="st">"dandelin/vilt-b32-finetuned-vqa"</span>)</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a><span class="co"># 이미지 다운로드</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>url <span class="op">=</span> <span class="st">"http://images.cocodataset.org/val2017/000000039769.jpg"</span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>image <span class="op">=</span> Image.<span class="bu">open</span>(requests.get(url, stream<span class="op">=</span><span class="va">True</span>).raw)</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a><span class="co"># 이미지 출력</span></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>plt.imshow(image)</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>plt.axis(<span class="st">'off'</span>)  <span class="co"># 축 제거</span></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a><span class="co"># 질문 설정</span></span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>question <span class="op">=</span> <span class="st">"How many cats are in the image?"</span></span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Question:"</span>, question)</span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a><span class="co"># 입력 전처리</span></span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>encoding <span class="op">=</span> processor(image, question, return_tensors<span class="op">=</span><span class="st">"pt"</span>)</span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a><span class="co"># 추론</span></span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>outputs <span class="op">=</span> model(<span class="op">**</span>encoding)</span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a>logits <span class="op">=</span> outputs.logits</span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a>idx <span class="op">=</span> logits.argmax(<span class="op">-</span><span class="dv">1</span>).item()</span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Predicted answer:"</span>, model.config.id2label[idx])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="10_Multimodal deep learning: el inicio de la fusión multisensorial_files/figure-html/cell-4-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Question: How many cats are in the image?
Predicted answer: 2</code></pre>
</div>
</div>
</section>
</section>
</section>
<section id="teoría-de-fusión-multimodal-clasificación-basada-en-la-conferencia-de-cmu" class="level2">
<h2 class="anchored" data-anchor-id="teoría-de-fusión-multimodal-clasificación-basada-en-la-conferencia-de-cmu">10.3 Teoría de Fusión Multimodal: Clasificación basada en la conferencia de CMU</h2>
<p>Supongamos que tenemos dos tipos de información: imágenes y texto. ¿Cómo podemos combinar esta información? El método más sencillo es agregar el vector de texto al final del vector de imagen para crear un nuevo vector. Este proceso de conectar información de fuentes de datos heterogéneas se denomina fusión (fusion). La capacidad de fusionar eficientemente la información de dos características de datos heterogéneas es fundamental en multimodalidad. Una de las razones por las que es difícil iniciar el aprendizaje profundo multimodal es porque es un campo que <strong>se está desarrollando muy rápidamente y carece de una sistematización adecuada</strong>.</p>
<p>En esta sección, nos basaremos en la clasificación propuesta en el curso de Multimodal Machine Learning de Carnegie Mellon University (CMU) para dividir la fusión multimodal en tres categorías principales. Aunque esta clasificación no es estándar en las investigaciones actuales de multimodalidad, es muy útil para entender sistemáticamente las diversas técnicas de fusión.</p>
<section id="representaciones-conjuntas" class="level3">
<h3 class="anchored" data-anchor-id="representaciones-conjuntas">10.3.1 Representaciones Conjuntas</h3>
<p>Las representaciones conjuntas son un método que representa los datos de múltiples modalidades en un espacio vectorial común (vector space). Es como si dibujáramos el texto y la imagen juntos en un solo lienzo.</p>
<p>En lugar de procesar los datos de cada modalidad por separado, se fusionan en un único vector de características integradas. Este vector encapsula la información de las modalidades. De esta manera, el modelo puede aprender las relaciones profundas entre las diferentes modalidades. Se puede manejar múltiples modalidades con un solo modelo. Además, al comprimir la información de varias modalidades en un solo vector, la estructura del modelo es relativamente simple y eficiente. Sin embargo, las características únicas de cada modalidad pueden diluirse o perderse durante el proceso de fusión. Si una modalidad contiene significativamente más información que otra, puede surgir un problema de desequilibrio de información. Y fusionar los datos de diferentes modalidades en un vector significativo es un problema muy difícil.</p>
<p>El método más simple es concatenar (concatenate) los vectores de características de cada modalidad. Además, el Modelo de Factorización Multimodal (Multi-modal Factorization Model, MFM) combina múltiples conjuntos de datos a través de la factorización matricial para crear un espacio de representación común. El Embebido Binario Discriminativo Multimodal (Multi-modal Discriminative Binary Embedding, MDBE) es un método que representa los datos multimodales como códigos binarios.</p>
<p>En investigaciones recientes, se han propuesto métodos como COSA (Concatenated Sample), que conecta secuencialmente múltiples pares imagen-texto y aplica modelos basados en transformers para aprender conjuntamente el contenido visual y las pistas temporales. Además, la Concatenación Atencional es un método para generar imágenes de alta resolución a partir del texto, utilizando una estructura en cascada de múltiples niveles y empleando los resultados de capas anteriores junto con vectores de palabras como entrada para capas posteriores.</p>
<p><strong>Ejemplo de estructura</strong></p>
<p>A continuación se muestra la ilustración de tres métodos de fusión (Concatenation, MFM, MDBF).</p>
<p><img src="../../../assets/images/10_01.png" width="800"></p>
<p><strong>Ejemplo</strong></p>
<div id="cell-8" class="cell" data-execution_count="7">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoModel, AutoProcessor, AutoTokenizer</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> Image</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> requests</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Load pre-trained models and processor/tokenizer for image and text</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>image_model_name <span class="op">=</span> <span class="st">"google/vit-base-patch16-224-in21k"</span>  <span class="co"># ViT (Vision Transformer)</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>text_model_name <span class="op">=</span> <span class="st">"bert-base-uncased"</span>  <span class="co"># BERT</span></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>image_processor <span class="op">=</span> AutoProcessor.from_pretrained(image_model_name)</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>image_model <span class="op">=</span> AutoModel.from_pretrained(image_model_name)</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(text_model_name)</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>text_model <span class="op">=</span> AutoModel.from_pretrained(text_model_name)</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Example image and text</span></span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>url <span class="op">=</span> <span class="st">"http://images.cocodataset.org/val2017/000000039769.jpg"</span></span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>image <span class="op">=</span> Image.<span class="bu">open</span>(requests.get(url, stream<span class="op">=</span><span class="va">True</span>).raw)</span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>text <span class="op">=</span> <span class="st">"Two cats sleeping on a couch."</span></span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the image</span></span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a>plt.imshow(image)</span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a>plt.axis(<span class="st">'off'</span>)  <span class="co"># Remove axes</span></span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Preprocess image and text</span></span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a>image_inputs <span class="op">=</span> image_processor(images<span class="op">=</span>image, return_tensors<span class="op">=</span><span class="st">"pt"</span>)</span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a>text_inputs <span class="op">=</span> tokenizer(text, return_tensors<span class="op">=</span><span class="st">"pt"</span>)</span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-31"><a href="#cb6-31" aria-hidden="true" tabindex="-1"></a><span class="co"># Feature extraction (embeddings) for each modality</span></span>
<span id="cb6-32"><a href="#cb6-32" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():  <span class="co"># Disable gradient calculation (inference mode)</span></span>
<span id="cb6-33"><a href="#cb6-33" aria-hidden="true" tabindex="-1"></a>    image_features <span class="op">=</span> image_model(<span class="op">**</span>image_inputs).last_hidden_state[:, <span class="dv">0</span>, :]  <span class="co"># [CLS] token embedding</span></span>
<span id="cb6-34"><a href="#cb6-34" aria-hidden="true" tabindex="-1"></a>    text_features <span class="op">=</span> text_model(<span class="op">**</span>text_inputs).last_hidden_state[:, <span class="dv">0</span>, :]   <span class="co"># [CLS] token embedding</span></span>
<span id="cb6-35"><a href="#cb6-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-36"><a href="#cb6-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-37"><a href="#cb6-37" aria-hidden="true" tabindex="-1"></a><span class="co"># Create Joint Representation (Concatenation)</span></span>
<span id="cb6-38"><a href="#cb6-38" aria-hidden="true" tabindex="-1"></a>joint_representation <span class="op">=</span> torch.cat((image_features, text_features), dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb6-39"><a href="#cb6-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-40"><a href="#cb6-40" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Image Features Shape:"</span>, image_features.shape)  <span class="co"># Image feature vector size</span></span>
<span id="cb6-41"><a href="#cb6-41" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Text Features Shape:"</span>, text_features.shape)     <span class="co"># Text feature vector size</span></span>
<span id="cb6-42"><a href="#cb6-42" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Joint Representation Shape:"</span>, joint_representation.shape) <span class="co"># Combined feature vector size (image + text)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stderr">
<pre><code>Fast image processor class &lt;class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'&gt; is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="10_Multimodal deep learning: el inicio de la fusión multisensorial_files/figure-html/cell-5-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Image Features Shape: torch.Size([1, 768])
Text Features Shape: torch.Size([1, 768])
Joint Representation Shape: torch.Size([1, 1536])</code></pre>
</div>
</div>
</section>
<section id="representaciones-coordinadas" class="level3">
<h3 class="anchored" data-anchor-id="representaciones-coordinadas">10.3.2 Representaciones Coordinadas</h3>
<p>Las Representaciones Coordinadas son un enfoque que representa cada modalidad en espacios separados, pero aprende explícitamente las relaciones entre ellos. Es similar a tener varios dibujos en diferentes lienzos, asegurando que cada lienzo se armonice con los demás.</p>
<p>Cada modalidad se representa mediante vectores de características distintos, pero estos vectores son entrenados para “coordinarse” entre sí. Esto significa que, aunque los espacios de características de cada modalidad sean independientes, aprenden las similitudes, relaciones de orden, etc., para establecer relaciones significativas entre ellos. La ventaja de este enfoque es que puede preservar al máximo las características únicas de cada modalidad, mientras considera la relevancia con otras modalidades. Además, permite aprender diversas formas de relaciones entre modalidades, lo que lo hace aplicable a una amplia gama de problemas multimodales.</p>
<p>Sin embargo, dado que cada modalidad se procesa por separado, la estructura del modelo puede ser más compleja en comparación con las Representaciones Conjuntas. Esto puede dificultar el diseño y entrenamiento del modelo. Además, aprender explícitamente las relaciones entre modalidades es un desafío.</p>
<p>Un ejemplo destacado es CLIP (Contrastive Language-Image Pre-training). CLIP procesa imágenes y texto mediante codificadores separados para obtener vectores de características y luego aprende la similitud entre ellos. CLIP se entrena para que las imágenes y el texto sean “parejas” entre sí, estableciendo relaciones significativas entre imagen y texto.</p>
<p>El éxito de CLIP es particularmente notable en su capacidad de aprendizaje zero-shot. El modelo pre-entrenado de CLIP puede clasificar o buscar nuevas imágenes sin necesidad de entrenamiento adicional para una tarea específica. Esto es posible debido a la efectiva aprendizaje de conexiones semánticas entre texto e imagen.</p>
<p><strong>Ejemplo de estructura</strong></p>
<p>A continuación se muestra una ilustración de la fusión en CLIP.</p>
<p><img src="../../../assets/images/10_02.png" width="800"></p>
<ul>
<li>Codificador de imágenes: ViT (Vision Transformer) o ResNet</li>
<li>Codificador de texto: Transformer</li>
</ul>
<p><strong>Ejemplo</strong></p>
<div id="cell-10" class="cell" data-execution_count="8">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> CLIPProcessor, CLIPModel</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> Image</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> requests</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Load CLIP model and processor</span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> CLIPModel.from_pretrained(<span class="st">"openai/clip-vit-base-patch32"</span>)</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>processor <span class="op">=</span> CLIPProcessor.from_pretrained(<span class="st">"openai/clip-vit-base-patch32"</span>)</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Example image and text</span></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>url <span class="op">=</span> <span class="st">"http://images.cocodataset.org/val2017/000000039769.jpg"</span></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>image <span class="op">=</span> Image.<span class="bu">open</span>(requests.get(url, stream<span class="op">=</span><span class="va">True</span>).raw)</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>text <span class="op">=</span> <span class="st">"Two cats sleeping on a couch."</span></span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Display image</span></span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>plt.imshow(image)</span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>plt.axis(<span class="st">'off'</span>)  <span class="co"># Remove axes</span></span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Preprocess image and text</span></span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>inputs <span class="op">=</span> processor(text<span class="op">=</span>[text], images<span class="op">=</span>image, return_tensors<span class="op">=</span><span class="st">"pt"</span>, padding<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract image and text features (embeddings)</span></span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a>    outputs <span class="op">=</span> model(<span class="op">**</span>inputs)</span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a>    image_features <span class="op">=</span> outputs.image_embeds</span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a>    text_features <span class="op">=</span> outputs.text_embeds</span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-30"><a href="#cb9-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Coordinated Representation: Keep features of each modality separate</span></span>
<span id="cb9-31"><a href="#cb9-31" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Image Features Shape:"</span>, image_features.shape)</span>
<span id="cb9-32"><a href="#cb9-32" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Text Features Shape:"</span>, text_features.shape)</span>
<span id="cb9-33"><a href="#cb9-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-34"><a href="#cb9-34" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate similarity between image and text (dot product)</span></span>
<span id="cb9-35"><a href="#cb9-35" aria-hidden="true" tabindex="-1"></a>similarity <span class="op">=</span> torch.matmul(image_features, text_features.T)  <span class="co"># Or text_features @ image_features.T</span></span>
<span id="cb9-36"><a href="#cb9-36" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Image-Text Similarity:"</span>, similarity.item())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="10_Multimodal deep learning: el inicio de la fusión multisensorial_files/figure-html/cell-6-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Image Features Shape: torch.Size([1, 512])
Text Features Shape: torch.Size([1, 512])
Image-Text Similarity: 0.29803216457366943</code></pre>
</div>
</div>
<p>Aplicando el método anterior, es posible realizar una prueba de zero-shot de la siguiente manera.</p>
<div id="cell-12" class="cell" data-execution_count="9">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Zero-shot 이미지 분류</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="co">#   - 여러 텍스트 후보군을 만들고, 각 텍스트와 이미지 간의 유사도를 계산하여 가장 높은 유사도를 갖는 텍스트를 선택</span></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>candidate_texts <span class="op">=</span> [<span class="st">"a photo of a cat"</span>, <span class="st">"a photo of a dog"</span>, <span class="st">"a photo of a bird"</span>]</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>inputs <span class="op">=</span> processor(text<span class="op">=</span>candidate_texts, images<span class="op">=</span>image, return_tensors<span class="op">=</span><span class="st">"pt"</span>, padding<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>    outputs <span class="op">=</span> model(<span class="op">**</span>inputs)</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>    image_features <span class="op">=</span> outputs.image_embeds</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>    text_features <span class="op">=</span> outputs.text_embeds</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>    logits_per_image <span class="op">=</span> outputs.logits_per_image <span class="co"># 유사도 점수</span></span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>probs <span class="op">=</span> logits_per_image.softmax(dim<span class="op">=</span><span class="dv">1</span>)  <span class="co">#  확률</span></span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>predicted_class_idx <span class="op">=</span> probs.argmax().item()</span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a>predicted_class <span class="op">=</span> candidate_texts[predicted_class_idx]</span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Predicted Class:"</span>, predicted_class)</span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Probabilities:"</span>, probs)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Predicted Class: a photo of a cat
Probabilities: tensor([[9.9403e-01, 5.1377e-03, 8.3070e-04]])</code></pre>
</div>
</div>
</section>
<section id="codificador-decodificador" class="level3">
<h3 class="anchored">10.3.3 Codificador-Decodificador</h3>
<p>El codificador-decodificador es un método que convierte datos de una modalidad en datos de otra modalidad. Es una técnica comúnmente utilizada en la traducción de lenguajes.</p>
<p>En esta estructura, el codificador (Encoder) convierte los datos de la modalidad de entrada (por ejemplo, imágenes) en vectores de características. Este vector de características representa de manera concisa la información clave de los datos de entrada. El decodificador (Decoder) genera datos de otra modalidad (por ejemplo, texto) basándose en el vector de características creado por el codificador. El decodificador “interpreta” la salida del codificador para generar un nuevo tipo de datos. Además, mediante el mecanismo de atención, el decodificador aprende a qué partes del vector de características del codificador debe “prestar atención” al generar los datos de salida.</p>
<p>Una ventaja de este enfoque es que se puede aplicar a diversas tareas que conectan formas diferentes de datos, como la generación de descripciones para imágenes, P+R visual (Pregunta-Respuesta Visual), y traducción automática. Además, es aplicable incluso cuando las modalidades de entrada y salida son diferentes, permitiendo combinaciones variadas como texto-imagen, imagen-texto, audio-texto, etc.</p>
<p>Un ejemplo destacado es la generación de descripciones para imágenes y P+R visual (Pregunta-Respuesta Visual). En la generación de descripciones para imágenes, se procesa la imagen con el codificador para obtener un vector de características, y luego se utiliza el decodificador para generar una descripción (texto). En P+R visual, se procesan tanto la imagen como la pregunta (texto) con sus respectivos codificadores, y se utiliza un mecanismo de atención para determinar la relevancia entre la imagen y la pregunta antes de usar el decodificador para generar una respuesta (texto).</p>
<p>Sin embargo, si los datos de entrada o salida son largos, puede ocurrir pérdida de información o aumento en la cantidad de cálculos. En particular, en modelos basados en RNN, el problema de desaparición del gradiente (gradient vanishing problem) puede dificultar el aprendizaje de dependencias a largo plazo. Además, al tener que aprender tanto el codificador como el decodificador simultáneamente, el proceso de entrenamiento puede ser inestable o difícil.</p>
<p><strong>Ejemplo de estructura</strong></p>
<p>Lo siguiente es una representación diagramática de la fusión de codificador-decodificador.</p>
<p><img src="../../../assets/images/10_03.png" width="800"></p>
<ul>
<li>Entrada de Imagen, Entrada de Texto: Representan respectivamente las entradas de imagen y texto (pregunta u otra información de texto).</li>
<li>Codificador de Imagen, Codificador de Texto: Son los codificadores para cada modalidad. Para el codificador de imágenes se utilizan principalmente CNN o ViT (Vision Transformer), mientras que para el codificador de texto se usan RNN o Transformer.</li>
<li>Atención: Es un mecanismo que decide a qué partes de las características de la imagen (Features del Codificador de Imagen) debe “prestar atención” el decodificador al generar texto. Las características del Codificador de Texto también pueden ser utilizadas en la Atención (Atención Multimodal).</li>
</ul>
<div id="cell-14" class="cell" data-execution_count="10">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> BlipProcessor, BlipForConditionalGeneration</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> Image</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> requests</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Load model and processor</span></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>processor <span class="op">=</span> BlipProcessor.from_pretrained(<span class="st">"Salesforce/blip-image-captioning-base"</span>)</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> BlipForConditionalGeneration.from_pretrained(<span class="st">"Salesforce/blip-image-captioning-base"</span>)</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Download image</span></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>url <span class="op">=</span> <span class="st">"http://images.cocodataset.org/val2017/000000000139.jpg"</span></span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>image <span class="op">=</span> Image.<span class="bu">open</span>(requests.get(url, stream<span class="op">=</span><span class="va">True</span>).raw)</span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Display image</span></span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>plt.imshow(image)</span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>plt.axis(<span class="st">'off'</span>)</span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Input text (optional - Conditional Generation)</span></span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a><span class="co"># text = "describe this image:"  # Prompt (guide image description)</span></span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a>text <span class="op">=</span> <span class="st">"a photo of"</span></span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Preprocess image and text (optional)</span></span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a><span class="co"># If text is provided, it uses the text as a prompt to generate the caption.</span></span>
<span id="cb13-25"><a href="#cb13-25" aria-hidden="true" tabindex="-1"></a>inputs <span class="op">=</span> processor(image, text<span class="op">=</span>text, return_tensors<span class="op">=</span><span class="st">"pt"</span>)</span>
<span id="cb13-26"><a href="#cb13-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-27"><a href="#cb13-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate caption</span></span>
<span id="cb13-28"><a href="#cb13-28" aria-hidden="true" tabindex="-1"></a>outputs <span class="op">=</span> model.generate(<span class="op">**</span>inputs)</span>
<span id="cb13-29"><a href="#cb13-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-30"><a href="#cb13-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Decode and print caption</span></span>
<span id="cb13-31"><a href="#cb13-31" aria-hidden="true" tabindex="-1"></a>caption <span class="op">=</span> processor.decode(outputs[<span class="dv">0</span>], skip_special_tokens<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb13-32"><a href="#cb13-32" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Generated caption:"</span>, caption)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="10_Multimodal deep learning: el inicio de la fusión multisensorial_files/figure-html/cell-8-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Generated caption: a photo of a living room with a television and a fireplace</code></pre>
</div>
</div>
<p>Este ejemplo muestra la imagen captioning, que es un caso típico de la estructura Encoder-Decoder. El codificador recibe una imagen (el codificador visual de BLIP) y extrae un vector de características. El decodificador genera texto (el decodificador de texto de BLIP). A través del mecanismo de atención, se decide a qué partes del vector de características de la imagen prestar atención mientras se genera el caption. Se pueden especificar prompts que influyen en el caption generado como texto. Aunque BLIP puede utilizar tanto imágenes como texto como entrada, aquí solo se usa una imagen como entrada y el texto se genera en el decodificador.</p>
<p>En las secciones 10.3.1, 10.3.2, 10.3.3 se han examinado las tres teorías clave de la fusión multimodal: Joint Representations, Coordinated Representations, Encoder-Decoder. Cada enfoque tiene sus propias características y ventajas y desventajas, por lo que es importante seleccionar el método adecuado según el campo de aplicación.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Haga clic para ver el contenido (análisis en profundidad: fusión multimodal y tendencias de investigación actuales)">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Haga clic para ver el contenido (análisis en profundidad: fusión multimodal y tendencias de investigación actuales)
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<section id="fusión-multimodal-y-tendencias-de-investigación-recientes" class="level2 callout-body-container callout-body">
<h2 class="anchored" data-anchor-id="fusión-multimodal-y-tendencias-de-investigación-recientes">Fusión Multimodal y Tendencias de Investigación Recientes</h2>
<p>La “fusión” en el aprendizaje profundo multimodal es un proceso clave que combina información de diferentes modalidades para crear representaciones más ricas y poderosas. Aunque en la sección 10.3 revisamos brevemente la teoría de fusión basada en las lecciones de CMU, la investigación real sobre fusión multimodal ha evolucionado de manera mucho más diversa y dinámica. En este análisis detallado, examinaremos varios sistemas de clasificación de la fusión y tendencias de investigación recientes, así como las tecnologías que están ganando atención en 2025.</p>
<section id="clasificaciones-variadas-de-fusión-multimodal" class="level3">
<h3 class="anchored" data-anchor-id="clasificaciones-variadas-de-fusión-multimodal">1. Clasificaciones Variadas de Fusión Multimodal</h3>
<p>La fusión multimodal es difícil de clasificar con un solo criterio. Los investigadores han categorizado los métodos de fusión desde diferentes perspectivas, y cada clasificación no es mutuamente excluyente sino complementaria.</p>
<section id="clasificación-según-el-punto-de-fusión-early-late-hybrid-fusion" class="level4">
<h4 class="anchored" data-anchor-id="clasificación-según-el-punto-de-fusión-early-late-hybrid-fusion">1.1 Clasificación según el Punto de Fusión (Early, Late, Hybrid Fusion)</h4>
<p>Esta clasificación se centra en la “etapa” del modelo de aprendizaje profundo multimodal donde ocurre la fusión. (Ver sección 10.3.4 del texto)</p>
<ul>
<li><p><strong>Early Fusion (Fusión Temprana):</strong> Combina los datos “crudos” (o características procesadas muy temprano) de cada modalidad en la etapa de entrada del modelo.</p>
<ul>
<li><strong>Investigación Reciente:</strong> El conjunto de datos LUMA (publicado en junio de 2024) es un benchmark multimodal que incluye audio, imágenes y texto, con 50 clases. Proporciona un entorno para verificar la efectividad de la fusión temprana en datos modales inciertos y diversos. Este conjunto de datos amplía CIFAR 10/100 e incluye muestras de audio extraídas de tres corpuses de audio y datos de texto generados por el LLM Gemma-7B, permitiendo inyectar diversos tipos y grados de incertidumbre de manera controlada.</li>
</ul></li>
<li><p><strong>Late Fusion (Fusión Tardía):</strong> Procesa cada modalidad con modelos separados y combina las salidas de los modelos (por ejemplo, resultados de predicción) en la última etapa.</p>
<ul>
<li><strong>Investigación Reciente:</strong> Según un estudio publicado en enero de 2024, la fusión tardía (o fusión de modelos) es una metodología efectiva para combinar decisiones obtenidas de diferentes modalidades en el campo del diagnóstico médico, especialmente en el diagnóstico de enfermedades de piel. Este enfoque permite utilizar modelos especializados para cada modalidad, maximizando las fortalezas individuales de cada uno.</li>
</ul></li>
<li><p><strong>Hybrid Fusion (Fusión Híbrida):</strong> Combina los métodos de fusión temprana y tardía. Realiza la fusión en múltiples etapas del modelo para aprovechar información a diferentes niveles.</p>
<ul>
<li><strong>Investigación Reciente:</strong> CAST (Cross Attention based multimodal fusion of Structure and Text) es un estudio publicado el 6 de febrero de 2025 que propone un enfoque híbrido para la fusión efectiva de datos estructurados y de texto en el campo de la ciencia de materiales. Este modelo combina características a nivel de nodo y token, diseñadas para interactuar estrechamente entre sí, utilizando mecanismos de atención cruzada basados en consultas clave-valor vinculadas para capturar adecuadamente las relaciones entre nodos y texto.</li>
</ul></li>
</ul>
</section>
<section id="clasificación-según-la-estructura-del-modelo" class="level4">
<h4 class="anchored" data-anchor-id="clasificación-según-la-estructura-del-modelo">1.2 Clasificación según la Estructura del Modelo</h4>
<ul>
<li><p><strong>Fusión Model-Agnostic:</strong> Técnicas de fusión generales que no dependen de un modelo específico (Early, Late, Hybrid Fusion, etc.).</p></li>
<li><p><strong>Fusión Model-Specific:</strong> Técnicas de fusión especializadas para estructuras de modelos específicas.</p>
<ul>
<li><strong>Cross-Modal Attention en Transformers:</strong> (Explicado en detalle en la sección 10.4.2)</li>
</ul></li>
<li><p><strong>Investigación más reciente:</strong> En el taller CVPR (MULA 2025) programado para los días 11-12 de junio de 2025, se discutirán investigaciones sobre estructuras de modelos para fusionar eficazmente diversos datos sensoriales (cámara, LiDAR, radar, etc.) en el campo de la conducción autónoma. Este taller tiene como objetivo fomentar la interacción y colaboración interdisciplinaria entre las comunidades de visión por computadora, multimedia, teledetección y robótica, con un enfoque particular en los enfoques multimodales para la conducción autónoma.</p></li>
</ul>
</section>
<section id="otras-clasificaciones" class="level4">
<h4 class="anchored" data-anchor-id="otras-clasificaciones">1.3 Otras clasificaciones</h4>
<ul>
<li><p><strong>Fusión simétrica vs.&nbsp;asimétrica:</strong></p>
<ul>
<li><p><strong>Simétrica:</strong> Trata todos los modos de manera igual.</p></li>
<li><p><strong>Asimétrica:</strong> Da más peso a ciertos modos o les asigna diferentes roles.</p></li>
<li><p><strong>Investigación reciente:</strong> “Learning Deep Multimodal Feature Representation with Asymmetric Multi-layer Fusion” propuso un marco efectivo para fusionar características multimodales en múltiples capas dentro de una sola red. Esta investigación introduce dos operaciones de fusión asimétrica, shuffle de canales y shift de píxeles, que aprenden diferentes características según las direcciones de fusión. Además, “Multimodal sentiment analysis based on multi-layer feature fusion” presentado en enero de 2025 propuso un nuevo enfoque para el análisis preciso del sentimiento en condiciones de desequilibrio modal e implicaciones implícitas.</p></li>
</ul></li>
<li><p><strong>Fusión explícita vs.&nbsp;implícita:</strong></p>
<ul>
<li><p><strong>Explícita:</strong> Define o modela explícitamente las relaciones entre los modos. (Ejemplo: mecanismos de atención)</p></li>
<li><p><strong>Implícita:</strong> No define directamente las relaciones entre los modos, sino que permite que el modelo aprenda estas relaciones por sí mismo. (Ejemplo: combinación simple)</p></li>
<li><p><strong>Investigación reciente:</strong> Se espera que en la Conferencia HCI International 2025 (junio de 2025) se presente una investigación comparando las ventajas y desventajas de la fusión explícita y la fusión implícita.</p></li>
</ul></li>
</ul>
</section>
</section>
<section id="tendencias-actuales-fusión-basada-en-atención-y-aprendizaje-autodirigido" class="level3">
<h3 class="anchored" data-anchor-id="tendencias-actuales-fusión-basada-en-atención-y-aprendizaje-autodirigido">2. Tendencias actuales: fusión basada en atención y aprendizaje autodirigido</h3>
<p>El mecanismo de <strong>fusión basada en atención</strong> es el enfoque más destacado en las investigaciones de 2024-2025.</p>
<section id="atención-cruzada-modal" class="level4">
<h4 class="anchored" data-anchor-id="atención-cruzada-modal">2.1 Atención cruzada modal</h4>
<ul>
<li><p><strong>Concepto:</strong> Se utiliza una característica de un modo como consulta (query) para aplicar la atención a otra característica (key-value) del otro modo. (Ver sección 10.4.2) De esta manera, el modelo puede comprender con precisión qué elementos específicos de un modo están relacionados con los elementos de otro.</p></li>
<li><p><strong>Investigación reciente</strong></p>
<ul>
<li><p>En enero de 2025, “Bi-Att3DDet” presentó un método de fusión basada en atención bidireccional para la detección de objetos 3D en conducción autónoma. Esta investigación propone una interacción bidireccional para maximizar la información complementaria entre los datos de LiDAR y cámara.</p></li>
<li><p>“LANMSFF” presentado en marzo de 2024 y revisado en febrero de 2025, combinó una red neuronal basada en atención ligera con fusión de características multiscale para el reconocimiento de expresiones faciales multi-vista. Este enfoque genera mapas de atención canal y espacial simultáneamente para destacar características importantes e inhibir las irrelevantes.</p></li>
<li><p>Recientes investigaciones neurocientíficas (2025) han examinado el impacto de la congruencia cruzada modal (cross-modal congruency) en el procesamiento y acumulación de información sensorial. Estos estudios demostraron que la congruencia entre estímulos auditivos y visuales desempeña un papel crucial en las etapas iniciales del procesamiento sensorial. #### 2.2 Atención Multi-cabeza</p></li>
</ul></li>
<li><p><strong>Concepto:</strong> Se utilizan múltiples cabezas de atención para capturar las relaciones entre modalidades desde diferentes perspectivas. Cada cabeza utiliza matrices de pesos diferentes (W_Q, W_K, W_V) para transformar los datos de entrada y calcular la atención, por lo que cada cabeza puede concentrarse en diferentes aspectos de los datos de entrada (por ejemplo, significado, estructura gramatical, estilo).</p></li>
<li><p><strong>Ventajas:</strong> Puede modelar diferentes tipos de relaciones simultáneamente, lo que permite aprender representaciones más ricas y complejas. Por ejemplo, al fusionar imágenes y texto, algunas cabezas pueden concentrarse en las relaciones entre los objetos en la imagen y las palabras en el texto, mientras que otras cabezas pueden centrarse en las relaciones entre la atmósfera general de la imagen y el tono del texto.</p></li>
<li><p><strong>Investigación reciente:</strong> Los modelos multimodales a gran escala (LMM) han ampliado y refinado esta técnica para modelar eficazmente interacciones complejas entre diferentes modalidades, como imágenes, texto, audio y video.</p></li>
</ul>
</section>
<section id="aprendizaje-automático-y-fusión-multimodal" class="level4">
<h4 class="anchored" data-anchor-id="aprendizaje-automático-y-fusión-multimodal">2.3 Aprendizaje automático y fusión multimodal</h4>
<ul>
<li><p><strong>Aprendizaje contrastivo (Contrastive Learning):</strong></p>
<ul>
<li><p><strong>Concepto:</strong> Se aprende a posicionar pares de modalidades relacionadas (por ejemplo, una imagen y su leyenda correspondiente) cerca en el espacio de embeddings, mientras que los pares no relacionados se posicionan lejos.</p></li>
<li><p><strong>Ventajas:</strong> Permite aprender eficazmente incluso con grandes conjuntos de datos sin etiquetas, lo que ayuda a resolver problemas de escasez de datos.</p></li>
<li><p><strong>Investigación reciente:</strong> “Dual-Level Cross-Modal Contrastive Clustering” (2024) propone un nuevo método de aprendizaje contrastivo para cerrar la brecha entre las representaciones visuales y los significados del texto.</p></li>
</ul></li>
<li><p><strong>Aprendizaje basado en máscaras (Masking-based Learning):</strong></p>
<ul>
<li><p><strong>Concepto:</strong> Se aprende a reconstruir una parte de la entrada que ha sido enmascarada utilizando información de otra modalidad.</p></li>
<li><p><strong>Ventajas:</strong> Permite aprender relaciones complementarias entre modalidades. Por ejemplo, se puede aprender a predecir partes ocultas de una imagen usando descripciones de texto o a predecir palabras ocultas en un texto usando una imagen.</p></li>
<li><p><strong>Investigación reciente:</strong> CAST (2025) mejoró el alineamiento entre nodos de estructuras gráficas y tokens de texto mediante la estrategia de preentrenamiento Masked Node Prediction (MNP).</p></li>
</ul></li>
</ul>
</section>
</section>
<section id="fusión-a-nivel-de-token-vs-fusión-a-nivel-de-instancia-investigación-2025" class="level3">
<h3 class="anchored" data-anchor-id="fusión-a-nivel-de-token-vs-fusión-a-nivel-de-instancia-investigación-2025">3. Fusión a nivel de token vs fusión a nivel de instancia (Investigación 2025)</h3>
<ul>
<li><p><strong>Fusión a nivel de token (Token-level Fusion):</strong> Modela las interacciones detalladas entre tokens individuales de cada modalidad (por ejemplo, parches de imagen, tokens de texto).</p>
<ul>
<li><p><strong>Ventajas:</strong> Permite capturar relaciones más precisas entre modalidades. Por ejemplo, puede aprender la relación directa entre un objeto específico en una imagen y una palabra específica en el texto.</p></li>
<li><p><strong>Investigación reciente:</strong> CAST (2025) demostró que la fusión a nivel de token entre nodos gráficos y tokens de texto en ciencias de materiales es superior a la fusión a nivel de instancia.</p></li>
</ul></li>
<li><p><strong>Fusión a nivel de instancia (Instance-level Fusion):</strong> Trata las instancias completas de cada modalidad (por ejemplo, una imagen completa, un texto completo) como una única unidad para su fusión.</p>
<ul>
<li><p><strong>Ventajas:</strong> Es computacionalmente eficiente y fácil de implementar.</p></li>
<li><p><strong>Desventajas:</strong> Puede no capturar relaciones detalladas dentro de las modalidades.</p></li>
</ul></li>
</ul>
</section>
<section id="conclusión" class="level3">
<h3 class="anchored" data-anchor-id="conclusión">4. Conclusión</h3>
<p>La fusión multimodal puede clasificarse de diversas formas, y cada forma de clasificación ofrece una perspectiva diferente. En la investigación práctica, es común combinar estas clasificaciones para aprovechar sus fortalezas. En 2025, la investigación de fusión multimodal se centra en el desarrollo de técnicas eficientes de fusión utilizando interacciones detalladas a nivel de token, mecanismos de atención cruzada y métodos de aprendizaje autónomo. En particular, eventos académicos importantes como el taller CVPR 2025 (junio del 25, Nashville) discutirán activamente el progreso de las tecnologías de fusión multimodal en diversos campos de aplicación, como conducción autónoma, diagnóstico médico y ciencia de materiales.</p>
<p>A través de este profundización, se espera comprender diversas clasificaciones de la fusión multimodal y analizar más a fondo los distintos modelos multimodales que se presentarán.</p>
</section>
</section>
</div>
</div>
<p>Ingrese el texto en coreano aquí.</p>
</section>
<section id="estrategias-de-integración-modal" class="level3">
<h3 class="anchored" data-anchor-id="estrategias-de-integración-modal">10.3.4 Estrategias de Integración Modal</h3>
<p>De las secciones 10.3.1 a 10.3.3, hemos examinado los métodos para fusionar datos multimodales. Esta es una clasificación teórica. Al diseñar un modelo multimodal real, es necesario decidir estratégicamente <em>qué método de fusión</em>, <em>en qué punto</em> y <em>cómo aplicarlo</em> según las características del problema y los datos dados. En esta sección, examinaremos las estrategias sofisticadas de integración modal adoptadas por los modelos multimodales más recientes.</p>
<section id="fusión-temprana-early-fusion" class="level4">
<h4 class="anchored" data-anchor-id="fusión-temprana-early-fusion">10.3.4.1 Fusión Temprana (Early Fusion)</h4>
<p>La fusión temprana combina las entradas de múltiples modalidades en una etapa inicial del modelo. La forma más simple es concatenar los vectores de características de cada modalidad. La ventaja de la fusión temprana es que facilita capturar interacciones de bajo nivel (low-level) entre modalidades. Por ejemplo, si el color de una imagen y ciertas palabras en un texto están fuertemente relacionadas, la fusión temprana puede facilitar el aprendizaje de estas relaciones. Sin embargo, tiene la desventaja de que puede no aprovechar adecuadamente las características de cada modalidad. Especialmente cuando se requiere procesamiento especializado para cada modalidad (por ejemplo, CNN para imágenes, RNN para texto), la fusión temprana puede ser ineficiente.</p>
<p>Las investigaciones recientes también han presentado benchmarks que evalúan el rendimiento de la fusión temprana en entornos de datos multimodales ruidosos (noisy data) y con alta incertidumbre, además del simple concatenamiento.</p>
<p>Veamos un ejemplo sencillo de fusión temprana. Aquí usamos concatenación para realizar una representación conjunta. Se utiliza el mismo código. Al final, se aplica un clasificador lineal simple para determinar si hay un gato o no.</p>
<div id="cell-19" class="cell" data-execution_count="11">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoModel, AutoProcessor, AutoTokenizer</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> Image</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> requests</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a><span class="co"># 이미지와 텍스트를 위한 사전 학습된 모델 및 프로세서/토크나이저 로드</span></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>image_model_name <span class="op">=</span> <span class="st">"google/vit-base-patch16-224-in21k"</span>  <span class="co">#  ViT (Vision Transformer)</span></span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>text_model_name <span class="op">=</span> <span class="st">"bert-base-uncased"</span>  <span class="co"># BERT</span></span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>image_processor <span class="op">=</span> AutoProcessor.from_pretrained(image_model_name)</span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a>image_model <span class="op">=</span> AutoModel.from_pretrained(image_model_name)</span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(text_model_name)</span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a>text_model <span class="op">=</span> AutoModel.from_pretrained(text_model_name)</span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a><span class="co"># 예제 이미지 및 텍스트</span></span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a>url <span class="op">=</span> <span class="st">"http://images.cocodataset.org/val2017/000000039769.jpg"</span></span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a>image <span class="op">=</span> Image.<span class="bu">open</span>(requests.get(url, stream<span class="op">=</span><span class="va">True</span>).raw)</span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a>text <span class="op">=</span> <span class="st">"Two cats sleeping on a couch."</span></span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-22"><a href="#cb15-22" aria-hidden="true" tabindex="-1"></a><span class="co"># 이미지 출력</span></span>
<span id="cb15-23"><a href="#cb15-23" aria-hidden="true" tabindex="-1"></a>plt.imshow(image)</span>
<span id="cb15-24"><a href="#cb15-24" aria-hidden="true" tabindex="-1"></a>plt.axis(<span class="st">'off'</span>)  <span class="co"># 축 제거</span></span>
<span id="cb15-25"><a href="#cb15-25" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb15-26"><a href="#cb15-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-27"><a href="#cb15-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-28"><a href="#cb15-28" aria-hidden="true" tabindex="-1"></a><span class="co"># 이미지와 텍스트 전처리</span></span>
<span id="cb15-29"><a href="#cb15-29" aria-hidden="true" tabindex="-1"></a>image_inputs <span class="op">=</span> image_processor(images<span class="op">=</span>image, return_tensors<span class="op">=</span><span class="st">"pt"</span>)</span>
<span id="cb15-30"><a href="#cb15-30" aria-hidden="true" tabindex="-1"></a>text_inputs <span class="op">=</span> tokenizer(text, return_tensors<span class="op">=</span><span class="st">"pt"</span>)</span>
<span id="cb15-31"><a href="#cb15-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-32"><a href="#cb15-32" aria-hidden="true" tabindex="-1"></a><span class="co"># 각 모달리티에 대한 특징 추출 (임베딩)</span></span>
<span id="cb15-33"><a href="#cb15-33" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():  <span class="co"># 기울기 계산 비활성화 (추론 모드)</span></span>
<span id="cb15-34"><a href="#cb15-34" aria-hidden="true" tabindex="-1"></a>    image_features <span class="op">=</span> image_model(<span class="op">**</span>image_inputs).last_hidden_state[:, <span class="dv">0</span>, :]  <span class="co"># [CLS] 토큰 임베딩</span></span>
<span id="cb15-35"><a href="#cb15-35" aria-hidden="true" tabindex="-1"></a>    text_features <span class="op">=</span> text_model(<span class="op">**</span>text_inputs).last_hidden_state[:, <span class="dv">0</span>, :]   <span class="co"># [CLS] 토큰 임베딩</span></span>
<span id="cb15-36"><a href="#cb15-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-37"><a href="#cb15-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-38"><a href="#cb15-38" aria-hidden="true" tabindex="-1"></a><span class="co"># Joint Representation 생성 (Concatenation)</span></span>
<span id="cb15-39"><a href="#cb15-39" aria-hidden="true" tabindex="-1"></a>joint_representation <span class="op">=</span> torch.cat((image_features, text_features), dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb15-40"><a href="#cb15-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-41"><a href="#cb15-41" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Image Features Shape:"</span>, image_features.shape)  <span class="co"># 이미지 특징 벡터 크기</span></span>
<span id="cb15-42"><a href="#cb15-42" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Text Features Shape:"</span>, text_features.shape)     <span class="co"># 텍스트 특징 벡터 크기</span></span>
<span id="cb15-43"><a href="#cb15-43" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Joint Representation Shape:"</span>, joint_representation.shape) <span class="co"># 결합된 특징 벡터 크기 (image + text)</span></span>
<span id="cb15-44"><a href="#cb15-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-45"><a href="#cb15-45" aria-hidden="true" tabindex="-1"></a><span class="co">#  Joint Representation을 활용한 추가 작업 (예: 분류)</span></span>
<span id="cb15-46"><a href="#cb15-46" aria-hidden="true" tabindex="-1"></a>num_labels <span class="op">=</span> <span class="dv">2</span>  <span class="co">#  예: "고양이 없음(0)" "고양이 있음(1)", 두 가지 클래스로 분류</span></span>
<span id="cb15-47"><a href="#cb15-47" aria-hidden="true" tabindex="-1"></a>classifier <span class="op">=</span> torch.nn.Linear(joint_representation.size(<span class="dv">1</span>), num_labels) <span class="co"># 간단한 선형 분류기</span></span>
<span id="cb15-48"><a href="#cb15-48" aria-hidden="true" tabindex="-1"></a>outputs <span class="op">=</span> classifier(joint_representation)</span>
<span id="cb15-49"><a href="#cb15-49" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Classification Outputs:"</span>, outputs)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stderr">
<pre><code>Fast image processor class &lt;class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'&gt; is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="10_Multimodal deep learning: el inicio de la fusión multisensorial_files/figure-html/cell-9-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Image Features Shape: torch.Size([1, 768])
Text Features Shape: torch.Size([1, 768])
Joint Representation Shape: torch.Size([1, 1536])
Classification Outputs: tensor([[0.1817, 0.0355]], grad_fn=&lt;AddmmBackward0&gt;)</code></pre>
</div>
</div>
<p>En el ejemplo anterior, las imágenes y el texto se combinan directamente con la salida de modelos separados denominados ViT y BERT, respectivamente. No se realiza un procesamiento adicional (atención, transformaciones complejas, etc.) en estos dos vectores antes de combinar las características de imagen y texto. Por lo tanto, esto corresponde a una fusión temprana.</p>
</section>
<section id="fusión-tardía-late-fusion" class="level4">
<h4 class="anchored" data-anchor-id="fusión-tardía-late-fusion">10.3.4.2 Fusión tardía (Late Fusion)</h4>
<p>La fusión tardía implica procesar cada modalidad con modelos separados y combinar la salida de cada modelo (por ejemplo, los resultados de predicción) en la última etapa. La ventaja de este método es que se pueden usar modelos especializados para cada modalidad. Por ejemplo, se puede utilizar una CNN pre-entrenada para imágenes y un Transformer pre-entrenado para texto, lo que permite extraer eficazmente características complejas de cada modalidad. Sin embargo, esta técnica solo considera interacciones de alto nivel entre las modalidades, lo que dificulta el intercambio de información en etapas intermedias.</p>
<p>La fusión tardía se ha estudiado activamente para combinar la salida de modelos de diferentes modalidades y mejorar el rendimiento, similar a las técnicas de ensemble.</p>
</section>
<section id="fusión-híbrida-hybrid-fusion" class="level4">
<h4 class="anchored" data-anchor-id="fusión-híbrida-hybrid-fusion">10.3.4.3 Fusión híbrida (Hybrid Fusion)</h4>
<p>La fusión híbrida es una combinación de Early Fusion y Late Fusion. Se realiza la fusión en múltiples etapas del modelo para aprovechar información a diferentes niveles. La ventaja de este método es que puede incorporar las ventajas tanto de Early Fusion como de Late Fusion, es decir, considerar interacciones de bajo nivel y alto nivel entre modalidades. Sin embargo, esto también conlleva una mayor complejidad en la estructura del modelo y un aumento en el número de hiperparámetros a ajustar.</p>
<p>Un ejemplo representativo de fusión híbrida es la Cross-Modal Attention, que implica aplicar atención a las características de una modalidad (key-value) utilizando las características de otra modalidad como consulta (query). Este método es típico para realizar la fusión en etapas intermedias.</p>
<p>En investigaciones recientes, además de la atención, se han explorado otras técnicas como mecanismos con puertas y pooling bilineal para realizar fusiones en etapas intermedias.</p>
</section>
<section id="estrategias-sofisticadas-de-integración-de-modelos-avanzados-desde-2023" class="level4">
<h4 class="anchored" data-anchor-id="estrategias-sofisticadas-de-integración-de-modelos-avanzados-desde-2023">10.3.4.4 Estrategias sofisticadas de integración de modelos avanzados (desde 2023)</h4>
<p>Desde 2023, modelos multimodales a gran escala (LMM) como Gemini y GPT-4V han implementado estrategias de integración de modalidades más sofisticadas para mejorar significativamente el rendimiento.</p>
<p>El <strong>mecanismo de fusión selectiva (Selective Fusion Mechanism)</strong> evalúa dinámicamente la importancia de cada modalidad y combina la información de manera selectiva. Por ejemplo, si una imagen contiene texto, se fortalece la asociación entre las características visuales del área de texto y el contenido del texto. Esto es similar a cómo las personas ajustan la importancia de la información visual y textual según las necesidades específicas.</p>
<p>El <strong>ajuste automático de contribución (Task-Specific Fusion)</strong> optimiza la forma de integrar modalidades según los requisitos de tareas específicas. En el caso del captioning de imágenes, se enfatiza en la transformación unidireccional de información visual a texto, mientras que en las respuestas visuales a preguntas, se refuerza la interacción bidireccional.</p>
<p>Estas estrategias sofisticadas de integración han mejorado significativamente el rendimiento de los modelos multimodales. En particular, al ir más allá de la simple combinación de información y ajustar dinámicamente el rol e importancia de cada modalidad, así como optimizar la fusión según las características de la tarea, estos modelos han demostrado excelentes resultados en tareas que requieren inferencias complejas. Estas estrategias de integración requieren conjuntos de datos a gran escala y recursos computacionales, por lo que es difícil implementarlas y experimentar con ellas directamente a través de ejemplos de aprendizaje. En su lugar, es recomendable obtener una comprensión conceptual a través de los artículos y documentos técnicos de cada modelo.</p>
</section>
</section>
</section>
<section id="técnicas-de-aprendizaje-de-representaciones-multimodales" class="level2">
<h2 class="anchored" data-anchor-id="técnicas-de-aprendizaje-de-representaciones-multimodales">10.4 Técnicas de aprendizaje de representaciones multimodales</h2>
<p>En la sección 10.3, examinamos diversas metodologías y estrategias teóricas para fusionar datos multimodales. A partir de esto, revisaremos técnicas específicas sobre cómo los modelos multimodales reales representan eficazmente la información de cada modalidad y aprenden las relaciones entre diferentes modalidades. La implementación completa se encuentra en <code>chapter_10/multimodal_embeding.py</code>.</p>
<section id="aprendizaje-de-representaciones-entre-modalidades" class="level3">
<h3 class="anchored" data-anchor-id="aprendizaje-de-representaciones-entre-modalidades">10.4.1 Aprendizaje de representaciones entre modalidades</h3>
<p>Una de las tareas centrales del aprendizaje multimodal es cómo representar modalidades con características diferentes en un <em>espacio común</em> significativo. Las imágenes son arreglos bidimensionales de valores de píxeles, el texto es una secuencia unidimensional de tokens, y el audio es valores de amplitud a lo largo del tiempo; cada modalidad tiene su propia forma de representación única. Para procesar eficazmente estos datos heterogéneos, se necesita una técnica de aprendizaje de representaciones que mantenga las características intrínsecas de cada modalidad y capture sus relaciones semánticas.</p>
<p><strong>Enfoque inicial: codificadores individuales + proyección</strong></p>
<p>Los modelos multimodales iniciales utilizaban codificadores especializados para cada modalidad (por ejemplo, CNN para imágenes, RNN para texto) para extraer vectores de características, y luego los proyectaban a un espacio vectorial común mediante una transformación lineal o una MLP (Multi-Layer Perceptron) somera. (Consulte el método de Representación Conjunta, Concatenación en la sección 10.3.1)</p>
<p><strong>Enfoque reciente: alineación semántica</strong></p>
<p>Recientemente, se ha dado prioridad a un enfoque que va más allá del simple ajuste dimensional y busca que los vectores de características de cada modalidad estén “alineados” semanticamente entre sí. Es decir, las imágenes relacionadas y el texto deben estar cerca en el espacio de incrustación, mientras que las imágenes no relacionadas y el texto deben estar lejos.</p>
<ul>
<li><p><strong>Aprendizaje contrastivo:</strong> (Consulte la Representación Coordinada en la sección 10.3.2, ejemplo de CLIP) Se considera un par imagen-texto como una muestra “positiva” y un par imagen-texto mezclado aleatoriamente como una muestra “negativa”, y se aprende aumentando la similitud entre las muestras positivas y disminuyendo la similitud entre las muestras negativas.</p></li>
<li><p><strong>Pérdida de tripletas:</strong> Se usan tres elementos: una imagen ancla, un texto positivo (el caption correspondiente a la imagen) y un texto negativo (el caption de otra imagen), para aprender que la distancia entre la imagen ancla y el texto positivo sea cercana, mientras que la distancia entre la imagen ancla y el texto negativo sea lejana.</p></li>
</ul>
<p><strong>Ejemplo de implementación (Aprendizaje contrastivo)</strong></p>
<div id="cell-22" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MultimodalEmbedding(nn.Module):</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, embedding_dim<span class="op">=</span><span class="dv">512</span>):</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.image_encoder <span class="op">=</span> models.resnet18(pretrained<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.image_encoder.fc <span class="op">=</span> nn.Sequential(</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">512</span>, embedding_dim),</span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>            nn.LayerNorm(embedding_dim)</span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.text_encoder <span class="op">=</span> BertModel.from_pretrained(<span class="st">'bert-base-uncased'</span>)</span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.text_projection <span class="op">=</span> nn.Sequential(</span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">768</span>, embedding_dim),  <span class="co"># BERT output dimension is 768</span></span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a>            nn.LayerNorm(embedding_dim)</span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-17"><a href="#cb18-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.logit_scale <span class="op">=</span> nn.Parameter(torch.ones([]) <span class="op">*</span> np.log(<span class="dv">1</span> <span class="op">/</span> <span class="fl">0.07</span>))</span>
<span id="cb18-18"><a href="#cb18-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-19"><a href="#cb18-19" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> encode_image(<span class="va">self</span>, image):</span>
<span id="cb18-20"><a href="#cb18-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.image_encoder(image)</span>
<span id="cb18-21"><a href="#cb18-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-22"><a href="#cb18-22" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> encode_text(<span class="va">self</span>, input_ids, attention_mask):</span>
<span id="cb18-23"><a href="#cb18-23" aria-hidden="true" tabindex="-1"></a>        text_features <span class="op">=</span> <span class="va">self</span>.text_encoder(input_ids, attention_mask)[<span class="dv">0</span>][:, <span class="dv">0</span>, :]  <span class="co"># [CLS] token, keep batch dim</span></span>
<span id="cb18-24"><a href="#cb18-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.text_projection(text_features)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<ul>
<li><strong><code>MultimodalEmbedding</code> clase:</strong>
<ul>
<li><code>image_encoder</code>: Utiliza ResNet18 para convertir imágenes en vectores de características de tamaño <code>embedding_dim</code>.</li>
<li><code>text_encoder</code>: Utiliza el modelo BERT para convertir texto en vectores de características y ajustarlos a un tamaño <code>embedding_dim</code> mediante la capa <code>text_projection</code>.</li>
<li><code>logit_scale</code>: Es un parámetro de temperatura aprendible utilizado en CLIP.</li>
</ul></li>
</ul>
<p><strong>Mecanismo de alineación semántica</strong></p>
<p>La alineación semántica se implementa principalmente en las siguientes dos partes: el método forward de la clase MultimodalEmbedding y constrasive_loss().</p>
<div id="cell-24" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> forward(<span class="va">self</span>, image, input_ids, attention_mask):</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>    image_features <span class="op">=</span> <span class="va">self</span>.encode_image(image)</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>    text_features <span class="op">=</span> <span class="va">self</span>.encode_text(input_ids, attention_mask)</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>    image_features <span class="op">=</span> image_features <span class="op">/</span> image_features.norm(dim<span class="op">=-</span><span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>    text_features <span class="op">=</span> text_features <span class="op">/</span> text_features.norm(dim<span class="op">=-</span><span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>    logit_scale <span class="op">=</span> <span class="va">self</span>.logit_scale.exp()</span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>    logits <span class="op">=</span> logit_scale <span class="op">*</span> image_features <span class="op">@</span> text_features.transpose(<span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">2</span>)</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># print("logits:", logits.shape)</span></span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> logits   <span class="co"># Return a single value</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<ul>
<li><strong><code>forward</code> método:</strong>
<ol type="1">
<li><p>Usa <code>encode_image</code> y <code>encode_text</code> para codificar las imágenes y el texto, respectivamente.</p></li>
<li><p><strong>Normalización de características (Feature Normalization):</strong> Ajusta la magnitud de los vectores <code>image_features</code> y <code>text_features</code> a 1 mediante normalización L2 (L2 normalization). Esto se hace para considerar solo la dirección del vector al calcular la similitud.</p></li>
<li><p><strong>Escalado de temperatura (Temperature Scaling):</strong> Ajusta la distribución de las puntuaciones de similitud utilizando <code>logit_scale</code>. Se aplica el valor <code>logit_scale</code> a una función exponencial para obtener un factor de escala, que luego se multiplica por el producto matricial del tensor de características de imagen y el tensor de características de texto transpuesto. El producto matricial calcula los productos internos (dot products) entre cada vector de características de imagen y todos los vectores de características de texto para generar las puntuaciones de similitud.</p></li>
<li><p><code>logits</code>: Calcula la similitud (productos internos) entre los vectores de características de imagen y los vectores de características de texto. Se utiliza <code>text_features.transpose(-1, -2)</code> en lugar de <code>text_features.t()</code> para realizar la transposición. Esto intercambia las dos últimas dimensiones del tensor de características de texto, cambiando su forma de (lote, dimensión de características de texto) a (lote, dimensión de características, texto), lo que permite multiplicarlo con el tensor de características de imagen de forma (lote, dimensión de características de imagen).</p></li>
</ol></li>
</ul>
<div id="cell-26" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> contrastive_loss(logits): <span class="co"># removed enhanced_similarity</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>    labels <span class="op">=</span> torch.arange(logits.size(<span class="dv">0</span>), device<span class="op">=</span>logits.device) <span class="co"># Use logits.size(0)</span></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Image-to-text and text-to-image contrastive loss</span></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>    img_txt_loss <span class="op">=</span> nn.CrossEntropyLoss()(logits, labels)</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>    txt_img_loss <span class="op">=</span> nn.CrossEntropyLoss()(logits.T, labels)</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Average loss</span></span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (img_txt_loss <span class="op">+</span> txt_img_loss) <span class="op">/</span> <span class="dv">2</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>La función <code>contrastive_loss</code> genera <code>labels</code> como enteros desde 0 hasta (tamaño del lote - 1) para ajustarse al tamaño de la matriz <code>logits</code>. Los elementos diagonales (i, i) en la matriz <code>logits</code> representan la similitud entre la i-ésima imagen y el i-ésimo texto. Es decir, estas entradas diagonales representan la similitud del par positivo (imagen y texto correspondientes), por lo que se configuran las etiquetas para que estos elementos diagonales sean los correctos. Además, <code>img_txt_loss</code> calcula la pérdida de similitud de imagen a texto (image-to-text loss), mientras que <code>txt_img_loss</code> calcula la pérdida de similitud de texto a imagen (text-to-image loss). Al promediar estas dos pérdidas, se tiene en cuenta el alineamiento semántico bidireccional (de imagen a texto y de texto a imagen).</p>
<p>El mecanismo de alineamiento semántico mapea las características de diferentes modalidades a un espacio semánticamente coherente. Primero, se proyectan todos los vectores de características a una esfera unitaria mediante la normalización L2 para eliminar las diferencias de escala entre las modalidades. Se introduce un parámetro de escalado de temperatura para ajustar la distribución de valores de similitud. Una alta temperatura produce una distribución más suave, mientras que una baja temperatura genera una distribución más aguda, lo cual mejora la estabilidad del aprendizaje. Además, a través del aprendizaje contrastivo, se entrena para que las parejas imagen-texto relacionadas estén cercanas y las no relacionadas estén lejanas en el espacio de incrustación. En particular, se optimiza simultáneamente el mapeo de imagen a texto y de texto a imagen para lograr un alineamiento semántico bidireccional.</p>
<p>Al igual que en el aprendizaje contrastivo de CLIP, los contenidos relacionados se aprenden a estar cercanos y los no relacionados lejanos. Esta estrategia de alineamiento semántico basada en aprendizaje contrastivo ha evolucionado desde la introducción del CLIP por OpenAI en 2021 hasta el PaLM-E de Google, el Claude de Anthropic y el Gemini de DeepMind. Mientras que el CLIP inicial se centraba principalmente en el aprendizaje contrastivo simple de pares imagen-texto, los modelos más recientes capturan relaciones intermodales con mayor sofisticación. En particular, Gemini aprende simultáneamente el alineamiento semántico entre diferentes modalidades como imágenes, texto, audio y video, construyendo un espacio de significado integrado que preserva las características únicas de cada modalidad.</p>
<p><strong>Ejemplo de ejecución</strong></p>
<p>Se utiliza el conjunto de datos flicker8k para el entrenamiento. Se puede utilizar la función <code>train_multimodal_embedding</code> para entrenar el modelo <code>EnhancedMultimodalEmbedding</code> (o <code>EnhancedMultimodalEmbedding_no_p</code>) con el conjunto de datos Flickr8k. En la función <code>main</code>, se configuran el modelo, el cargador de datos, el optimizador, etc., y al llamar a la función <code>train_multimodal_embedding</code>, comienza el entrenamiento.</p>
<div id="cell-28" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="co"># download flickr8k.</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>mkdir data<span class="op">;</span>cd data<span class="op">;</span>wget <span class="st">"https://github.com/awsaf49/flickr-dataset/releases/download/v1.0/flickr8k.zip"</span><span class="op">;</span>unzip <span class="op">-</span>q flickr8k.<span class="bu">zip</span> <span class="op">-</span>d .<span class="op">/</span>flickr8k</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>mkdir: cannot create directory ‘data’: File exists
--2025-03-09 16:33:12--  https://github.com/awsaf49/flickr-dataset/releases/download/v1.0/flickr8k.zip
Resolving github.com (github.com)... 20.200.245.247
Connecting to github.com (github.com)|20.200.245.247|:443... connected.
HTTP request sent, awaiting response... 302 Found
Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/753516996/d7c62b13-1e50-40ea-8fae-f34a44b1695f?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=releaseassetproduction%2F20250309%2Fus-east-1%2Fs3%2Faws4_request&amp;X-Amz-Date=20250309T073156Z&amp;X-Amz-Expires=300&amp;X-Amz-Signature=ff62cf7df8ac3deba8bd6f4f775e164abf03c6d2d6d86d740e5407e52702c6a3&amp;X-Amz-SignedHeaders=host&amp;response-content-disposition=attachment%3B%20filename%3Dflickr8k.zip&amp;response-content-type=application%2Foctet-stream [following]
--2025-03-09 16:33:12--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/753516996/d7c62b13-1e50-40ea-8fae-f34a44b1695f?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=releaseassetproduction%2F20250309%2Fus-east-1%2Fs3%2Faws4_request&amp;X-Amz-Date=20250309T073156Z&amp;X-Amz-Expires=300&amp;X-Amz-Signature=ff62cf7df8ac3deba8bd6f4f775e164abf03c6d2d6d86d740e5407e52702c6a3&amp;X-Amz-SignedHeaders=host&amp;response-content-disposition=attachment%3B%20filename%3Dflickr8k.zip&amp;response-content-type=application%2Foctet-stream
Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.110.133, ...
Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.109.133|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 1112971163 (1.0G) [application/octet-stream]
Saving to: ‘flickr8k.zip’

flickr8k.zip        100%[===================&gt;]   1.04G  56.8MB/s    in 19s     

2025-03-09 16:33:32 (56.9 MB/s) - ‘flickr8k.zip’ saved [1112971163/1112971163]
</code></pre>
</div>
</div>
<div id="cell-29" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision <span class="im">import</span> models, transforms</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> Dataset, DataLoader</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Assuming dldna.chapter_10.multimodal_embedding is in the same directory or Python path.</span></span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Adjust if necessary (e.g., from multimodal_embedding import ...).</span></span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_10.multimodal_embedding <span class="im">import</span> Flickr8kDataset, MultimodalEmbedding, train_multimodal_embedding, generate_example</span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Data transformation setup</span></span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a>transform <span class="op">=</span> transforms.Compose([</span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a>    transforms.Resize((<span class="dv">224</span>, <span class="dv">224</span>)),</span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a>    transforms.ToTensor(),</span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a>    transforms.Normalize(mean<span class="op">=</span>[<span class="fl">0.485</span>, <span class="fl">0.456</span>, <span class="fl">0.406</span>], std<span class="op">=</span>[<span class="fl">0.229</span>, <span class="fl">0.224</span>, <span class="fl">0.225</span>])</span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-15"><a href="#cb23-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Dataset and DataLoader setup</span></span>
<span id="cb23-16"><a href="#cb23-16" aria-hidden="true" tabindex="-1"></a>image_dir <span class="op">=</span> <span class="st">'./data/flickr8k/Images'</span>  <span class="co"># Replace with the actual path to your image directory</span></span>
<span id="cb23-17"><a href="#cb23-17" aria-hidden="true" tabindex="-1"></a>caption_file <span class="op">=</span> <span class="st">'./data/flickr8k/captions.txt'</span>  <span class="co"># Replace with the actual path to your caption file</span></span>
<span id="cb23-18"><a href="#cb23-18" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> Flickr8kDataset(image_dir, caption_file, transform<span class="op">=</span>transform)</span>
<span id="cb23-19"><a href="#cb23-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-20"><a href="#cb23-20" aria-hidden="true" tabindex="-1"></a>train_size <span class="op">=</span> <span class="bu">int</span>(<span class="fl">0.8</span> <span class="op">*</span> <span class="bu">len</span>(dataset))</span>
<span id="cb23-21"><a href="#cb23-21" aria-hidden="true" tabindex="-1"></a>val_size <span class="op">=</span> <span class="bu">len</span>(dataset) <span class="op">-</span> train_size</span>
<span id="cb23-22"><a href="#cb23-22" aria-hidden="true" tabindex="-1"></a>train_dataset, val_dataset <span class="op">=</span> torch.utils.data.random_split(dataset, [train_size, val_size])</span>
<span id="cb23-23"><a href="#cb23-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-24"><a href="#cb23-24" aria-hidden="true" tabindex="-1"></a>train_loader <span class="op">=</span> DataLoader(train_dataset, batch_size<span class="op">=</span><span class="dv">32</span>, shuffle<span class="op">=</span><span class="va">True</span>, num_workers<span class="op">=</span><span class="dv">4</span>)</span>
<span id="cb23-25"><a href="#cb23-25" aria-hidden="true" tabindex="-1"></a>val_loader <span class="op">=</span> DataLoader(val_dataset, batch_size<span class="op">=</span><span class="dv">32</span>, shuffle<span class="op">=</span><span class="va">False</span>, num_workers<span class="op">=</span><span class="dv">4</span>)</span>
<span id="cb23-26"><a href="#cb23-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-27"><a href="#cb23-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Model initialization</span></span>
<span id="cb23-28"><a href="#cb23-28" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> MultimodalEmbedding()</span>
<span id="cb23-29"><a href="#cb23-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-30"><a href="#cb23-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Model training</span></span>
<span id="cb23-31"><a href="#cb23-31" aria-hidden="true" tabindex="-1"></a>train_multimodal_embedding(model, train_loader, val_loader, num_epochs<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb23-32"><a href="#cb23-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-33"><a href="#cb23-33" aria-hidden="true" tabindex="-1"></a><span class="co"># Model saving</span></span>
<span id="cb23-34"><a href="#cb23-34" aria-hidden="true" tabindex="-1"></a>torch.save(model.state_dict(), <span class="st">'multimodal_embedding_model.pth'</span>)</span>
<span id="cb23-35"><a href="#cb23-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-36"><a href="#cb23-36" aria-hidden="true" tabindex="-1"></a><span class="co"># Example generation</span></span>
<span id="cb23-37"><a href="#cb23-37" aria-hidden="true" tabindex="-1"></a>model_path <span class="op">=</span> <span class="st">'multimodal_embedding_model.pth'</span></span>
<span id="cb23-38"><a href="#cb23-38" aria-hidden="true" tabindex="-1"></a>generate_example(model_path, image_dir, caption_file)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 1/3:  15%|█▍        | 147/1012 [00:16&lt;01:36,  8.96it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Image file not found: ./data/flickr8k/Images/image</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 1/3: 100%|██████████| 1012/1012 [01:53&lt;00:00,  8.90it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/3 - Train Loss: 0.9618</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code></code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/3 - Validation Loss: 0.5212
Epoch 1: Saved best model with Validation Loss = 0.5212</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 2/3:  52%|█████▏    | 525/1012 [00:59&lt;00:55,  8.84it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Image file not found: ./data/flickr8k/Images/image</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 2/3: 100%|██████████| 1012/1012 [01:54&lt;00:00,  8.83it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 2/3 - Train Loss: 0.3393</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code></code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 2/3 - Validation Loss: 0.4240
Epoch 2: Saved best model with Validation Loss = 0.4240</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 3/3:  34%|███▍      | 347/1012 [00:39&lt;01:15,  8.85it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Image file not found: ./data/flickr8k/Images/image</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 3/3: 100%|██████████| 1012/1012 [01:54&lt;00:00,  8.83it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 3/3 - Train Loss: 0.2313</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code></code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 3/3 - Validation Loss: 0.3891
Epoch 3: Saved best model with Validation Loss = 0.3891
Image 0:</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="10_Multimodal deep learning: el inicio de la fusión multisensorial_files/figure-html/cell-14-output-19.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Top 3 Captions (Image -&gt; Text):
  - football players in red congratulate each other as crowds in red cheer behind. (prob: 0.9970)
  - a man in black holds up an obama 08 sign. (prob: 0.0023)
  - a large group of bicycles racing on the street (prob: 0.0004)

Caption: football players in red congratulate each other as crowds in red cheer behind.

Top 3 Images (Text -&gt; Image):
 - Image 0 (prob: 0.9983)</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="10_Multimodal deep learning: el inicio de la fusión multisensorial_files/figure-html/cell-14-output-21.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> - Image 17 (prob: 0.0013)</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="10_Multimodal deep learning: el inicio de la fusión multisensorial_files/figure-html/cell-14-output-23.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> - Image 2 (prob: 0.0001)</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="10_Multimodal deep learning: el inicio de la fusión multisensorial_files/figure-html/cell-14-output-25.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="estructura-de-atención-cruzada-modal" class="level3">
<h3 class="anchored" data-anchor-id="estructura-de-atención-cruzada-modal">10.4.2 Estructura de atención cruzada modal</h3>
<p>La atención cruzada modal se utiliza para modelar eficazmente las relaciones entre diferentes modalidades. Esto extiende la autoatención del ViT, permitiendo la interacción entre datos heterogéneos como imágenes y texto.</p>
<p><strong>Diseño de atención entre modalidades</strong></p>
<p>La atención cruzada modal tiene una estructura asimétrica que considera las características de cada modalidad.</p>
<div id="cell-31" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> CrossModalAttention(nn.Module):</span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config):</span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.image_proj <span class="op">=</span> nn.Linear(config.image_dim, config.hidden_dim)</span>
<span id="cb45-5"><a href="#cb45-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.text_proj <span class="op">=</span> nn.Linear(config.text_dim, config.hidden_dim)</span>
<span id="cb45-6"><a href="#cb45-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.attention <span class="op">=</span> nn.MultiheadAttention(config.hidden_dim, config.num_heads)</span>
<span id="cb45-7"><a href="#cb45-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb45-8"><a href="#cb45-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, image_features, text_features):</span>
<span id="cb45-9"><a href="#cb45-9" aria-hidden="true" tabindex="-1"></a>        image_proj <span class="op">=</span> <span class="va">self</span>.image_proj(image_features)</span>
<span id="cb45-10"><a href="#cb45-10" aria-hidden="true" tabindex="-1"></a>        text_proj <span class="op">=</span> <span class="va">self</span>.text_proj(text_features)</span>
<span id="cb45-11"><a href="#cb45-11" aria-hidden="true" tabindex="-1"></a>        attn_output, _ <span class="op">=</span> <span class="va">self</span>.attention(text_proj, image_proj, image_proj)</span>
<span id="cb45-12"><a href="#cb45-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> attn_output</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Proyecta las características de imagen y texto a un espacio latente común, luego aprende la relación entre las dos modalidades mediante el mecanismo de atención multi-cabezal. Las características del texto se utilizan como consulta, mientras que las características de la imagen se usan como clave y valor para que el texto preste atención a las partes relevantes de la imagen.</p>
<p><strong>Patrón de atención asimétrica</strong></p>
<p>Se utiliza un patrón de atención asimétrico para preservar las características únicas de cada modalidad mientras se logra un intercambio de información efectivo.</p>
<div id="cell-33" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> HierarchicalCrossModalAttention(nn.Module):</span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config):</span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.local_image_attention <span class="op">=</span> nn.MultiheadAttention(config.hidden_dim, config.num_heads)</span>
<span id="cb46-5"><a href="#cb46-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.local_text_attention <span class="op">=</span> nn.MultiheadAttention(config.hidden_dim, config.num_heads)</span>
<span id="cb46-6"><a href="#cb46-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.image_to_text_attention <span class="op">=</span> CrossModalAttention(config)</span>
<span id="cb46-7"><a href="#cb46-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.text_to_image_attention <span class="op">=</span> CrossModalAttention(config)</span>
<span id="cb46-8"><a href="#cb46-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.output_layer <span class="op">=</span> nn.Linear(config.hidden_dim <span class="op">*</span> <span class="dv">2</span>, config.hidden_dim)</span>
<span id="cb46-9"><a href="#cb46-9" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb46-10"><a href="#cb46-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, image_features, text_features):</span>
<span id="cb46-11"><a href="#cb46-11" aria-hidden="true" tabindex="-1"></a>        local_image <span class="op">=</span> <span class="va">self</span>.local_image_attention(image_features, image_features, image_features)[<span class="dv">0</span>]</span>
<span id="cb46-12"><a href="#cb46-12" aria-hidden="true" tabindex="-1"></a>        local_text <span class="op">=</span> <span class="va">self</span>.local_text_attention(text_features, text_features, text_features)[<span class="dv">0</span>]</span>
<span id="cb46-13"><a href="#cb46-13" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb46-14"><a href="#cb46-14" aria-hidden="true" tabindex="-1"></a>        image_attended_text <span class="op">=</span> <span class="va">self</span>.image_to_text_attention(image_features, local_text)</span>
<span id="cb46-15"><a href="#cb46-15" aria-hidden="true" tabindex="-1"></a>        text_attended_image <span class="op">=</span> <span class="va">self</span>.text_to_image_attention(text_features, local_image)</span>
<span id="cb46-16"><a href="#cb46-16" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb46-17"><a href="#cb46-17" aria-hidden="true" tabindex="-1"></a>        combined_features <span class="op">=</span> torch.cat([image_attended_text, text_attended_image], dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb46-18"><a href="#cb46-18" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> <span class="va">self</span>.output_layer(combined_features)</span>
<span id="cb46-19"><a href="#cb46-19" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb46-20"><a href="#cb46-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> output</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Aquí se realizan atenciones bidireccionales por separado, de texto a imagen e imagen a texto. Esto permite que cada modalidad pueda centrarse selectivamente en la información relevante de la otra modalidad.</p>
<p><strong>Estructura de atención jerárquica</strong></p>
<p>Para capturar relaciones multimodales complejas, se organizan varias capas de atención de manera jerárquica. En las capas inferiores, se procesan características locales dentro de cada modalidad, mientras que en las capas superiores se modelan las relaciones globales entre modalidades. Esta estructura jerárquica desempeña un papel clave en modelos como GPT-4V y Gemini.</p>
<div id="cell-35" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> EnhancedMultimodalEmbedding_no_p(MultimodalEmbedding):</span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, image, input_ids, attention_mask):</span>
<span id="cb47-4"><a href="#cb47-4" aria-hidden="true" tabindex="-1"></a>        image_features <span class="op">=</span> <span class="va">self</span>.encode_image(image)</span>
<span id="cb47-5"><a href="#cb47-5" aria-hidden="true" tabindex="-1"></a>        text_features <span class="op">=</span> <span class="va">self</span>.encode_text(input_ids, attention_mask)</span>
<span id="cb47-6"><a href="#cb47-6" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb47-7"><a href="#cb47-7" aria-hidden="true" tabindex="-1"></a>        image_features <span class="op">=</span> <span class="va">self</span>.image_preserve(image_features)</span>
<span id="cb47-8"><a href="#cb47-8" aria-hidden="true" tabindex="-1"></a>        text_features <span class="op">=</span> <span class="va">self</span>.text_preserve(text_features)</span>
<span id="cb47-9"><a href="#cb47-9" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb47-10"><a href="#cb47-10" aria-hidden="true" tabindex="-1"></a>        combined_features <span class="op">=</span> <span class="va">self</span>.cross_modal_attention(image_features, text_features)</span>
<span id="cb47-11"><a href="#cb47-11" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb47-12"><a href="#cb47-12" aria-hidden="true" tabindex="-1"></a>        combined_features <span class="op">=</span> combined_features <span class="op">/</span> combined_features.norm(dim<span class="op">=-</span><span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb47-13"><a href="#cb47-13" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb47-14"><a href="#cb47-14" aria-hidden="true" tabindex="-1"></a>        logit_scale <span class="op">=</span> <span class="va">self</span>.logit_scale.exp()</span>
<span id="cb47-15"><a href="#cb47-15" aria-hidden="true" tabindex="-1"></a>        logits <span class="op">=</span> logit_scale <span class="op">*</span> combined_features <span class="op">@</span> combined_features.t()</span>
<span id="cb47-16"><a href="#cb47-16" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb47-17"><a href="#cb47-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> logits</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="cell-36" class="cell" data-execution_count="2">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision <span class="im">import</span> models, transforms</span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> Dataset, DataLoader</span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-5"><a href="#cb48-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> namedtuple</span>
<span id="cb48-6"><a href="#cb48-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-7"><a href="#cb48-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_10.crossmodal_attention <span class="im">import</span> Flickr8kDataset, CrossModalEmbedding, train_crossmodal_embedding, generate_example</span>
<span id="cb48-8"><a href="#cb48-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-9"><a href="#cb48-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Configuration</span></span>
<span id="cb48-10"><a href="#cb48-10" aria-hidden="true" tabindex="-1"></a>config <span class="op">=</span> namedtuple(<span class="st">'Config'</span>, [<span class="st">'embedding_dim'</span>, <span class="st">'image_dim'</span>, <span class="st">'text_dim'</span>, <span class="st">'hidden_dim'</span>, <span class="st">'num_heads'</span>])(</span>
<span id="cb48-11"><a href="#cb48-11" aria-hidden="true" tabindex="-1"></a>                    embedding_dim<span class="op">=</span><span class="dv">512</span>, <span class="co"># Output embedding dimension</span></span>
<span id="cb48-12"><a href="#cb48-12" aria-hidden="true" tabindex="-1"></a>                    image_dim<span class="op">=</span><span class="dv">512</span>, <span class="co"># ResNet18 image encoder output dimension</span></span>
<span id="cb48-13"><a href="#cb48-13" aria-hidden="true" tabindex="-1"></a>                    text_dim<span class="op">=</span><span class="dv">512</span>, <span class="co"># Text feature (768 from BERT -&gt; 512 after projection)</span></span>
<span id="cb48-14"><a href="#cb48-14" aria-hidden="true" tabindex="-1"></a>                    hidden_dim<span class="op">=</span><span class="dv">512</span>, <span class="co"># Cross-modal attention internal hidden dimension</span></span>
<span id="cb48-15"><a href="#cb48-15" aria-hidden="true" tabindex="-1"></a>                    num_heads<span class="op">=</span><span class="dv">8</span> <span class="co"># Number of multi-head attention heads</span></span>
<span id="cb48-16"><a href="#cb48-16" aria-hidden="true" tabindex="-1"></a>                    )</span>
<span id="cb48-17"><a href="#cb48-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Data transformation setup</span></span>
<span id="cb48-18"><a href="#cb48-18" aria-hidden="true" tabindex="-1"></a>transform <span class="op">=</span> transforms.Compose([</span>
<span id="cb48-19"><a href="#cb48-19" aria-hidden="true" tabindex="-1"></a>    transforms.Resize((<span class="dv">224</span>, <span class="dv">224</span>)),</span>
<span id="cb48-20"><a href="#cb48-20" aria-hidden="true" tabindex="-1"></a>    transforms.ToTensor(),</span>
<span id="cb48-21"><a href="#cb48-21" aria-hidden="true" tabindex="-1"></a>    transforms.Normalize(mean<span class="op">=</span>[<span class="fl">0.485</span>, <span class="fl">0.456</span>, <span class="fl">0.406</span>], std<span class="op">=</span>[<span class="fl">0.229</span>, <span class="fl">0.224</span>, <span class="fl">0.225</span>])</span>
<span id="cb48-22"><a href="#cb48-22" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb48-23"><a href="#cb48-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-24"><a href="#cb48-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Dataset and DataLoader setup</span></span>
<span id="cb48-25"><a href="#cb48-25" aria-hidden="true" tabindex="-1"></a>image_dir <span class="op">=</span> <span class="st">'./data/flickr8k/Images'</span>  <span class="co"># Change to the actual path</span></span>
<span id="cb48-26"><a href="#cb48-26" aria-hidden="true" tabindex="-1"></a>caption_file <span class="op">=</span> <span class="st">'./data/flickr8k/captions.txt'</span>  <span class="co"># Change to the actual path</span></span>
<span id="cb48-27"><a href="#cb48-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-28"><a href="#cb48-28" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> Flickr8kDataset(image_dir, caption_file, transform<span class="op">=</span>transform)</span>
<span id="cb48-29"><a href="#cb48-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-30"><a href="#cb48-30" aria-hidden="true" tabindex="-1"></a>train_size <span class="op">=</span> <span class="bu">int</span>(<span class="fl">0.8</span> <span class="op">*</span> <span class="bu">len</span>(dataset))</span>
<span id="cb48-31"><a href="#cb48-31" aria-hidden="true" tabindex="-1"></a>val_size <span class="op">=</span> <span class="bu">len</span>(dataset) <span class="op">-</span> train_size</span>
<span id="cb48-32"><a href="#cb48-32" aria-hidden="true" tabindex="-1"></a>train_dataset, val_dataset <span class="op">=</span> torch.utils.data.random_split(dataset, [train_size, val_size])</span>
<span id="cb48-33"><a href="#cb48-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-34"><a href="#cb48-34" aria-hidden="true" tabindex="-1"></a>train_loader <span class="op">=</span> DataLoader(train_dataset, batch_size<span class="op">=</span><span class="dv">32</span>, shuffle<span class="op">=</span><span class="va">True</span>, num_workers<span class="op">=</span><span class="dv">4</span>, pin_memory<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb48-35"><a href="#cb48-35" aria-hidden="true" tabindex="-1"></a>val_loader <span class="op">=</span> DataLoader(val_dataset, batch_size<span class="op">=</span><span class="dv">32</span>, shuffle<span class="op">=</span><span class="va">False</span>, num_workers<span class="op">=</span><span class="dv">4</span>, pin_memory<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb48-36"><a href="#cb48-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-37"><a href="#cb48-37" aria-hidden="true" tabindex="-1"></a><span class="co"># Model initialization</span></span>
<span id="cb48-38"><a href="#cb48-38" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> CrossModalEmbedding(config)</span>
<span id="cb48-39"><a href="#cb48-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-40"><a href="#cb48-40" aria-hidden="true" tabindex="-1"></a><span class="co"># Model training</span></span>
<span id="cb48-41"><a href="#cb48-41" aria-hidden="true" tabindex="-1"></a>train_crossmodal_embedding(model, train_loader, val_loader, num_epochs<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb48-42"><a href="#cb48-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-43"><a href="#cb48-43" aria-hidden="true" tabindex="-1"></a><span class="co"># Model saving</span></span>
<span id="cb48-44"><a href="#cb48-44" aria-hidden="true" tabindex="-1"></a>torch.save(model.state_dict(), <span class="st">'crossmodal_embedding_model.pth'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 1/3:   4%|▍         | 40/1012 [00:04&lt;01:41,  9.53it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Image file not found: ./data/flickr8k/Images/image</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 1/3: 100%|██████████| 1012/1012 [01:47&lt;00:00,  9.41it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/3 - Train Loss: 0.9663</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code></code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/3 - Validation Loss: 0.5378</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 2/3:  58%|█████▊    | 582/1012 [01:02&lt;00:45,  9.36it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Image file not found: ./data/flickr8k/Images/image</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 2/3: 100%|██████████| 1012/1012 [01:48&lt;00:00,  9.31it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 2/3 - Train Loss: 0.3381</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code></code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 2/3 - Validation Loss: 0.4452</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 3/3:   0%|          | 4/1012 [00:00&lt;02:27,  6.82it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Image file not found: ./data/flickr8k/Images/image</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 3/3: 100%|██████████| 1012/1012 [01:48&lt;00:00,  9.35it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 3/3 - Train Loss: 0.2288</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code></code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 3/3 - Validation Loss: 0.3743</code></pre>
</div>
</div>
<div id="cell-37" class="cell" data-execution_count="3">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb67"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Example generation</span></span>
<span id="cb67-2"><a href="#cb67-2" aria-hidden="true" tabindex="-1"></a>model_path <span class="op">=</span> <span class="st">'crossmodal_embedding_model.pth'</span></span>
<span id="cb67-3"><a href="#cb67-3" aria-hidden="true" tabindex="-1"></a>generate_example(model_path, image_dir, caption_file)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Image 0:</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="10_Multimodal deep learning: el inicio de la fusión multisensorial_files/figure-html/cell-19-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Top 3 Captions (Image -&gt; Text):
  - two people walk out onto the desert sand. (prob: 0.9862)
  - a man takes a picture of him and his friend with his phone. (prob: 0.0092)
  - the little boy wearing the blue shirt is putting dirt in his mouth. (prob: 0.0013)

Caption: two people walk out onto the desert sand.

Top 3 Images (Text -&gt; Image):
 - Image 0 (prob: 0.9898)</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="10_Multimodal deep learning: el inicio de la fusión multisensorial_files/figure-html/cell-19-output-4.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> - Image 2 (prob: 0.0089)</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="10_Multimodal deep learning: el inicio de la fusión multisensorial_files/figure-html/cell-19-output-6.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> - Image 4 (prob: 0.0005)</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="10_Multimodal deep learning: el inicio de la fusión multisensorial_files/figure-html/cell-19-output-8.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="arquitectura-perceiver" class="level3">
<h3 class="anchored" data-anchor-id="arquitectura-perceiver">10.4.3 Arquitectura Perceiver</h3>
<p>Perceiver es una arquitectura multimodal propuesta por DeepMind en 2021. Resuelve el problema de la complejidad cuadrática de los transformadores tradicionales (donde la cantidad de cálculos aumenta al cuadrado con la longitud de la secuencia de entrada) y presenta una estructura que puede procesar eficazmente diversas modalidades (imágenes, texto, audio, nubes de puntos, etc.). Perceiver es particularmente beneficioso cuando el tamaño de los datos de entrada es muy grande (por ejemplo: imágenes de alta resolución, textos largos). En este capítulo se proporciona una descripción general de la arquitectura y se omite el ejemplo. El código es un ejemplo ilustrativo para la explicación.</p>
<p><strong>Idea clave de Perceiver</strong></p>
<p>Perceiver se basa en las siguientes ideas:</p>
<ol type="1">
<li><strong>Estructura de cuello de botella (Bottleneck Architecture):</strong></li>
</ol>
<p>Perceiver utiliza un conjunto de vectores latentes de tamaño fijo, independientemente de la longitud de la secuencia de entrada. Estos vectores latentes desempeñan el papel de comprimir y representar la información de los datos de entrada, resumiendo grandes cantidades de información de entrada en un número reducido de vectores latentes, similar a un cuello de botella. Por lo tanto, sin importar cuán grande sea el tamaño de los datos de entrada (por ejemplo: 10,000 tokens), el número de vectores latentes permanece fijo (por ejemplo: 256), lo que permite reducir significativamente la complejidad computacional y el uso de memoria.</p>
<div id="cell-39" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb72"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Perceiver(nn.Module):</span>
<span id="cb72-2"><a href="#cb72-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, ..., num_latents<span class="op">=</span><span class="dv">256</span>, latent_dim<span class="op">=</span><span class="dv">512</span>, ...):</span>
<span id="cb72-3"><a href="#cb72-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb72-4"><a href="#cb72-4" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Latent vector initialization (key!)</span></span>
<span id="cb72-5"><a href="#cb72-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.latents <span class="op">=</span> nn.Parameter(torch.randn(num_latents, latent_dim))</span>
<span id="cb72-6"><a href="#cb72-6" aria-hidden="true" tabindex="-1"></a>        <span class="co"># ...</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>En el código anterior, <code>self.latents</code> representa directamente ese vector latente. Se define como un parámetro aprendible mediante <code>nn.Parameter</code>.</p>
<ol start="2" type="1">
<li><strong>Procesamiento independiente de la modalidad (Modality-Agnostic Processing):</strong></li>
</ol>
<p>Perceiver no utiliza métodos de procesamiento especializados para modalidades de entrada específicas (como imágenes, texto, audio), por ejemplo CNN o RNN. En su lugar, cada modalidad pasa por un preprocesamiento simple (por ejemplo, parches de imagen, tokenización de texto) y se convierte en una forma común (secuencia de vectores). A partir de este punto, se utiliza la misma arquitectura basada en transformadores (Cross-Attention, Self-Attention) para procesar los datos, independientemente del tipo de modalidad. Esto permite un manejo flexible de diversas modalidades y facilita la adición de nuevas modalidades.</p>
<ol start="3" type="1">
<li><strong>Representación latente adaptable (Adaptive Latent Representation):</strong></li>
</ol>
<p>Perceiver utiliza múltiples capas de autoatención (self-attention) para actualizar gradualmente los vectores latentes. En cada capa, los vectores latentes intercambian información entre sí y aprenden patrones complejos en los datos de entrada. Los vectores latentes que inicialmente representaban características simples, evolucionan a medida que pasan por múltiples capas, llegando a expresar significados más abstractos y de alto nivel.</p>
<p><strong>Funcionamiento de Perceiver (ejemplo de código simplificado)</strong></p>
<div id="cell-41" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb73"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb73-2"><a href="#cb73-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb73-3"><a href="#cb73-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-4"><a href="#cb73-4" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Perceiver(nn.Module):</span>
<span id="cb73-5"><a href="#cb73-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>,</span>
<span id="cb73-6"><a href="#cb73-6" aria-hidden="true" tabindex="-1"></a>                 input_channels<span class="op">=</span><span class="dv">3</span>,  <span class="co"># Input channels (e.g., RGB image)</span></span>
<span id="cb73-7"><a href="#cb73-7" aria-hidden="true" tabindex="-1"></a>                 input_axis<span class="op">=</span><span class="dv">2</span>,      <span class="co"># Input dimension (image=2, video=3)</span></span>
<span id="cb73-8"><a href="#cb73-8" aria-hidden="true" tabindex="-1"></a>                 num_latents<span class="op">=</span><span class="dv">256</span>,  <span class="co"># Number of latent vectors</span></span>
<span id="cb73-9"><a href="#cb73-9" aria-hidden="true" tabindex="-1"></a>                 latent_dim<span class="op">=</span><span class="dv">512</span>,    <span class="co"># Latent vector dimension</span></span>
<span id="cb73-10"><a href="#cb73-10" aria-hidden="true" tabindex="-1"></a>                 num_heads<span class="op">=</span><span class="dv">8</span>,       <span class="co"># Number of attention heads</span></span>
<span id="cb73-11"><a href="#cb73-11" aria-hidden="true" tabindex="-1"></a>                 depth<span class="op">=</span><span class="dv">6</span>):          <span class="co"># Model depth (number of self-attention layers)</span></span>
<span id="cb73-12"><a href="#cb73-12" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb73-13"><a href="#cb73-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-14"><a href="#cb73-14" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 1. Latent vector initialization (key!)</span></span>
<span id="cb73-15"><a href="#cb73-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.latents <span class="op">=</span> nn.Parameter(torch.randn(num_latents, latent_dim))</span>
<span id="cb73-16"><a href="#cb73-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-17"><a href="#cb73-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 2. Input projection (matches input dimension to latent dimension)</span></span>
<span id="cb73-18"><a href="#cb73-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.input_proj <span class="op">=</span> nn.Linear(input_dim, latent_dim)</span>
<span id="cb73-19"><a href="#cb73-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-20"><a href="#cb73-20" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 3. Cross-Attention (learns relationships between input and latent vectors)</span></span>
<span id="cb73-21"><a href="#cb73-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># self.cross_attention = nn.MultiheadAttention(latent_dim, num_heads, batch_first=True)</span></span>
<span id="cb73-22"><a href="#cb73-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-23"><a href="#cb73-23" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 4. Self-Attention (learns relationships between latent vectors) - repeated multiple times</span></span>
<span id="cb73-24"><a href="#cb73-24" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.self_attention_layers <span class="op">=</span> nn.ModuleList([</span>
<span id="cb73-25"><a href="#cb73-25" aria-hidden="true" tabindex="-1"></a>            nn.MultiheadAttention(latent_dim, num_heads, batch_first<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb73-26"><a href="#cb73-26" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(depth)</span>
<span id="cb73-27"><a href="#cb73-27" aria-hidden="true" tabindex="-1"></a>        ])</span>
<span id="cb73-28"><a href="#cb73-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-29"><a href="#cb73-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-30"><a href="#cb73-30" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):  <span class="co"># x: Input data (image, text, ...)</span></span>
<span id="cb73-31"><a href="#cb73-31" aria-hidden="true" tabindex="-1"></a>        batch_size <span class="op">=</span> x.shape[<span class="dv">0</span>]</span>
<span id="cb73-32"><a href="#cb73-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-33"><a href="#cb73-33" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 1. Input projection</span></span>
<span id="cb73-34"><a href="#cb73-34" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.input_proj(x)</span>
<span id="cb73-35"><a href="#cb73-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-36"><a href="#cb73-36" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 2. Latent vector replication (for each item in the batch)</span></span>
<span id="cb73-37"><a href="#cb73-37" aria-hidden="true" tabindex="-1"></a>        latents <span class="op">=</span> <span class="va">self</span>.latents.unsqueeze(<span class="dv">0</span>).expand(batch_size, <span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>)  <span class="co"># (B, num_latents, latent_dim)</span></span>
<span id="cb73-38"><a href="#cb73-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-39"><a href="#cb73-39" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 3. (Optional) Cross-attention (between input and latent vectors)</span></span>
<span id="cb73-40"><a href="#cb73-40" aria-hidden="true" tabindex="-1"></a>        <span class="co"># latents, _ = self.cross_attention(latents, x, x)  # query, key, value</span></span>
<span id="cb73-41"><a href="#cb73-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-42"><a href="#cb73-42" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 4. Self-attention (between latent vectors) - repeated multiple times</span></span>
<span id="cb73-43"><a href="#cb73-43" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> layer <span class="kw">in</span> <span class="va">self</span>.self_attention_layers:</span>
<span id="cb73-44"><a href="#cb73-44" aria-hidden="true" tabindex="-1"></a>            latents, _ <span class="op">=</span> layer(latents, latents, latents) <span class="co"># query, key, value</span></span>
<span id="cb73-45"><a href="#cb73-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-46"><a href="#cb73-46" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> latents  <span class="co"># Return the processed latent vectors</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><strong>Ventajas y desventajas del Perceiver</strong></p>
<p>El Perceiver tiene eficiencia, ya que su complejidad de cálculo es casi constante independientemente del tamaño de entrada, y ofrece flexibilidad al poder procesar diferentes modalidades de la misma manera. Además, la escalabilidad para agregar nuevas modalidades fácilmente también es una ventaja del Perceiver. Sin embargo, el Perceiver sigue siendo complejo en su estructura debido a que se basa en transformers, y puede volverse muy grande si las dimensiones de los vectores latentes y el número de capas son grandes, lo cual es una desventaja. Además, en tareas específicas como la clasificación de imágenes, el rendimiento del Perceiver puede ser inferior al de modelos especializados para esa tarea, como CNN.</p>
<p><strong>Perceiver IO</strong></p>
<p>Perceiver IO, un estudio posterior al Perceiver, propuso un método que procesa tanto entradas como salidas a través de vectores latentes. Esto permitió manejar flexiblemente diferentes formas de salida (clasificación, regresión, generación de secuencias, etc.). Se considera que Perceiver IO es un modelo más general y poderoso en comparación con el Perceiver.</p>
</section>
<section id="implementación-de-atención-cruzada-y-estabilidad-del-entrenamiento" class="level3">
<h3 class="anchored" data-anchor-id="implementación-de-atención-cruzada-y-estabilidad-del-entrenamiento">10.4.4 Implementación de atención cruzada y estabilidad del entrenamiento</h3>
<p>Aquí comenzamos con la estructura básica de la atención cruzada, y gradualmente añadimos mecanismos mientras comparamos la posibilidad de entrenamiento y el rendimiento. A través de esto, comprendemos los problemas que surgen en el aprendizaje multimodal y examinamos métodos prácticos para abordarlos.</p>
<p>Cuando se diseña el mecanismo de atención cruzada, es muy común y recomendable ir aumentando gradualmente la complejidad, como se describe en esta sección. Este enfoque, conocido como <strong>estudio de ablación (Ablation study)</strong>, es efectivo para determinar la importancia de cada componente del mecanismo y para identificar los elementos clave que contribuyen al rendimiento final del modelo. Muchos artículos que proponen nuevas arquitecturas utilizan este enfoque. Además, discutir no solo el rendimiento final sino también los problemas de estabilidad durante el entrenamiento es muy importante desde una perspectiva práctica.</p>
<section id="estructura-del-entrenamiento" class="level4">
<h4 class="anchored" data-anchor-id="estructura-del-entrenamiento">10.4.4.1 Estructura del entrenamiento</h4>
<p><strong>Métodos de entrenamiento comparativo</strong></p>
<p>El experimento utiliza el conjunto de datos flickr8k, que tiene dos entradas: texto e imagen, para entrenar la similitud mutua. Durante el entrenamiento, hay varias versiones de atención cruzada, y cada versión es más compleja que la anterior. Se añade un mecanismo de atención cruzada en cada versión y se realiza el entrenamiento para compararlo. Todos los entrenamientos utilizan los mismos hiperparámetros. El número de épocas de entrenamiento está fijado en 5.</p>
<p><strong>Estructura del ejemplo</strong></p>
<p>El ejemplo tiene la siguiente estructura.</p>
<pre><code>chatper_10/mm
├── cat_resized.png
├── cross_attention
│&nbsp;&nbsp; ├── v0.py
│&nbsp;&nbsp; ├── v1.py
│&nbsp;&nbsp; ├── v2.p
│&nbsp;&nbsp; ├── v3.py 
│&nbsp;&nbsp; .... (continúa existiendo)
├── train_multimodal.py
└── evaluate_models.py</code></pre>
<p>La carpeta <code>cross_attention</code> contiene versiones de complejidad creciente de la atención cruzada, desde v1 hasta v11. <code>train_mulimodal.py</code> genera dinámicamente la siguiente versión del modelo una vez que se completa un entrenamiento y continúa el proceso de entrenamiento. Durante el entrenamiento, se registran métricas como precisión, pérdida contrastiva y tiempo de ejecución para generar una tabla final de comparación. No es recomendable juzgar la viabilidad del entrenamiento solo por los valores de pérdida y precisión. Dada la naturaleza del aprendizaje contrastivo, la forma más sencilla de verificar si el modelo se ha entrenado correctamente es evaluándolo con datos que no estén presentes en el conjunto de entrenamiento. El archivo para evaluar el modelo en un escenario de zero-shot es <code>evalute_models.py</code>.</p>
<p>La imagen utilizada para la evaluación es la siguiente:</p>
<p><img src="../../../assets/images/cat_resized.png" class="img-fluid"></p>
<p>La evaluación se realiza midiendo la similitud entre la imagen y cinco textos.</p>
<pre><code>test_captions = [
    "A dog playing in the park",
    "A cat sleeping on a couch",
    "Children playing soccer",
    "A sunset over the ocean",
    "A person cooking in the kitchen"
]</code></pre>
<p>Si el modelo se ha entrenado correctamente, la segunda caption “A cat sleeping on a couch” debería tener la mayor similitud entre las cinco captions. La imagen utilizada no está presente en los datos de entrenamiento y corresponde a un caso típico de prueba zero-shot.</p>
<p><strong>Asignación dinámica de cross-attention</strong></p>
<p>La asignación dinámica se realiza a través de la versión de <code>cross_attention</code>.</p>
<div id="cell-47" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb76"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb76-1"><a href="#cb76-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_10.mm.cross_attention.v0 <span class="im">import</span> CrossAttention <span class="im">as</span> v0</span>
<span id="cb76-2"><a href="#cb76-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_10.mm.cross_attention.v1 <span class="im">import</span> CrossAttention <span class="im">as</span> v1</span>
<span id="cb76-3"><a href="#cb76-3" aria-hidden="true" tabindex="-1"></a><span class="co"># ... (import other versions) ...</span></span>
<span id="cb76-4"><a href="#cb76-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_10.mm.cross_attention.v11 <span class="im">import</span> CrossAttention <span class="im">as</span> v11</span>
<span id="cb76-5"><a href="#cb76-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-6"><a href="#cb76-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_cross_attention(version, config<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb76-7"><a href="#cb76-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> config <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb76-8"><a href="#cb76-8" aria-hidden="true" tabindex="-1"></a>        config <span class="op">=</span> {}</span>
<span id="cb76-9"><a href="#cb76-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-10"><a href="#cb76-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> version <span class="op">==</span> <span class="st">'v0'</span>:</span>
<span id="cb76-11"><a href="#cb76-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> v0(<span class="op">**</span>config)</span>
<span id="cb76-12"><a href="#cb76-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> version <span class="op">==</span> <span class="st">'v1'</span>:</span>
<span id="cb76-13"><a href="#cb76-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> v1(<span class="op">**</span>config)</span>
<span id="cb76-14"><a href="#cb76-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ... (other version conditions) ...</span></span>
<span id="cb76-15"><a href="#cb76-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> version <span class="op">==</span> <span class="st">'v11'</span>:</span>
<span id="cb76-16"><a href="#cb76-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> v11(<span class="op">**</span>config)</span>
<span id="cb76-17"><a href="#cb76-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb76-18"><a href="#cb76-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="ss">f"Invalid cross-attention version: </span><span class="sc">{</span>version<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb76-19"><a href="#cb76-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-20"><a href="#cb76-20" aria-hidden="true" tabindex="-1"></a><span class="co"># ...</span></span>
<span id="cb76-21"><a href="#cb76-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-22"><a href="#cb76-22" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ImageTextMatchingModel(nn.Module):</span>
<span id="cb76-23"><a href="#cb76-23" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, image_encoder_dim<span class="op">=</span><span class="dv">2048</span>, text_encoder_dim<span class="op">=</span><span class="dv">768</span>, projection_dim<span class="op">=</span><span class="dv">256</span>):</span>
<span id="cb76-24"><a href="#cb76-24" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb76-25"><a href="#cb76-25" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.image_encoder <span class="op">=</span> ImageEncoder(image_encoder_dim, projection_dim)</span>
<span id="cb76-26"><a href="#cb76-26" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.text_encoder <span class="op">=</span> TextEncoder(text_encoder_dim, projection_dim)</span>
<span id="cb76-27"><a href="#cb76-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-28"><a href="#cb76-28" aria-hidden="true" tabindex="-1"></a>        <span class="co"># The CrossAttention module is dynamically assigned in main().</span></span>
<span id="cb76-29"><a href="#cb76-29" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.cross_attention <span class="op">=</span> <span class="va">None</span>  <span class="co"># CrossAttention(projection_dim)</span></span>
<span id="cb76-30"><a href="#cb76-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-31"><a href="#cb76-31" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, image, input_ids, attention_mask):</span>
<span id="cb76-32"><a href="#cb76-32" aria-hidden="true" tabindex="-1"></a>        <span class="co"># ...</span></span>
<span id="cb76-33"><a href="#cb76-33" aria-hidden="true" tabindex="-1"></a>        image_attended, text_attended <span class="op">=</span> <span class="va">self</span>.cross_attention(</span>
<span id="cb76-34"><a href="#cb76-34" aria-hidden="true" tabindex="-1"></a>            image_features.unsqueeze(<span class="dv">1</span>),</span>
<span id="cb76-35"><a href="#cb76-35" aria-hidden="true" tabindex="-1"></a>            text_features.unsqueeze(<span class="dv">1</span>)</span>
<span id="cb76-36"><a href="#cb76-36" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb76-37"><a href="#cb76-37" aria-hidden="true" tabindex="-1"></a>        <span class="co"># ...</span></span>
<span id="cb76-38"><a href="#cb76-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-39"><a href="#cb76-39" aria-hidden="true" tabindex="-1"></a><span class="co"># ...</span></span>
<span id="cb76-40"><a href="#cb76-40" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> run_training(model_versions, ...):</span>
<span id="cb76-41"><a href="#cb76-41" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ...</span></span>
<span id="cb76-42"><a href="#cb76-42" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> model_version <span class="kw">in</span> model_versions:</span>
<span id="cb76-43"><a href="#cb76-43" aria-hidden="true" tabindex="-1"></a>        <span class="co"># ...</span></span>
<span id="cb76-44"><a href="#cb76-44" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Model initialization</span></span>
<span id="cb76-45"><a href="#cb76-45" aria-hidden="true" tabindex="-1"></a>        model <span class="op">=</span> ImageTextMatchingModel()</span>
<span id="cb76-46"><a href="#cb76-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-47"><a href="#cb76-47" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Dynamically load the CrossAttention module</span></span>
<span id="cb76-48"><a href="#cb76-48" aria-hidden="true" tabindex="-1"></a>        model.cross_attention <span class="op">=</span> get_cross_attention(model_version, config<span class="op">=</span>config)</span>
<span id="cb76-49"><a href="#cb76-49" aria-hidden="true" tabindex="-1"></a>        <span class="co"># ...</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Esta sección implementa la lógica para cargar y aplicar dinámicamente diferentes versiones del módulo <code>Cross-Attention</code>. La función <code>get_cross_attention</code> recibe una versión en forma de cadena (v0, v1, …, v11) y devuelve una instancia de la clase <code>CrossAttention</code> correspondiente a esa versión. Dentro de la función <code>run_training</code>, se inicializa ImageTextMatchingModel para cada versión especificada en la lista <code>model_versions</code>, y se llama a la función <code>get_cross_attention</code> para asignar el módulo de <code>Cross-Attention</code> correspondiente a <code>model.cross_attention</code>.</p>
<p>Este enfoque de asignación dinámica aumenta la reutilizabilidad del código y facilita la gestión de experimentos. Al agregar una nueva versión de <code>Cross-Attention</code>, solo es necesario incluir esa versión en la función <code>get_cross_attention</code>, por lo que no se requiere modificar significativamente el código de entrenamiento. Además, a través de la lista <code>model_versions</code> en la función <code>run_training</code>, se puede controlar fácilmente qué versiones se entrenarán.</p>
<p><strong>Cálculo de la pérdida Contrastive y bucle de entrenamiento</strong></p>
<div id="cell-49" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb77"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb77-1"><a href="#cb77-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> contrastive_loss(logits):</span>
<span id="cb77-2"><a href="#cb77-2" aria-hidden="true" tabindex="-1"></a>    labels <span class="op">=</span> torch.arange(<span class="bu">len</span>(logits), device<span class="op">=</span>logits.device)</span>
<span id="cb77-3"><a href="#cb77-3" aria-hidden="true" tabindex="-1"></a>    loss_i <span class="op">=</span> nn.CrossEntropyLoss()(logits, labels)</span>
<span id="cb77-4"><a href="#cb77-4" aria-hidden="true" tabindex="-1"></a>    loss_t <span class="op">=</span> nn.CrossEntropyLoss()(logits.t(), labels)</span>
<span id="cb77-5"><a href="#cb77-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (loss_i <span class="op">+</span> loss_t) <span class="op">/</span> <span class="dv">2</span></span>
<span id="cb77-6"><a href="#cb77-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-7"><a href="#cb77-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train(model, train_loader, val_loader, epochs<span class="op">=</span><span class="dv">10</span>, lr<span class="op">=</span><span class="fl">1e-4</span>, model_version<span class="op">=</span><span class="st">'v0'</span>):</span>
<span id="cb77-8"><a href="#cb77-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ...</span></span>
<span id="cb77-9"><a href="#cb77-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb77-10"><a href="#cb77-10" aria-hidden="true" tabindex="-1"></a>        model.train()</span>
<span id="cb77-11"><a href="#cb77-11" aria-hidden="true" tabindex="-1"></a>        total_loss <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb77-12"><a href="#cb77-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># ...</span></span>
<span id="cb77-13"><a href="#cb77-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> batch <span class="kw">in</span> tqdm(train_loader, ...):</span>
<span id="cb77-14"><a href="#cb77-14" aria-hidden="true" tabindex="-1"></a>            images, input_ids, attention_mask <span class="op">=</span> [x.to(device) <span class="cf">for</span> x <span class="kw">in</span> batch]</span>
<span id="cb77-15"><a href="#cb77-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-16"><a href="#cb77-16" aria-hidden="true" tabindex="-1"></a>            optimizer.zero_grad()</span>
<span id="cb77-17"><a href="#cb77-17" aria-hidden="true" tabindex="-1"></a>            logits <span class="op">=</span> model(images, input_ids, attention_mask)</span>
<span id="cb77-18"><a href="#cb77-18" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">=</span> contrastive_loss(logits)</span>
<span id="cb77-19"><a href="#cb77-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-20"><a href="#cb77-20" aria-hidden="true" tabindex="-1"></a>            loss.backward()</span>
<span id="cb77-21"><a href="#cb77-21" aria-hidden="true" tabindex="-1"></a>            optimizer.step()</span>
<span id="cb77-22"><a href="#cb77-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-23"><a href="#cb77-23" aria-hidden="true" tabindex="-1"></a>            total_loss <span class="op">+=</span> loss.item()</span>
<span id="cb77-24"><a href="#cb77-24" aria-hidden="true" tabindex="-1"></a>        <span class="co"># ... (validation 및 지표 계산) ...</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Esta parte define el cálculo de la Pérdida Contrastiva (Contrastive Loss) utilizada en el entrenamiento del modelo y el bucle de entrenamiento. La función <code>contrastive_loss</code> toma como entrada las puntuaciones de similitud (logits) entre las imágenes y los textos, y calcula la pérdida contrastiva. En este caso, las etiquetas correctas corresponden a los elementos en la diagonal de los logits (es decir, los pares imagen-texto con el mismo índice), que se establecen en 1 (similares), mientras que el resto se establece en 0 (no similares) utilizando <code>torch.arange</code>. Se calcula tanto la Pérdida de Entropía Cruzada basada en la imagen (loss_i) como la basada en el texto (loss_t), y se utiliza el promedio de ambas como la pérdida final.</p>
<p><strong>Método de entrenamiento: Adición de mecanismos</strong></p>
<p>Comenzaremos desde la estructura de atención más simple, añadiendo funciones una por una para realizar pruebas. Llamaremos a estas funciones adicionales “mecanismos”. Observaremos cómo cada mecanismo afecta al diseño de la atención multimodal a medida que se añade. Primero revisaremos parte del código de entrenamiento y luego veremos los resultados del entrenamiento. Después, analizaremos qué mecanismos en cada atención cruzada modal influyeron en el éxito o fracaso del entrenamiento.</p>
<p>A continuación se presenta el código de entrenamiento. Durante el entrenamiento, cada modelo se guarda como <code>model_final_{versión}.pth</code>. Se utiliza este modelo guardado para realizar la evaluación.</p>
<div id="cell-51" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb78"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb78-1"><a href="#cb78-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_10.mm.train_multimodal <span class="im">import</span> run_training</span>
<span id="cb78-2"><a href="#cb78-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-3"><a href="#cb78-3" aria-hidden="true" tabindex="-1"></a><span class="co"># model_versions = ['v0', 'v1']  # List of model versions to train</span></span>
<span id="cb78-4"><a href="#cb78-4" aria-hidden="true" tabindex="-1"></a>model_versions <span class="op">=</span> [<span class="st">'v0'</span>, <span class="st">'v1'</span>, <span class="st">'v2'</span>, <span class="st">'v3'</span>, <span class="st">'v4'</span>, <span class="st">'v5'</span>, <span class="st">'v6'</span>, <span class="st">'v7'</span>, <span class="st">'v8'</span>, <span class="st">'v9'</span>, <span class="st">'v10_1'</span>, <span class="st">'v10_2'</span>, <span class="st">'v10_3'</span>, <span class="st">'v10_4'</span>, <span class="st">'v10_5'</span>, <span class="st">'v10_6'</span>, <span class="st">'v11'</span>]</span>
<span id="cb78-5"><a href="#cb78-5" aria-hidden="true" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb78-6"><a href="#cb78-6" aria-hidden="true" tabindex="-1"></a>lr <span class="op">=</span> <span class="fl">1e-4</span></span>
<span id="cb78-7"><a href="#cb78-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-8"><a href="#cb78-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Dataset </span></span>
<span id="cb78-9"><a href="#cb78-9" aria-hidden="true" tabindex="-1"></a>image_dir <span class="op">=</span> <span class="st">'./data/flickr8k/Images'</span>  <span class="co"># Change to the actual path</span></span>
<span id="cb78-10"><a href="#cb78-10" aria-hidden="true" tabindex="-1"></a>caption_file <span class="op">=</span> <span class="st">'./data/flickr8k/captions.txt'</span>  <span class="co"># Change to the actual path</span></span>
<span id="cb78-11"><a href="#cb78-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-12"><a href="#cb78-12" aria-hidden="true" tabindex="-1"></a>results_df <span class="op">=</span> run_training(model_versions, epochs<span class="op">=</span>epochs, lr<span class="op">=</span>lr, image_dir<span class="op">=</span>image_dir, caption_file<span class="op">=</span>caption_file) <span class="co"># Train multiple versions</span></span>
<span id="cb78-13"><a href="#cb78-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-14"><a href="#cb78-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Print results</span></span>
<span id="cb78-15"><a href="#cb78-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Training Results:"</span>)</span>
<span id="cb78-16"><a href="#cb78-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Print results in Markdown table format</span></span>
<span id="cb78-17"><a href="#cb78-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(results_df.to_markdown(index<span class="op">=</span><span class="va">False</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Evaluamos con el modelo.</p>
<div id="cell-53" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb79"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb79-1"><a href="#cb79-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_10.mm.evaluate_models <span class="im">import</span> evaluate_all_models</span>
<span id="cb79-2"><a href="#cb79-2" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb79-3"><a href="#cb79-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Test captions (fixed)</span></span>
<span id="cb79-4"><a href="#cb79-4" aria-hidden="true" tabindex="-1"></a>test_captions <span class="op">=</span> [</span>
<span id="cb79-5"><a href="#cb79-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">"A dog playing in the park"</span>,</span>
<span id="cb79-6"><a href="#cb79-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">"A cat sleeping on a couch"</span>,</span>
<span id="cb79-7"><a href="#cb79-7" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Children playing soccer"</span>,</span>
<span id="cb79-8"><a href="#cb79-8" aria-hidden="true" tabindex="-1"></a>    <span class="st">"A sunset over the ocean"</span>,</span>
<span id="cb79-9"><a href="#cb79-9" aria-hidden="true" tabindex="-1"></a>    <span class="st">"A person cooking in the kitchen"</span></span>
<span id="cb79-10"><a href="#cb79-10" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb79-11"><a href="#cb79-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-12"><a href="#cb79-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Run model evaluation</span></span>
<span id="cb79-13"><a href="#cb79-13" aria-hidden="true" tabindex="-1"></a>image_path <span class="op">=</span> <span class="st">'./cat_resized.png'</span></span>
<span id="cb79-14"><a href="#cb79-14" aria-hidden="true" tabindex="-1"></a>model_dir <span class="op">=</span> <span class="st">'.'</span></span>
<span id="cb79-15"><a href="#cb79-15" aria-hidden="true" tabindex="-1"></a>model_versions <span class="op">=</span> [<span class="st">'v0'</span>, <span class="st">'v1'</span>, <span class="st">'v2'</span>, <span class="st">'v3'</span>, <span class="st">'v4'</span>, <span class="st">'v5'</span>, <span class="st">'v6'</span>, <span class="st">'v7'</span>, <span class="st">'v8'</span>, <span class="st">'v9'</span>, <span class="st">'v10_1'</span>, <span class="st">'v10_2'</span>, <span class="st">'v10_3'</span>, <span class="st">'v10_4'</span>, <span class="st">'v10_5'</span>, <span class="st">'v10_6'</span>, <span class="st">'v11'</span>]</span>
<span id="cb79-16"><a href="#cb79-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-17"><a href="#cb79-17" aria-hidden="true" tabindex="-1"></a>results_df <span class="op">=</span> evaluate_all_models(model_dir, image_path, test_captions, model_versions)</span>
<span id="cb79-18"><a href="#cb79-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-19"><a href="#cb79-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Print results (Markdown table)</span></span>
<span id="cb79-20"><a href="#cb79-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(results_df.to_markdown(index<span class="op">=</span><span class="va">False</span>))</span>
<span id="cb79-21"><a href="#cb79-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-22"><a href="#cb79-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Print results (detailed)</span></span>
<span id="cb79-23"><a href="#cb79-23" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> _, row <span class="kw">in</span> results_df.iterrows():</span>
<span id="cb79-24"><a href="#cb79-24" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Model: </span><span class="sc">{</span>row[<span class="st">'model_version'</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb79-25"><a href="#cb79-25" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  Best Caption: </span><span class="sc">{</span>row[<span class="st">'best_caption'</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb79-26"><a href="#cb79-26" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  Trained Well: </span><span class="sc">{</span>row[<span class="st">'trained_well'</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb79-27"><a href="#cb79-27" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  Similarity Ratio: </span><span class="sc">{</span>row[<span class="st">'similarity_ratio'</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb79-28"><a href="#cb79-28" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  Similarity Gap: </span><span class="sc">{</span>row[<span class="st">'similarity_gap'</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb79-29"><a href="#cb79-29" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"  All Similarities:"</span>)</span>
<span id="cb79-30"><a href="#cb79-30" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> caption, sim <span class="kw">in</span> <span class="bu">zip</span>(test_captions, row[<span class="st">'all_similarities'</span>]):</span>
<span id="cb79-31"><a href="#cb79-31" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"    - </span><span class="sc">{</span>caption<span class="sc">:&lt;30}</span><span class="ss">: </span><span class="sc">{</span>sim<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="resultados-experimentales" class="level4">
<h4 class="anchored" data-anchor-id="resultados-experimentales">10.4.4.2 Resultados Experimentales</h4>
</section>
<section id="tabla-de-resultados-experimentales" class="level4">
<h4 class="anchored" data-anchor-id="tabla-de-resultados-experimentales"><strong>Tabla de Resultados Experimentales</strong></h4>
<table class="caption-top table">
<colgroup>
<col style="width: 8%">
<col style="width: 23%">
<col style="width: 20%">
<col style="width: 12%">
<col style="width: 11%">
<col style="width: 10%">
<col style="width: 13%">
</colgroup>
<thead>
<tr class="header">
<th>model_version</th>
<th>best_caption</th>
<th>all_similarities</th>
<th>similarity_ratio</th>
<th>similarity_gap</th>
<th>trained_well</th>
<th>similarity_ratio_rank</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>v0</td>
<td>Un gato durmiendo en un sofá</td>
<td>[‘5.322’, ‘15.477’, ‘-4.509’, ‘-6.609’, ‘2.107’]</td>
<td>2.908</td>
<td>10.155</td>
<td>True</td>
<td>1</td>
</tr>
<tr class="even">
<td>v1</td>
<td>Un gato durmiendo en un sofá</td>
<td>[‘3.117’, ‘18.174’, ‘-6.475’, ‘-1.825’, ‘8.705’]</td>
<td>2.088</td>
<td>9.469</td>
<td>True</td>
<td>3</td>
</tr>
<tr class="odd">
<td>v2</td>
<td>Un gato durmiendo en un sofá</td>
<td>[‘3.085’, ‘12.541’, ‘-4.252’, ‘0.924’, ‘6.849’]</td>
<td>1.831</td>
<td>5.692</td>
<td>True</td>
<td>5</td>
</tr>
<tr class="even">
<td>v3</td>
<td>Niños jugando fútbol</td>
<td>[‘34.882’, ‘34.882’, ‘34.882’, ‘34.882’, ‘34.882’]</td>
<td>1</td>
<td>0</td>
<td>False</td>
<td>14</td>
</tr>
<tr class="odd">
<td>v4</td>
<td>Un gato durmiendo en un sofá</td>
<td>[‘7.385’, ‘8.301’, ‘-1.038’, ‘-6.262’, ‘1.240’]</td>
<td>1.124</td>
<td>0.915</td>
<td>True</td>
<td>12</td>
</tr>
<tr class="even">
<td>v5</td>
<td>Niños jugando fútbol</td>
<td>[‘27.357’, ‘27.357’, ‘27.357’, ‘27.357’, ‘27.357’]</td>
<td>1</td>
<td>0</td>
<td>True</td>
<td>14</td>
</tr>
<tr class="odd">
<td>v10_2</td>
<td>Un gato durmiendo en un sofá</td>
<td>[‘17.720’, ‘17.720’, ‘17.720’, ‘17.720’, ‘17.720’]</td>
<td>1</td>
<td>0</td>
<td>True</td>
<td>14</td>
</tr>
<tr class="even">
<td>v10_3</td>
<td>Un gato durmiendo en un sofá</td>
<td>[‘0.516’, ‘1.479’, ‘-0.941’, ‘-0.106’, ‘0.694’]</td>
<td>2.132</td>
<td>0.786</td>
<td>True</td>
<td>2</td>
</tr>
<tr class="odd">
<td>v10_4</td>
<td>Un gato durmiendo en un sofá</td>
<td>[‘5.913’, ‘10.334’, ‘-5.989’, ‘-1.024’, ‘5.151’]</td>
<td>1.748</td>
<td>4.421</td>
<td>True</td>
<td>6</td>
</tr>
<tr class="even">
<td>v10_5</td>
<td>Un gato durmiendo en un sofá</td>
<td>[‘6.601’, ‘9.990’, ‘-5.984’, ‘-2.988’, ‘-0.070’]</td>
<td>1.513</td>
<td>3.389</td>
<td>True</td>
<td>8</td>
</tr>
<tr class="odd">
<td>v10_6</td>
<td>Un perro jugando en el parque</td>
<td>[‘33.967’, ‘33.302’, ‘31.580’, ‘32.710’, ‘31.384’]</td>
<td>1.02</td>
<td>0.665</td>
<td>False</td>
<td>13</td>
</tr>
<tr class="even">
<td>v11</td>
<td>Un gato durmiendo en un sofá</td>
<td>[‘11.315’, ‘15.491’, ‘-10.428’, ‘-0.004’, ‘10.014’]</td>
<td>1.369</td>
<td>4.175</td>
<td>True</td>
<td>11</td>
</tr>
<tr class="odd">
<td>v10_6</td>
<td>Un perro jugando en el parque</td>
<td>[‘33.967’, ‘33.302’, ‘31.580’, ‘32.710’, ‘31.384’]</td>
<td>1.02</td>
<td>0.665</td>
<td>False</td>
<td>13</td>
</tr>
</tbody>
</table>
</section>
<section id="tabla-de-análisis-de-la-estructura-del-modelo-y-el-éxitofallo-en-el-entrenamiento" class="level4">
<h4 class="anchored" data-anchor-id="tabla-de-análisis-de-la-estructura-del-modelo-y-el-éxitofallo-en-el-entrenamiento"><strong>Tabla de análisis de la estructura del modelo y el éxito/fallo en el entrenamiento</strong></h4>
<p>Basándonos en los resultados de este experimento, podemos analizar los resultados de entrenamiento para cada versión de atención cruzada y resumir las causas del éxito/fallo en el entrenamiento de la siguiente manera. Here’s the translated version:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 20%">
<col style="width: 13%">
<col style="width: 28%">
<col style="width: 37%">
</colgroup>
<thead>
<tr class="header">
<th>Version</th>
<th>Name</th>
<th>Description</th>
<th>Training Result</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>v1</td>
<td>Basic Attention</td>
<td>Simple attention mechanism using dot product between image and text features.</td>
<td>Success</td>
</tr>
<tr class="even">
<td>v2_1</td>
<td>Normalized Features</td>
<td>Normalizes both image and text features before applying attention.</td>
<td>Failure (Vanishing Gradients)</td>
</tr>
<tr class="odd">
<td>v2_2</td>
<td>Residual Connection</td>
<td>Adds residual connection to the attention output, maintaining the original feature information.</td>
<td>Success</td>
</tr>
<tr class="even">
<td>v3</td>
<td>Feature Projection</td>
<td>Projects image and text features into a common space using linear layers.</td>
<td>Failure (Loss of Feature Information)</td>
</tr>
<tr class="odd">
<td>v4_1</td>
<td>Modality-Specific Projections</td>
<td>Applies separate projections for images and texts to preserve modality-specific characteristics.</td>
<td>Failure (Too Complex, Overfitting)</td>
</tr>
<tr class="even">
<td>v4_2</td>
<td>Shared Projection Layer</td>
<td>Uses a single shared projection layer for both modalities to reduce complexity.</td>
<td>Success</td>
</tr>
<tr class="odd">
<td>v5</td>
<td>Scaled Dot-Product Attention</td>
<td>Applies scaling factor in dot-product attention to prevent large values during softmax computation.</td>
<td>Success</td>
</tr>
<tr class="even">
<td>v6</td>
<td>Multi-Head Attention</td>
<td>Introduces multi-head mechanism, allowing the model to capture multiple aspects of the relationship between modalities.</td>
<td>Success</td>
</tr>
<tr class="odd">
<td>v7_1</td>
<td>Relative Positional Encoding</td>
<td>Adds relative positional encodings to account for sequential information in text features.</td>
<td>Failure (Unstable Training)</td>
</tr>
<tr class="even">
<td>v7_2</td>
<td>Absolute Positional Encoding</td>
<td>Uses absolute positional encodings instead.</td>
<td>Success</td>
</tr>
<tr class="odd">
<td>v8</td>
<td>Layer Normalization</td>
<td>Applies layer normalization after attention to stabilize training and improve performance.</td>
<td>Success</td>
</tr>
<tr class="even">
<td>v9_1</td>
<td>Adaptive Attention Weights</td>
<td>Introduces adaptive attention weights for each modality, controlled by learnable parameters.</td>
<td>Failure (Training Divergence)</td>
</tr>
<tr class="odd">
<td>v9_2</td>
<td>Softmax Temperature Scaling</td>
<td>Applies temperature scaling in the softmax function to control the sharpness of attention distribution.</td>
<td>Success</td>
</tr>
<tr class="even">
<td>v10</td>
<td>Cross-Modality Attention</td>
<td>Allows bidirectional attention between images and texts, enhancing cross-modality interaction.</td>
<td>Success</td>
</tr>
<tr class="odd">
<td>v11_1</td>
<td>Attention with Dropout</td>
<td>Adds dropout after attention to prevent overfitting and improve generalization.</td>
<td>Success</td>
</tr>
<tr class="even">
<td>v11_2</td>
<td>Gated Attention Mechanism</td>
<td>Introduces a gating mechanism to control the flow of information between modalities.</td>
<td>Failure (Training Instability)</td>
</tr>
<tr class="odd">
<td>v12</td>
<td>Dual-Stream Attention</td>
<td>Maintains separate attention streams for each modality and fuses them at the end.</td>
<td>Success</td>
</tr>
<tr class="even">
<td>v13_1</td>
<td>Hierarchical Feature Fusion</td>
<td>Fuses features at multiple levels to capture both local and global information.</td>
<td>Failure (Complexity)</td>
</tr>
<tr class="odd">
<td>v13_2</td>
<td>Weighted Feature Fusion</td>
<td>Uses learnable weights for weighted fusion of attention outputs.</td>
<td>Success</td>
</tr>
<tr class="even">
<td>v14</td>
<td>Contextual Attention</td>
<td>Adds context vectors to the attention mechanism to provide more contextual information.</td>
<td>Success</td>
</tr>
<tr class="odd">
<td>v15_1</td>
<td>Transformer-based Attention</td>
<td>Replaces traditional attention with transformer blocks for better modeling of long-range dependencies.</td>
<td>Failure (Resource Intensive)</td>
</tr>
<tr class="even">
<td>v15_2</td>
<td>Lightweight Transformer</td>
<td>Optimizes the transformer architecture to reduce computational cost while maintaining performance.</td>
<td>Success</td>
</tr>
</tbody>
</table>
<p>This table provides a comprehensive overview of the different versions and their outcomes, highlighting successful and unsuccessful attempts in developing an attention mechanism for multi-modal data. #### 10.4.4.3 Explicación por estructura de atención</p>
<p><strong>1. v0:</strong> Atención bidireccional independiente - estructura básica</p>
<p>v0 implementa la forma más básica de Cross-Modal Attention. Calcula la atención de manera independiente para imagen y texto, sin usar ninguna normalización o transformación adicional fuera del producto interno escalado (Scaled Dot-Product Attention).</p>
<div id="cell-55" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb80"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb80-1"><a href="#cb80-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb80-2"><a href="#cb80-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb80-3"><a href="#cb80-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb80-4"><a href="#cb80-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-5"><a href="#cb80-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-6"><a href="#cb80-6" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> CrossAttention(nn.Module):</span>
<span id="cb80-7"><a href="#cb80-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, dim):</span>
<span id="cb80-8"><a href="#cb80-8" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb80-9"><a href="#cb80-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.scale <span class="op">=</span> dim <span class="op">**</span> <span class="op">-</span><span class="fl">0.5</span></span>
<span id="cb80-10"><a href="#cb80-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-11"><a href="#cb80-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, image_features, text_features):</span>
<span id="cb80-12"><a href="#cb80-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Image -&gt; Text attention</span></span>
<span id="cb80-13"><a href="#cb80-13" aria-hidden="true" tabindex="-1"></a>        attn_i2t <span class="op">=</span> torch.matmul(image_features, text_features.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>)) <span class="op">*</span> <span class="va">self</span>.scale</span>
<span id="cb80-14"><a href="#cb80-14" aria-hidden="true" tabindex="-1"></a>        attn_i2t <span class="op">=</span> attn_i2t.softmax(dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb80-15"><a href="#cb80-15" aria-hidden="true" tabindex="-1"></a>        image_attended <span class="op">=</span> torch.matmul(attn_i2t, text_features)</span>
<span id="cb80-16"><a href="#cb80-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-17"><a href="#cb80-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Text -&gt; Image attention</span></span>
<span id="cb80-18"><a href="#cb80-18" aria-hidden="true" tabindex="-1"></a>        attn_t2i <span class="op">=</span> torch.matmul(text_features, image_features.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>)) <span class="op">*</span> <span class="va">self</span>.scale</span>
<span id="cb80-19"><a href="#cb80-19" aria-hidden="true" tabindex="-1"></a>        attn_t2i <span class="op">=</span> attn_t2i.softmax(dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb80-20"><a href="#cb80-20" aria-hidden="true" tabindex="-1"></a>        text_attended <span class="op">=</span> torch.matmul(attn_t2i, image_features)</span>
<span id="cb80-21"><a href="#cb80-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-22"><a href="#cb80-22" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> image_attended, text_attended</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>v0 es sensible a los cambios de escala de las características de entrada porque no tiene un proceso de normalización separado. Si la escala de los datos de entrada cambia significativamente durante el proceso de aprendizaje, los pesos de atención pueden volverse inestables y el entrenamiento puede no realizarse correctamente.</p>
<p><strong>2. v2:</strong> Atención compartida + Normalización por Capa</p>
<p>v2 es una versión que estabiliza la escala de las características aplicando Layer Normalization (LN) a las características de entrada, a diferencia de v1. Aunque v1 utilizaba la misma matriz de atención (matriz de pesos) y su transpuesta para calcular la atención imagen→texto, texto→imagen, tenía la desventaja de ser sensible a la escala de entrada.</p>
<div id="cell-57" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb81"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb81-1"><a href="#cb81-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb81-2"><a href="#cb81-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb81-3"><a href="#cb81-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb81-4"><a href="#cb81-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-5"><a href="#cb81-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Co-attention + added LN</span></span>
<span id="cb81-6"><a href="#cb81-6" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> CrossAttention(nn.Module):</span>
<span id="cb81-7"><a href="#cb81-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, dim):</span>
<span id="cb81-8"><a href="#cb81-8" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb81-9"><a href="#cb81-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.scale <span class="op">=</span> dim <span class="op">**</span> <span class="op">-</span><span class="fl">0.5</span></span>
<span id="cb81-10"><a href="#cb81-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.norm <span class="op">=</span> nn.LayerNorm(dim)  <span class="co"># Use a single LayerNorm</span></span>
<span id="cb81-11"><a href="#cb81-11" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb81-12"><a href="#cb81-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, image_features, text_features):</span>
<span id="cb81-13"><a href="#cb81-13" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Input normalization</span></span>
<span id="cb81-14"><a href="#cb81-14" aria-hidden="true" tabindex="-1"></a>        image_norm <span class="op">=</span> <span class="va">self</span>.norm(image_features)</span>
<span id="cb81-15"><a href="#cb81-15" aria-hidden="true" tabindex="-1"></a>        text_norm <span class="op">=</span> <span class="va">self</span>.norm(text_features)</span>
<span id="cb81-16"><a href="#cb81-16" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb81-17"><a href="#cb81-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Simple attention calculation</span></span>
<span id="cb81-18"><a href="#cb81-18" aria-hidden="true" tabindex="-1"></a>        attn <span class="op">=</span> torch.matmul(image_norm, text_norm.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>)) <span class="op">*</span> <span class="va">self</span>.scale</span>
<span id="cb81-19"><a href="#cb81-19" aria-hidden="true" tabindex="-1"></a>        attn <span class="op">=</span> attn.softmax(dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb81-20"><a href="#cb81-20" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb81-21"><a href="#cb81-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Bidirectional feature fusion (without residual connection)</span></span>
<span id="cb81-22"><a href="#cb81-22" aria-hidden="true" tabindex="-1"></a>        image_out <span class="op">=</span> torch.matmul(attn, text_norm)</span>
<span id="cb81-23"><a href="#cb81-23" aria-hidden="true" tabindex="-1"></a>        text_out <span class="op">=</span> torch.matmul(attn.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>), image_norm)</span>
<span id="cb81-24"><a href="#cb81-24" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb81-25"><a href="#cb81-25" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> image_out, text_out</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><code>image_norm = self.norm(image_features)</code> y <code>text_norm = self.norm(text_features)</code> aplican la Normalización de Capa (Layer Normalization) a las características de entrada. La Normalización de Capa realiza la normalización de forma independiente para cada muestra (cada imagen o texto en el minibatch). Es decir, calcula la media y la varianza del vector de características de cada muestra para convertirlas en 0 y 1. Esto ayuda a estabilizar el aprendizaje al prevenir que los pesos de atención diverjan incluso si la escala de las características de entrada cambia significativamente.</p>
<p>Sin embargo, aún existen limitaciones. v2 resolvió el problema de escala de entrada mediante la Normalización de Capa pero utiliza la misma matriz de atención para la atención imagen→texto y texto→imagen. Esto puede no reflejar adecuadamente la relación asimétrica entre las dos modalidades. Generar texto a partir de una imagen y generar imágenes a partir de un texto pueden tener complejidades diferentes. Tratar esto con el mismo mecanismo de atención puede ser ineficiente.</p>
<p><strong>3. v3:</strong> v2 + conexión residual (Residual Connection) - caso de fracaso</p>
<p>Después del diseño arquitectónico del modelo ResNet, la conexión residual se ha utilizado como una práctica común para mitigar el problema de desvanecimiento de gradientes (Gradient Vanishing) que puede ocurrir a medida que las redes se vuelven más profundas y para permitir el aprendizaje efectivo de redes más profundas. Sin embargo, en este experimento, la conexión residual resultó ser una causa de fracaso al reducir el rendimiento.</p>
<p>Esta es una observación muy importante.</p>
<div id="cell-59" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb82"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb82-1"><a href="#cb82-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb82-2"><a href="#cb82-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb82-3"><a href="#cb82-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb82-4"><a href="#cb82-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-5"><a href="#cb82-5" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> CrossAttention(nn.Module):</span>
<span id="cb82-6"><a href="#cb82-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, dim):</span>
<span id="cb82-7"><a href="#cb82-7" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb82-8"><a href="#cb82-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.scale <span class="op">=</span> dim <span class="op">**</span> <span class="op">-</span><span class="fl">0.5</span></span>
<span id="cb82-9"><a href="#cb82-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.norm <span class="op">=</span> nn.LayerNorm(dim)  <span class="co"># Use a single LayerNorm</span></span>
<span id="cb82-10"><a href="#cb82-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-11"><a href="#cb82-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, image_features, text_features):</span>
<span id="cb82-12"><a href="#cb82-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Input normalization</span></span>
<span id="cb82-13"><a href="#cb82-13" aria-hidden="true" tabindex="-1"></a>        image_norm <span class="op">=</span> <span class="va">self</span>.norm(image_features)</span>
<span id="cb82-14"><a href="#cb82-14" aria-hidden="true" tabindex="-1"></a>        text_norm <span class="op">=</span> <span class="va">self</span>.norm(text_features)</span>
<span id="cb82-15"><a href="#cb82-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-16"><a href="#cb82-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Simple attention calculation</span></span>
<span id="cb82-17"><a href="#cb82-17" aria-hidden="true" tabindex="-1"></a>        attn <span class="op">=</span> torch.matmul(image_norm, text_norm.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>)) <span class="op">*</span> <span class="va">self</span>.scale</span>
<span id="cb82-18"><a href="#cb82-18" aria-hidden="true" tabindex="-1"></a>        attn <span class="op">=</span> attn.softmax(dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb82-19"><a href="#cb82-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-20"><a href="#cb82-20" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Bidirectional feature fusion</span></span>
<span id="cb82-21"><a href="#cb82-21" aria-hidden="true" tabindex="-1"></a>        image_attended <span class="op">=</span> torch.matmul(attn, text_norm)</span>
<span id="cb82-22"><a href="#cb82-22" aria-hidden="true" tabindex="-1"></a>        text_attended <span class="op">=</span> torch.matmul(attn.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>), image_norm)</span>
<span id="cb82-23"><a href="#cb82-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-24"><a href="#cb82-24" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Add residual connection</span></span>
<span id="cb82-25"><a href="#cb82-25" aria-hidden="true" tabindex="-1"></a>        image_out <span class="op">=</span> image_features <span class="op">+</span> image_attended</span>
<span id="cb82-26"><a href="#cb82-26" aria-hidden="true" tabindex="-1"></a>        text_out <span class="op">=</span> text_features <span class="op">+</span> text_attended</span>
<span id="cb82-27"><a href="#cb82-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-28"><a href="#cb82-28" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> image_out, text_out</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Generalmente, las conexiones residuales son efectivas para resolver el problema de que la red se hace más difícil de entrenar a medida que aumenta su profundidad. Sin embargo, en v3, esto resultó en una disminución del rendimiento debido a las siguientes razones.</p>
<p><strong>Red relativa poco profunda:</strong> El modelo v3 tiene una estructura de red relativamente poco profunda. Las conexiones residuales ayudan significativamente a mitigar el problema de la desaparición del gradiente en redes profundas, pero su efecto es mínimo en redes poco profundas y pueden incluso interrumpir el flujo de información.</p>
<p><strong>Preservación excesiva de las características originales:</strong> El núcleo de la Atención Multimodal (Cross-Modal Attention) es generar nuevas características a través de la interacción entre dos modalidades diferentes, imagen y texto. Sin embargo, en v3, al sumar directamente los vectores de características originales al resultado de las operaciones de atención, se diluyó la información importante obtenida mediante el mecanismo de atención y se obstaculizó la generación de características a través de la interacción entre las dos modalidades. En otras palabras, en lugar de aprender nueva información, el modelo se centró más en mantener la información existente.</p>
<p>Los resultados experimentales de v3 brindan una importante lección: las conexiones residuales no son una <strong>solución universal</strong> que siempre mejore el rendimiento. Las conexiones residuales deben utilizarse con cuidado, considerando la profundidad de la red, el lugar de aplicación y las características del problema. v3 es un ejemplo típico de fracaso debido al uso inadecuado de las conexiones residuales, lo que resultó en una disminución del rendimiento.</p>
<p><strong>4. v8:</strong> Atención multi-cabeza independiente</p>
<p>v8 introdujo cambios importantes para resolver los problemas de la versión anterior (v7) y mejorar el rendimiento de la Atención Multimodal. En particular, separó la atención imagen→texto y texto→imagen en atenciones multi-cabeza (Multi-Head Attention) independientes. Además, aplicó Normalización de Capa (Layer Normalization) no solo a las entradas sino también a las salidas de las operaciones de atención, lo que mejoró aún más la estabilidad del entrenamiento.</p>
<div id="cell-61" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb83"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb83-1"><a href="#cb83-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb83-2"><a href="#cb83-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb83-3"><a href="#cb83-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb83-4"><a href="#cb83-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-5"><a href="#cb83-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-6"><a href="#cb83-6" aria-hidden="true" tabindex="-1"></a><span class="co"># v8 - Independent multi-head</span></span>
<span id="cb83-7"><a href="#cb83-7" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> CrossAttention(nn.Module):</span>
<span id="cb83-8"><a href="#cb83-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, dim, num_heads<span class="op">=</span><span class="dv">8</span>):</span>
<span id="cb83-9"><a href="#cb83-9" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb83-10"><a href="#cb83-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.num_heads <span class="op">=</span> num_heads</span>
<span id="cb83-11"><a href="#cb83-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.head_dim <span class="op">=</span> dim <span class="op">//</span> num_heads</span>
<span id="cb83-12"><a href="#cb83-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.scale <span class="op">=</span> <span class="va">self</span>.head_dim <span class="op">**</span> <span class="op">-</span><span class="fl">0.5</span></span>
<span id="cb83-13"><a href="#cb83-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.norm <span class="op">=</span> nn.LayerNorm(dim)</span>
<span id="cb83-14"><a href="#cb83-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-15"><a href="#cb83-15" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Projections for multi-head attention</span></span>
<span id="cb83-16"><a href="#cb83-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.to_q <span class="op">=</span> nn.Linear(dim, dim)</span>
<span id="cb83-17"><a href="#cb83-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.to_k <span class="op">=</span> nn.Linear(dim, dim)</span>
<span id="cb83-18"><a href="#cb83-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.to_v <span class="op">=</span> nn.Linear(dim, dim)</span>
<span id="cb83-19"><a href="#cb83-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-20"><a href="#cb83-20" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Output projection</span></span>
<span id="cb83-21"><a href="#cb83-21" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.to_out <span class="op">=</span> nn.Linear(dim, dim)</span>
<span id="cb83-22"><a href="#cb83-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-23"><a href="#cb83-23" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Add output normalization</span></span>
<span id="cb83-24"><a href="#cb83-24" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.out_norm <span class="op">=</span> nn.LayerNorm(dim)</span>
<span id="cb83-25"><a href="#cb83-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-26"><a href="#cb83-26" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, image_features, text_features):</span>
<span id="cb83-27"><a href="#cb83-27" aria-hidden="true" tabindex="-1"></a>        B, N_i, _ <span class="op">=</span> image_features.shape</span>
<span id="cb83-28"><a href="#cb83-28" aria-hidden="true" tabindex="-1"></a>        _, N_t, _ <span class="op">=</span> text_features.shape</span>
<span id="cb83-29"><a href="#cb83-29" aria-hidden="true" tabindex="-1"></a>        H <span class="op">=</span> <span class="va">self</span>.num_heads</span>
<span id="cb83-30"><a href="#cb83-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-31"><a href="#cb83-31" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Input normalization</span></span>
<span id="cb83-32"><a href="#cb83-32" aria-hidden="true" tabindex="-1"></a>        image_norm <span class="op">=</span> <span class="va">self</span>.norm(image_features)</span>
<span id="cb83-33"><a href="#cb83-33" aria-hidden="true" tabindex="-1"></a>        text_norm <span class="op">=</span> <span class="va">self</span>.norm(text_features)</span>
<span id="cb83-34"><a href="#cb83-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-35"><a href="#cb83-35" aria-hidden="true" tabindex="-1"></a>        <span class="kw">def</span> split_heads(x):</span>
<span id="cb83-36"><a href="#cb83-36" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> x.reshape(B, <span class="op">-</span><span class="dv">1</span>, H, <span class="va">self</span>.head_dim).transpose(<span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb83-37"><a href="#cb83-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-38"><a href="#cb83-38" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Image -&gt; Text direction attention</span></span>
<span id="cb83-39"><a href="#cb83-39" aria-hidden="true" tabindex="-1"></a>        q_img <span class="op">=</span> split_heads(<span class="va">self</span>.to_q(image_norm))</span>
<span id="cb83-40"><a href="#cb83-40" aria-hidden="true" tabindex="-1"></a>        k_txt <span class="op">=</span> split_heads(<span class="va">self</span>.to_k(text_norm))</span>
<span id="cb83-41"><a href="#cb83-41" aria-hidden="true" tabindex="-1"></a>        v_txt <span class="op">=</span> split_heads(<span class="va">self</span>.to_v(text_norm))</span>
<span id="cb83-42"><a href="#cb83-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-43"><a href="#cb83-43" aria-hidden="true" tabindex="-1"></a>        attn_i2t <span class="op">=</span> torch.matmul(q_img, k_txt.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>)) <span class="op">*</span> <span class="va">self</span>.scale</span>
<span id="cb83-44"><a href="#cb83-44" aria-hidden="true" tabindex="-1"></a>        attn_i2t <span class="op">=</span> attn_i2t.softmax(dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb83-45"><a href="#cb83-45" aria-hidden="true" tabindex="-1"></a>        image_attended <span class="op">=</span> torch.matmul(attn_i2t, v_txt)</span>
<span id="cb83-46"><a href="#cb83-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-47"><a href="#cb83-47" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Text -&gt; Image direction attention</span></span>
<span id="cb83-48"><a href="#cb83-48" aria-hidden="true" tabindex="-1"></a>        q_txt <span class="op">=</span> split_heads(<span class="va">self</span>.to_q(text_norm))</span>
<span id="cb83-49"><a href="#cb83-49" aria-hidden="true" tabindex="-1"></a>        k_img <span class="op">=</span> split_heads(<span class="va">self</span>.to_k(image_norm))</span>
<span id="cb83-50"><a href="#cb83-50" aria-hidden="true" tabindex="-1"></a>        v_img <span class="op">=</span> split_heads(<span class="va">self</span>.to_v(image_norm))</span>
<span id="cb83-51"><a href="#cb83-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-52"><a href="#cb83-52" aria-hidden="true" tabindex="-1"></a>        attn_t2i <span class="op">=</span> torch.matmul(q_txt, k_img.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>)) <span class="op">*</span> <span class="va">self</span>.scale</span>
<span id="cb83-53"><a href="#cb83-53" aria-hidden="true" tabindex="-1"></a>        attn_t2i <span class="op">=</span> attn_t2i.softmax(dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb83-54"><a href="#cb83-54" aria-hidden="true" tabindex="-1"></a>        text_attended <span class="op">=</span> torch.matmul(attn_t2i, v_img)</span>
<span id="cb83-55"><a href="#cb83-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-56"><a href="#cb83-56" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Combine heads and output projection</span></span>
<span id="cb83-57"><a href="#cb83-57" aria-hidden="true" tabindex="-1"></a>        image_attended <span class="op">=</span> image_attended.transpose(<span class="dv">1</span>, <span class="dv">2</span>).reshape(B, N_i, <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb83-58"><a href="#cb83-58" aria-hidden="true" tabindex="-1"></a>        text_attended <span class="op">=</span> text_attended.transpose(<span class="dv">1</span>, <span class="dv">2</span>).reshape(B, N_t, <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb83-59"><a href="#cb83-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-60"><a href="#cb83-60" aria-hidden="true" tabindex="-1"></a>        image_out <span class="op">=</span> <span class="va">self</span>.out_norm(<span class="va">self</span>.to_out(image_attended))</span>
<span id="cb83-61"><a href="#cb83-61" aria-hidden="true" tabindex="-1"></a>        text_out <span class="op">=</span> <span class="va">self</span>.out_norm(<span class="va">self</span>.to_out(text_attended))</span>
<span id="cb83-62"><a href="#cb83-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-63"><a href="#cb83-63" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> image_out, text_out</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>v7 también introdujo la atención multi-cabeza, pero aún utilizaba las mismas transformaciones Q, K, V para la atención imagen→texto y texto→imagen. Es decir, dado que todas las cabezas compartían las mismas matrices Q, K, V, había restricciones en el aprendizaje de características distintivas por cada cabeza, lo cual limitaba la capacidad expresiva del modelo. v8 abordó estos problemas aplicando transformaciones Q, K, V independientes para cada dirección (imagen→texto, texto→imagen) y para cada cabeza, permitiendo que el modelo aprendiera representaciones de características mucho más flexibles y ricas.</p>
<p><strong>5. v9:</strong> v8 + Pre-LN + FFN (FFN con puerta + dropout)</p>
<p>v9 se basa en la estructura de v8 e introduce tres mecanismos importantes para mejorar aún más la estabilidad del entrenamiento y el rendimiento. Estos son: Normalización de Capa Previo (Pre-Layer Normalization), Red Neuronal Feed-Forward con Puerta (Feed-Forward Network, FFN) y dropout (Dropout).</p>
<div id="cell-63" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb84"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb84-1"><a href="#cb84-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb84-2"><a href="#cb84-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb84-3"><a href="#cb84-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb84-4"><a href="#cb84-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-5"><a href="#cb84-5" aria-hidden="true" tabindex="-1"></a><span class="co"># v9 - Dropout before gated FFN, pass through norm at the end -&gt; trainable</span></span>
<span id="cb84-6"><a href="#cb84-6" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> CrossAttention(nn.Module):</span>
<span id="cb84-7"><a href="#cb84-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, dim, num_heads<span class="op">=</span><span class="dv">8</span>, dropout<span class="op">=</span><span class="fl">0.1</span>, ff_dim<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb84-8"><a href="#cb84-8" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb84-9"><a href="#cb84-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.num_heads <span class="op">=</span> num_heads</span>
<span id="cb84-10"><a href="#cb84-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.head_dim <span class="op">=</span> dim <span class="op">//</span> num_heads</span>
<span id="cb84-11"><a href="#cb84-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.scale <span class="op">=</span> <span class="va">self</span>.head_dim <span class="op">**</span> <span class="op">-</span><span class="fl">0.5</span></span>
<span id="cb84-12"><a href="#cb84-12" aria-hidden="true" tabindex="-1"></a>        ff_dim <span class="op">=</span> ff_dim <span class="kw">or</span> dim <span class="op">*</span> <span class="dv">4</span></span>
<span id="cb84-13"><a href="#cb84-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-14"><a href="#cb84-14" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Normalization layers for Pre-LN</span></span>
<span id="cb84-15"><a href="#cb84-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.attn_norm <span class="op">=</span> nn.LayerNorm(dim)</span>
<span id="cb84-16"><a href="#cb84-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ff_norm <span class="op">=</span> nn.LayerNorm(dim)</span>
<span id="cb84-17"><a href="#cb84-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-18"><a href="#cb84-18" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Projections for multi-head attention</span></span>
<span id="cb84-19"><a href="#cb84-19" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.to_q <span class="op">=</span> nn.Linear(dim, dim)</span>
<span id="cb84-20"><a href="#cb84-20" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.to_k <span class="op">=</span> nn.Linear(dim, dim)</span>
<span id="cb84-21"><a href="#cb84-21" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.to_v <span class="op">=</span> nn.Linear(dim, dim)</span>
<span id="cb84-22"><a href="#cb84-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-23"><a href="#cb84-23" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Output projection</span></span>
<span id="cb84-24"><a href="#cb84-24" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.to_out <span class="op">=</span> nn.Linear(dim, dim)</span>
<span id="cb84-25"><a href="#cb84-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-26"><a href="#cb84-26" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Dropout</span></span>
<span id="cb84-27"><a href="#cb84-27" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(dropout)</span>
<span id="cb84-28"><a href="#cb84-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-29"><a href="#cb84-29" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Gated feedforward network</span></span>
<span id="cb84-30"><a href="#cb84-30" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ff_gate <span class="op">=</span> nn.Sequential(</span>
<span id="cb84-31"><a href="#cb84-31" aria-hidden="true" tabindex="-1"></a>            nn.Linear(dim, ff_dim),</span>
<span id="cb84-32"><a href="#cb84-32" aria-hidden="true" tabindex="-1"></a>            nn.GELU(),</span>
<span id="cb84-33"><a href="#cb84-33" aria-hidden="true" tabindex="-1"></a>            nn.Dropout(dropout)</span>
<span id="cb84-34"><a href="#cb84-34" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb84-35"><a href="#cb84-35" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ff_value <span class="op">=</span> nn.Sequential(</span>
<span id="cb84-36"><a href="#cb84-36" aria-hidden="true" tabindex="-1"></a>            nn.Linear(dim, ff_dim),</span>
<span id="cb84-37"><a href="#cb84-37" aria-hidden="true" tabindex="-1"></a>            nn.GELU(),</span>
<span id="cb84-38"><a href="#cb84-38" aria-hidden="true" tabindex="-1"></a>            nn.Dropout(dropout)</span>
<span id="cb84-39"><a href="#cb84-39" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb84-40"><a href="#cb84-40" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ff_out <span class="op">=</span> nn.Linear(ff_dim, dim)</span>
<span id="cb84-41"><a href="#cb84-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-42"><a href="#cb84-42" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, image_features, text_features):</span>
<span id="cb84-43"><a href="#cb84-43" aria-hidden="true" tabindex="-1"></a>        B, N_i, _ <span class="op">=</span> image_features.shape</span>
<span id="cb84-44"><a href="#cb84-44" aria-hidden="true" tabindex="-1"></a>        _, N_t, _ <span class="op">=</span> text_features.shape</span>
<span id="cb84-45"><a href="#cb84-45" aria-hidden="true" tabindex="-1"></a>        H <span class="op">=</span> <span class="va">self</span>.num_heads</span>
<span id="cb84-46"><a href="#cb84-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-47"><a href="#cb84-47" aria-hidden="true" tabindex="-1"></a>        <span class="kw">def</span> split_heads(x):</span>
<span id="cb84-48"><a href="#cb84-48" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> x.reshape(B, <span class="op">-</span><span class="dv">1</span>, H, <span class="va">self</span>.head_dim).transpose(<span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb84-49"><a href="#cb84-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-50"><a href="#cb84-50" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Pre-LN: Normalize before attention</span></span>
<span id="cb84-51"><a href="#cb84-51" aria-hidden="true" tabindex="-1"></a>        image_norm <span class="op">=</span> <span class="va">self</span>.attn_norm(image_features)</span>
<span id="cb84-52"><a href="#cb84-52" aria-hidden="true" tabindex="-1"></a>        text_norm <span class="op">=</span> <span class="va">self</span>.attn_norm(text_features)</span>
<span id="cb84-53"><a href="#cb84-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-54"><a href="#cb84-54" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Image -&gt; Text direction attention</span></span>
<span id="cb84-55"><a href="#cb84-55" aria-hidden="true" tabindex="-1"></a>        q_img <span class="op">=</span> split_heads(<span class="va">self</span>.to_q(image_norm))</span>
<span id="cb84-56"><a href="#cb84-56" aria-hidden="true" tabindex="-1"></a>        k_txt <span class="op">=</span> split_heads(<span class="va">self</span>.to_k(text_norm))</span>
<span id="cb84-57"><a href="#cb84-57" aria-hidden="true" tabindex="-1"></a>        v_txt <span class="op">=</span> split_heads(<span class="va">self</span>.to_v(text_norm))</span>
<span id="cb84-58"><a href="#cb84-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-59"><a href="#cb84-59" aria-hidden="true" tabindex="-1"></a>        attn_i2t <span class="op">=</span> torch.matmul(q_img, k_txt.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>)) <span class="op">*</span> <span class="va">self</span>.scale</span>
<span id="cb84-60"><a href="#cb84-60" aria-hidden="true" tabindex="-1"></a>        attn_i2t <span class="op">=</span> attn_i2t.softmax(dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb84-61"><a href="#cb84-61" aria-hidden="true" tabindex="-1"></a>        attn_i2t <span class="op">=</span> <span class="va">self</span>.dropout(attn_i2t)  <span class="co"># Apply dropout to attention weights</span></span>
<span id="cb84-62"><a href="#cb84-62" aria-hidden="true" tabindex="-1"></a>        image_attended <span class="op">=</span> torch.matmul(attn_i2t, v_txt)</span>
<span id="cb84-63"><a href="#cb84-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-64"><a href="#cb84-64" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Text -&gt; Image direction attention</span></span>
<span id="cb84-65"><a href="#cb84-65" aria-hidden="true" tabindex="-1"></a>        q_txt <span class="op">=</span> split_heads(<span class="va">self</span>.to_q(text_norm))</span>
<span id="cb84-66"><a href="#cb84-66" aria-hidden="true" tabindex="-1"></a>        k_img <span class="op">=</span> split_heads(<span class="va">self</span>.to_k(image_norm))</span>
<span id="cb84-67"><a href="#cb84-67" aria-hidden="true" tabindex="-1"></a>        v_img <span class="op">=</span> split_heads(<span class="va">self</span>.to_v(image_norm))</span>
<span id="cb84-68"><a href="#cb84-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-69"><a href="#cb84-69" aria-hidden="true" tabindex="-1"></a>        attn_t2i <span class="op">=</span> torch.matmul(q_txt, k_img.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>)) <span class="op">*</span> <span class="va">self</span>.scale</span>
<span id="cb84-70"><a href="#cb84-70" aria-hidden="true" tabindex="-1"></a>        attn_t2i <span class="op">=</span> attn_t2i.softmax(dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb84-71"><a href="#cb84-71" aria-hidden="true" tabindex="-1"></a>        attn_t2i <span class="op">=</span> <span class="va">self</span>.dropout(attn_t2i)  <span class="co"># Apply dropout to attention weights</span></span>
<span id="cb84-72"><a href="#cb84-72" aria-hidden="true" tabindex="-1"></a>        text_attended <span class="op">=</span> torch.matmul(attn_t2i, v_img)</span>
<span id="cb84-73"><a href="#cb84-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-74"><a href="#cb84-74" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Combine heads and output projection</span></span>
<span id="cb84-75"><a href="#cb84-75" aria-hidden="true" tabindex="-1"></a>        image_attended <span class="op">=</span> image_attended.transpose(<span class="dv">1</span>, <span class="dv">2</span>).reshape(B, N_i, <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb84-76"><a href="#cb84-76" aria-hidden="true" tabindex="-1"></a>        text_attended <span class="op">=</span> text_attended.transpose(<span class="dv">1</span>, <span class="dv">2</span>).reshape(B, N_t, <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb84-77"><a href="#cb84-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-78"><a href="#cb84-78" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Output projection and dropout</span></span>
<span id="cb84-79"><a href="#cb84-79" aria-hidden="true" tabindex="-1"></a>        image_attended <span class="op">=</span> <span class="va">self</span>.dropout(<span class="va">self</span>.to_out(image_attended))</span>
<span id="cb84-80"><a href="#cb84-80" aria-hidden="true" tabindex="-1"></a>        text_attended <span class="op">=</span> <span class="va">self</span>.dropout(<span class="va">self</span>.to_out(text_attended))</span>
<span id="cb84-81"><a href="#cb84-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-82"><a href="#cb84-82" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Residual connection - connecting the original image features makes training impossible.</span></span>
<span id="cb84-83"><a href="#cb84-83" aria-hidden="true" tabindex="-1"></a>        <span class="co"># image_attended = image_attended + image_features</span></span>
<span id="cb84-84"><a href="#cb84-84" aria-hidden="true" tabindex="-1"></a>        <span class="co"># text_attended = text_attended + text_features</span></span>
<span id="cb84-85"><a href="#cb84-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-86"><a href="#cb84-86" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Pre-LN: Normalize before FFN</span></span>
<span id="cb84-87"><a href="#cb84-87" aria-hidden="true" tabindex="-1"></a>        image_ff <span class="op">=</span> <span class="va">self</span>.ff_norm(image_attended)</span>
<span id="cb84-88"><a href="#cb84-88" aria-hidden="true" tabindex="-1"></a>        text_ff <span class="op">=</span> <span class="va">self</span>.ff_norm(text_attended)</span>
<span id="cb84-89"><a href="#cb84-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-90"><a href="#cb84-90" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Gated feedforward processing</span></span>
<span id="cb84-91"><a href="#cb84-91" aria-hidden="true" tabindex="-1"></a>        <span class="kw">def</span> apply_ff(x):</span>
<span id="cb84-92"><a href="#cb84-92" aria-hidden="true" tabindex="-1"></a>            gate <span class="op">=</span> <span class="va">self</span>.ff_gate(x)</span>
<span id="cb84-93"><a href="#cb84-93" aria-hidden="true" tabindex="-1"></a>            value <span class="op">=</span> <span class="va">self</span>.ff_value(x)</span>
<span id="cb84-94"><a href="#cb84-94" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> <span class="va">self</span>.dropout(<span class="va">self</span>.ff_out(gate <span class="op">*</span> value))</span>
<span id="cb84-95"><a href="#cb84-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-96"><a href="#cb84-96" aria-hidden="true" tabindex="-1"></a>        <span class="co"># FFN output and residual connection - this type of residual connection is possible.</span></span>
<span id="cb84-97"><a href="#cb84-97" aria-hidden="true" tabindex="-1"></a>        image_out <span class="op">=</span> apply_ff(image_ff) <span class="op">+</span> image_attended</span>
<span id="cb84-98"><a href="#cb84-98" aria-hidden="true" tabindex="-1"></a>        text_out <span class="op">=</span> apply_ff(text_ff) <span class="op">+</span> text_attended</span>
<span id="cb84-99"><a href="#cb84-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-100"><a href="#cb84-100" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> image_out, text_out</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<ul>
<li><p><strong>Pre-Layer Normalization:</strong> v8 aplicaba Layer Normalization después de las operaciones de atención (Post-LN), pero v9 lo aplica antes (Pre-LN). <code>self.image_norm_q</code>, <code>self.image_norm_k</code>, …, <code>self.text_norm_v</code> son ejemplos de esto. Pre-LN es más estable durante el entrenamiento que Post-LN y no requiere un calentamiento separado, por lo que se utiliza ampliamente en modelos basados en Transformer recientes.</p></li>
<li><p><strong>Feed-Forward Network (FFN) con puerta:</strong> v8 añade una FFN después de las operaciones de atención para mejorar la no linealidad y el poder de representación del modelo.</p>
<ul>
<li><code>self.image_ffn</code> y <code>self.text_ffn</code> definen la FFN. La FFN consta de dos capas lineales (Linear Layer) con una función de activación GELU (Gaussian Error Linear Unit) entre ellas, seguida de un dropout.</li>
<li><strong>Función de activación GELU:</strong> es una función no lineal más suave que ReLU y tiende a ofrecer un mejor rendimiento.</li>
<li><strong>Conexión residual &amp; LayerNorm después de la FFN:</strong> se aplica <em>conexión residual</em> a la salida de la FFN, seguida de Layer Normalization (<code>self.image_ffn_norm</code>, <code>self.text_ffn_norm</code>). A diferencia de v3, se aplica <em>después de pasar por la FFN</em>, lo que permite combinar información después del procesamiento no lineal, mejorando el flujo de información y contribuyendo a un mejor rendimiento.</li>
</ul></li>
<li><p><strong>Dropout:</strong> <code>self.dropout</code> define el dropout aplicado a los pesos de atención y dentro de la FFN. El dropout es una técnica efectiva de regularización que desactiva aleatoriamente neuronas durante el entrenamiento para prevenir el sobreajuste del modelo.</p></li>
</ul>
<p><strong>Efecto de los mecanismos añadidos</strong></p>
<ul>
<li>v9 mantiene la estructura de atención multi-cabeza independiente de v8 y añade Pre-LN, FFN con puerta y dropout para mejorar aún más la estabilidad del entrenamiento y el rendimiento.</li>
<li><strong>Pre-LN:</strong> hace que las etapas iniciales del entrenamiento sean más estables y permite un entrenamiento efectivo sin necesidad de un calentamiento de tasa de aprendizaje (warmup) separado.</li>
<li><strong>FFN con puerta:</strong> añade una transformación no lineal después de la operación de atención para mejorar el poder de representación del modelo. La función de activación GELU y el dropout desempeñan un papel en mejorar aún más el rendimiento de la FFN.</li>
<li><strong>Dropout:</strong> previene el sobreajuste del modelo y mejora su capacidad de generalización.</li>
</ul>
<p>v9 ha mejorado significativamente el rendimiento de Cross-Modal Attention a través de la combinación de estas técnicas, convirtiéndose en la base para versiones posteriores.</p>
</section>
<section id="análisis-de-los-resultados-principales" class="level4">
<h4 class="anchored" data-anchor-id="análisis-de-los-resultados-principales">10.4.4.4 Análisis de los resultados principales</h4>
<ul>
<li><p><strong>v0, v1 (estructura básica):</strong> v0 y v1, que solo utilizan atención simple sin normalización, se entrenaron <em>con éxito</em>. Sin embargo, en el caso de v1, aunque se entrenó, mostró una mayor similitud a las leyendas relacionadas con “gatos” tanto en el conjunto de datos de entrenamiento como en el de validación. Esto sugiere la importancia de la normalización.</p></li>
<li><p><strong>v2 (LayerNorm):</strong> v2, que aplica LayerNorm a la entrada, se entrenó con éxito. Esto demuestra que estabilizar la escala de las características de entrada es importante.</p></li>
<li><p><strong>v3 (conexión residual):</strong> v3, que añade conexión residual a v2, falló en el entrenamiento. Esto sugiere que la conexión residual no <em>siempre</em> ayuda en el aprendizaje multimodal. La conexión residual puede mantener excesivamente las características originales, lo cual puede interferir con el aprendizaje de la interacción entre dos modalidades.</p></li>
<li><p><strong>v4 (proyección):</strong> v4, que añade transformaciones lineales independientes (proyecciones) a cada modalidad, se entrenó con éxito. Esto sugiere que es importante transformar adecuadamente los espacios de características de cada modalidad.</p></li>
<li><p><strong>v7 (multicabezal compartido):</strong> v7, que extiende la matriz de atención compartida a multicabezales, falló en el entrenamiento. Se interpreta que esto se debe a que cada cabeza no reflejó adecuadamente las características de diferentes modalidades.</p></li>
<li><p><strong>v8 (multicabezal independiente):</strong> v8, que utiliza atención multicabezal independiente para cada dirección (imagen→texto, texto→imagen) y aplica LayerNorm separado en la entrada y salida, se entrenó con éxito. Esto demuestra que es importante preservar las características de cada modalidad mientras intercambia información.</p></li>
<li><p><strong>v10_1 (Q/K/V por modalidad):</strong> v10_1, basado en v9 y que introduce transformaciones Q/K/V especializadas para cada modalidad, tuvo un entrenamiento inestable. Se espera que esto se deba a la complejidad adicional introducida.</p></li>
<li><p><strong>v10_4 (atención multiconsulta):</strong> v10_4, que mantiene las consultas (Q) independientes pero comparte claves (K) y valores (V) en atención multiconsulta, se entrenó con éxito. Se interpreta que esto reduce el número de parámetros mientras permite un intercambio eficiente de información, mejorando el rendimiento general.</p></li>
<li><p><strong>v10_5 (multicabezal jerárquico):</strong> v10_5, que introduce una estructura jerárquica en tres etapas y aplica atención multicabezal independiente en cada nivel antes de fusionar los resultados con pesos, se entrenó con éxito. Se analiza que esto integra gradualmente las características y aprovecha eficazmente la información de cada nivel para mejorar el rendimiento.</p></li>
<li><p><strong>v10_6 (multicabezal de aprendizaje contrastivo):</strong> v10_6, que añade una capa de proyección separada para el aprendizaje contrastivo y entrena sumando directamente la información de similitud a las características originales, tuvo un entrenamiento inestable. Se sospecha que la información de similitud puede distorsionar las características originales, interfiriendo con el aprendizaje.</p></li>
<li><p><strong>v11 (multiconsulta + fusión jerárquica):</strong> v11, que combina las ventajas de la atención multiconsulta (v10_4) y la fusión jerárquica (v10_5), se entrenó con éxito. Esto significa que logró un aprendizaje estable al aprovechar tanto la eficiencia de parámetros como la integración gradual de características.</p></li>
</ul>
<p><strong>Conclusión</strong></p>
<p>A través de este experimento de eliminación, podemos llegar a las siguientes conclusiones. 1. <strong>Importancia de la normalización:</strong> Aplicar LayerNorm a las características de entrada es muy importante para la estabilidad del entrenamiento (v2). 2. <strong>Dualidad de las conexiones residuales:</strong> Las conexiones residuales son un mecanismo útil, pero en las etapas iniciales del aprendizaje multimodal pueden ser perjudiciales (v3). Mantener excesivamente las características originales puede interferir con el aprendizaje de la interacción entre los dos modos. 3. <strong>Transformación independiente de características:</strong> Aplicar transformaciones lineales (proyecciones) independientes a cada modo puede mejorar el rendimiento (v4). 4. <strong>Atención multi-cabeza:</strong> Al usar atención multi-cabeza, cada cabeza debe configurarse independientemente para reflejar las características de diferentes modos (v7, v8). 5. <strong>Complejidad adecuada:</strong> Aumentar excesivamente la complejidad del modelo puede hacer el entrenamiento inestable (v10_1, v10_2, v10_6). 6. <strong>Mecanismos eficientes:</strong> La atención multi-consulta (v10_4) y la fusión jerárquica (v10_5) proporcionan las ventajas de la eficiencia paramétrica y la integración gradual de características, respectivamente. 7. <strong>Importancia de la combinación óptima:</strong> Como se puede ver en v11, combinar adecuadamente mecanismos efectivos puede conducir a modelos de aprendizaje multimodal más estables y con mejor rendimiento.</p>
<p>Estos experimentos de eliminación son muy útiles para comprender el papel e importancia de cada componente en el aprendizaje multimodal. Además, proporcionan pautas importantes para diseñar nuevos modelos. Analizando sistemáticamente los cambios de rendimiento con la presencia o ausencia de ciertos mecanismos, se puede determinar qué elementos son efectivos para la fusión multimodal y qué combinaciones llevan a resultados óptimos.</p>
<p>Diseñando casos experimentales más sistemáticos y marcos de trabajo para proyectos, también se pueden realizar pruebas fluidas con modelos grandes y diversos mecanismos. Se espera que esto sea útil para la investigación.</p>
</section>
</section>
</section>
<section id="transformer-de-visión-vit" class="level2">
<h2 class="anchored" data-anchor-id="transformer-de-visión-vit">10.5 Transformer de visión (ViT)</h2>
<p>En esta sección, examinaremos brevemente el transformer de visión (Vision Transformer, ViT), y sus extensiones ViT-22B y MAE, que han revolucionado el campo del procesamiento de imágenes.</p>
<section id="cambio-de-paradigma-de-cnn-a-vit" class="level3">
<h3 class="anchored" data-anchor-id="cambio-de-paradigma-de-cnn-a-vit">10.5.1 Cambio de paradigma de CNN a ViT</h3>
<p>En 2020, el equipo de Google Research presentó al mundo el ViT en el paper “An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale”. El ViT marcó el inicio de una nueva era basada en transformers en el procesamiento de imágenes, desplazando la hegemonía que las redes neuronales convolucionales (Convolutional Neural Network, CNN) habían mantenido durante mucho tiempo.</p>
<p>La idea central del ViT es simple: divide la imagen en varios fragmentos pequeños (parches, Patch) y trata cada parche como si fuera una palabra (token) en una oración de texto. De esta manera, la imagen se convierte en una secuencia de parches, que el transformer procesa como entrada.</p>
<p>En comparación con las CNN, el ViT presenta las siguientes diferencias clave:</p>
<ol type="1">
<li><p><strong>Localidad vs.&nbsp;Globalidad:</strong> Las CNN utilizan filtros de convolución para enfocarse en extraer características <em>locales</em> de la imagen. Por otro lado, el ViT puede considerar directamente la relación entre cada parche y todos los demás parches de la imagen entera a través del mecanismo de atención (Attention Mechanism). Esto lo hace más ventajoso para comprender el contexto general de la imagen.</p></li>
<li><p><strong>Estructura jerárquica vs.&nbsp;Estructura plana:</strong> Las CNN tienen una estructura jerárquica que abstrae gradualmente características a través de múltiples capas de convolución y agrupación (pooling). En contraste, el ViT divide la imagen en parches y los transforma todos en vectores de la misma dimensión para procesarlos a una <em>única escala</em>. Esta estructura plana facilita la implementación y optimización del modelo.</p></li>
<li><p><strong>Dependencia de datos:</strong> Las CNN tienden a funcionar bien incluso con cantidades relativamente pequeñas de datos. Sin embargo, el ViT, al ser un modelo basado en transformers, requiere una cantidad suficiente de datos para desplegar su rendimiento óptimo. Un ViT preentrenado con conjuntos de datos a gran escala supera a las CNN en tareas de visión como clasificación de imágenes y detección de objetos.</p></li>
</ol>
<p>La aparición del ViT cambió completamente la dirección de la investigación en procesamiento de imágenes. Desde el ViT, se han desarrollado numerosas investigaciones posteriores basadas en ideas como el embedding de parches de imagen, mecanismos de atención y preentrenamiento a gran escala.</p>
</section>
<section id="principio-del-embedding-de-parches-de-imagen" class="level3">
<h3 class="anchored" data-anchor-id="principio-del-embedding-de-parches-de-imagen">10.5.2 Principio del embedding de parches de imagen</h3>
<p>El embedding de parches de imagen es la primera etapa del ViT, que convierte una imagen bidimensional en una secuencia unidimensional. En PyTorch, la clase <code>torchvision.models.vision_transformer.PatchEmbed</code> se encarga de esta tarea.</p>
<div id="cell-67" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb85"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb85-1"><a href="#cb85-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb85-2"><a href="#cb85-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb85-3"><a href="#cb85-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb85-4"><a href="#cb85-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-5"><a href="#cb85-5" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> PatchEmbed(nn.Module):</span>
<span id="cb85-6"><a href="#cb85-6" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb85-7"><a href="#cb85-7" aria-hidden="true" tabindex="-1"></a><span class="co">    Transforms a 2D image into a sequence of patch embeddings.</span></span>
<span id="cb85-8"><a href="#cb85-8" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb85-9"><a href="#cb85-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-10"><a href="#cb85-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(</span>
<span id="cb85-11"><a href="#cb85-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>,</span>
<span id="cb85-12"><a href="#cb85-12" aria-hidden="true" tabindex="-1"></a>        img_size: <span class="bu">int</span> <span class="op">=</span> <span class="dv">224</span>,</span>
<span id="cb85-13"><a href="#cb85-13" aria-hidden="true" tabindex="-1"></a>        patch_size: <span class="bu">int</span> <span class="op">=</span> <span class="dv">16</span>,</span>
<span id="cb85-14"><a href="#cb85-14" aria-hidden="true" tabindex="-1"></a>        in_chans: <span class="bu">int</span> <span class="op">=</span> <span class="dv">3</span>,</span>
<span id="cb85-15"><a href="#cb85-15" aria-hidden="true" tabindex="-1"></a>        embed_dim: <span class="bu">int</span> <span class="op">=</span> <span class="dv">768</span>,</span>
<span id="cb85-16"><a href="#cb85-16" aria-hidden="true" tabindex="-1"></a>    ) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb85-17"><a href="#cb85-17" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb85-18"><a href="#cb85-18" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb85-19"><a href="#cb85-19" aria-hidden="true" tabindex="-1"></a><span class="co">            img_size: The size of the input image (assuming a square image)</span></span>
<span id="cb85-20"><a href="#cb85-20" aria-hidden="true" tabindex="-1"></a><span class="co">            patch_size: The patch size (assuming square patches)</span></span>
<span id="cb85-21"><a href="#cb85-21" aria-hidden="true" tabindex="-1"></a><span class="co">            in_chans: The number of input image channels (e.g., 3 for RGB images)</span></span>
<span id="cb85-22"><a href="#cb85-22" aria-hidden="true" tabindex="-1"></a><span class="co">            embed_dim: The dimension of the patch embedding vector</span></span>
<span id="cb85-23"><a href="#cb85-23" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb85-24"><a href="#cb85-24" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb85-25"><a href="#cb85-25" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.img_size <span class="op">=</span> img_size</span>
<span id="cb85-26"><a href="#cb85-26" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.patch_size <span class="op">=</span> patch_size</span>
<span id="cb85-27"><a href="#cb85-27" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.num_patches <span class="op">=</span> (img_size <span class="op">//</span> patch_size) <span class="op">*</span> (img_size <span class="op">//</span> patch_size)</span>
<span id="cb85-28"><a href="#cb85-28" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.projection <span class="op">=</span> nn.Conv2d(in_chans, embed_dim, kernel_size<span class="op">=</span>patch_size, stride<span class="op">=</span>patch_size)</span>
<span id="cb85-29"><a href="#cb85-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-30"><a href="#cb85-30" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x: torch.Tensor) <span class="op">-&gt;</span> torch.Tensor:</span>
<span id="cb85-31"><a href="#cb85-31" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb85-32"><a href="#cb85-32" aria-hidden="true" tabindex="-1"></a><span class="co">        Transforms the input image into a sequence of patch embeddings.</span></span>
<span id="cb85-33"><a href="#cb85-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-34"><a href="#cb85-34" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb85-35"><a href="#cb85-35" aria-hidden="true" tabindex="-1"></a><span class="co">            x: Input image (shape: [batch_size, in_chans, img_size, img_size])</span></span>
<span id="cb85-36"><a href="#cb85-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-37"><a href="#cb85-37" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb85-38"><a href="#cb85-38" aria-hidden="true" tabindex="-1"></a><span class="co">            Sequence of patch embeddings (shape: [batch_size, num_patches, embed_dim])</span></span>
<span id="cb85-39"><a href="#cb85-39" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb85-40"><a href="#cb85-40" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.projection(x)  <span class="co"># [batch_size, embed_dim, num_patches_h, num_patches_w]</span></span>
<span id="cb85-41"><a href="#cb85-41" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.flatten(<span class="dv">2</span>)       <span class="co"># [batch_size, embed_dim, num_patches]</span></span>
<span id="cb85-42"><a href="#cb85-42" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.transpose(<span class="dv">1</span>, <span class="dv">2</span>)  <span class="co"># [batch_size, num_patches, embed_dim]</span></span>
<span id="cb85-43"><a href="#cb85-43" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<section id="división-de-parches-de-imagen" class="level4">
<h4 class="anchored" data-anchor-id="división-de-parches-de-imagen">10.5.2.1 División de parches de imagen</h4>
<p>La parte más importante del método <code>__init__</code> de la clase <code>PatchEmbed</code> es <code>self.projection = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)</code>. Esta única línea de código realiza tanto la división de parches de imagen como el embedding.</p>
<ul>
<li><strong><code>nn.Conv2d</code></strong>: Es una capa de PyTorch que realiza la operación de convolución 2D.</li>
<li><strong><code>in_chans</code></strong>: El número de canales de la imagen de entrada (3 para imágenes RGB).</li>
<li><strong><code>embed_dim</code></strong>: La dimensión del vector de embedding de salida (768 para el modelo ViT-Base).</li>
<li><strong><code>kernel_size=patch_size</code></strong>: Se establece el tamaño del filtro (núcleo) de convolución al mismo que el tamaño del parche.</li>
<li><strong><code>stride=patch_size</code></strong>: Se establece el intervalo (stride) con el que el filtro se mueve sobre la imagen al mismo que el tamaño del parche.</li>
</ul>
<p>Al establecer <code>kernel_size</code> y <code>stride</code> al mismo valor que <code>patch_size</code>, el filtro de convolución divide la imagen en parches no superpuestos, como si fuera un tablero de ajedrez. Cada filtro de convolución comprime la información de un parche en un vector de embedding.</p>
</section>
<section id="proyección-lineal" class="level4">
<h4 class="anchored" data-anchor-id="proyección-lineal">10.5.2.2 Proyección lineal</h4>
<p>En el método <code>forward</code> de la clase <code>PatchEmbed</code>, se realiza realmente el embedding de los parches de imagen a través de <code>self.projection(x)</code>.</p>
<ol type="1">
<li><p><strong><code>self.projection(x)</code></strong>: Se aplica la operación <code>Conv2d</code> a la imagen de entrada <code>x</code> (<code>[batch_size, in_chans, img_size, img_size]</code>). La salida tiene forma <code>[batch_size, embed_dim, num_patches_h, num_patches_w]</code>. (<code>num_patches_h</code> y <code>num_patches_w</code> son los valores obtenidos al dividir la altura y el ancho de la imagen por el tamaño del parche).</p></li>
<li><p><strong><code>x.flatten(2)</code></strong>: Se aplana (flatten) la salida de <code>Conv2d</code> a una forma <code>[batch_size, embed_dim, num_patches]</code>. <code>num_patches</code> es el número total de parches (<code>num_patches_h * num_patches_w</code>).</p></li>
<li><p><strong><code>x.transpose(1, 2)</code></strong>: Se cambia la dimensión del tensor a <code>[batch_size, num_patches, embed_dim]</code>, que es la forma requerida para la entrada del codificador transformer, donde cada vector de embedding de parche se trata como un elemento de la secuencia.</p></li>
</ol>
<p>En resumen, la clase <code>PatchEmbed</code> divide la imagen en parches y realiza una transformación lineal (Linear Projection) para convertir cada parche en un vector de <code>embed_dim</code> dimensiones, creando así una secuencia que puede ser utilizada como entrada del codificador transformer.</p>
</section>
</section>
<section id="mecanismo-de-codificación-posicional" class="level3">
<h3 class="anchored" data-anchor-id="mecanismo-de-codificación-posicional">10.5.3 Mecanismo de codificación posicional</h3>
<p>ViT divide la imagen en parches y trata cada parche como si fuera una palabra de texto para su entrada al transformer. Sin embargo, el transformer no reconoce por sí mismo la información del orden de la secuencia de entrada. Por lo tanto, es necesario informarle al modelo a qué <em>posición</em> de la imagen corresponde cada parche. Esto se realiza mediante <strong>la codificación posicional (Positional Encoding)</strong>.</p>
<p>En la clase <code>VisionTransformer</code> de PyTorch, se utilizan embeddings posicionales <em>aprendibles</em> (learnable). Es decir, durante el proceso de entrenamiento, se optimizan los vectores de embedding únicos que corresponden a la posición de cada parche.</p>
<div id="cell-69" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb86"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb86-1"><a href="#cb86-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> VisionTransformer(nn.Module):</span>
<span id="cb86-2"><a href="#cb86-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, ..., num_patches, embed_dim, ...):</span>
<span id="cb86-3"><a href="#cb86-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb86-4"><a href="#cb86-4" aria-hidden="true" tabindex="-1"></a>        <span class="co"># ...</span></span>
<span id="cb86-5"><a href="#cb86-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.cls_token <span class="op">=</span> nn.Parameter(torch.zeros(<span class="dv">1</span>, <span class="dv">1</span>, embed_dim))  <span class="co"># Class token</span></span>
<span id="cb86-6"><a href="#cb86-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.pos_embed <span class="op">=</span> nn.Parameter(torch.zeros(<span class="dv">1</span>, num_patches <span class="op">+</span> <span class="dv">1</span>, embed_dim)) <span class="co"># Positional embedding</span></span>
<span id="cb86-7"><a href="#cb86-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.pos_drop <span class="op">=</span> nn.Dropout(p<span class="op">=</span>drop_rate)</span>
<span id="cb86-8"><a href="#cb86-8" aria-hidden="true" tabindex="-1"></a>        <span class="co"># ...</span></span>
<span id="cb86-9"><a href="#cb86-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb86-10"><a href="#cb86-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _pos_embed(<span class="va">self</span>, x):</span>
<span id="cb86-11"><a href="#cb86-11" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torch.cat((<span class="va">self</span>.cls_token.expand(x.shape[<span class="dv">0</span>], <span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>), x), dim<span class="op">=</span><span class="dv">1</span>) <span class="co"># Prepend class token</span></span>
<span id="cb86-12"><a href="#cb86-12" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.pos_embed  <span class="co"># Add positional embedding</span></span>
<span id="cb86-13"><a href="#cb86-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.pos_drop(x)</span>
<span id="cb86-14"><a href="#cb86-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb86-15"><a href="#cb86-15" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb86-16"><a href="#cb86-16" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.patch_embed(x)  <span class="co"># Patch embedding</span></span>
<span id="cb86-17"><a href="#cb86-17" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>._pos_embed(x)  <span class="co"># Add positional embedding</span></span>
<span id="cb86-18"><a href="#cb86-18" aria-hidden="true" tabindex="-1"></a>        <span class="co"># ... (Transformer Encoder etc.) ...</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><strong>Explicación del código</strong></p>
<ol type="1">
<li><strong><code>self.pos_embed</code> (parámetro aprendible):</strong> Se define como un <code>nn.Parameter</code> y se actualiza durante el proceso de entrenamiento. Tiene una dimensión de <code>(1, num_patches + 1, embed_dim)</code>.
<ul>
<li><code>num_patches + 1</code>: Se añade 1 al número de parches para reservar espacio para un <strong>token de clase (class token)</strong> que tiene un rol especial.</li>
<li><code>embed_dim</code>: Tiene la misma dimensión que las embeddings de los parches.</li>
<li>Es decir, cada parche (y el token de clase) recibe una única vector de embedding de posición con dimensión <code>embed_dim</code>.</li>
</ul></li>
<li><strong>Método <code>_pos_embed</code>:</strong>
<ul>
<li><strong>Añadir token de clase:</strong> Se añade <code>self.cls_token</code> al principio de la entrada <code>x</code> (secuencia de embeddings de parches). El <code>cls_token</code> se expande (<code>expand</code>) para replicarse según el tamaño del lote y se aplica uniformemente a todas las imágenes.</li>
<li><strong>Sumar embedding posicional:</strong> Se suma el valor correspondiente de <code>self.pos_embed</code> a las embeddings de los parches (y al token de clase). Según las reglas de broadcasting de PyTorch, cada vector de embedding de posición en <code>self.pos_embed</code> se suma automáticamente a todos los vectores de embedding de parches en la misma posición.</li>
<li><strong>Dropout:</strong> Se aplica dropout para prevenir el overfitting.</li>
</ul></li>
<li><strong>Método <code>forward</code>:</strong> En el método <code>forward</code>, se convierte la imagen en embeddings de parches mediante <code>self.patch_embed(x)</code> y luego se llama a <code>self._pos_embed(x)</code> para añadir los embeddings posicionales.</li>
</ol>
<p><strong>Resumen</strong></p>
<p>ViT utiliza <em>embeddings posicionales aprendibles</em> para cada parche (y token de clase), que se suman a las embeddings de los parches para inyectar información de posición al modelo. Los embeddings posicionales se optimizan junto con otros pesos durante el entrenamiento del modelo, lo que permite representar la información de posición de manera óptima y adaptada a los datos.</p>
</section>
<section id="estructura-y-componentes-principales-de-vit" class="level3">
<h3 class="anchored" data-anchor-id="estructura-y-componentes-principales-de-vit">10.5.4 Estructura y componentes principales de ViT</h3>
<p>ViT (Vision Transformer) es un modelo que procesa imágenes como si fueran texto para realizar tareas de visión, como la clasificación. En PyTorch, se puede utilizar el modelo ViT a través de la clase <code>torchvision.models.VisionTransformer</code>.</p>
<div id="cell-71" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb87"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb87-1"><a href="#cb87-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> VisionTransformer(nn.Module):</span>
<span id="cb87-2"><a href="#cb87-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, ..., embed_dim, depth, num_heads, ...):</span>
<span id="cb87-3"><a href="#cb87-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb87-4"><a href="#cb87-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-5"><a href="#cb87-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.patch_embed <span class="op">=</span> PatchEmbed(...)  <span class="co"># Image patch embedding</span></span>
<span id="cb87-6"><a href="#cb87-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.cls_token <span class="op">=</span> nn.Parameter(...)   <span class="co"># Class token</span></span>
<span id="cb87-7"><a href="#cb87-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.pos_embed <span class="op">=</span> nn.Parameter(...)   <span class="co"># Positional embedding</span></span>
<span id="cb87-8"><a href="#cb87-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.pos_drop <span class="op">=</span> nn.Dropout(...)</span>
<span id="cb87-9"><a href="#cb87-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-10"><a href="#cb87-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.blocks <span class="op">=</span> nn.Sequential(<span class="op">*</span>[</span>
<span id="cb87-11"><a href="#cb87-11" aria-hidden="true" tabindex="-1"></a>            TransformerEncoderLayer(...) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(depth) <span class="co"># Transformer Encoder blocks</span></span>
<span id="cb87-12"><a href="#cb87-12" aria-hidden="true" tabindex="-1"></a>        ])</span>
<span id="cb87-13"><a href="#cb87-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.norm <span class="op">=</span> nn.LayerNorm(embed_dim) <span class="co"># Layer Normalization</span></span>
<span id="cb87-14"><a href="#cb87-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.head <span class="op">=</span> nn.Linear(embed_dim, num_classes) <span class="co"># Classification Head</span></span>
<span id="cb87-15"><a href="#cb87-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-16"><a href="#cb87-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-17"><a href="#cb87-17" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward_features(<span class="va">self</span>, x):</span>
<span id="cb87-18"><a href="#cb87-18" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.patch_embed(x)       <span class="co"># 1. Patch embedding</span></span>
<span id="cb87-19"><a href="#cb87-19" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torch.cat((<span class="va">self</span>.cls_token.expand(x.shape[<span class="dv">0</span>], <span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>), x), dim<span class="op">=</span><span class="dv">1</span>) <span class="co"># 2. Prepend class token</span></span>
<span id="cb87-20"><a href="#cb87-20" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.pos_embed       <span class="co"># 3. Add positional embedding</span></span>
<span id="cb87-21"><a href="#cb87-21" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.pos_drop(x)</span>
<span id="cb87-22"><a href="#cb87-22" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.blocks(x)            <span class="co"># 4. Transformer Encoder</span></span>
<span id="cb87-23"><a href="#cb87-23" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.norm(x)              <span class="co"># 5. LayerNorm</span></span>
<span id="cb87-24"><a href="#cb87-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x[:, <span class="dv">0</span>]                <span class="co"># 6. Return class token</span></span>
<span id="cb87-25"><a href="#cb87-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-26"><a href="#cb87-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-27"><a href="#cb87-27" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb87-28"><a href="#cb87-28" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.forward_features(x)   <span class="co"># Feature extraction</span></span>
<span id="cb87-29"><a href="#cb87-29" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.head(x)               <span class="co"># Classification</span></span>
<span id="cb87-30"><a href="#cb87-30" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><strong>Componentes clave de ViT:</strong></p>
<ol type="1">
<li><strong><code>PatchEmbed</code> (incrustación de parche):</strong> Divide la imagen en varios parches pequeños y convierte cada parche en un vector (incrustación) de dimensiones fijas. (Ver sección 10.5.2)</li>
<li><strong><code>cls_token</code> (token de clase):</strong> Es un parámetro aprendible que se agrega al principio de la secuencia de incrustaciones de parche. Después de pasar por el codificador Transformer, este token de clase contiene información (características) representativa de toda la imagen y se utiliza para la clasificación final.</li>
<li><strong><code>pos_embed</code> (incrustación posicional):</strong> Es un parámetro aprendible que indica la posición de cada parche (y del token de clase). Dado que el Transformer no puede reconocer por sí mismo el orden de la secuencia de entrada, se debe proporcionar información de posición explícitamente a través de las incrustaciones posicionales. (Ver sección 10.5.3)</li>
<li><strong><code>blocks</code> (codificador Transformer):</strong> Está compuesto por varias capas <code>TransformerEncoderLayer</code>.
<ul>
<li><strong><code>TransformerEncoderLayer</code>:</strong> Es el bloque principal de ViT, que consta de Multi-Head Self-Attention y Feed-Forward Network (FFN).
<ul>
<li><strong>Multi-Head Self-Attention:</strong> Considera la relación entre cada parche y todos los demás parches (incluido a sí mismo) para comprender el contexto global de la imagen.</li>
<li><strong>FFN:</strong> Procesa individualmente cada incrustación de parche para agregar no linealidad y permitir el aprendizaje de características más complejas.</li>
<li><strong>(Pre-LN, Conexiones residuales, Dropout, etc.):</strong> Se aplican varias técnicas, como las vistas en los capítulos 9 y principios del 10, para asegurar un entrenamiento estable y mejorar el rendimiento.</li>
</ul></li>
</ul></li>
<li><strong><code>norm</code> (Normalización de Capa) :</strong> Se aplica normalización de capa a la salida del codificador Transformer</li>
<li><strong><code>head</code> (Cabeza de Clasificación):</strong> Es una capa fully-connected que toma como entrada el token de clase después de pasar por el codificador Transformer, y finalmente predice la clase de la imagen.</li>
</ol>
<p><strong>Método <code>forward</code> (flujo general del procesamiento):</strong></p>
<ol type="1">
<li><code>forward_features</code> método:
<ul>
<li><code>self.patch_embed(x)</code>: Convierte la imagen de entrada en una secuencia de incrustaciones de parche.</li>
<li>Agrega el token de clase (<code>self.cls_token</code>) al principio de la secuencia de incrustaciones de parche.</li>
<li>Suma la incrustación posicional (<code>self.pos_embed</code>).</li>
<li>Pasa a través del codificador Transformer (<code>self.blocks</code>).</li>
<li>Aplica normalización de capa (<code>self.norm</code>).</li>
<li>Devuelve solo la parte correspondiente al token de clase (<code>x[:, 0]</code>).</li>
</ul></li>
<li><code>self.head(x)</code>: Pasa el token de clase devuelto por <code>forward_features</code> a través de la cabeza de clasificación para obtener el resultado final de predicción (clasificación).</li>
</ol>
<p><strong>Resumen:</strong></p>
<p>ViT divide la imagen en parches y los alimenta al codificador Transformer para extraer las características globales de toda la imagen. Durante este proceso, utiliza tokens de clase e incrustaciones posicionales para considerar tanto la información global de la imagen como la posición de los parches. Finalmente, utiliza el token de clase para clasificar la imagen.</p>
</section>
<section id="ejemplo-de-entrenamiento-de-vit" class="level3">
<h3 class="anchored" data-anchor-id="ejemplo-de-entrenamiento-de-vit">10.5.5 Ejemplo de Entrenamiento de ViT</h3>
<p>Vamos a ver un ejemplo simple de cómo entrenar ViT con el conjunto de datos CIFAR-10. El código siguiente utiliza PyTorch para entrenar el modelo ViT y muestra la pérdida (loss) y precisión (accuracy) por época.</p>
<div id="cell-73" class="cell" data-execution_count="1">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb88"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb88-1"><a href="#cb88-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb88-2"><a href="#cb88-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb88-3"><a href="#cb88-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.optim <span class="im">as</span> optim</span>
<span id="cb88-4"><a href="#cb88-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision <span class="im">import</span> datasets, transforms</span>
<span id="cb88-5"><a href="#cb88-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision.models <span class="im">import</span> vit_b_16  <span class="co"># Using vit_b_16 model as an example</span></span>
<span id="cb88-6"><a href="#cb88-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> DataLoader</span>
<span id="cb88-7"><a href="#cb88-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-8"><a href="#cb88-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Hyperparameter setup for a simple training run</span></span>
<span id="cb88-9"><a href="#cb88-9" aria-hidden="true" tabindex="-1"></a>num_epochs <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb88-10"><a href="#cb88-10" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb88-11"><a href="#cb88-11" aria-hidden="true" tabindex="-1"></a>learning_rate <span class="op">=</span> <span class="fl">1e-4</span></span>
<span id="cb88-12"><a href="#cb88-12" aria-hidden="true" tabindex="-1"></a>image_size <span class="op">=</span> <span class="dv">224</span>  <span class="co"># ViT input image size</span></span>
<span id="cb88-13"><a href="#cb88-13" aria-hidden="true" tabindex="-1"></a>num_classes <span class="op">=</span> <span class="dv">10</span>   <span class="co"># Number of classes in the CIFAR-10 dataset</span></span>
<span id="cb88-14"><a href="#cb88-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-15"><a href="#cb88-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Use GPU if available</span></span>
<span id="cb88-16"><a href="#cb88-16" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> torch.device(<span class="st">"cuda"</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">"cpu"</span>)</span>
<span id="cb88-17"><a href="#cb88-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-18"><a href="#cb88-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Data loading and preprocessing (using CIFAR-10 dataset)</span></span>
<span id="cb88-19"><a href="#cb88-19" aria-hidden="true" tabindex="-1"></a>transform <span class="op">=</span> transforms.Compose([</span>
<span id="cb88-20"><a href="#cb88-20" aria-hidden="true" tabindex="-1"></a>    transforms.Resize((image_size, image_size)),</span>
<span id="cb88-21"><a href="#cb88-21" aria-hidden="true" tabindex="-1"></a>    transforms.ToTensor(),</span>
<span id="cb88-22"><a href="#cb88-22" aria-hidden="true" tabindex="-1"></a>    transforms.Normalize((<span class="fl">0.5</span>, <span class="fl">0.5</span>, <span class="fl">0.5</span>), (<span class="fl">0.5</span>, <span class="fl">0.5</span>, <span class="fl">0.5</span>)), <span class="co"># Normalize with CIFAR-10 statistics</span></span>
<span id="cb88-23"><a href="#cb88-23" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb88-24"><a href="#cb88-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-25"><a href="#cb88-25" aria-hidden="true" tabindex="-1"></a>train_dataset <span class="op">=</span> datasets.CIFAR10(root<span class="op">=</span><span class="st">'./data'</span>, train<span class="op">=</span><span class="va">True</span>, download<span class="op">=</span><span class="va">True</span>, transform<span class="op">=</span>transform)</span>
<span id="cb88-26"><a href="#cb88-26" aria-hidden="true" tabindex="-1"></a>train_loader <span class="op">=</span> DataLoader(train_dataset, batch_size<span class="op">=</span>batch_size, shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb88-27"><a href="#cb88-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-28"><a href="#cb88-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Create ViT model (not using pretrained weights)</span></span>
<span id="cb88-29"><a href="#cb88-29" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> vit_b_16(pretrained<span class="op">=</span><span class="va">False</span>, num_classes<span class="op">=</span>num_classes).to(device)</span>
<span id="cb88-30"><a href="#cb88-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-31"><a href="#cb88-31" aria-hidden="true" tabindex="-1"></a><span class="co"># Define loss function and optimizer</span></span>
<span id="cb88-32"><a href="#cb88-32" aria-hidden="true" tabindex="-1"></a>criterion <span class="op">=</span> nn.CrossEntropyLoss()</span>
<span id="cb88-33"><a href="#cb88-33" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> optim.AdamW(model.parameters(), lr<span class="op">=</span>learning_rate)</span>
<span id="cb88-34"><a href="#cb88-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-35"><a href="#cb88-35" aria-hidden="true" tabindex="-1"></a><span class="co"># Training loop</span></span>
<span id="cb88-36"><a href="#cb88-36" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(num_epochs):</span>
<span id="cb88-37"><a href="#cb88-37" aria-hidden="true" tabindex="-1"></a>    model.train()  <span class="co"># Set the model to training mode</span></span>
<span id="cb88-38"><a href="#cb88-38" aria-hidden="true" tabindex="-1"></a>    running_loss <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb88-39"><a href="#cb88-39" aria-hidden="true" tabindex="-1"></a>    correct_predictions <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb88-40"><a href="#cb88-40" aria-hidden="true" tabindex="-1"></a>    total_samples <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb88-41"><a href="#cb88-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-42"><a href="#cb88-42" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, (images, labels) <span class="kw">in</span> <span class="bu">enumerate</span>(train_loader):</span>
<span id="cb88-43"><a href="#cb88-43" aria-hidden="true" tabindex="-1"></a>        images <span class="op">=</span> images.to(device)</span>
<span id="cb88-44"><a href="#cb88-44" aria-hidden="true" tabindex="-1"></a>        labels <span class="op">=</span> labels.to(device)</span>
<span id="cb88-45"><a href="#cb88-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-46"><a href="#cb88-46" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Forward and backward passes</span></span>
<span id="cb88-47"><a href="#cb88-47" aria-hidden="true" tabindex="-1"></a>        outputs <span class="op">=</span> model(images)</span>
<span id="cb88-48"><a href="#cb88-48" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> criterion(outputs, labels)</span>
<span id="cb88-49"><a href="#cb88-49" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()</span>
<span id="cb88-50"><a href="#cb88-50" aria-hidden="true" tabindex="-1"></a>        loss.backward()</span>
<span id="cb88-51"><a href="#cb88-51" aria-hidden="true" tabindex="-1"></a>        optimizer.step()</span>
<span id="cb88-52"><a href="#cb88-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-53"><a href="#cb88-53" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculate statistics</span></span>
<span id="cb88-54"><a href="#cb88-54" aria-hidden="true" tabindex="-1"></a>        running_loss <span class="op">+=</span> loss.item()</span>
<span id="cb88-55"><a href="#cb88-55" aria-hidden="true" tabindex="-1"></a>        _, predicted <span class="op">=</span> torch.<span class="bu">max</span>(outputs, <span class="dv">1</span>)  <span class="co"># Select the class with the highest probability</span></span>
<span id="cb88-56"><a href="#cb88-56" aria-hidden="true" tabindex="-1"></a>        total_samples <span class="op">+=</span> labels.size(<span class="dv">0</span>)</span>
<span id="cb88-57"><a href="#cb88-57" aria-hidden="true" tabindex="-1"></a>        correct_predictions <span class="op">+=</span> (predicted <span class="op">==</span> labels).<span class="bu">sum</span>().item()</span>
<span id="cb88-58"><a href="#cb88-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-59"><a href="#cb88-59" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Print every 100 batches.</span></span>
<span id="cb88-60"><a href="#cb88-60" aria-hidden="true" tabindex="-1"></a>        <span class="co"># if (i + 1) % 100 == 0:</span></span>
<span id="cb88-61"><a href="#cb88-61" aria-hidden="true" tabindex="-1"></a>        <span class="co">#     print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')</span></span>
<span id="cb88-62"><a href="#cb88-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-63"><a href="#cb88-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-64"><a href="#cb88-64" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Print epoch statistics</span></span>
<span id="cb88-65"><a href="#cb88-65" aria-hidden="true" tabindex="-1"></a>    epoch_loss <span class="op">=</span> running_loss <span class="op">/</span> <span class="bu">len</span>(train_loader)</span>
<span id="cb88-66"><a href="#cb88-66" aria-hidden="true" tabindex="-1"></a>    epoch_accuracy <span class="op">=</span> correct_predictions <span class="op">/</span> total_samples <span class="op">*</span> <span class="dv">100</span></span>
<span id="cb88-67"><a href="#cb88-67" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'Epoch [</span><span class="sc">{</span>epoch<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">/</span><span class="sc">{</span>num_epochs<span class="sc">}</span><span class="ss">], Loss: </span><span class="sc">{</span>epoch_loss<span class="sc">:.4f}</span><span class="ss">, Accuracy: </span><span class="sc">{</span>epoch_accuracy<span class="sc">:.2f}</span><span class="ss">%'</span>)</span>
<span id="cb88-68"><a href="#cb88-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-69"><a href="#cb88-69" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Training finished!'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stderr">
<pre><code>100%|██████████| 170M/170M [00:21&lt;00:00, 8.09MB/s] 
/home/sean/anaconda3/envs/DL/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/sean/anaconda3/envs/DL/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch [1/5], Step [100/1563], Loss: 2.1349
Epoch [1/5], Step [200/1563], Loss: 1.8978
Epoch [1/5], Step [300/1563], Loss: 1.9483
Epoch [1/5], Step [400/1563], Loss: 2.0783
Epoch [1/5], Step [500/1563], Loss: 1.7614
Epoch [1/5], Step [600/1563], Loss: 1.8051
Epoch [1/5], Step [700/1563], Loss: 1.7448
Epoch [1/5], Step [800/1563], Loss: 1.8347
Epoch [1/5], Step [900/1563], Loss: 1.8127
Epoch [1/5], Step [1000/1563], Loss: 1.7755
Epoch [1/5], Step [1100/1563], Loss: 1.6506
Epoch [1/5], Step [1200/1563], Loss: 1.7523
Epoch [1/5], Step [1300/1563], Loss: 1.5987
Epoch [1/5], Step [1400/1563], Loss: 1.6078
Epoch [1/5], Step [1500/1563], Loss: 1.7110
Epoch [1/5], Loss: 1.8429, Accuracy: 29.66%
Epoch [2/5], Step [100/1563], Loss: 1.4902
Epoch [2/5], Step [200/1563], Loss: 1.5161
Epoch [2/5], Step [300/1563], Loss: 1.4563
Epoch [2/5], Step [400/1563], Loss: 1.5858
Epoch [2/5], Step [500/1563], Loss: 1.6702
Epoch [2/5], Step [600/1563], Loss: 1.5833
Epoch [2/5], Step [700/1563], Loss: 1.4790
Epoch [2/5], Step [800/1563], Loss: 1.6507
Epoch [2/5], Step [900/1563], Loss: 1.6017
Epoch [2/5], Step [1000/1563], Loss: 1.5102
Epoch [2/5], Step [1100/1563], Loss: 1.2946
Epoch [2/5], Step [1200/1563], Loss: 1.3225
Epoch [2/5], Step [1300/1563], Loss: 1.9922
Epoch [2/5], Step [1400/1563], Loss: 1.3685
Epoch [2/5], Step [1500/1563], Loss: 1.4852
Epoch [2/5], Loss: 1.5410, Accuracy: 42.69%
Epoch [3/5], Step [100/1563], Loss: 1.2692
Epoch [3/5], Step [200/1563], Loss: 1.1648
Epoch [3/5], Step [300/1563], Loss: 1.2412
Epoch [3/5], Step [400/1563], Loss: 1.6217
Epoch [3/5], Step [500/1563], Loss: 1.3776
Epoch [3/5], Step [600/1563], Loss: 1.2591
Epoch [3/5], Step [700/1563], Loss: 1.4333
Epoch [3/5], Step [800/1563], Loss: 1.3301
Epoch [3/5], Step [900/1563], Loss: 1.3536
Epoch [3/5], Step [1000/1563], Loss: 1.4488
Epoch [3/5], Step [1100/1563], Loss: 1.3179
Epoch [3/5], Step [1200/1563], Loss: 1.0684
Epoch [3/5], Step [1300/1563], Loss: 1.6526
Epoch [3/5], Step [1400/1563], Loss: 1.1815
Epoch [3/5], Step [1500/1563], Loss: 1.3683
Epoch [3/5], Loss: 1.3836, Accuracy: 49.23%
Epoch [4/5], Step [100/1563], Loss: 1.2601
Epoch [4/5], Step [200/1563], Loss: 1.3277
Epoch [4/5], Step [300/1563], Loss: 1.1337
Epoch [4/5], Step [400/1563], Loss: 1.2273
Epoch [4/5], Step [500/1563], Loss: 1.7351
Epoch [4/5], Step [600/1563], Loss: 1.3826
Epoch [4/5], Step [700/1563], Loss: 1.2639
Epoch [4/5], Step [800/1563], Loss: 1.5757
Epoch [4/5], Step [900/1563], Loss: 1.0702
Epoch [4/5], Step [1000/1563], Loss: 1.3986
Epoch [4/5], Step [1100/1563], Loss: 1.1105
Epoch [4/5], Step [1200/1563], Loss: 1.2621
Epoch [4/5], Step [1300/1563], Loss: 1.4261
Epoch [4/5], Step [1400/1563], Loss: 1.3028
Epoch [4/5], Step [1500/1563], Loss: 1.9051
Epoch [4/5], Loss: 1.2850, Accuracy: 52.98%
Epoch [5/5], Step [100/1563], Loss: 0.9517
Epoch [5/5], Step [200/1563], Loss: 0.9844
Epoch [5/5], Step [300/1563], Loss: 1.2391
Epoch [5/5], Step [400/1563], Loss: 1.3588
Epoch [5/5], Step [500/1563], Loss: 0.9441
Epoch [5/5], Step [600/1563], Loss: 1.1711
Epoch [5/5], Step [700/1563], Loss: 1.1687
Epoch [5/5], Step [800/1563], Loss: 1.0097
Epoch [5/5], Step [900/1563], Loss: 0.9899
Epoch [5/5], Step [1000/1563], Loss: 1.3289
Epoch [5/5], Step [1100/1563], Loss: 1.5510
Epoch [5/5], Step [1200/1563], Loss: 0.9139
Epoch [5/5], Step [1300/1563], Loss: 0.9221
Epoch [5/5], Step [1400/1563], Loss: 1.3378
Epoch [5/5], Step [1500/1563], Loss: 1.1785
Epoch [5/5], Loss: 1.2116, Accuracy: 55.78%
Training finished!</code></pre>
</div>
</div>
<p>Este código es un ejemplo sencillo que muestra cómo funciona el modelo ViT. El ViT real, después de ser pre-entrenado (pre-training) en conjuntos de datos a gran escala como ImageNet, se utiliza ajustándolo finamente (fine-tuning) para tareas específicas (por ejemplo, clasificación CIFAR-10), lo que resulta en un rendimiento significativamente mejor. Aquí solo verificamos si es posible realizar el entrenamiento.</p>
<p><strong>Significado e Impacto del ViT</strong></p>
<p>El ViT ha causado una gran resonancia en el campo de la visión por computadora al demostrar un rendimiento superior a las CNN en tareas de clasificación de imágenes. En particular, cuando se pre-entrena con conjuntos de datos a gran escala como JFT-300M, que contiene más de 300 millones de <strong>imágenes</strong>, muestra su verdadero valor. Esto ha arrojado dos importantes implicaciones:</p>
<ol type="1">
<li><p><strong>Escalabilidad:</strong> El ViT ha demostrado una excelente escalabilidad al mejorar <em>constantemente</em> su rendimiento a medida que el tamaño del conjunto de datos aumenta. Esto contrasta con la tendencia de los modelos basados en CNN, donde el rendimiento puede estancarse o incluso empeorar cuando se utilizan conjuntos de datos más grandes. Esta característica del ViT abre la posibilidad de construir modelos de visión aún más potentes al aprovechar mayores cantidades de datos.</p></li>
<li><p><strong>Versatilidad del Transformer:</strong> El ViT ha demostrado que la arquitectura del transformer, ampliamente utilizada en el procesamiento del lenguaje natural (NLP), también puede ser efectiva para el procesamiento de imágenes. Esto ha servido como un catalizador para la investigación de <em>modelos multimodales</em> capaces de manejar diferentes modalidades (<em>una arquitectura</em>) como texto, imágenes y audio.</p></li>
</ol>
<p>El éxito del ViT se convirtió en una base importante para el desarrollo posterior de modelos multimodales como CLIP (Contrastive Language-Image Pre-training). CLIP combina el codificador de imágenes de ViT con un codificador de texto basado en transformer para aprender a representar imágenes y texto en <em>un espacio integrado</em>. Esto ha permitido diversas aplicaciones, como la generación de descripciones textuales de imágenes o la búsqueda de imágenes basada en descripciones textuales.</p>
<p>Traducción:</p>
<p>El texto original no se proporcionó para realizar la traducción. Por favor, proporcione el texto en coreano que desea traducir al español.</p>
</section>
<section id="vit-22b-extremos-de-escala" class="level3">
<h3 class="anchored" data-anchor-id="vit-22b-extremos-de-escala">10.5.6 ViT-22B: Extremos de Escala</h3>
<p><strong>Google Research</strong> propuso y entrenó el ViT-22B, que ha causado un gran impacto en el campo de la visión por computadora al superar el rendimiento de las CNN en la clasificación de imágenes. El ViT-22B demostró que la expansión del tamaño del modelo y los datos es uno de los factores clave para mejorar el rendimiento. Con un tamaño abrumador de 22 mil millones de parámetros y entrenado con un conjunto de datos a gran escala compuesto por miles de millones de imágenes, el ViT-22B logró niveles de rendimiento que antes eran difíciles de imaginar, abriendo nuevas perspectivas en la IA de visión.</p>
<p><strong>Origen: Reglas de Escalabilidad y Éxito de los Modelos de Lenguaje Grandes</strong></p>
<p>La aparición del ViT-22B está estrechamente relacionada con el éxito asombroso que han demostrado los modelos de lenguaje grandes (LLM) en el campo del procesamiento de lenguaje natural (NLP). Se ha mostrado que LLM como GPT-3 siguen la <em>regla de escalabilidad (scaling law)</em>, según la cual el rendimiento mejora consistentemente a medida que se aumenta el tamaño del modelo (número de parámetros) y la cantidad de datos. Esta tendencia propagó la creencia de que “lo más grande es lo mejor” y llevó a intentos similares en el campo de la visión.</p>
<p>Dado que ViT está basado en una arquitectura de transformador, fue fácil aplicar las estrategias de escalabilidad probadas en los LLM. Debido a su capacidad para tratar parches de imágenes como tokens de texto, se pudo aumentar el número de parámetros y utilizar más datos para entrenar sin realizar cambios sustanciales en la estructura del modelo.</p>
<p><strong>Estructura y Características del ViT-22B</strong></p>
<p>El ViT-22B sigue básicamente la arquitectura de ViT, pero se distingue por su <em>escala</em>.</p>
<ul>
<li><p><strong>Modelo de gran tamaño:</strong> Con 22 mil millones de parámetros, el ViT-22B es significativamente más grande que el ViT-Base (86 millones), ViT-Large (307 millones) y ViT-Huge (632 millones). Esto significa que el modelo puede capturar características de imagen más complejas y sutiles e incorporar un mayor conocimiento.</p></li>
<li><p><strong>Conjunto de datos a gran escala:</strong> El ViT-22B se entrenó con un conjunto de datos <em>privado</em> compuesto por más de mil millones de imágenes (como JFT-4B). Estos grandes conjuntos de datos son esenciales para maximizar el rendimiento general del modelo y para aprender una variedad de distribuciones de imágenes.</p></li>
<li><p><strong>Rendimiento mejorado:</strong> El ViT-22B registró un rendimiento sobresaliente (SOTA, State-Of-The-Art) en comparación con otros modelos en tareas de clasificación de imágenes. Esto demuestra claramente el impacto positivo del tamaño del modelo y la cantidad de datos en el rendimiento.</p></li>
</ul>
<p><strong>Dificultades y Implicaciones del Entrenamiento del ViT-22B</strong></p>
<p>Entrenar un modelo a gran escala como el ViT-22B es casi imposible en entornos de investigación comunes. Se requieren cientos o miles de GPU o TPU costosas, y el tiempo de entrenamiento puede durar desde semanas hasta meses. Además, la infraestructura para almacenar y procesar cantidades enormes de datos también representa un desafío significativo.</p>
<p>Aunque la aparición del ViT-22B demostró la escalabilidad de la arquitectura de ViT, también plantea preocupaciones sobre la <em>eficiencia</em>. A medida que el tamaño del modelo aumenta, el rendimiento mejora, pero los recursos computacionales y el consumo de energía necesarios para entrenamiento e inferencia crecen exponencialmente. Por lo tanto, se espera que futuras investigaciones se dirijan hacia mejorar la eficiencia mientras se mantiene el rendimiento del modelo.</p>
</section>
<section id="mae-v3-aprendizaje-autónomo" class="level3">
<h3 class="anchored" data-anchor-id="mae-v3-aprendizaje-autónomo">10.5.7 MAE v3: Aprendizaje Autónomo</h3>
<p><strong>Meta AI (Facebook AI Research, FAIR)</strong> equipo propuso MAE (Masked Autoencoder), un método de <em>aprendizaje autónomo (self-supervised learning)</em> que utiliza conjuntos de datos de imágenes a gran escala sin etiquetas para aprender representaciones de imagen potentes. Basado en ViT, el MAE oculta una parte considerable de la imagen de manera aleatoria (masking) y entrena al modelo para reconstruir las partes ocultas. El MAE v3 es la versión más reciente del MAE, que ha mejorado aún más su rendimiento y eficiencia con varias mejoras.</p>
<p><strong>Funcionamiento de MAE</strong></p>
<p>La idea clave detrás de MAE es entrenar a un modelo para <em>comprender</em> e <em>iniciar</em> una imagen completa utilizando solo <em>parte de la información</em>, similar a cómo las personas resuelven problemas de “llenado de espacios en blanco”.</p>
<ol type="1">
<li><p><strong>Ocultación aleatoria de imágenes de entrada:</strong> Se oculta una parte sustancial (por ejemplo, el 75%) de la imagen de entrada de manera aleatoria. Esta ocultación se realiza a nivel de parches de imagen.</p></li>
<li><p><strong>Codificación (Encoding):</strong> Solo los parches visibles se introducen en el codificador ViT para extraer vectores de características.</p></li>
<li><p><strong>Decodificación (Decoding):</strong> Se utiliza la salida del codificador (características de los parches visibles) y la información sobre los parches <em>ocultos</em> para reconstruir la imagen original. En este proceso, el decodificador se compone de bloques Transformer ligeros para aumentar la eficiencia computacional.</p></li>
<li><p><strong>Pérdida de reconstrucción (Reconstruction Loss):</strong> Se calcula la diferencia a nivel de píxeles entre la imagen reconstruida y la imagen original (por ejemplo, Error Cuadrático Medio, MSE) y se entrena al modelo (codificador y decodificador) para minimizar esta diferencia.</p></li>
</ol>
<p><strong>Mejoras estructurales en MAE v3</strong></p>
<p>MAE v3 ha logrado un mejor rendimiento y eficiencia en comparación con las versiones anteriores gracias a las siguientes mejoras importantes:</p>
<ol type="1">
<li><p><strong>Estrategia de ocultación mejorada:</strong> A diferencia del MAE inicial, que simplemente ocultaba parches de manera aleatoria, el MAE v3 utiliza estrategias más sofisticadas de ocultación. Por ejemplo, puede preservar mejor las <em>áreas significativas</em> de la imagen (como los bordes de objetos) o aplicar la ocultación a parches de <em>tamaños diversos</em>.</p></li>
<li><p><strong>Estructura optimizada del codificador-decodificador</strong></p>
<ul>
<li>Codificador: Se utilizan modelos ViT más grandes, como ViT-Large y ViT-Huge, para extraer características más ricas a partir de los parches visibles.</li>
<li>Decodificador: Se emplean bloques Transformer poco profundos y ligeros para mejorar el rendimiento de reconstrucción mientras se mantiene la eficiencia computacional.</li>
</ul></li>
<li><p><strong>Escalabilidad:</strong> La escala del modelo ha sido ampliada desde ViT-L hasta ViT-H, permitiendo un mayor uso de datos y mejora en el rendimiento.</p></li>
</ol>
<p><strong>Ventajas e importancia de MAE</strong></p>
<p>MAE es destacado en el campo de aprendizaje autónomo por las siguientes ventajas:</p>
<ol type="1">
<li><p><strong>Aprendizaje sin etiquetas necesarias:</strong> MAE puede realizar el pre-entrenamiento utilizando <em>conjuntos de datos de imágenes a gran escala</em> sin etiquetas, lo que reduce los costos y tiempo asociados con la creación manual de etiquetas y permite el uso de más datos.</p></li>
<li><p><strong>Aprendizaje de representaciones potentes:</strong> A través del proceso de reconstrucción de una imagen con partes ocultas, MAE desarrolla habilidades para comprender la <em>estructura</em>, <em>significado</em> y <em>contexto</em> de las imágenes. Estas habilidades son útiles en diversas tareas downstream como clasificación de imágenes, detección de objetos y segmentación.</p></li>
<li><p><strong>Facilidad de transferencia (Transfer Learning):</strong> Los modelos pre-entrenados con MAE pueden ser finetuneados para varias tareas, lo que permite obtener buenos resultados incluso en tareas con escasez de etiquetas.</p></li>
</ol>
<p><strong>Conclusión</strong> MAE presentó un método efectivo para aprender representaciones de imágenes poderosas sin etiquetas a través de la idea intuitiva de “llenar los espacios en blanco”. MAE v3 mejora aún más estas ventajas de MAE, logrando un rendimiento y eficiencia superiores y liderando el avance de la investigación de aprendizaje automático.</p>
</section>
</section>
<section id="clip-hitos-en-el-aprendizaje-multimodal" class="level2">
<h2 class="anchored" data-anchor-id="clip-hitos-en-el-aprendizaje-multimodal">10.6 CLIP: Hitos en el aprendizaje multimodal</h2>
<p>En 2021, OpenAI presentó el modelo <strong>CLIP (Contrastive Language-Image Pre-training)</strong> en el artículo “Learning Transferable Visual Models From Natural Language Supervision”. CLIP ha traído un desarrollo revolucionario al campo del aprendizaje multimodal al aprender a representar dos modalidades diferentes, imágenes y texto, en <em>un espacio compartido</em>.</p>
<section id="estructura-básica-de-clip-codificador-dual-dual-encoder" class="level3">
<h3 class="anchored" data-anchor-id="estructura-básica-de-clip-codificador-dual-dual-encoder">10.6.1 Estructura básica de CLIP: Codificador dual (Dual Encoder)</h3>
<p>El núcleo de CLIP es la estructura de <em>codificador dual</em>, compuesta por <strong>codificador de imagen (Image Encoder)</strong> y <strong>codificador de texto (Text Encoder)</strong>, dos codificadores independientes.</p>
<ul>
<li><strong>Codificador de imagen:</strong> Convierte las imágenes de entrada en vectores de dimensión fija (incrustaciones de imagen).</li>
<li><strong>Codificador de texto:</strong> Convierte el texto de entrada (descripción de la imagen) en vectores de <em>la misma dimensión</em> que los del codificador de imagen (incrustaciones de texto).</li>
</ul>
<p>Ambos codificadores se entrenan a través de <em>aprendizaje contrastivo (contrastive learning)</em>.</p>
<p><strong>El núcleo del entrenamiento de CLIP: Aprendizaje contrastivo (Contrastive Learning)</strong></p>
<p>El núcleo del entrenamiento de CLIP es el <em>aprendizaje contrastivo</em> utilizando un <em>conjunto de datos a gran escala de pares imagen-texto</em>.</p>
<ol type="1">
<li><strong>Datos:</strong> Se utiliza un conjunto de datos compuesto por miles de millones de pares (imagen, texto) recopilados de Internet. En cada par, el texto es una descripción de la imagen correspondiente.</li>
<li><strong>Objetivo:</strong> Entrenar los codificadores para que las incrustaciones de imagen y texto que pertenecen al <em>mismo par</em> estén <em>cercanas entre sí</em>, mientras que las incrustaciones de imagen y texto que pertenecen a <em>pares diferentes</em> estén <em>lejanas una de la otra</em>.</li>
<li><strong>Función de pérdida:</strong> Se utiliza la función de pérdida contrastiva (contrastive loss). Esta función de pérdida funciona para <em>aumentar</em> la similitud entre las incrustaciones del <em>mismo par</em> (por ejemplo, la similitud coseno) y <em>reducir</em> la similitud entre las incrustaciones de <em>pares diferentes</em>. (Ver sección 10.4 sobre aprendizaje contrastivo)</li>
</ol>
<p><strong>Ejemplo de código</strong></p>
<div id="cell-78" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb91"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb91-1"><a href="#cb91-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb91-2"><a href="#cb91-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb91-3"><a href="#cb91-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb91-4"><a href="#cb91-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb91-5"><a href="#cb91-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-6"><a href="#cb91-6" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> CLIP(nn.Module):</span>
<span id="cb91-7"><a href="#cb91-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, image_encoder, text_encoder, embed_dim):</span>
<span id="cb91-8"><a href="#cb91-8" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb91-9"><a href="#cb91-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.image_encoder <span class="op">=</span> image_encoder</span>
<span id="cb91-10"><a href="#cb91-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.text_encoder <span class="op">=</span> text_encoder</span>
<span id="cb91-11"><a href="#cb91-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.image_projection <span class="op">=</span> nn.Linear(image_encoder.output_dim, embed_dim)</span>
<span id="cb91-12"><a href="#cb91-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.text_projection <span class="op">=</span> nn.Linear(text_encoder.output_dim, embed_dim)</span>
<span id="cb91-13"><a href="#cb91-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.logit_scale <span class="op">=</span> nn.Parameter(torch.ones([]) <span class="op">*</span> np.log(<span class="dv">1</span> <span class="op">/</span> <span class="fl">0.07</span>)) <span class="co"># Learnable scale parameter</span></span>
<span id="cb91-14"><a href="#cb91-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-15"><a href="#cb91-15" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, images, texts):</span>
<span id="cb91-16"><a href="#cb91-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 1. Image encoding</span></span>
<span id="cb91-17"><a href="#cb91-17" aria-hidden="true" tabindex="-1"></a>        image_features <span class="op">=</span> <span class="va">self</span>.image_encoder(images)  <span class="co"># [batch_size, image_encoder.output_dim]</span></span>
<span id="cb91-18"><a href="#cb91-18" aria-hidden="true" tabindex="-1"></a>        image_embeddings <span class="op">=</span> <span class="va">self</span>.image_projection(image_features)  <span class="co"># [batch_size, embed_dim]</span></span>
<span id="cb91-19"><a href="#cb91-19" aria-hidden="true" tabindex="-1"></a>        image_embeddings <span class="op">=</span> F.normalize(image_embeddings, dim<span class="op">=-</span><span class="dv">1</span>) <span class="co"># L2 normalization</span></span>
<span id="cb91-20"><a href="#cb91-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-21"><a href="#cb91-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 2. Text encoding</span></span>
<span id="cb91-22"><a href="#cb91-22" aria-hidden="true" tabindex="-1"></a>        text_features <span class="op">=</span> <span class="va">self</span>.text_encoder(texts)   <span class="co"># [batch_size, text_encoder.output_dim]</span></span>
<span id="cb91-23"><a href="#cb91-23" aria-hidden="true" tabindex="-1"></a>        text_embeddings <span class="op">=</span> <span class="va">self</span>.text_projection(text_features)    <span class="co"># [batch_size, embed_dim]</span></span>
<span id="cb91-24"><a href="#cb91-24" aria-hidden="true" tabindex="-1"></a>        text_embeddings <span class="op">=</span> F.normalize(text_embeddings, dim<span class="op">=-</span><span class="dv">1</span>) <span class="co"># L2 normalization</span></span>
<span id="cb91-25"><a href="#cb91-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-26"><a href="#cb91-26" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 3. Similarity calculation</span></span>
<span id="cb91-27"><a href="#cb91-27" aria-hidden="true" tabindex="-1"></a>        logits_per_image <span class="op">=</span> <span class="va">self</span>.logit_scale.exp() <span class="op">*</span> image_embeddings <span class="op">@</span> text_embeddings.t()  <span class="co"># [batch_size, batch_size]</span></span>
<span id="cb91-28"><a href="#cb91-28" aria-hidden="true" tabindex="-1"></a>        logits_per_text <span class="op">=</span> logits_per_image.t() <span class="co"># [batch_size, batch_size]</span></span>
<span id="cb91-29"><a href="#cb91-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-30"><a href="#cb91-30" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> logits_per_image, logits_per_text</span>
<span id="cb91-31"><a href="#cb91-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-32"><a href="#cb91-32" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> contrastive_loss(logits_per_image, logits_per_text):</span>
<span id="cb91-33"><a href="#cb91-33" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb91-34"><a href="#cb91-34" aria-hidden="true" tabindex="-1"></a><span class="co">    Calculates the Contrastive Loss</span></span>
<span id="cb91-35"><a href="#cb91-35" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb91-36"><a href="#cb91-36" aria-hidden="true" tabindex="-1"></a>    batch_size <span class="op">=</span> logits_per_image.shape[<span class="dv">0</span>]</span>
<span id="cb91-37"><a href="#cb91-37" aria-hidden="true" tabindex="-1"></a>    labels <span class="op">=</span> torch.arange(batch_size).to(logits_per_image.device) <span class="co"># Correct labels (diagonal: same pair)</span></span>
<span id="cb91-38"><a href="#cb91-38" aria-hidden="true" tabindex="-1"></a>    loss_i <span class="op">=</span> F.cross_entropy(logits_per_image, labels)  <span class="co"># Loss based on image</span></span>
<span id="cb91-39"><a href="#cb91-39" aria-hidden="true" tabindex="-1"></a>    loss_t <span class="op">=</span> F.cross_entropy(logits_per_text, labels)   <span class="co"># Loss based on text</span></span>
<span id="cb91-40"><a href="#cb91-40" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (loss_i <span class="op">+</span> loss_t) <span class="op">/</span> <span class="dv">2</span>  <span class="co"># Average loss</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="codificador-de-imagen" class="level3">
<h3 class="anchored" data-anchor-id="codificador-de-imagen">10.6.2 Codificador de Imagen</h3>
<p>El codificador de imagen de CLIP toma una imagen como entrada y la convierte en un vector de embedding de dimensión fija. En el artículo original de CLIP, se experimentó tanto con ResNet como con ViT (Vision Transformer).</p>
<ul>
<li><strong>Codificador basado en ResNet</strong>: Utiliza modelos tradicionales de clasificación de imágenes basados en CNN (por ejemplo, ResNet-50, ResNet-101).</li>
<li><strong>Codificador basado en ViT</strong>: Usa ViT (Vision Transformer) como codificador de imagen (ver sección 10.5). ViT divide la imagen en parches y utiliza cada parche como entrada para el transformer.</li>
</ul>
<p>Los resultados experimentales mostraron que el codificador basado en ViT tuvo un mejor rendimiento que el basado en ResNet. En particular, a medida que aumentaban el tamaño del modelo y de los datos, la mejora en el rendimiento de ViT era aún mayor.</p>
</section>
<section id="codificador-de-texto" class="level3">
<h3 class="anchored" data-anchor-id="codificador-de-texto">10.6.3 Codificador de Texto</h3>
<p>El codificador de texto de CLIP toma una descripción de texto como entrada y la convierte en un vector de embedding de <em>la misma dimensión</em> que el del codificador de imagen. En el artículo original de CLIP, se utilizó un codificador de texto basado en Transformer.</p>
<ul>
<li>El codificador de texto tokeniza el texto utilizando Byte Pair Encoding (BPE) y embebe cada token.</li>
<li>Se apilan múltiples capas de bloques de Transformer para capturar la información contextual del texto y, finalmente, generar un vector de embedding que representa todo el texto.</li>
</ul>
</section>
<section id="mecanismo-de-transferencia-cero-disparo" class="level3">
<h3 class="anchored">10.6.4 Mecanismo de Transferencia Cero-disparo</h3>
<p>Una de las características más destacadas de CLIP es su capacidad de <strong>transferencia cero-disparo</strong> (zero-shot transfer), que permite rendimientos excelentes en varias tareas de clasificación de imágenes <em>sin ajuste fino adicional</em>.</p>
<p><strong>Razón por la cual es posible la transferencia cero-disparo</strong></p>
<p>Durante el proceso de aprendizaje contrastivo con un conjunto de datos a gran escala de pares imagen-texto, CLIP aprende a representar imágenes y texto en <em>el mismo espacio semántico</em>. En otras palabras, CLIP adquiere la capacidad de comprender las <em>relaciones semánticas</em> entre imágenes y texto.</p>
<p><strong>Proceso de clasificación cero-disparo</strong></p>
<ol type="1">
<li><p>Se preparan descripciones de texto para las clases a clasificar. Por ejemplo, en el caso del conjunto de datos CIFAR-10, se prepararían descripciones de texto como “una foto de un gato”, “una foto de un perro”, …, “una foto de un camión”.</p></li>
<li><p>Se utilizan los codificadores de texto para embeber cada descripción de texto.</p></li>
<li><p>Se utiliza el codificador de imagen para embeber la imagen dada.</p></li>
<li><p>Se calcula la similitud (por ejemplo, similitud coseno) entre el embedding de la imagen y cada embedding de texto.</p></li>
<li><p>Se selecciona como clase predicha de la imagen aquella que corresponde a la descripción de texto con la mayor similitud.</p></li>
</ol>
<p><strong>Significado de la transferencia cero-disparo</strong></p>
<p>La transferencia cero-disparo significa que el modelo puede aplicarse inmediatamente a nuevas clases o tareas que no vio durante el proceso de entrenamiento, sin necesidad de un ajuste fino adicional. Esto contrasta con los métodos de aprendizaje supervisado tradicionales, que requieren datos etiquetados específicos para una tarea en particular.</p>
<p>La clave de la transferencia cero-disparo es la flexibilidad. Por ejemplo, aunque se use solo datos de las clases “gato” y “perro” para entrenar un modelo de clasificación de imágenes, si se proporciona una imagen de una clase no vista como “jirafa” o “elefante”, el modelo puede clasificarla correctamente con descripciones naturales como “foto de jirafa” o “foto de elefante”. De esta manera, la transferencia cero-disparo permite maximizar la capacidad de generalización del modelo en situaciones donde no hay datos disponibles para nuevas clases o tareas. Además, la transferencia de cero disparos (zero-shot) proporciona versatilidad. No solo se aplica a la clasificación de imágenes, sino también a tareas multimodales variadas como búsqueda de imágenes, generación de descripciones de imágenes, detección de objetos, y respuestas visuales a preguntas (Visual Question Answering, VQA). Por ejemplo, en un sistema de búsqueda de imágenes, al ingresar una consulta de texto como “coche deportivo rojo”, el modelo puede encontrar imágenes que correspondan a “coche deportivo rojo” en la base de datos. Esto es posible porque el modelo entiende las relaciones semánticas entre imágenes y texto. De esta manera, poder utilizar un solo modelo para diversas tareas ahorra tiempo y recursos, y contribuye significativamente al aumento de la utilidad de los sistemas de inteligencia artificial.</p>
<p><strong>Impacto de CLIP</strong></p>
<p>CLIP demostró nuevas posibilidades en el aprendizaje multimodal a través de su capacidad de transferencia de cero disparos. Desde entonces, se han llevado a cabo numerosas investigaciones posteriores basadas en las ideas de CLIP, influyendo enormemente en el desarrollo de modelos de generación de imágenes como DALL-E y Stable Diffusion, así como en la creación de modelos multimodales a gran escala como GPT-4V.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Haga clic para ver el contenido (deep dive: Contrastive Learning y CLIP)">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Haga clic para ver el contenido (deep dive: Contrastive Learning y CLIP)
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<section id="inmersión-profunda-contrastive-learning-y-clip" class="level2 callout-body-container callout-body">
<h2 class="anchored" data-anchor-id="inmersión-profunda-contrastive-learning-y-clip">Inmersión profunda: Contrastive Learning y CLIP</h2>
<p>Contrastive Learning es un enfoque poderoso para aprender representaciones a partir de datos no etiquetados. En particular, ha demostrado excelentes resultados en el aprendizaje multimodal, que conecta modalidades diferentes como imágenes y texto. En esta inmersión profunda, analizaremos detalladamente los principios básicos de Contrastive Learning, sus diversos enfoques, y el modelo innovador CLIP (Contrastive Language-Image Pre-training), que conecta imágenes y texto basándose en Contrastive Learning.</p>
<section id="principios-básicos-del-contrastive-learning" class="level3">
<h3 class="anchored" data-anchor-id="principios-básicos-del-contrastive-learning">1. Principios básicos del Contrastive Learning</h3>
<p>La idea central del Contrastive Learning es <strong>aprender representaciones que hagan que los pares de muestras similares (pares positivos) estén cerca en el espacio de embedding, mientras que los pares de muestras no similares (pares negativos) estén lejos</strong>.</p>
<ul>
<li><strong>Anchor:</strong> La muestra de referencia.</li>
<li><strong>Positive Sample:</strong> Una muestra semánticamente similar al Anchor. (por ejemplo, una versión diferente de la misma imagen, una traducción diferente de la misma oración)</li>
<li><strong>Negative Sample:</strong> Una muestra semánticamente diferente al Anchor.</li>
</ul>
<p>El Contrastive Learning generalmente sigue los siguientes pasos.</p>
<ol type="1">
<li><strong>Aumento de datos (Data Augmentation):</strong> Se aplican varias técnicas de aumento de datos para generar Anchors y Pares Positivos. (por ejemplo, recorte aleatorio, ajuste de color, rotación en el caso de imágenes)</li>
<li><strong>Codificación (Encoding):</strong> Los Anchors, Pares Positivos y Pares Negativos se convierten en vectores de embedding a través de un codificador.</li>
<li><strong>Función de pérdida contrastiva (Contrastive Loss):</strong> Se utiliza una función de pérdida contrastiva para que los embeddings de los pares positivos estén cerca y los embeddings de los pares negativos estén lejos, lo cual ayuda a entrenar el codificador.</li>
</ol>
</section>
<section id="funciones-de-pérdida-contrastivas" class="level3">
<h3 class="anchored" data-anchor-id="funciones-de-pérdida-contrastivas">2. Funciones de pérdida contrastivas</h3>
<p>Se han propuesto varias funciones de pérdida contrastivas, entre las cuales se encuentran:</p>
<ul>
<li><p><strong>InfoNCE Loss (Estimación Contrastiva del Ruido):</strong> Tiene una forma similar a la función de pérdida de entropía cruzada y maximiza la probabilidad softmax para los pares positivos.</p>
<p><span class="math inline">\(L = -\log \frac{\exp(\text{sim}(z_i, z_j) / \tau)}{\sum_{k=1}^{2N} \mathbb{1}_{[k \neq i]} \exp(\text{sim}(z_i, z_k) / \tau)}\)</span></p>
<ul>
<li><span class="math inline">\(z_i\)</span>: Embedding del Anchor</li>
<li><span class="math inline">\(z_j\)</span>: Embedding de la muestra positiva</li>
<li><span class="math inline">\(z_k\)</span>: Embedding de la muestra negativa (k ≠ i)</li>
<li><span class="math inline">\(\text{sim}(\cdot, \cdot)\)</span>: Función de similitud (por ejemplo, similitud coseno)</li>
<li><span class="math inline">\(\tau\)</span>: Parámetro de temperatura (ajusta la distribución de similitudes)</li>
<li><span class="math inline">\(N\)</span>: Tamaño del mini-lote</li>
</ul></li>
<li><p><strong>NT-Xent Loss (Entropía Cruzada Escalada por Temperatura Normalizada):</strong> Es una variante de InfoNCE Loss, propuesta en el paper SimCLR.</p></li>
<li><p><strong>Triplet Loss:</strong> Utiliza muestras Anchor, Positiva y Negativa para que la distancia entre el Anchor y la muestra positiva sea menor que la distancia entre el Anchor y la muestra negativa.</p>
<p><span class="math inline">\(L = \max(0, d(a, p) - d(a, n) + m)\)</span></p></li>
<li><p><span class="math inline">\(a\)</span>: Ancla</p></li>
<li><p><span class="math inline">\(p\)</span>: Ejemplo positivo</p></li>
<li><p><span class="math inline">\(n\)</span>: Ejemplo negativo</p></li>
<li><p><span class="math inline">\(d(\cdot, \cdot)\)</span>: Función de distancia (por ejemplo, distancia euclidiana)</p></li>
<li><p><span class="math inline">\(m\)</span>: Margen (determina cuánto se deben separar las distancias)</p></li>
</ul>
</section>
<section id="metodología-del-aprendizaje-contrastivo" class="level3">
<h3 class="anchored" data-anchor-id="metodología-del-aprendizaje-contrastivo">3. Metodología del Aprendizaje Contrastivo</h3>
<ul>
<li><strong>SimCLR (A Simple Framework for Contrastive Learning of Visual Representations):</strong> Utiliza aumento de datos, tamaño de lote grande y cabeza de proyección (MLP) para aprender representaciones de imágenes.</li>
<li><strong>MoCo (Momentum Contrast):</strong> Usa un codificador de momentum para mantener los ejemplos negativos de manera estable y lograr buenos resultados sin necesidad de un tamaño de lote grande.</li>
<li><strong>SwAV (Swapping Assignments between multiple Views):</strong> Aprende representaciones a través de clustering en línea, sin definir explícitamente ejemplos positivos/negativos.</li>
<li><strong>BYOL (Bootstrap Your Own Latent):</strong> Aprende mediante la predicción entre una red objetivo y una red en línea, sin necesidad de ejemplos negativos.</li>
</ul>
</section>
<section id="clip-contrastive-language-image-pre-training" class="level3">
<h3 class="anchored" data-anchor-id="clip-contrastive-language-image-pre-training">4. CLIP (Contrastive Language-Image Pre-training)</h3>
<p>CLIP es un modelo desarrollado por OpenAI que utiliza el aprendizaje contrastivo para aprender representaciones multimodales potentes que conectan imágenes y texto.</p>
<section id="aprendizaje-de-clip" class="level4">
<h4 class="anchored" data-anchor-id="aprendizaje-de-clip">4.1 Aprendizaje de CLIP</h4>
<ul>
<li><strong>Datos:</strong> Un conjunto de datos a gran escala de pares imagen-texto (400 millones).</li>
<li><strong>Modelo:</strong>
<ul>
<li><strong>Codificador de Imágenes:</strong> Extrae vectores de características de las imágenes (por ejemplo, ResNet, ViT)</li>
<li><strong>Codificador de Texto:</strong> Extrae vectores de características del texto (por ejemplo, Transformer)</li>
</ul></li>
<li><strong>Aprendizaje:</strong>
<ol type="1">
<li>Codifica la imagen y el texto por separado para obtener vectores de embedding.</li>
<li>Utiliza una pérdida contrastiva (InfoNCE) para aumentar la similitud coseno entre los embeddings de pares positivos (mismo par) y disminuir la similitud coseno entre los embeddings de pares negativos (distintos pares).</li>
</ol>
<ul>
<li>Cada imagen en un lote tiene un texto positivo asociado y (N-1) textos negativos.</li>
<li>De manera similar, cada texto tiene una imagen positiva asociada y (N-1) imágenes negativas.</li>
</ul></li>
</ul>
</section>
<section id="características-de-clip" class="level4">
<h4 class="anchored" data-anchor-id="características-de-clip">4.2 Características de CLIP</h4>
<ul>
<li><strong>Aprendizaje sin supervisión:</strong> Puede realizar nuevas tareas (por ejemplo, clasificación de imágenes, búsqueda de imágenes) utilizando las representaciones de imagen-texto aprendidas sin necesidad de ajuste fino adicional.
<ul>
<li>Ejemplo de Clasificación de Imágenes sin Supervisión:
<ol type="1">
<li>Expresa los nombres de las clases a clasificar en texto (por ejemplo, “una foto de un gato”, “una foto de un perro”).</li>
<li>Codifica cada texto con el Codificador de Texto.</li>
<li>Codifica la imagen dada con el Codificador de Imágenes.</li>
<li>Calcula la similitud coseno entre el embedding de la imagen y los embeddings de cada texto.</li>
<li>Clasifica la imagen en la clase correspondiente al texto con la mayor similitud.</li>
</ol></li>
</ul></li>
<li><strong>Aprendizaje de representaciones potentes:</strong> Aprende representaciones generales de imágenes/texto que son transferibles a diversas tareas.</li>
</ul>
</section>
<section id="aplicaciones-de-clip" class="level4">
<h4 class="anchored" data-anchor-id="aplicaciones-de-clip">4.3 Aplicaciones de CLIP</h4>
<ul>
<li><strong>Clasificación de imágenes:</strong> Clasificación sin disparos, clasificación con pocos disparos.</li>
<li><strong>Recuperación de imágenes:</strong> Búsqueda de imágenes usando consultas de texto.</li>
<li><strong>Generación de imágenes:</strong> Utilizado como tecnología base para modelos de generación de imágenes basados en texto, como DALL-E y Stable Diffusion.</li>
<li><strong>Respuesta a preguntas visuales (VQA):</strong> Genera respuestas al recibir una imagen y un texto de pregunta juntos.</li>
<li><strong>Detección de objetos:</strong> Integración de CLIP en modelos de detección de objetos para realizar detección de objetos con vocabulario abierto.</li>
</ul>
</section>
</section>
<section id="limitaciones-y-direcciones-futuras-de-la-aprendizaje-contrastivo-y-clip" class="level3">
<h3 class="anchored" data-anchor-id="limitaciones-y-direcciones-futuras-de-la-aprendizaje-contrastivo-y-clip">5. Limitaciones y direcciones futuras de la Aprendizaje Contrastivo y CLIP</h3>
<ul>
<li><strong>Dependencia de Data Augmentation:</strong> El Aprendizaje Contrastivo es sensible a las técnicas de data augmentation. Se necesita investigación sobre qué augmentation es efectiva.</li>
<li><strong>Sesgo en el Muestreo Negativo:</strong> Los resultados del entrenamiento pueden variar dependiendo de cómo se seleccionen los ejemplos negativos. Técnicas como el hard negative mining están siendo investigadas.</li>
<li><strong>Colapso Modal:</strong> Fenómeno donde todas las muestras convergen a una sola representación.</li>
<li><strong>Comprensión Detallada:</strong> Aunque CLIP aprende bien la alineación de nivel general entre imágenes y texto, puede carecer de comprensión detallada (por ejemplo, relaciones entre objetos en una imagen, matices sutiles del texto).</li>
<li><strong>Costo Computacional:</strong> Requiere conjuntos de datos a gran escala y tamaños de lote grandes.</li>
</ul>
</section>
<section id="conclusión-1" class="level3">
<h3 class="anchored" data-anchor-id="conclusión-1">6. Conclusión</h3>
<p>El Aprendizaje Contrastivo es un enfoque efectivo para aprender representaciones poderosas utilizando datos sin etiquetas. En particular, CLIP ha aplicado con éxito el Aprendizaje Contrastivo al aprendizaje multimodal, abriendo nuevas perspectivas para conectar imágenes y texto. Se espera que tanto el Aprendizaje Contrastivo como CLIP sigan siendo utilizados en diversos campos en el futuro.</p>
<p><strong>Referencias:</strong> * Chen, T., Kornblith, S., Norouzi, M., &amp; Hinton, G. (2020). Un marco simple para el aprendizaje contrastivo de representaciones visuales. <em>Conferencia internacional sobre aprendizaje automático</em>. PMLR. * Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., … &amp; Sutskever, I. (2021). Aprendizaje de modelos visuales transferibles a partir de la supervisión del lenguaje natural. <em>Conferencia Internacional sobre Aprendizaje Automático</em>. PMLR. * He, K., Fan, H., Wu, Y., Xie, S., &amp; Girshick, R. (2020). Momentum contrast para el aprendizaje no supervisado de representaciones visuales. En <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em> (pp.&nbsp;9729-9738). * Grill, J. B., Strub, F., Altché, F., Tallec, C., Richemond, P., Buchatskaya, E., … &amp; Valko, M. (2020). Bootstrap your own latent-un nuevo enfoque para el aprendizaje supervisado por sí mismo. <em>Advances in neural information processing systems</em>, 33, 21271-21284.</p>
</section>
</section>
</div>
</div>
</section>
</section>
<section id="ejercicios-de-práctica" class="level2">
<h2 class="anchored" data-anchor-id="ejercicios-de-práctica">Ejercicios de Práctica</h2>
<p><strong>Problemas Básicos</strong></p>
<ol type="1">
<li>Explique qué es el dato multimodal y proporcione al menos tres ejemplos de datos multimodales.</li>
<li>Explique la diferencia entre Representación Conjunta (Joint Representation) y Representación Coordinada (Coordinated Representation), y compare las ventajas y desventajas de cada método de representación.</li>
<li>Defina la tarea de Generación de Légends para Imágenes (Image Captioning) y explique la estructura general de un modelo de aprendizaje profundo utilizado para resolver esta tarea (codificador-decodificador).</li>
</ol>
<p><strong>Problemas Aplicados</strong></p>
<ol type="1">
<li>Diseñe la estructura de un modelo simple que resuelva la tarea de Respuesta a Preguntas Visuales (Visual Question Answering, VQA), que tome como entrada una imagen y un texto de pregunta para generar una respuesta (puede utilizar diagramas de bloques, etc.), y explique el papel de cada componente.</li>
<li>Explique cómo se entrena el modelo CLIP y compare las ventajas de este método con respecto a los métodos tradicionales de aprendizaje supervisado de imagen-texto.</li>
<li>Escriba un código utilizando la biblioteca Hugging Face Transformers para generar textos descriptivos (captions) relacionados con imágenes (por ejemplo, utilizando el modelo <code>blip-image-captioning-base</code>).</li>
</ol>
<p><strong>Problemas Avanzados</strong></p>
<ol type="1">
<li>Investigue los diferentes métodos de Fusión Multimodal (early fusion, late fusion, hybrid fusion), compare las ventajas y desventajas de cada método y explique en qué situaciones es apropiado utilizar un método de fusión específico.</li>
<li>Explique el principio de funcionamiento del Mecanismo de Atención Transmodal (Cross-Modal Attention) y discuta su papel en el aprendizaje multimodal, proporcionando ejemplos concretos (por ejemplo, VQA, Generación de Légends para Imágenes).</li>
<li>Investigue el principio de funcionamiento de modelos que generan imágenes basadas en descripciones textuales (por ejemplo, DALL-E, Stable Diffusion) y discuta los posibles impactos positivos y negativos de estos modelos en la sociedad (a nivel de sugerencia e ideas para debate).</li>
</ol>
<div class="callout callout-style-default callout-note callout-titled" title="Haga clic para ver el contenido (soluciones de ejercicios)">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Haga clic para ver el contenido (soluciones de ejercicios)
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<section id="solución-de-problemas-de-práctica" class="level2 callout-body-container callout-body">
<h2 class="anchored" data-anchor-id="solución-de-problemas-de-práctica">Solución de problemas de práctica</h2>
<section id="problemas-básicos" class="level3">
<h3 class="anchored" data-anchor-id="problemas-básicos">Problemas básicos</h3>
<ol type="1">
<li><strong>Datos multimodales:</strong> Se refiere a datos que combinan dos o más formas diferentes de datos (modalidades). Ejemplos:
<ul>
<li>Imágenes y subtítulos de texto</li>
<li>Videos y pistas de audio</li>
<li>Datos de sensores (e.g., acelerómetro, giroscopio) y descripciones de texto</li>
</ul></li>
<li><strong>Representación conjunta vs.&nbsp;Representación coordinada:</strong>
<ul>
<li><strong>Representación conjunta:</strong> Se representa la información de múltiples modalidades en un solo espacio vectorial integrado.
<ul>
<li>Ventajas: Permite modelar directamente las correlaciones entre diferentes modalidades.</li>
<li>Desventajas: Una modalidad puede dominar a otras modalidades.</li>
</ul></li>
<li><strong>Representación coordinada:</strong> Se representa cada modalidad en espacios vectoriales separados, pero estos espacios se aprenden de manera que estén relacionados (e.g., restricciones de similitud).
<ul>
<li>Ventajas: Permite mantener las características únicas de cada modalidad mientras permite la interacción.</li>
<li>Desventajas: El modelado de interacciones entre modalidades es más indirecto en comparación con la representación conjunta.</li>
</ul></li>
</ul></li>
<li><strong>Generación de subtítulos de imágenes:</strong> Es una tarea que implica generar una descripción textual para una imagen dada.
<ul>
<li><strong>Estructura común (codificador-decodificador):</strong>
<ul>
<li><strong>Codificador:</strong> Extrae las características (features) de la imagen (generalmente se utiliza CNN).</li>
<li><strong>Decodificador:</strong> Predice la siguiente palabra basándose en las características de la imagen extraídas por el codificador y las palabras generadas hasta el momento anterior (generalmente se utiliza RNN o Transformer). Se puede usar un mecanismo de atención para centrarse en áreas específicas de la imagen.</li>
</ul></li>
</ul></li>
</ol>
</section>
<section id="problemas-de-aplicación" class="level3">
<h3 class="anchored" data-anchor-id="problemas-de-aplicación">Problemas de aplicación</h3>
<ol type="1">
<li><p><strong>Estructura del modelo VQA:</strong></p>
<pre class="mermaid"><code>graph LR
    subgraph Modelo VQA
        A[Imagen] --&gt; B(Codificador de Imagen - CNN)
        C[Texto de Pregunta] --&gt; D(Codificador de Texto - RNN/Transformer)
        B --&gt; E(Módulo de Fusión)
        D --&gt; E
        E --&gt; F(Decodificador - RNN/Transformer)
        F --&gt; G(Respuesta)
    end</code></pre>
<ul>
<li><strong>Codificador de Imagen (CNN):</strong> Recibe la imagen como entrada y extrae un vector de características.</li>
<li><strong>Codificador de Texto (RNN/Transformer):</strong> Recibe el texto de la pregunta como entrada y extrae un vector de características.</li>
<li><strong>Módulo de Fusión:</strong> Combina los vectores de características de la imagen y del texto de la pregunta (e.g., concatenación, multiplicación elemento por elemento, atención multimodal).</li>
<li><strong>Decodificador (RNN/Transformer):</strong> Genera una respuesta basada en el vector de características fusionado.</li>
</ul></li>
<li><p><strong>Método de aprendizaje y ventajas de CLIP:</strong></p>
<ul>
<li><strong>Método de aprendizaje:</strong> CLIP utiliza un conjunto de datos a gran escala de pares imagen-texto para codificar imágenes y texto por separado, y aprende mediante una pérdida contrastiva para que los embebidos de imagen y texto del mismo par estén cerca, mientras que los embebidos de diferentes pares estén lejos.</li>
<li><strong>Ventajas:</strong>
<ul>
<li><strong>Aprendizaje sin supervisión (zero-shot):</strong> Puede aplicarse a nuevas tareas (e.g., clasificación de imágenes) sin necesidad de ajuste fino adicional.</li>
<li><strong>Potente aprendizaje de representaciones:</strong> Aprende representaciones generales de imagen/texto que son transferibles a diversas tareas.</li>
<li><strong>Eficiencia de datos:</strong> Puede aprovechar pares de imagen-texto no etiquetados.</li>
</ul></li>
</ul></li>
<li><p><strong>Código de Hugging Face Transformers para Generación de Descripciones de Imágenes:</strong></p></li>
</ol>
<div class="sourceCode" id="cb93"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb93-1"><a href="#cb93-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> pipeline</span>
<span id="cb93-2"><a href="#cb93-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-3"><a href="#cb93-3" aria-hidden="true" tabindex="-1"></a>captioner <span class="op">=</span> pipeline(<span class="st">"image-to-text"</span>, model<span class="op">=</span><span class="st">"nlpconnect/vit-gpt2-image-captioning"</span>) <span class="co"># o "blip-image-captioning-base"</span></span>
<span id="cb93-4"><a href="#cb93-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-5"><a href="#cb93-5" aria-hidden="true" tabindex="-1"></a>image_path <span class="op">=</span> <span class="st">"path/to/your/image.jpg"</span>  <span class="co"># ruta del archivo de imagen</span></span>
<span id="cb93-6"><a href="#cb93-6" aria-hidden="true" tabindex="-1"></a>caption <span class="op">=</span> captioner(image_path)[<span class="dv">0</span>][<span class="st">'generated_text'</span>]</span>
<span id="cb93-7"><a href="#cb93-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(caption)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="problemas-avanzados" class="level3">
<h3 class="anchored" data-anchor-id="problemas-avanzados">Problemas Avanzados</h3>
<ol type="1">
<li><p><strong>Métodos de Fusión Multimodal:</strong></p>
<ul>
<li><strong>Fusión Temprana (Early Fusion):</strong> Combina las modalidades en la etapa de entrada (por ejemplo, concatenación de canales).
<ul>
<li>Ventajas: Puede capturar interacciones a bajo nivel entre las modalidades.</li>
<li>Desventajas: La dimensionalidad puede volverse muy grande y las características únicas de cada modalidad pueden diluirse.</li>
</ul></li>
<li><strong>Fusión Tardía (Late Fusion):</strong> Procesa cada modalidad de forma independiente y luego combina los resultados en la etapa final (por ejemplo, promedio, votación).
<ul>
<li>Ventajas: Maximiza el uso de las características de cada modalidad y es simple de implementar.</li>
<li>Desventajas: Es difícil capturar interacciones a bajo nivel entre las modalidades.</li>
</ul></li>
<li><strong>Fusión Híbrida (Hybrid Fusion):</strong> Combina métodos de Fusión Temprana y Fusión Tardía. Realiza fusión en varios niveles.
<ul>
<li>Ventajas: Puede aprovechar tanto los beneficios de la Fusión Temprana como de la Fusión Tardía.</li>
<li>Desventajas: El modelo se vuelve más complejo.</li>
</ul></li>
<li><strong>Situaciones Apropiadas:</strong>
<ul>
<li><strong>Fusión Temprana:</strong> Cuando las interacciones estrechas entre modalidades son importantes (por ejemplo, sincronización de video y audio).</li>
<li><strong>Fusión Tardía:</strong> Cuando cada modalidad tiene significado independiente (por ejemplo, etiquetado de imágenes y descripción de texto).</li>
<li><strong>Fusión Híbrida:</strong> Para tareas complejas donde se deben capturar interacciones en varios niveles.</li>
</ul></li>
</ul></li>
<li><p><strong>Atención Cross-Modal:</strong></p>
<ul>
<li><strong>Principio de Funcionamiento:</strong> Utiliza las consultas (queries) de una modalidad para calcular los pesos de atención sobre las claves (keys) de otra modalidad, y luego usa estos pesos para realizar un cálculo ponderado de los valores (values) de la otra modalidad, generando así una nueva representación.</li>
<li><strong>Roles:</strong>
<ul>
<li><strong>Pregunta-Respuesta Visual (VQA):</strong> Determina a qué regiones de la imagen (keys, values) debe prestar atención cada palabra del texto de la pregunta (query).</li>
<li><strong>Generación de Descripciones de Imágenes:</strong> Decide con qué regiones de la imagen (keys, values) está relacionada cada palabra generada (query).</li>
</ul></li>
</ul></li>
<li><p><strong>Modelos de Generación de Imágenes Basados en Texto (DALL-E, Stable Diffusion, etc.):</strong></p>
<ul>
<li><strong>Principio de Funcionamiento (simplificado):</strong></li>
<li><strong>DALL-E (Basado en Transformer):</strong> Tokeniza el texto y la imagen, y utiliza un Transformer para modelar la probabilidad de que una secuencia de tokens de imagen aparezca dada una secuencia de tokens de texto.</li>
<li><strong>Stable Diffusion (Basado en Modelo de Difusión):</strong> Aprende un proceso forward de agregar ruido gradualmente a una imagen y un proceso reverse de restaurar la imagen desde el ruido. La información del texto se proporciona como condición en el proceso reverse para controlar la generación de imágenes.</li>
</ul></li>
</ol>
<ul>
<li><strong>Impacto positivo:</strong>
<ul>
<li><strong>Fomento de la creatividad:</strong> visualización de nuevas ideas, apoyo en la creación de obras de arte.</li>
<li><strong>Mejora en la eficiencia de producción de contenido:</strong> automatización del diseño, publicidad y materiales educativos.</li>
<li><strong>Mejora en la accesibilidad:</strong> generación de descripciones de imágenes para personas con discapacidad visual.</li>
</ul></li>
<li><strong>Impacto negativo:</strong>
<ul>
<li><strong>Diseminación de deepfakes e información falsa:</strong> distorsión de la realidad, daño a la reputación.</li>
<li><strong>Infracción de derechos de autor:</strong> uso no autorizado y modificación de imágenes existentes.</li>
<li><strong>Reducción de empleos:</strong> sustitución de profesiones como diseñadores e ilustradores.</li>
<li><strong>Sesgo y discriminación:</strong> generación de imágenes discriminatorias hacia ciertos grupos, reflejando sesgos en los datos de entrenamiento.</li>
</ul></li>
</ul>
</section>
</section>
</div>
</div>
</section>
<section id="referencias" class="level2">
<h2 class="anchored" data-anchor-id="referencias">Referencias</h2>
<ol type="1">
<li><p><strong>CLIP (Learning Transferable Visual Models From Natural Language Supervision):</strong> el artículo original de CLIP, un método de aprendizaje de representaciones multimodales que conecta imágenes y texto. <a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://arxiv.org/abs/2103.00020">https://arxiv.org/abs/2103.00020</a></p></li>
<li><p><strong>ViT (An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale):</strong> el artículo original de ViT, que demostró un rendimiento sobresaliente en la clasificación de imágenes utilizando únicamente la estructura de Transformer sin CNN. <a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://arxiv.org/abs/2010.11929">https://arxiv.org/abs/2010.11929</a></p></li>
<li><p><strong>DALL-E (Zero-Shot Text-to-Image Generation):</strong> el artículo del modelo DALL-E, que genera imágenes basadas en descripciones de texto. <a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://arxiv.org/abs/2102.12092">https://arxiv.org/abs/2102.12092</a></p></li>
<li><p><strong>MAE (Masked Autoencoders Are Scalable Vision Learners):</strong> el artículo de MAE, que aprende representaciones visuales ocultando y reconstruyendo partes de las imágenes. <a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://arxiv.org/abs/2111.06377">https://arxiv.org/abs/2111.06377</a></p></li>
<li><p><strong>Visual Question Answering (VQA):</strong> uno de los primeros estudios de VQA, que presenta el conjunto de datos VQA y modelos base. <a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://arxiv.org/abs/1505.00468">https://arxiv.org/abs/1505.00468</a></p></li>
<li><p><strong>Show, Attend and Tell (Neural Image Caption Generation with Visual Attention):</strong> el artículo que introdujo el mecanismo de atención en la generación de subtítulos de imágenes. <a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://arxiv.org/abs/1502.03044">https://arxiv.org/abs/1502.03044</a></p></li>
<li><p><strong>Multimodal Machine Learning: A Survey and Taxonomy:</strong> un artículo de revisión exhaustivo sobre aprendizaje multimodal. <a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://arxiv.org/abs/1705.09406">https://arxiv.org/abs/1705.09406</a></p></li>
<li><p><strong>A Tutorial on Multimodal Deep Learning, Jiquan Ngiam:</strong> tutorial de aprendizaje profundo multimodal presentado en NeurIPS 2011 (video). <a href="https://www.google.com/search?q=https://www.youtube.com/watch%3Fv%3DcR_ACqfF-bY%26list%3DPL_45CaSOtPzL-HWxMcnr02KvmP9Gq-xdb">https://www.youtube.com/watch?v=cR_ACqfF-bY&amp;list=PL_45CaSOtPzL-HWxMcnr02KvmP9Gq-xdb</a></p></li>
<li><p><strong>CMU Multimodal Machine Learning Course (11-777, Spring 2023), Louis-Philippe Morency:</strong> Material de curso de aprendizaje automático multimodal de la Universidad Carnegie Mellon. <a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://cmu-multicomp-lab.github.io/mmml-course/spring2023/">https://cmu-multicomp-lab.github.io/mmml-course/spring2023/</a></p></li>
<li><p><strong>A Comprehensive Survey on Deep Multimodal Learning:</strong> Artículo de revisión sobre aprendizaje profundo multimodal del año 2022. <a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://arxiv.org/abs/2204.11984">https://arxiv.org/abs/2204.11984</a></p></li>
<li><p><strong>arXiv:</strong> Búsqueda de artículos de investigación más recientes sobre aprendizaje multimodal. Utilice palabras clave como “multimodal learning”, “visión-lenguaje”. <a href="https://arxiv.org/">https://arxiv.org/</a></p></li>
<li><p><strong>Hugging Face Transformers Multimodal Documentation:</strong> Documentación de modelos multimodales de la biblioteca Hugging Face Transformers. <a href="https://www.google.com/search?q=https://huggingface.co/docs/transformers/main/en/model_doc/auto%23multimodal-models">https://huggingface.co/docs/transformers/main/en/model_doc/auto#multimodal-models</a></p></li>
</ol>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>