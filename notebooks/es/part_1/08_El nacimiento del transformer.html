<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>el-nacimiento-del-transformer – Deep Learning DNA: Surviving Architectures and Essential Principles</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap-f507c7d0488cb7630e20aad62ad8c2aa.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script>window.MathJax = {loader: {load: ['[tex]/boldsymbol']},tex: {packages: {'[+]': ['boldsymbol']}}};</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../../notebooks/es/part_1/01_El inicio del aprendizaje profundo.html">part_1</a></li><li class="breadcrumb-item"><a href="../../../notebooks/es/part_1/08_El nacimiento del transformer.html">8. El nacimiento del transformer</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../../../">Español</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Language</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index_de.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Deutsch</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index_en.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">English</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index_es.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Español</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">한국어</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index_zh.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">中文</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/es/part_1/00_Introducción.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introducción</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">part_1</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/es/part_1/01_El inicio del aprendizaje profundo.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">1. El inicio del aprendizaje profundo</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/es/part_1/02_Matemáticas de deep learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">2. Matemáticas de deep learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/es/part_1/03_marco de aprendizaje profundo.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">3. marco de aprendizaje profundo</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/es/part_1/04_función de activación.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">4. función de activación</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/es/part_1/05_Optimización y visualización.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">5. Optimización y visualización</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/es/part_1/06_Sobreajuste y desarrollo de técnicas de solución.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">6. Sobreajuste y desarrollo de técnicas de solución</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/es/part_1/07_Evolución de las redes neuronales convolucionales.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">7. Evolución de las redes neuronales convolucionales</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/es/part_1/08_El nacimiento del transformer.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">8. El nacimiento del transformer</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/es/part_1/09_La evolución del transformer.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">9. La evolución del transformer</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/es/part_1/10_Multimodal deep learning: el inicio de la fusión multisensorial.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">10. Multimodal deep learning: el inicio de la fusión multisensorial</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/es/part_1/11_Multimodal deep learning: inteligencia más allá de los límites.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">11. Multimodal deep learning: inteligencia más allá de los límites</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">la vanguardia del deep learning</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/es/la vanguardia del deep learning/01_SLM: pequeño pero poderoso modelo de lenguaje.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">1. SLM: pequeño pero poderoso modelo de lenguaje</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/es/la vanguardia del deep learning/02_conducción autónoma.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">2. conducción autónoma</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#capítulo-nacimiento-del-transformer" id="toc-capítulo-nacimiento-del-transformer" class="nav-link active" data-scroll-target="#capítulo-nacimiento-del-transformer">8 Capítulo Nacimiento del Transformer</a>
  <ul class="collapse">
  <li><a href="#transformer---revolución-en-el-procesamiento-secuencial" id="toc-transformer---revolución-en-el-procesamiento-secuencial" class="nav-link" data-scroll-target="#transformer---revolución-en-el-procesamiento-secuencial">8.1 Transformer - Revolución en el Procesamiento Secuencial</a></li>
  <li><a href="#el-proceso-de-evolución-del-transformer" id="toc-el-proceso-de-evolución-del-transformer" class="nav-link" data-scroll-target="#el-proceso-de-evolución-del-transformer">8.2 El proceso de evolución del Transformer</a>
  <ul class="collapse">
  <li><a href="#los-límites-del-rnn-y-el-nacimiento-de-la-atención" id="toc-los-límites-del-rnn-y-el-nacimiento-de-la-atención" class="nav-link" data-scroll-target="#los-límites-del-rnn-y-el-nacimiento-de-la-atención">8.2.1 Los límites del RNN y el nacimiento de la atención</a></li>
  <li><a href="#conceptos-básicos-de-la-atención" id="toc-conceptos-básicos-de-la-atención" class="nav-link" data-scroll-target="#conceptos-básicos-de-la-atención">8.2.2 Conceptos básicos de la atención</a></li>
  <li><a href="#evolución-hacia-la-autoatención" id="toc-evolución-hacia-la-autoatención" class="nav-link" data-scroll-target="#evolución-hacia-la-autoatención">8.2.3 Evolución hacia la autoatención</a></li>
  <li><a href="#atención-multi-cabeza-y-procesamiento-en-paralelo" id="toc-atención-multi-cabeza-y-procesamiento-en-paralelo" class="nav-link" data-scroll-target="#atención-multi-cabeza-y-procesamiento-en-paralelo">8.2.4 Atención multi-cabeza y procesamiento en paralelo</a></li>
  <li><a href="#análisis-detallado-de-la-atención-multi-cabeza-multi-head-attention" id="toc-análisis-detallado-de-la-atención-multi-cabeza-multi-head-attention" class="nav-link" data-scroll-target="#análisis-detallado-de-la-atención-multi-cabeza-multi-head-attention">Análisis detallado de la Atención Multi-Cabeza (Multi-Head Attention)</a></li>
  <li><a href="#estrategias-de-enmascaramiento-para-el-aprendizaje-paralelo" id="toc-estrategias-de-enmascaramiento-para-el-aprendizaje-paralelo" class="nav-link" data-scroll-target="#estrategias-de-enmascaramiento-para-el-aprendizaje-paralelo">8.2.5 Estrategias de enmascaramiento para el aprendizaje paralelo</a></li>
  <li><a href="#evolución-del-significado-de-head-de-cabeza-a-cerebro" id="toc-evolución-del-significado-de-head-de-cabeza-a-cerebro" class="nav-link" data-scroll-target="#evolución-del-significado-de-head-de-cabeza-a-cerebro">8.2.6 Evolución del significado de “head”: de “cabeza” a “cerebro”</a></li>
  </ul></li>
  <li><a href="#procesamiento-de-la-información-de-posición" id="toc-procesamiento-de-la-información-de-posición" class="nav-link" data-scroll-target="#procesamiento-de-la-información-de-posición">8.3 Procesamiento de la información de posición</a>
  <ul class="collapse">
  <li><a href="#importancia-de-la-información-secuencial" id="toc-importancia-de-la-información-secuencial" class="nav-link" data-scroll-target="#importancia-de-la-información-secuencial">8.3.1 Importancia de la información secuencial</a></li>
  <li><a href="#diseño-del-codificador-posicional" id="toc-diseño-del-codificador-posicional" class="nav-link" data-scroll-target="#diseño-del-codificador-posicional">8.3.2 Diseño del codificador posicional</a></li>
  </ul></li>
  <li><a href="#arquitectura-completa-del-transformer" id="toc-arquitectura-completa-del-transformer" class="nav-link" data-scroll-target="#arquitectura-completa-del-transformer">8.4 Arquitectura completa del transformer</a>
  <ul class="collapse">
  <li><a href="#integración-de-componentes-básicos" id="toc-integración-de-componentes-básicos" class="nav-link" data-scroll-target="#integración-de-componentes-básicos">8.4.1 Integración de componentes básicos</a></li>
  <li><a href="#composición-del-codificador" id="toc-composición-del-codificador" class="nav-link" data-scroll-target="#composición-del-codificador">8.4.2 Composición del codificador</a></li>
  <li><a href="#estructura-del-decodificador" id="toc-estructura-del-decodificador" class="nav-link" data-scroll-target="#estructura-del-decodificador">8.4.3 Estructura del decodificador</a></li>
  <li><a href="#explicación-de-la-estructura-completa" id="toc-explicación-de-la-estructura-completa" class="nav-link" data-scroll-target="#explicación-de-la-estructura-completa">8.4.4 Explicación de la estructura completa</a></li>
  </ul></li>
  <li><a href="#ejemplos-de-transformer" id="toc-ejemplos-de-transformer" class="nav-link" data-scroll-target="#ejemplos-de-transformer">8.5 Ejemplos de transformer</a>
  <ul class="collapse">
  <li><a href="#tarea-de-copia-simple" id="toc-tarea-de-copia-simple" class="nav-link" data-scroll-target="#tarea-de-copia-simple">8.5.1 Tarea de copia simple</a></li>
  <li><a href="#tarea-de-copia-simple-1" id="toc-tarea-de-copia-simple-1" class="nav-link" data-scroll-target="#tarea-de-copia-simple-1">8.5.1 Tarea de copia simple</a></li>
  <li><a href="#problema-de-suma-de-dígitos" id="toc-problema-de-suma-de-dígitos" class="nav-link" data-scroll-target="#problema-de-suma-de-dígitos">8.5.2 Problema de suma de dígitos</a></li>
  <li><a href="#tarea-de-análisis-sintáctico" id="toc-tarea-de-análisis-sintáctico" class="nav-link" data-scroll-target="#tarea-de-análisis-sintáctico">8.5.3 Tarea de análisis sintáctico</a></li>
  </ul></li>
  <li><a href="#conclusión" id="toc-conclusión" class="nav-link" data-scroll-target="#conclusión">Conclusión</a></li>
  <li><a href="#ejercicios-de-práctica" id="toc-ejercicios-de-práctica" class="nav-link" data-scroll-target="#ejercicios-de-práctica">Ejercicios de Práctica</a>
  <ul class="collapse">
  <li><a href="#problemas-básicos" id="toc-problemas-básicos" class="nav-link" data-scroll-target="#problemas-básicos">Problemas Básicos</a></li>
  <li><a href="#problemas-de-aplicación" id="toc-problemas-de-aplicación" class="nav-link" data-scroll-target="#problemas-de-aplicación">Problemas de Aplicación</a></li>
  <li><a href="#problemas-avanzados" id="toc-problemas-avanzados" class="nav-link" data-scroll-target="#problemas-avanzados">Problemas Avanzados</a></li>
  </ul></li>
  <li><a href="#referencia" id="toc-referencia" class="nav-link" data-scroll-target="#referencia">Referencia</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../../notebooks/es/part_1/01_El inicio del aprendizaje profundo.html">part_1</a></li><li class="breadcrumb-item"><a href="../../../notebooks/es/part_1/08_El nacimiento del transformer.html">8. El nacimiento del transformer</a></li></ol></nav></header>




<p><a href="https://colab.research.google.com/github/Quantum-Intelligence-Frontier/dldna/blob/main/notebooks/es/part_1/08_el_nacimiento_del_transformer.ipynb" target="_parent"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Abrir en Colab"> </a></p>
<section id="capítulo-nacimiento-del-transformer" class="level1">
<h1>8 Capítulo Nacimiento del Transformer</h1>
<blockquote class="blockquote">
<p>“Attention is all you need.” - Ashish Vaswani et al., NeurIPS 2017.</p>
</blockquote>
<p>2017 fue un año especial en la historia del procesamiento de lenguaje natural. Esto se debe a que Google presentó el Transformer en su artículo “Attention is All You Need”. Este avance es comparable a la revolución que AlexNet trajo al reconocimiento visual en 2012. Con la aparición del Transformer, el procesamiento de lenguaje natural (NLP) entró en una nueva era. A partir de entonces, surgieron modelos de lenguaje poderosos basados en Transformers, como BERT y GPT, abriendo un nuevo capítulo en la historia de la inteligencia artificial.</p>
<p><strong>Notas</strong></p>
<p>El Capítulo 8 reconstruye el proceso de desarrollo del Transformer por parte del equipo de investigación de Google <em>de manera dramática</em>. Basándose en el artículo original, blogs de investigación, presentaciones académicas y otros materiales, se ha intentado describir vividamente las preocupaciones y procesos de resolución de problemas que los investigadores <strong>podrían haber enfrentado</strong>. En este proceso, se aclara que algunas partes han sido reconstruidas con base en <strong>razonamientos y imaginación razonables</strong>.</p>
<section id="transformer---revolución-en-el-procesamiento-secuencial" class="level2">
<h2 class="anchored" data-anchor-id="transformer---revolución-en-el-procesamiento-secuencial">8.1 Transformer - Revolución en el Procesamiento Secuencial</h2>
<blockquote class="blockquote">
<p><strong>Desafío:</strong> ¿Cómo superar las limitaciones fundamentales de los modelos basados en redes neuronales recurrentes (RNN)?</p>
<p><strong>Angustia del investigador:</strong> En ese momento, los modelos basados en RNN, LSTM y GRU eran dominantes en el campo del procesamiento de lenguaje natural. Sin embargo, estos modelos tenían que procesar secuencias de entrada de manera secuencial, lo que hacía imposible la paralelización y provocaba problemas de dependencia a largo plazo al procesar oraciones largas. Los investigadores necesitaban superar estas limitaciones fundamentales y desarrollar una nueva arquitectura que fuera más rápida, eficiente y capaz de comprender mejor contextos extensos.</p>
</blockquote>
<p>El procesamiento de lenguaje natural había estado atrapado durante mucho tiempo en las limitaciones del procesamiento secuencial. El procesamiento secuencial implica procesar una oración palabra por palabra o token a token en orden. Al igual que cómo los humanos leen un texto palabra por palabra, RNN y LSTM también tenían que procesar la entrada de manera secuencial. Este tipo de procesamiento secuencial presentaba dos problemas graves: 1) no se podía aprovechar eficientemente el hardware de procesamiento paralelo como GPUs, y 2) al procesar oraciones largas, la información del comienzo (palabras) no se transmitía adecuadamente a las partes posteriores, conocido como el “problema de dependencia a largo plazo (long-range dependency problem)”. En otras palabras, cuando los elementos relacionados dentro de una oración estaban muy separados, no podían ser procesados correctamente.</p>
<p>El mecanismo de atención, que apareció en 2014, resolvió parcialmente estos problemas. A diferencia de RNN tradicionales, donde el decodificador solo consultaba el último estado oculto del codificador, la atención permitía al decodificador considerar todos los estados ocultos del codificador. Sin embargo, aún había limitaciones fundamentales. La estructura misma de RNN estaba basada en el procesamiento secuencial, por lo que seguían teniendo que procesar una palabra a la vez. Como resultado, no era posible realizar un procesamiento paralelo con GPUs y, por lo tanto, el tiempo de procesamiento para secuencias largas era considerable.</p>
<p>En 2017, el equipo de investigación de Google desarrolló el Transformer para mejorar significativamente el rendimiento en traducción automática. El Transformer resolvió estas limitaciones fundamentales al eliminar completamente las RNN y adoptar un enfoque basado únicamente en la atención propia (self-attention).</p>
<p>El Transformer tiene tres ventajas clave: 1. Procesamiento paralelo: puede procesar todas las posiciones de una secuencia simultáneamente, maximizando el uso de GPUs. 2. Dependencia global: todos los tokens pueden definir directamente la intensidad de su relación con todos los demás tokens. 3. Manejo flexible de la información de posición: a través del codificado posicional, representa eficazmente la información de orden mientras se adapta flexiblemente a secuencias de diferentes longitudes. El transformer pronto se convirtió en la base de potentes modelos de lenguaje como BERT y GPT, e incluso se expandió a otros campos, como el Vision Transformer. El transformer no fue solo una nueva arquitectura simple, sino que llevó a un replanteamiento fundamental del procesamiento de información en el deep learning. En particular, esto condujo al éxito de ViT (Vision Transformer) en el campo de la visión por computadora, convirtiéndose en un fuerte competidor para las CNN.</p>
</section>
<section id="el-proceso-de-evolución-del-transformer" class="level2">
<h2 class="anchored" data-anchor-id="el-proceso-de-evolución-del-transformer">8.2 El proceso de evolución del Transformer</h2>
<p>A principios de 2017, el equipo de investigación de Google se encontró con un obstáculo en el campo de la traducción automática. En ese momento, los modelos secuencia a secuencia (seq-to-seq) basados en RNN, que eran predominantes, tenían un problema crónico: su rendimiento disminuía significativamente al procesar oraciones largas. Aunque el equipo de investigación hizo esfuerzos multidireccionales para mejorar la estructura del RNN, estos solo fueron medidas temporales y no una solución fundamental. En medio de este desafío, un investigador puso su atención en el mecanismo de atención publicado en 2014 (Bahdanau et al., 2014). “Si la atención había mitigado el problema de las dependencias a larga distancia, ¿no sería posible procesar secuencias solo con atención, sin necesidad de RNN?”</p>
<p>Muchas personas experimentan confusión al conocer por primera vez el mecanismo de atención, especialmente en los conceptos de Q, K y V. De hecho, la forma inicial de la atención se presentó como “puntuación de alineamiento” en el artículo de Bahdanau de 2014. Esta puntuación indicaba qué parte del codificador debía enfocar el decodificador al generar una palabra de salida y, esencialmente, era un valor que representaba <strong>la correlación entre dos vectores</strong>.</p>
<p>Es probable que el equipo de investigación haya comenzado con la pregunta práctica: “¿Cómo se pueden cuantificar las relaciones entre palabras?”. Empezaron con la idea relativamente simple de calcular la similitud entre vectores y usar estos valores como pesos para sintetizar información contextual. De hecho, en los primeros documentos de diseño del equipo de investigación de Google (“Transformers: Iterative Self-Attention and Processing for Various Tasks”), se utilizaba un método similar a “puntuación de alineamiento” para representar las relaciones entre palabras, en lugar de los términos Q, K y V.</p>
<p>A continuación, seguiremos el proceso que los investigadores de Google siguieron para resolver el problema y entenderemos el mecanismo de atención. Comenzaremos con la idea básica de calcular la similitud entre vectores y explicaremos paso a paso cómo llegaron a completar finalmente la arquitectura del Transformer.</p>
<section id="los-límites-del-rnn-y-el-nacimiento-de-la-atención" class="level3">
<h3 class="anchored" data-anchor-id="los-límites-del-rnn-y-el-nacimiento-de-la-atención">8.2.1 Los límites del RNN y el nacimiento de la atención</h3>
<p>El equipo de investigación primero quiso comprender claramente los límites del RNN. A través de experimentos, confirmaron que a medida que aumentaba la longitud de las oraciones, especialmente cuando superaban las 50 palabras, la puntuación BLEU disminuía drásticamente. Un problema aún mayor era que, debido al procesamiento secuencial del RNN, incluso con el uso de GPU, era difícil mejorar significativamente la velocidad. Para superar estas limitaciones, el equipo realizó un análisis profundo del mecanismo de atención propuesto por Bahdanau et al.&nbsp;(2014). La atención permitía que el decodificador consultara todos los estados del codificador, lo que mitigaba el problema de las dependencias a larga distancia. A continuación se presenta una implementación básica del mecanismo de atención.</p>
<div id="cell-3" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip install dldna[colab] <span class="co"># in Colab</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="co"># !pip install dldna[all] # in your local</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>load_ext autoreload</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>autoreload <span class="dv">2</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="cell-4" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Example word vectors (3-dimensional)</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>word_vectors <span class="op">=</span> {</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">'time'</span>: np.array([<span class="fl">0.2</span>, <span class="fl">0.8</span>, <span class="fl">0.3</span>]),   <span class="co"># In reality, these would be hundreds of dimensions</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">'flies'</span>: np.array([<span class="fl">0.7</span>, <span class="fl">0.2</span>, <span class="fl">0.9</span>]),</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>    <span class="st">'like'</span>: np.array([<span class="fl">0.3</span>, <span class="fl">0.5</span>, <span class="fl">0.2</span>]),</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>    <span class="st">'an'</span>: np.array([<span class="fl">0.1</span>, <span class="fl">0.3</span>, <span class="fl">0.4</span>]),</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>    <span class="st">'arrow'</span>: np.array([<span class="fl">0.8</span>, <span class="fl">0.1</span>, <span class="fl">0.6</span>])</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> calculate_similarity_matrix(word_vectors):</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Calculates the similarity matrix between word vectors."""</span></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> np.vstack(<span class="bu">list</span>(word_vectors.values()))</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.dot(X, X.T)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>The autoreload extension is already loaded. To reload it, use:
  %reload_ext autoreload</code></pre>
</div>
</div>
</section>
<section id="conceptos-básicos-de-la-atención" class="level3">
<h3 class="anchored" data-anchor-id="conceptos-básicos-de-la-atención">8.2.2 Conceptos básicos de la atención</h3>
<p>El contenido explicado en esta sección proviene del documento de diseño inicial “Transformers: Iterative Self-Attention and Processing for Various Tasks”. A continuación, analizaremos paso a paso el código utilizado para explicar los conceptos básicos de la atención. Primero, solo consideremos la matriz de similitud (pasos 1 y 2 del código fuente). Las palabras suelen tener cientos de dimensiones. Aquí, por ejemplo, se representan con vectores de 3 dimensiones. Cuando se forman en una matriz, cada columna es un vector de palabra. Al transponer esta matriz, los vectores de palabra se convierten en vectores fila. Al realizar la operación entre estas dos matrices, cada elemento (i, j) representa el producto escalar entre el i-ésimo y el j-ésimo vector de palabra, lo que indica la distancia (similitud) entre las dos palabras.</p>
<div id="cell-6" class="cell" data-execution_count="8">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> visualize_similarity_matrix(words, similarity_matrix):</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Visualizes the similarity matrix in ASCII art format."""</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>    max_word_len <span class="op">=</span> <span class="bu">max</span>(<span class="bu">len</span>(word) <span class="cf">for</span> word <span class="kw">in</span> words)</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>    col_width <span class="op">=</span> max_word_len <span class="op">+</span> <span class="dv">4</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>    header <span class="op">=</span> <span class="st">" "</span> <span class="op">*</span> (col_width) <span class="op">+</span> <span class="st">""</span>.join(<span class="ss">f"</span><span class="sc">{</span>word<span class="sc">:</span><span class="op">&gt;</span>{col_width}<span class="sc">}</span><span class="ss">"</span> <span class="cf">for</span> word <span class="kw">in</span> words)</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(header)</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, word <span class="kw">in</span> <span class="bu">enumerate</span>(words):</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>        row_str <span class="op">=</span> <span class="ss">f"</span><span class="sc">{</span>word<span class="sc">:</span><span class="op">&lt;</span>{col_width}<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>        row_values <span class="op">=</span> [<span class="ss">f"</span><span class="sc">{</span>similarity_matrix[i, j]<span class="sc">:.2f}</span><span class="ss">"</span> <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(words))]</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>        row_str <span class="op">+=</span> <span class="st">""</span>.join(<span class="ss">f"[</span><span class="sc">{</span>value<span class="sc">:</span><span class="op">&gt;</span>{col_width<span class="op">-</span><span class="dv">2</span>}<span class="sc">}</span><span class="ss">]"</span> <span class="cf">for</span> value <span class="kw">in</span> row_values)</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(row_str)</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Example word vectors (in practice, these would have hundreds of dimensions)</span></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>word_vectors <span class="op">=</span> {</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>    <span class="st">'time'</span>: np.array([<span class="fl">0.2</span>, <span class="fl">0.8</span>, <span class="fl">0.3</span>]),</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>    <span class="st">'flies'</span>: np.array([<span class="fl">0.7</span>, <span class="fl">0.2</span>, <span class="fl">0.9</span>]),</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>    <span class="st">'like'</span>: np.array([<span class="fl">0.3</span>, <span class="fl">0.5</span>, <span class="fl">0.2</span>]),</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>    <span class="st">'an'</span>: np.array([<span class="fl">0.1</span>, <span class="fl">0.3</span>, <span class="fl">0.4</span>]),</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>    <span class="st">'arrow'</span>: np.array([<span class="fl">0.8</span>, <span class="fl">0.1</span>, <span class="fl">0.6</span>])</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>words <span class="op">=</span> <span class="bu">list</span>(word_vectors.keys()) <span class="co"># Preserve order</span></span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Convert word vectors into a matrix</span></span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.vstack([word_vectors[word] <span class="cf">for</span> word <span class="kw">in</span> words])</span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Calculate the similarity matrix (dot product)</span></span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>similarity_matrix <span class="op">=</span> calculate_similarity_matrix(word_vectors)</span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a><span class="co"># Print results</span></span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Input matrix shape:"</span>, X.shape)</span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Input matrix:</span><span class="ch">\n</span><span class="st">"</span>, X)</span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Input matrix transpose:</span><span class="ch">\n</span><span class="st">"</span>, X.T)</span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Similarity matrix shape:"</span>, similarity_matrix.shape)</span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Similarity matrix:"</span>) <span class="co"># Output from visualize_similarity_matrix</span></span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a>visualize_similarity_matrix(words, similarity_matrix)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Input matrix shape: (5, 3)
Input matrix:
 [[0.2 0.8 0.3]
 [0.7 0.2 0.9]
 [0.3 0.5 0.2]
 [0.1 0.3 0.4]
 [0.8 0.1 0.6]]

Input matrix transpose:
 [[0.2 0.7 0.3 0.1 0.8]
 [0.8 0.2 0.5 0.3 0.1]
 [0.3 0.9 0.2 0.4 0.6]]

Similarity matrix shape: (5, 5)
Similarity matrix:
              time    flies     like       an    arrow
time     [   0.77][   0.57][   0.52][   0.38][   0.42]
flies    [   0.57][   1.34][   0.49][   0.49][   1.12]
like     [   0.52][   0.49][   0.38][   0.26][   0.41]
an       [   0.38][   0.49][   0.26][   0.26][   0.35]
arrow    [   0.42][   1.12][   0.41][   0.35][   1.01]</code></pre>
</div>
</div>
<p>Por ejemplo, el valor del elemento (1,2) de la matriz de similitud 0.57 representa la distancia (similitud) entre los vectores de times en el eje de las filas y flies en el eje de las columnas. Esto se puede expresar matemáticamente de la siguiente manera.</p>
<ul>
<li>Matriz X de vectores de palabras de la oración</li>
</ul>
<p><span class="math inline">\(\mathbf{X} = \begin{bmatrix}
\mathbf{x_1} \\
\mathbf{x_2} \\
\vdots \\
\mathbf{x_n}
\end{bmatrix}\)</span></p>
<ul>
<li>Transpuesta de X</li>
</ul>
<p><span class="math inline">\(\mathbf{X}^T = \begin{bmatrix}
\mathbf{x_1}^T &amp; \mathbf{x_2}^T &amp; \cdots &amp; \mathbf{x_n}^T
\end{bmatrix}\)</span></p>
<ul>
<li>Operación <span class="math inline">\(\mathbf{X}\mathbf{X}^T\)</span></li>
</ul>
<p><span class="math inline">\(\mathbf{X}\mathbf{X}^T = \begin{bmatrix}
\mathbf{x_1} \cdot \mathbf{x_1} &amp; \mathbf{x_1} \cdot \mathbf{x_2} &amp; \cdots &amp; \mathbf{x_1} \cdot \mathbf{x_n} \\
\mathbf{x_2} \cdot \mathbf{x_1} &amp; \mathbf{x_2} \cdot \mathbf{x_2} &amp; \cdots &amp; \mathbf{x_2} \cdot \mathbf{x_n} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\mathbf{x_n} \cdot \mathbf{x_1} &amp; \mathbf{x_n} \cdot \mathbf{x_2} &amp; \cdots &amp; \mathbf{x_n} \cdot \mathbf{x_n}
\end{bmatrix}\)</span></p>
<ul>
<li>Cada elemento (i,j)</li>
</ul>
<p><span class="math inline">\((\mathbf{X}\mathbf{X}^T)_{ij} = \mathbf{x_i} \cdot \mathbf{x_j} = \sum_{k=1}^d x_{ik}x_{jk}\)</span></p>
<p>Cada elemento de esta matriz n×n es el producto escalar entre dos vectores de palabras, y por lo tanto representa la distancia (similitud) entre las dos palabras. Esto son los “puntajes de atención”.</p>
<p>El siguiente es el paso de convertir la matriz de similitud en una matriz de pesos utilizando el softmax, que consta de 3 etapas.</p>
<div id="cell-9" class="cell" data-execution_count="9">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. Convert similarities to weights (probability distribution) (softmax)</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> softmax(x):</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>    exp_x <span class="op">=</span> np.exp(x <span class="op">-</span> np.<span class="bu">max</span>(x, axis<span class="op">=-</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>))  <span class="co"># trick for stability</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> exp_x <span class="op">/</span> exp_x.<span class="bu">sum</span>(axis<span class="op">=-</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>attention_weights <span class="op">=</span> softmax(similarity_matrix)</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Attention weights shape:"</span>, attention_weights.shape)</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Attention weights:</span><span class="ch">\n</span><span class="st">"</span>, attention_weights)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Attention weights shape: (5, 5)
Attention weights:
 [[0.25130196 0.20574865 0.19571417 0.17014572 0.1770895 ]
 [0.14838442 0.32047566 0.13697608 0.13697608 0.25718775]
 [0.22189237 0.21533446 0.19290396 0.17109046 0.19877876]
 [0.20573742 0.22966017 0.18247272 0.18247272 0.19965696]
 [0.14836389 0.29876818 0.14688764 0.13833357 0.26764673]]</code></pre>
</div>
</div>
<p>Los pesos de atención se aplican utilizando la función softmax. Realizan dos transformaciones clave:</p>
<ol type="1">
<li>Convertir las puntuaciones de similitud a valores entre 0 y 1.</li>
<li>Asegurar que la suma de cada fila sea 1, convirtiéndolas en una distribución de probabilidad.</li>
</ol>
<p>Al convertir la matriz de similitud en pesos, se expresa probabilísticamente la relevancia de una palabra con respecto a las demás palabras. Dado que tanto los ejes de filas como columnas siguen el orden de las palabras en la oración, la primera fila de pesos corresponde a la fila de la palabra ‘time’, y las columnas representan todas las palabras de la oración. Por lo tanto,</p>
<ol type="1">
<li>Se expresa la relación entre ‘time’ y otras palabras (‘flies’, ‘like’, ‘an’, ‘arrow’) en términos de valores de probabilidad.</li>
<li>La suma de estos valores de probabilidad es 1.</li>
<li>Un valor de probabilidad alto indica una mayor relevancia.</li>
</ol>
<p>Estos pesos transformados se utilizan en el siguiente paso para ponderar la oración. Al aplicar estas ponderaciones, cada palabra de la oración refleja cuánta información contiene. Esto equivale a decidir qué tan atentas deben ser las palabras al “referirse” a la información de otras palabras.</p>
<div id="cell-11" class="cell" data-execution_count="17">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 4. Generate contextualized representations using the weights</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>contextualized_vectors <span class="op">=</span> np.dot(attention_weights, X)</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Contextualized vectors shape:"</span>, contextualized_vectors.shape)</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Contextualized vectors:</span><span class="ch">\n</span><span class="st">"</span>, contextualized_vectors)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
Contextualized vectors shape: (5, 3)
Contextualized vectors:
 [[0.41168487 0.40880105 0.47401919]
 [0.51455048 0.31810231 0.56944172]
 [0.42911583 0.38823778 0.48665295]
 [0.43462426 0.37646585 0.49769319]
 [0.51082753 0.32015331 0.55869952]]</code></pre>
</div>
</div>
<p>El producto punto entre la matriz de pesos y la matriz de palabras (compuesta por vectores de palabras) necesita una interpretación. Si asumimos que la primera fila de <code>attention_weights</code> es [0.5, 0.2, 0.1, 0.1, 0.1], cada valor representa la probabilidad de relevancia de ‘time’ con las demás palabras. La primera fila de pesos puede expresarse como <span class="math inline">\(\begin{bmatrix} \alpha_{11} &amp; \alpha_{12} &amp; \alpha_{13} &amp; \alpha_{14} &amp; \alpha_{15} \end{bmatrix}\)</span>, por lo que la operación con la matriz de palabras para esta fila de pesos se puede expresar así:</p>
<p><span class="math inline">\(\begin{bmatrix} \alpha_{11} &amp; \alpha_{12} &amp; \alpha_{13} &amp; \alpha_{14} &amp; \alpha_{15} \end{bmatrix} \begin{bmatrix} \vec{v}_{\text{time}} \ \vec{v}_{\text{flies}} \ \vec{v}_{\text{like}} \ \vec{v}_{\text{an}} \ \vec{v}_{\text{arrow}} \end{bmatrix}\)</span></p>
<p>Esto se puede representar en código Python de la siguiente manera.</p>
<div id="cell-13" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>time_contextualized <span class="op">=</span> <span class="fl">0.5</span><span class="op">*</span>time_vector <span class="op">+</span> <span class="fl">0.2</span><span class="op">*</span>flies_vector <span class="op">+</span> <span class="fl">0.1</span><span class="op">*</span>like_vector <span class="op">+</span> <span class="fl">0.1</span><span class="op">*</span>an_vector <span class="op">+</span> <span class="fl">0.1</span><span class="op">*</span>arrow_vector</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="co"># 0.5는 time과 time의 관련도 확률값</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="co"># 0.2는 time과 files의 관련도 확률값</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>La operación multiplica estas probabilidades (donde el tiempo está relacionado con la probabilidad de cada palabra) por los vectores originales de cada palabra y luego suma todos. Como resultado, <strong>el nuevo vector de ‘time’ refleja un promedio ponderado de los significados de las otras palabras</strong>, según su relevancia. El punto clave es que se calcula un promedio ponderado. Por lo tanto, fue necesario un paso previo para obtener la matriz de pesos que se utiliza para calcular el promedio ponderado.</p>
<p>La forma del vector contextualizado final es (5, 3), ya que esto resulta de multiplicar una matriz de pesos de atención de tamaño (5,5) por una matriz de vectores de palabras X de tamaño (5,3), lo que da como resultado (5,5) @ (5,3) = (5,3).</p>
<p>Sure, please provide the Korean text you want to be translated into Spanish.</p>
</section>
<section id="evolución-hacia-la-autoatención" class="level3">
<h3 class="anchored">8.2.3 Evolución hacia la autoatención</h3>
<p>El equipo de investigación de Google analizó el mecanismo de atención básico (sección 8.2.2) y descubrió varias <strong>limitaciones</strong>. El problema más importante era que los vectores de palabras realizaban tareas múltiples, como el <strong>cálculo de similitud</strong> y la <strong>transmisión de información</strong>, lo cual resultaba ineficiente. Por ejemplo, la palabra “bank” puede tener <em>significados diferentes</em> según el contexto, como “banco” o “orilla del río”, y por lo tanto, sus <em>relaciones con otras palabras</em> también deben ser diferentes. Sin embargo, un <strong>único vector</strong> no podía representar adecuadamente estos diversos significados y relaciones.</p>
<p>El equipo buscó una forma de optimizar cada rol de manera <strong>independiente</strong>. Esto se asemeja a cómo en las CNN, los filtros aprenden a extraer características de imágenes de manera <em>aprendible</em>, permitiendo que en el mecanismo de atención, cada rol tenga representaciones <em>aprendidas</em> especializadas. Esta idea comenzó con la transformación de los vectores de palabras en espacios para diferentes roles.</p>
<p><strong>Limitaciones del concepto básico (ejemplo de código)</strong></p>
<div id="cell-17" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> basic_self_attention(word_vectors):</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>    similarity_matrix <span class="op">=</span> np.dot(word_vectors, word_vectors.T)</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>    attention_weights <span class="op">=</span> softmax(similarity_matrix)</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>    contextualized_vectors <span class="op">=</span> np.dot(attention_weights, word_vectors)</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> contextualized_vectors</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>En el siguiente código, <code>word_vectors</code> cumple tres roles simultáneos:</p>
<ol type="1">
<li><strong>Sujeto de cálculo de similitud:</strong> Se utiliza para calcular la similitud con otras palabras.</li>
<li><strong>Objeto de cálculo de similitud:</strong> Se le calcula la similitud desde otras palabras.</li>
<li><strong>Transmisión de información:</strong> Se usa en el promedio ponderado para generar el vector contextual final.</li>
</ol>
<p><strong>Primer mejoramiento: Separación del rol de transmisión de información</strong></p>
<p>El equipo de investigación primero separó el <strong>rol de transmisión de información</strong>. El método más simple para separar los roles de un vector en álgebra lineal es usar una <em>matriz de aprendizaje</em> separada para realizar una <em>transformación lineal (linear transformation)</em> del vector a un nuevo espacio.</p>
<div id="cell-19" class="cell" data-execution_count="10">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> improved_self_attention(word_vectors, W_similarity, W_content):</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>    similarity_vectors <span class="op">=</span> np.dot(word_vectors, W_similarity)</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>    content_vectors <span class="op">=</span> np.dot(word_vectors, W_content)</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate similarity by taking the dot product between similarity_vectors</span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>    attention_scores <span class="op">=</span> np.dot(similarity_vectors, similarity_vectors.T)</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Convert to probability distribution using softmax</span></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>    attention_weights <span class="op">=</span> softmax(attention_scores)</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Generate the final contextualized representation by multiplying weights and content_vectors</span></span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>    contextualized_vectors <span class="op">=</span> np.dot(attention_weights, content_vectors)</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> contextualized_vectors</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<ul>
<li><code>W_similarity</code>: matriz <em>aprendible</em> que proyecta los vectores de palabras a un espacio optimizado para el cálculo de similitud.</li>
<li><code>W_content</code>: matriz <em>aprendible</em> que proyecta los vectores de palabras a un espacio optimizado para la transmisión de información.</li>
</ul>
<p>Con esta mejora, <code>similarity_vectors</code> se especializó en el cálculo de similitud y <code>content_vectors</code> en la transmisión de información. Esto sentó las bases para el concepto previo de agregación de información a través del Value.</p>
<p><strong>Segunda Mejora: Separación total del papel de similitud (nacimiento de Q, K)</strong></p>
<p>El siguiente paso fue separar el proceso de cálculo de similitud en dos roles distintos. En lugar de que <code>similarity_vectors</code> desempeñara tanto el rol de “hacer preguntas” (Query) como el de “dar respuestas” (Key), estos dos roles se desarrollaron para estar <em>completamente separados</em>.</p>
<div id="cell-21" class="cell" data-execution_count="11">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SelfAttention(nn.Module):</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, embed_dim):</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 각각의 역할을 위한 독립적인 선형 변환</span></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.q <span class="op">=</span> nn.Linear(embed_dim, embed_dim)  <span class="co"># 질문(Query)을 위한 변환</span></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.k <span class="op">=</span> nn.Linear(embed_dim, embed_dim)  <span class="co"># 답변(Key)을 위한 변환</span></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.v <span class="op">=</span> nn.Linear(embed_dim, embed_dim)  <span class="co"># 정보 전달(Value)을 위한 변환</span></span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>        Q <span class="op">=</span> <span class="va">self</span>.q(x)  <span class="co"># 질문자로서의 표현</span></span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>        K <span class="op">=</span> <span class="va">self</span>.k(x)  <span class="co"># 응답자로서의 표현</span></span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>        V <span class="op">=</span> <span class="va">self</span>.v(x)  <span class="co"># 전달할 정보의 표현</span></span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 질문과 답변 간의 관련성(유사도) 계산</span></span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a>        scores <span class="op">=</span> torch.matmul(Q, K.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>))</span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a>        weights <span class="op">=</span> F.softmax(scores, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 관련성에 따른 정보 집계 (가중 평균)</span></span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.matmul(weights, V)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><strong>Significado de la separación del espacio Q, K, V</strong></p>
<p>Intercambiar el orden de Q y K (<span class="math inline">\(QK^T\)</span> en lugar de <span class="math inline">\(KQ^T\)</span>) también nos da una matriz de similitud idéntica desde un punto de vista matemático. Si solo consideramos las matemáticas, ¿por qué se nombran estos dos espacios como “consulta (Query)”, “clave (Key)”? La clave está en <em>optimizar separadamente los espacios para mejorar el cálculo de la similitud</em>. Estos nombres parecen surgir del hecho de que el mecanismo de atención del modelo Transformer se inspira en los sistemas de recuperación de información (Information Retrieval). En los sistemas de búsqueda, “consulta (Query)” representa la información que el usuario busca y “clave (Key)” juega un papel similar a las palabras clave de cada documento. La atención imita el proceso de buscar información relevante calculando la similitud entre consultas y claves.</p>
<p>Por ejemplo:</p>
<ul>
<li>“I need to deposit money in the bank” (banco)</li>
<li>“The river bank is covered with flowers” (orilla del río)</li>
</ul>
<p>En las dos oraciones anteriores, “bank” tiene diferentes significados dependiendo del contexto. Al separar los espacios Q y K,</p>
<ul>
<li>“bank” y otras palabras se distribuyen de <em>formas diferentes</em> en los espacios Q y K para optimizar el cálculo de similitud.</li>
<li>En un contexto financiero, los vectores se colocan para que la similitud con ‘money’, ‘deposit’ sea alta en ambos espacios.</li>
<li>En un contexto geográfico, los vectores se distribuyen para que la similitud con ‘river’, ‘covered’ sea alta.</li>
</ul>
<p>En otras palabras, el par Q-K <em>realiza el producto interno en dos espacios optimizados</em> para calcular la similitud. Lo importante es que los espacios Q y K están <em>optimizados a través del aprendizaje</em>. Es probable que el equipo de investigación de Google haya descubierto que las matrices Q y K se optimizan durante el proceso de aprendizaje para funcionar de manera similar a consultas y claves.</p>
<p><strong>Importancia de la separación del espacio Q, K</strong></p>
<p>Otra ventaja obtenida al separar Q y K es <em>aumentar la flexibilidad</em>. Si Q y K están en el mismo espacio, los métodos de cálculo de similitud pueden estar limitados (por ejemplo, similitud simétrica). Sin embargo, al separar Q y K, se pueden aprender relaciones más complejas y asimétricas (por ejemplo, “A es la causa de B”). Además, a través de transformaciones diferentes (<span class="math inline">\(W^Q\)</span>, <span class="math inline">\(W^K\)</span>), Q y K pueden representar los roles de cada palabra con mayor detalle, aumentando la expresividad del modelo. Finalmente, al separar los espacios Q y K, se clarifican mejor los objetivos de optimización de cada espacio: el espacio Q aprende a representar adecuadamente las consultas y el espacio K aprende a representar adecuadamente las respuestas.</p>
<p><strong>El papel de V</strong></p>
<p>Si Q y K son espacios para calcular similitud, V es un espacio que <em>contiene la información real</em> que se va a transmitir. La transformación al espacio V se optimiza para expresar mejor la información semántica de las palabras. Mientras que Q y K determinan “qué información de qué palabras se reflejará”, V se encarga de “qué información real se transmitirá”. En el ejemplo de “bank”,</p>
<ul>
<li>Q y K calculan la similitud con palabras relacionadas con finanzas según el contexto,</li>
<li>V expresa la información semántica del ‘bank’ como institución financiera.</li>
</ul>
<p>Esta separación en tres espacios optimiza independientemente “cómo se encuentra la información (Q, K)” y “qué contenido de la información se transmite (V)”, similar a cómo las CNN separan “qué patrones encontrarán (aprendizaje del filtro)” y “cómo expresar los patrones encontrados (aprendizaje del canal)”.</p>
<p><strong>Expresión matemática de la atención</strong></p>
<p>El mecanismo final de atención se expresa con la siguiente fórmula:</p>
<p><span class="math display">\[\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V\]</span> * <span class="math inline">\(Q \in \mathbb{R}^{n \times d_k}\)</span>: Matriz de consulta * <span class="math inline">\(K \in \mathbb{R}^{n \times d_k}\)</span>: Matriz de clave * <span class="math inline">\(V \in \mathbb{R}^{n \times d_v}\)</span>: Matriz de valor (<span class="math inline">\(d_v\)</span> es generalmente igual a <span class="math inline">\(d_k\)</span>) * <span class="math inline">\(n\)</span>: Longitud de la secuencia * <span class="math inline">\(d_k\)</span>: Dimensión de los vectores de consulta y clave * <span class="math inline">\(d_v\)</span>: Dimensión del vector de valor * <span class="math inline">\(\frac{QK^T}{\sqrt{d_k}}\)</span>: Scaled Dot-Product Attention. A medida que las dimensiones aumentan, los valores de producto interno también lo hacen para evitar la desaparición del gradiente al pasar por la función softmax.</p>
<p>Esta estructura avanzada se convirtió en un elemento clave de los transformadores y posteriormente en la base de modelos de lenguaje modernos como BERT y GPT.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Haga clic para ver el contenido (inmersión teórica: comprensión integral del mecanismo de autoatención y teoría más reciente)">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Haga clic para ver el contenido (inmersión teórica: comprensión integral del mecanismo de autoatención y teoría más reciente)
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<section id="comprensión-integral-del-mecanismo-de-atención-propia-y-teoría-más-reciente" class="level2 callout-body-container callout-body">
<h2 class="anchored" data-anchor-id="comprensión-integral-del-mecanismo-de-atención-propia-y-teoría-más-reciente">Comprensión integral del mecanismo de atención propia y teoría más reciente</h2>
<section id="principios-matemáticos-y-complejidad-computacional" class="level3">
<h3 class="anchored" data-anchor-id="principios-matemáticos-y-complejidad-computacional">1. Principios matemáticos y complejidad computacional</h3>
<p>La autoatención genera nuevas representaciones que reflejan el contexto al calcular la relación entre cada palabra en una secuencia de entrada con todas las demás palabras, incluyéndose a sí misma. Este proceso se compone principalmente de tres etapas.</p>
<ol type="1">
<li><p><strong>Generación de Query, Key, Value:</strong></p>
<p>Para cada vector de incrustación (embedding) de palabra (<span class="math inline">\(x_i\)</span>) en la secuencia de entrada, se aplican tres transformaciones lineales para generar los vectores Query (<span class="math inline">\(q_i\)</span>), Key (<span class="math inline">\(k_i\)</span>), y Value (<span class="math inline">\(v_i\)</span>). Estas transformaciones se realizan usando matrices de pesos aprendibles (<span class="math inline">\(W^Q\)</span>,<span class="math inline">\(W^K\)</span>,<span class="math inline">\(W^V\)</span>).</p>
<p><span class="math inline">\(q_i = x_i W^Q\)</span></p>
<p><span class="math inline">\(k_i = x_i W^K\)</span></p>
<p><span class="math inline">\(v_i = x_i W^V\)</span></p>
<p><span class="math inline">\(W^Q, W^K, W^V \in \mathbb{R}^{d_{model} \times d_k}\)</span> : matrices de pesos aprendibles. (<span class="math inline">\(d_{model}\)</span>: dimensión del embedding,<span class="math inline">\(d_k\)</span>: dimensión de los vectores query, key, value)</p></li>
<li><p><strong>Cálculo y normalización de las puntuaciones de atención</strong></p>
<p>Se calcula la puntuación de atención (attention score) para cada par de palabras tomando el producto punto (dot product) entre los vectores Query y Key.</p>
<p><span class="math display">\[\text{score}(q_i, k_j) = q_i \cdot k_j^T\]</span></p>
<p>Esta puntuación indica cuán relacionadas están las dos palabras. Después del cálculo del producto punto, se realiza una escala (scaling) para evitar que los valores sean demasiado grandes y mitigar el problema de desvanecimiento del gradiente (gradient vanishing). La escala se aplica dividiendo por la raíz cuadrada de la dimensión del vector Key (<span class="math inline">\(d_k\)</span>).</p>
<p><span class="math display">\[\text{scaled score}(q_i, k_j) = \frac{q_i \cdot k_j^T}{\sqrt{d_k}}\]</span></p>
<p>Finalmente, se aplica la función softmax para normalizar las puntuaciones de atención y obtener los pesos de atención (attention weight) para cada palabra.</p>
<p><span class="math display">\[\alpha_{ij} = \text{softmax}(\text{scaled score}(q_i, k_j)) = \frac{\exp(\text{scaled score}(q_i, k_j))}{\sum_{l=1}^{n} \exp(\text{scaled score}(q_i, k_l))}\]</span></p>
<p>Aquí,<span class="math inline">\(\alpha_{ij}\)</span> es el peso de atención que la<span class="math inline">\(i\)</span>-ésima palabra da a la<span class="math inline">\(j\)</span>-ésima palabra,<span class="math inline">\(n\)</span> es la longitud de la secuencia.</p></li>
<li><p><strong>Cálculo del promedio ponderado</strong></p>
<p>Se calcula el promedio ponderado (weighted average) de los vectores Value (<span class="math inline">\(v_j\)</span>) utilizando los pesos de atención (<span class="math inline">\(\alpha_{ij}\)</span>). Este promedio ponderado se convierte en un vector contextual (<span class="math inline">\(c_i\)</span>) que resume la información de todas las palabras en la secuencia de entrada.</p></li>
</ol>
<p><span class="math display">\[c_i = \sum_{j=1}^{n} \alpha_{ij} v_j\]</span></p>
<p><strong>Representación del proceso completo en forma matricial</strong></p>
<p>Dada una matriz de incrustaciones de entrada <span class="math inline">\(X \in \mathbb{R}^{n \times d_{model}}\)</span>, el proceso completo de autoatención se puede expresar como:</p>
<p><span class="math display">\[\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V\]</span></p>
<p>Aquí,<span class="math inline">\(Q = XW^Q\)</span>,<span class="math inline">\(K = XW^K\)</span>,<span class="math inline">\(V = XW^V\)</span>.</p>
<p><strong>Complejidad computacional</strong></p>
<p>La complejidad computacional de la autoatención es <span class="math inline">\(O(n^2)\)</span> con respecto a la longitud de la secuencia de entrada (<span class="math inline">\(n\)</span>). Esto se debe a que cada palabra debe calcular su relación con todas las demás palabras. * <strong>Cálculo de <span class="math inline">\(QK^T\)</span>:</strong> Se necesita un cálculo de <span class="math inline">\(O(n^2d_k)\)</span> ya que se realiza la operación de producto interno entre <span class="math inline">\(n\)</span> vectores de consulta y <span class="math inline">\(n\)</span> vectores clave. * <strong>Operación softmax:</strong> Se necesita una complejidad de cálculo de <span class="math inline">\(O(n^2)\)</span>, ya que se realiza la operación softmax para calcular los pesos de atención para cada consulta con respecto a las <span class="math inline">\(n\)</span> claves. * <strong>Promedio ponderado con <span class="math inline">\(V\)</span>:</strong> Se necesita una complejidad de cálculo de <span class="math inline">\(O(n^2d_k)\)</span> ya que se deben multiplicar <span class="math inline">\(n\)</span> vectores valor y <span class="math inline">\(n\)</span> pesos de atención.</p>
</section>
<section id="ampliación-desde-la-perspectiva-del-aprendizaje-de-máquinas" class="level3">
<h3 class="anchored" data-anchor-id="ampliación-desde-la-perspectiva-del-aprendizaje-de-máquinas">2. Ampliación desde la perspectiva del aprendizaje de máquinas</h3>
<section id="función-kernel-asimétrica" class="level4">
<h4 class="anchored" data-anchor-id="función-kernel-asimétrica">2.1 Función kernel asimétrica</h4>
<p>Interpretación de la atención como una función kernel asimétrica: <span class="math inline">\(K(Q_i, K_j) = \exp\left(\frac{Q_i \cdot K_j}{\sqrt{d_k}}\right)\)</span></p>
<p>Este kernel aprende un mapeo de características que reconstruye el espacio de entrada.</p>
</section>
<section id="análisis-de-descomposición-en-valores-singulares-svd" class="level4">
<h4 class="anchored" data-anchor-id="análisis-de-descomposición-en-valores-singulares-svd">2.2 Análisis de descomposición en valores singulares (SVD)</h4>
<p>Descomposición SVD asimétrica de la matriz de atención:</p>
<p><span class="math inline">\(A = U\Sigma V^T \quad \text{donde } \Sigma = \text{diag}(\sigma_1, \sigma_2, ...)\)</span></p>
<p>-<span class="math inline">\(U\)</span>: direcciones principales del espacio de consulta (patrones de solicitud de contexto) -<span class="math inline">\(V\)</span>: direcciones principales del espacio clave (patrones de suministro de información) -<span class="math inline">\(\sigma_i\)</span>: intensidad de interacción (observación de concentración explicativa ≥0.9)</p>
</section>
</section>
<section id="modelos-y-dinámicas-basados-en-energía" class="level3">
<h3 class="anchored" data-anchor-id="modelos-y-dinámicas-basados-en-energía">3. Modelos y dinámicas basados en energía</h3>
<section id="formalización-de-la-función-de-energía" class="level4">
<h4 class="anchored" data-anchor-id="formalización-de-la-función-de-energía">3.1 Formalización de la función de energía</h4>
<p><span class="math inline">\(E(Q,K,V) = -\sum_{i,j} \frac{Q_i \cdot K_j}{\sqrt{d_k}}V_j + \text{función de partición logarítmica}\)</span></p>
<p>La salida se interpreta como un proceso de minimización de energía:</p>
<p><span class="math inline">\(\text{Salida} = \arg\min_V E(Q,K,V)\)</span></p>
</section>
<section id="equivalencia-con-redes-de-hopfield" class="level4">
<h4 class="anchored" data-anchor-id="equivalencia-con-redes-de-hopfield">3.2 Equivalencia con redes de Hopfield</h4>
<p>Ecuaciones de red de Hopfield continua: <span class="math inline">\(\tau\frac{dX}{dt} = -X + \text{softmax}(XWX^T)XW\)</span></p>
<p>donde <span class="math inline">\(\tau\)</span> es una constante de tiempo, y <span class="math inline">\(W\)</span> es la matriz de intensidades de conexión aprendida.</p>
</section>
</section>
<section id="análisis-de-estabilidad" class="level3">
<h3 class="anchored" data-anchor-id="análisis-de-estabilidad">4. Análisis de estabilidad</h3>
<section id="estabilidad-de-lyapunov" class="level4">
<h4 class="anchored" data-anchor-id="estabilidad-de-lyapunov">5.1 Estabilidad de Lyapunov</h4>
<p><span class="math inline">\(V(X) = \|X - X^*\|^2\)</span> función decreciente</p>
<p>Las actualizaciones de atención garantizan la estabilidad asintótica.</p>
</section>
<section id="interpretación-en-el-dominio-de-frecuencia" class="level4">
<h4 class="anchored" data-anchor-id="interpretación-en-el-dominio-de-frecuencia">5.2 Interpretación en el dominio de frecuencia</h4>
<p>Espectro de atención después de aplicar transformada de Fourier:</p>
<p><span class="math inline">\(\mathcal{F}(A)_{kl} = \sum_{m,n} A_{mn}e^{-i2\pi(mk/M+nl/N)}\)</span></p>
<p>Los componentes de baja frecuencia capturan más del 80% de la información.</p>
</section>
</section>
<section id="interpretación-teórica-de-la-información" class="level3">
<h3 class="anchored" data-anchor-id="interpretación-teórica-de-la-información">6. Interpretación teórica de la información</h3>
<section id="maximización-de-información-mutua" class="level4">
<h4 class="anchored" data-anchor-id="maximización-de-información-mutua">6.1 Maximización de información mutua</h4>
<p><span class="math inline">\(\max I(X;Y) = H(Y) - H(Y|X) \quad \text{s.t. } Y = \text{Attention}(X)\)</span></p>
<p>La softmax genera la distribución óptima que maximiza la entropía <span class="math inline">\(H(Y)\)</span>.</p>
</section>
<section id="análisis-de-relación-señal-ruido-snr" class="level4">
<h4 class="anchored" data-anchor-id="análisis-de-relación-señal-ruido-snr">6.2 Análisis de relación señal-ruido (SNR)</h4>
<p>Atenuación del SNR con respecto a la profundidad de capa <span class="math inline">\(l\)</span>:</p>
<p><span class="math inline">\(\text{SNR}^{(l)} \propto e^{-0.2l} \quad \text{(basado en ResNet-50)}\)</span></p>
</section>
</section>
<section id="inspiración-neurocientífica" class="level3">
<h3 class="anchored" data-anchor-id="inspiración-neurocientífica">7. Inspiración neurocientífica</h3>
<section id="área-visual-v4" class="level4">
<h4 class="anchored" data-anchor-id="área-visual-v4">7.1 Área visual V4</h4>
<ul>
<li>Neuronas selectivas a la dirección ≈ cabezas de atención que responden a patrones específicos</li>
<li>Estructura jerárquica del campo receptivo ≈ atención multiscale</li>
</ul>
</section>
<section id="memoria-de-trabajo-prefrontal" class="level4">
<h4 class="anchored" data-anchor-id="memoria-de-trabajo-prefrontal">7.2 Memoria de trabajo prefrontal</h4>
<ul>
<li>Activación sostenida de neuronas ≈ procesamiento de dependencias a largo plazo en la atención</li>
<li>Mecanismo de retención de contexto ≈ técnica de masking en el decodificador</li>
</ul>
</section>
</section>
<section id="modelado-matemático-avanzado" class="level3">
<h3 class="anchored" data-anchor-id="modelado-matemático-avanzado">8. Modelado matemático avanzado</h3>
<section id="expansión-de-la-red-tensorial" class="level4">
<h4 class="anchored" data-anchor-id="expansión-de-la-red-tensorial">8.1 Expansión de la red tensorial</h4>
<p>Representación MPO (Operador Producto Matricial)</p>
<p><span class="math inline">\(A_{ij} = \sum_{\alpha=1}^r Q_{i\alpha}K_{j\alpha}\)</span> donde <span class="math inline">\(r\)</span> es la dimensión del enlace de la red tensorial</p>
</section>
<section id="interpretación-geométrica-diferencial" class="level4">
<h4 class="anchored" data-anchor-id="interpretación-geométrica-diferencial">8.2 Interpretación geométrica diferencial</h4>
<p>Curvatura riemanniana de la variedad de atención <span class="math inline">\(R_{ijkl} = \partial_i\Gamma_{jk}^m - \partial_j\Gamma_{ik}^m + \Gamma_{il}^m\Gamma_{jk}^l - \Gamma_{jl}^m\Gamma_{ik}^l\)</span></p>
<p>Es posible estimar las limitaciones de la capacidad expresiva del modelo a través del análisis de curvatura</p>
</section>
</section>
<section id="tendencias-de-investigación-recientes-2025" class="level3">
<h3 class="anchored" data-anchor-id="tendencias-de-investigación-recientes-2025">9. Tendencias de investigación recientes (2025)</h3>
<ol type="1">
<li><p><strong>Atención cuántica</strong></p>
<ul>
<li>Representación de consultas/llaves en estados de superposición cuántica: <span class="math inline">\(|\psi_Q\rangle = \sum c_i|i\rangle\)</span></li>
<li>Aceleración del producto interno cuántico</li>
</ul></li>
<li><p><strong>Optimización bioinspirada</strong></p>
<ul>
<li>Aplicación de la plasticidad dependiente del tiempo de las espinas (STDP)</li>
</ul>
<p><span class="math inline">\(\Delta W_{ij} \propto x_i x_j - \beta W_{ij}\)</span></p></li>
<li><p><strong>Ajuste energético dinámico</strong></p>
<ul>
<li>Afinado en tiempo real de funciones de energía basado en aprendizaje meta</li>
<li>Simulación integrada con motores físicos</li>
</ul></li>
</ol>
<hr>
</section>
<section id="referencias" class="level3">
<h3 class="anchored" data-anchor-id="referencias">Referencias</h3>
<ol type="1">
<li>Vaswani et al., “Attention Is All You Need”, NeurIPS 2017<br>
</li>
<li>Choromanski et al., “Rethinking Attention with Performers”, ICLR 2021<br>
</li>
<li>Ramsauer et al., “Hopfield Networks is All You Need”, ICLR 2021<br>
</li>
<li>Wang et al., “Linformer: Self-Attention with Linear Complexity”, arXiv 2020<br>
</li>
<li>Chen et al., “Theoretical Analysis of Self-Attention via Signal Propagation”, NeurIPS 2023</li>
</ol>
</section>
</section>
</div>
</div>
</section>
<section id="atención-multi-cabeza-y-procesamiento-en-paralelo" class="level3">
<h3 class="anchored" data-anchor-id="atención-multi-cabeza-y-procesamiento-en-paralelo">8.2.4 Atención multi-cabeza y procesamiento en paralelo</h3>
<p>El equipo de investigación de Google ideó una manera de mejorar aún más el rendimiento de la autoatención, planteando la idea de “¿Qué tal si capturamos diferentes tipos de relaciones <em>en múltiples espacios de atención pequeños</em> en lugar de un solo gran espacio de atención?”. Al igual que varios expertos analizan un problema desde sus respectivas perspectivas, pensaron que considerar diversos aspectos de la secuencia de entrada simultáneamente podría proporcionar información contextual más rica.</p>
<p>Basándose en esta idea, el equipo de investigación diseñó la <strong>atención multi-cabeza (Multi-Head Attention)</strong>, que divide los vectores Q, K y V en varios espacios pequeños para calcular la atención en paralelo. En el artículo original (“Attention is All You Need”), se procesó un embedding de 512 dimensiones dividiéndolo en 8 cabezas (heads) de 64 dimensiones cada una. Posteriormente, modelos como BERT expandieron aún más esta estructura (por ejemplo: BERT-base divide 768 dimensiones en 12 cabezas de 64 dimensiones).</p>
<p><strong>Funcionamiento de la atención multi-cabeza</strong></p>
<div id="cell-25" class="cell" data-execution_count="12">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> math</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MultiHeadAttention(nn.Module):</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config):</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span> config.hidden_size <span class="op">%</span> config.num_attention_heads <span class="op">==</span> <span class="dv">0</span></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.d_k <span class="op">=</span> config.hidden_size <span class="op">//</span> config.num_attention_heads  <span class="co"># Dimension of each head</span></span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.h <span class="op">=</span> config.num_attention_heads  <span class="co"># Number of heads</span></span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Linear transformation layers for Q, K, V, and output</span></span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.linear_layers <span class="op">=</span> nn.ModuleList([</span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>            nn.Linear(config.hidden_size, config.hidden_size)</span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">4</span>)  <span class="co"># For Q, K, V, and output</span></span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a>        ])</span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(config.attention_probs_dropout_prob) <span class="co"># added</span></span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.attention_weights <span class="op">=</span> <span class="va">None</span> <span class="co"># added</span></span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> attention(<span class="va">self</span>, query, key, value, mask<span class="op">=</span><span class="va">None</span>): <span class="co"># separate function</span></span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a>        scores <span class="op">=</span> torch.matmul(query, key.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>)) <span class="op">/</span> math.sqrt(<span class="va">self</span>.d_k) <span class="co"># scaled dot product</span></span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-25"><a href="#cb14-25" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> mask <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb14-26"><a href="#cb14-26" aria-hidden="true" tabindex="-1"></a>            scores <span class="op">=</span> scores.masked_fill(mask <span class="op">==</span> <span class="dv">0</span>, <span class="op">-</span><span class="fl">1e9</span>)</span>
<span id="cb14-27"><a href="#cb14-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-28"><a href="#cb14-28" aria-hidden="true" tabindex="-1"></a>        p_attn <span class="op">=</span> scores.softmax(dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb14-29"><a href="#cb14-29" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.attention_weights <span class="op">=</span> p_attn.detach()  <span class="co"># Store attention weights</span></span>
<span id="cb14-30"><a href="#cb14-30" aria-hidden="true" tabindex="-1"></a>        p_attn <span class="op">=</span> <span class="va">self</span>.dropout(p_attn)</span>
<span id="cb14-31"><a href="#cb14-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-32"><a href="#cb14-32" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.matmul(p_attn, value), p_attn</span>
<span id="cb14-33"><a href="#cb14-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-34"><a href="#cb14-34" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, query, key, value, mask<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb14-35"><a href="#cb14-35" aria-hidden="true" tabindex="-1"></a>        batch_size <span class="op">=</span> query.size(<span class="dv">0</span>)</span>
<span id="cb14-36"><a href="#cb14-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-37"><a href="#cb14-37" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 1) Linear projections in batch from d_model =&gt; h x d_k</span></span>
<span id="cb14-38"><a href="#cb14-38" aria-hidden="true" tabindex="-1"></a>        query, key, value <span class="op">=</span> [l(x).view(batch_size, <span class="op">-</span><span class="dv">1</span>, <span class="va">self</span>.h, <span class="va">self</span>.d_k).transpose(<span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb14-39"><a href="#cb14-39" aria-hidden="true" tabindex="-1"></a>                             <span class="cf">for</span> l, x <span class="kw">in</span> <span class="bu">zip</span>(<span class="va">self</span>.linear_layers, (query, key, value))]</span>
<span id="cb14-40"><a href="#cb14-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-41"><a href="#cb14-41" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 2) Apply attention on all the projected vectors in batch.</span></span>
<span id="cb14-42"><a href="#cb14-42" aria-hidden="true" tabindex="-1"></a>        x, attn <span class="op">=</span> <span class="va">self</span>.attention(query, key, value, mask<span class="op">=</span>mask)</span>
<span id="cb14-43"><a href="#cb14-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-44"><a href="#cb14-44" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 3) "Concat" using a view and apply a final linear.</span></span>
<span id="cb14-45"><a href="#cb14-45" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.transpose(<span class="dv">1</span>, <span class="dv">2</span>).contiguous().view(batch_size, <span class="op">-</span><span class="dv">1</span>, <span class="va">self</span>.h <span class="op">*</span> <span class="va">self</span>.d_k)</span>
<span id="cb14-46"><a href="#cb14-46" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.linear_layers[<span class="op">-</span><span class="dv">1</span>](x)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="análisis-detallado-de-la-atención-multi-cabeza-multi-head-attention" class="level3">
<h3 class="anchored" data-anchor-id="análisis-detallado-de-la-atención-multi-cabeza-multi-head-attention">Análisis detallado de la Atención Multi-Cabeza (Multi-Head Attention)</h3>
<p><strong>Estructura del código (<code>__init__</code> y <code>forward</code>)</strong></p>
<p>El código de la atención multi-cabeza está compuesto principalmente por los métodos de inicialización (<code>__init__</code>) y propagación hacia adelante (<code>forward</code>). Examinaremos detalladamente el rol y las operaciones específicas de cada método.</p>
<ul>
<li><strong>Método <code>__init__</code></strong>:
<ul>
<li><code>d_k</code>: Representa la dimensión de cada cabeza de atención. Este valor es el resultado de dividir el tamaño oculto del modelo por el número de cabezas de atención (num_attention_heads), y determina la cantidad de información que cada cabeza procesará.</li>
<li><code>h</code>: Configura el número de cabezas de atención. Este valor es un hiperparámetro que decide cuántas perspectivas diferentes el modelo considerará de las entradas.</li>
<li><code>linear_layers</code>: Crea cuatro capas de transformación lineal en total para la consulta (Q), clave (K), valor (V) y la salida final. Estas capas convierten la entrada para adaptarla a cada cabeza, y luego combinan los resultados de todas las cabezas al final.</li>
</ul></li>
<li><strong>Método <code>forward</code></strong>:
<ol type="1">
<li><strong>Transformación lineal y división</strong>:
<ul>
<li>Realiza una transformación lineal en <code>query</code>, <code>key</code>, <code>value</code> utilizando <code>self.linear_layers</code>. Este proceso convierte la entrada en un formato adecuado para cada cabeza.</li>
<li>Utiliza la función <code>view</code> para cambiar la forma del tensor de (batch_size, sequence_length, hidden_size) a (batch_size, sequence_length, h, d_k). Esto divide toda la entrada en h cabezas.</li>
<li>Usa la función <code>transpose</code> para reorganizar las dimensiones del tensor de (batch_size, sequence_length, h, d_k) a (batch_size, h, sequence_length, d_k). Ahora cada cabeza está lista para realizar cálculos de atención de forma independiente.</li>
</ul></li>
<li><strong>Aplicación de la atención</strong>:
<ul>
<li>Llama a la función <code>attention</code>, que implementa la atención por producto punto escalado (Scaled Dot-Product Attention), para calcular los pesos de atención y los resultados de cada cabeza.</li>
</ul></li>
<li><strong>Combinación y transformación lineal final</strong>:
<ul>
<li>Utiliza <code>transpose</code> y <code>contiguous</code> para revertir el resultado (<code>x</code>) a la forma (batch_size, sequence_length, h, d_k).</li>
<li>Usa la función <code>view</code> para integrar los resultados en una forma (batch_size, sequence_length, h * d_k), es decir, (batch_size, sequence_length, hidden_size).</li>
<li>Finalmente, aplica <code>self.linear_layers[-1]</code> para generar la salida final. Esta transformación lineal combina los resultados de todas las cabezas y produce una salida en el formato deseado por el modelo.</li>
</ul></li>
</ol></li>
<li><strong>Método <code>attention</code> (atención por producto punto escalado)</strong>:
<ul>
<li>Esta función es donde se realiza el mecanismo de atención en cada cabeza, devolviendo los resultados de cada cabeza y los pesos de atención.</li>
<li><strong>Núcleo:</strong> Al calcular <code>scores</code>, se divide la dimensión del vector <code>key</code> por la raíz cuadrada de <span class="math inline">\(d_k\)</span> (<span class="math inline">\(\sqrt{d_k}\)</span>), lo cual es un paso crucial para el escalado.
<ul>
<li><strong>Objetivo:</strong> Evitar que los valores de producto punto (<span class="math inline">\(QK^T\)</span>) se vuelvan demasiado grandes, lo que puede llevar a una excesiva amplificación de las entradas de la función softmax. Esto ayuda a mitigar el problema de desaparición del gradiente (gradient vanishing), estabiliza el aprendizaje y mejora el rendimiento del modelo.</li>
</ul></li>
</ul></li>
</ul>
<hr>
<p><strong>El rol de cada cabeza y las ventajas de la atención multi-cabeza</strong> La atención multi-cabeza se puede comparar con el uso de varios “pequeños lentes” para observar un objeto desde diferentes ángulos. Cada cabeza transforma independientemente las consultas (Q), claves (K) y valores (V) y realiza cálculos de atención. De esta manera, se extraen información enfocándose en diferentes subespacios dentro de la secuencia de entrada completa.</p>
<ul>
<li><strong>Captura de diversas relaciones</strong>: Cada cabeza puede especializarse en aprender diferentes tipos de relaciones lingüísticas. Por ejemplo, una cabeza podría centrarse en las relaciones sujeto-verbo, otra en las relaciones adjetivo-sustantivo, y otra en las relaciones entre pronombres y sus antecedentes.</li>
<li><strong>Eficiencia computacional</strong>: Cada cabeza calcula la atención en dimensiones relativamente pequeñas (d_k). Esto es más eficiente en términos de costo computacional que calcular la atención en una sola dimensión grande.</li>
<li><strong>Procesamiento paralelo</strong>: Los cálculos de cada cabeza son independientes entre sí. Por lo tanto, es posible el procesamiento paralelo utilizando GPU, lo que aumenta significativamente la velocidad de cálculo.</li>
</ul>
<p><strong>Casos de análisis reales</strong></p>
<p>Los resultados de investigaciones muestran que las diferentes cabezas de atención multi-cabeza efectivamente capturan características lingüísticas distintas. Por ejemplo, en el artículo <a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://arxiv.org/abs/1906.04341">“What does BERT Look At? An Analysis of BERT’s Attention”</a>, se analizó la atención multi-cabeza del modelo BERT, revelando que algunas cabezas juegan un papel más importante en el reconocimiento de estructuras sintácticas de oraciones, mientras que otras son cruciales para capturar similitudes semánticas entre palabras.</p>
<hr>
<p><strong>Expresiones matemáticas</strong></p>
<ul>
<li><strong>Total</strong>: <span class="math inline">\(\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \ldots, \text{head}_h)W^O\)</span></li>
<li><strong>Cada cabeza</strong>: <span class="math inline">\(\text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)\)</span></li>
<li><strong>Función de atención</strong>: <span class="math inline">\(\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V\)</span></li>
</ul>
<p><strong>Notación explicada</strong>:</p>
<ul>
<li><span class="math inline">\(h\)</span>: número de cabezas</li>
<li><span class="math inline">\(W_i^Q \in \mathbb{R}^{d_{\text{model}} \times d_k}\)</span>: matriz de transformación de Query para la i-ésima cabeza</li>
<li><span class="math inline">\(W_i^K \in \mathbb{R}^{d_{\text{model}} \times d_k}\)</span>: matriz de transformación de Key para la i-ésima cabeza</li>
<li><span class="math inline">\(W_i^V \in \mathbb{R}^{d_{\text{model}} \times d_v}\)</span>: matriz de transformación de Value para la i-ésima cabeza</li>
<li><span class="math inline">\(W^O\)</span>: matriz de transformación lineal final que proyecta las salidas concatenadas a la dimensión original del embedding (<span class="math inline">\(d_{model}\)</span>)</li>
</ul>
<p><strong>Importancia de la transformación lineal final (<span class="math inline">\(W^O\)</span>)</strong>: La transformación lineal adicional (<span class="math inline">\(W^O\)</span>) que proyecta las salidas concatenadas de cada cabeza de vuelta a la dimensión original del embedding (<span class="math inline">\(d_{model}\)</span>) desempeña un papel crucial.</p>
<ul>
<li><strong>Integración de información</strong>: Integra de manera equilibrada y estable los diferentes puntos de vista extraídos por las cabezas, enriqueciendo la representación del contexto general.</li>
<li><strong>Combinación óptima</strong>: Durante el proceso de aprendizaje, se aprende cómo combinar la información de cada cabeza de manera más efectiva. Esto es similar al principio de combinar las predicciones de modelos individuales en un modelo de ensemble no con un simple promedio, sino utilizando pesos aprendidos.</li>
</ul>
<hr>
<p><strong>Conclusión</strong></p>
<p>La atención multi-cabeza es un mecanismo fundamental que permite a los modelos transformer capturar eficientemente la información contextual de secuencias de entrada y aumentar la velocidad de cálculo mediante procesamiento paralelo con GPU. Esto ha permitido a los transformers demostrar un rendimiento sobresaliente en una variedad de tareas de procesamiento de lenguaje natural.</p>
</section>
<section id="estrategias-de-enmascaramiento-para-el-aprendizaje-paralelo" class="level3">
<h3 class="anchored" data-anchor-id="estrategias-de-enmascaramiento-para-el-aprendizaje-paralelo">8.2.5 Estrategias de enmascaramiento para el aprendizaje paralelo</h3>
<p>Después de implementar la atención multi-cabeza, el equipo de investigación se enfrentó a un problema importante durante el proceso de aprendizaje real. Este problema era la <strong>“fuga de información (information leakage)”</strong>, donde el modelo predecía una palabra actual basándose en palabras futuras. Por ejemplo, en la frase “The cat ___ on the mat”, cuando se intenta predecir la palabra que falta, el modelo podría ver con anticipación la palabra “mat” y fácilmente predecir “sits”.</p>
<p><strong>Necesidad de enmascaramiento: prevención de fuga de información</strong></p>
<p>Esta fuga de información resulta en que el modelo no desarrolle habilidades de inferencia reales, sino que simplemente “vea” las respuestas. Aunque el modelo puede mostrar un alto rendimiento en los datos de entrenamiento, tiene problemas para predecir correctamente con datos nuevos (datos futuros).</p>
<p>Para abordar este problema, el equipo de investigación introdujo una estrategia de <strong>enmascaramiento (masking)</strong> cuidadosamente diseñada. En el Transformer se utilizan dos tipos de máscaras.</p>
<ol type="1">
<li><strong>Máscara causal (Causal Mask, Look-Ahead Mask):</strong> Bloquea el acceso a la información futura en modelos autoregresivos.</li>
<li><strong>Máscara de relleno (Padding Mask):</strong> Elimina el impacto de los tokens de relleno sin sentido al procesar secuencias de longitud variable.</li>
</ol>
<p><strong>1. Máscara causal (Causal Mask)</strong></p>
<p>La máscara causal tiene el papel de ocultar la información futura. Ejecutando el código siguiente, se puede visualizar cómo se enmascaran las partes correspondientes a la información futura en la matriz de puntuaciones de atención.</p>
<div id="cell-28" class="cell" data-execution_count="4">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_08.visualize_masking <span class="im">import</span> visualize_causal_mask</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>visualize_causal_mask()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>1. Original attention score matrix:
                       I        love        deep    learning
I           [      0.90][      0.70][      0.30][      0.20]
love        [      0.60][      0.80][      0.90][      0.40]
deep        [      0.20][      0.50][      0.70][      0.90]
learning    [      0.40][      0.30][      0.80][      0.60]

Each row represents the attention scores from the current position to all positions
--------------------------------------------------

2. Lower triangular mask (1: allowed, 0: blocked):
                       I        love        deep    learning
I           [      1.00][      0.00][      0.00][      0.00]
love        [      1.00][      1.00][      0.00][      0.00]
deep        [      1.00][      1.00][      1.00][      0.00]
learning    [      1.00][      1.00][      1.00][      1.00]

Only the diagonal and below are 1, the rest are 0
--------------------------------------------------

3. Mask converted to -inf:
                       I        love        deep    learning
I           [   1.0e+00][      -inf][      -inf][      -inf]
love        [   1.0e+00][   1.0e+00][      -inf][      -inf]
deep        [   1.0e+00][   1.0e+00][   1.0e+00][      -inf]
learning    [   1.0e+00][   1.0e+00][   1.0e+00][   1.0e+00]

Converting 0 to -inf so that it becomes 0 after softmax
--------------------------------------------------

4. Attention scores with mask applied:
                       I        love        deep    learning
I           [       1.9][      -inf][      -inf][      -inf]
love        [       1.6][       1.8][      -inf][      -inf]
deep        [       1.2][       1.5][       1.7][      -inf]
learning    [       1.4][       1.3][       1.8][       1.6]

Future information (upper triangle) is masked with -inf
--------------------------------------------------

5. Final attention weights (after softmax):
                       I        love        deep    learning
I           [      1.00][      0.00][      0.00][      0.00]
love        [      0.45][      0.55][      0.00][      0.00]
deep        [      0.25][      0.34][      0.41][      0.00]
learning    [      0.22][      0.20][      0.32][      0.26]

The sum of each row becomes 1, and future information is masked to 0</code></pre>
</div>
</div>
<p><strong>Estructura de procesamiento de secuencia y matrices</strong></p>
<p>Explicaré por qué la información futura toma la forma de una matriz triangular superior usando como ejemplo la frase “I love deep learning”. El orden de las palabras es [I(0), love(1), deep(2), learning(3)]. En la matriz de puntuaciones de atención (<span class="math inline">\(QK^T\)</span>), tanto las filas como las columnas siguen este orden de palabras.</p>
<div id="cell-30" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>attention_scores <span class="op">=</span> [</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>    [<span class="fl">0.9</span>, <span class="fl">0.7</span>, <span class="fl">0.3</span>, <span class="fl">0.2</span>],  <span class="co"># I -&gt; I, love, deep, learning</span></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>    [<span class="fl">0.6</span>, <span class="fl">0.8</span>, <span class="fl">0.9</span>, <span class="fl">0.4</span>],  <span class="co"># love -&gt; I, love, deep, learning</span></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>    [<span class="fl">0.2</span>, <span class="fl">0.5</span>, <span class="fl">0.7</span>, <span class="fl">0.9</span>],  <span class="co"># deep -&gt; I, love, deep, learning</span></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>    [<span class="fl">0.4</span>, <span class="fl">0.3</span>, <span class="fl">0.8</span>, <span class="fl">0.6</span>]   <span class="co"># learning -&gt; I, love, deep, learning</span></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<ul>
<li>Cada fila de Q es el vector de consulta de la palabra que se está procesando actualmente.</li>
<li>Cada columna de K (dado que K ha sido transpuesta) es el vector clave de la palabra a la que se hace referencia.</li>
</ul>
<p>Interpretando las matrices anteriores:</p>
<ol type="1">
<li>Fila 1 (I): [I] → [I, love, deep, learning] y su relación</li>
<li>Fila 2 (love): [love] → [I, love, deep, learning] y su relación</li>
<li>Fila 3 (deep): [deep] → [I, love, deep, learning] y su relación</li>
<li>Fila 4 (learning): [learning] → [I, love, deep, learning] y su relación</li>
</ol>
<p>Al procesar la palabra “deep” (fila 3)</p>
<ul>
<li>Disponible para referencia: [I, love, deep] (palabras que han aparecido hasta el momento)</li>
<li>No disponible para referencia: [learning] (palabras futuras que aún no han aparecido)</li>
</ul>
<p>Por lo tanto, en base a la fila, las palabras futuras de la columna correspondiente (información futura) se convierten en la parte <strong>triangular superior (upper triangular)</strong>. Por el contrario, las palabras disponibles para referencia son la parte <strong>triangular inferior (lower triangular)</strong>.</p>
<p>La máscara de causalidad llena la parte triangular inferior con 1 y la parte triangular superior con 0, luego cambia los 0 de la parte triangular superior a <span class="math inline">\(-\infty\)</span>. <span class="math inline">\(-\infty\)</span> se convierte en 0 cuando pasa por la función softmax. La matriz de máscaras simplemente se suma a la matriz de puntuaciones de atención. Como resultado, en la matriz de puntuaciones de atención después de aplicar la softmax, la información futura se bloquea al convertirse en 0.</p>
<p><strong>2. Máscara de relleno (Padding Mask)</strong></p>
<p>En el procesamiento del lenguaje natural, las longitudes de las oraciones varían. Para el procesamiento por lotes (batch), todas las oraciones deben tener la misma longitud, por lo que los espacios vacíos en las oraciones más cortas se rellenan con tokens de relleno (PAD). Sin embargo, estos tokens de relleno no tienen significado y no deben incluirse en el cálculo de atención.</p>
<div id="cell-32" class="cell" data-execution_count="14">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_08.visualize_masking <span class="im">import</span> visualize_padding_mask</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>visualize_padding_mask()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
2. Create padding mask (1: valid token, 0: padding token):
tensor([[[1., 1., 1., 1.]],

        [[1., 1., 1., 0.]],

        [[1., 1., 1., 1.]],

        [[1., 1., 1., 1.]]])

Positions that are not padding (0) are 1, padding positions are 0
--------------------------------------------------

3. Original attention scores (first sentence):
                       I        love        deep    learning
I           [      0.90][      0.70][      0.30][      0.20]
love        [      0.60][      0.80][      0.90][      0.40]
deep        [      0.20][      0.50][      0.70][      0.90]
learning    [      0.40][      0.30][      0.80][      0.60]

Attention scores at each position
--------------------------------------------------

4. Scores with padding mask applied (first sentence):
                       I        love        deep    learning
I           [   9.0e-01][   7.0e-01][   3.0e-01][   2.0e-01]
love        [   6.0e-01][   8.0e-01][   9.0e-01][   4.0e-01]
deep        [   2.0e-01][   5.0e-01][   7.0e-01][   9.0e-01]
learning    [   4.0e-01][   3.0e-01][   8.0e-01][   6.0e-01]

The scores at padding positions are masked with -inf
--------------------------------------------------

5. Final attention weights (first sentence):
                       I        love        deep    learning
I           [      0.35][      0.29][      0.19][      0.17]
love        [      0.23][      0.28][      0.31][      0.19]
deep        [      0.17][      0.22][      0.27][      0.33]
learning    [      0.22][      0.20][      0.32][      0.26]

The weights at padding positions become 0, and the sum of the weights at the remaining positions is 1</code></pre>
</div>
</div>
<p>Tomemos como ejemplo las siguientes oraciones.</p>
<ul>
<li>“I love ML” → [I, love, ML, PAD]</li>
<li>“Deep learning is fun” → [Deep, learning, is, fun]</li>
</ul>
<p>Aquí, la primera oración tiene solo 3 palabras, por lo que se llena el final con PAD. La máscara de padding elimina el efecto de estos tokens PAD. Se genera una máscara que marca las palabras reales con 1 y los tokens de padding con 0, y 2. hace que los puntajes de atención en las posiciones de padding sean <span class="math inline">\(-\infty\)</span> para que se conviertan en 0 después de pasar por la softmax.</p>
<p>En consecuencia, se obtiene el siguiente efecto.</p>
<ol type="1">
<li>Las palabras reales pueden intercambiar libremente su atención entre ellas.</li>
<li>Los tokens de padding están completamente excluidos del cálculo de atención.</li>
<li>El contexto se forma solo con las partes significativas de cada oración.</li>
</ol>
<div id="cell-34" class="cell" data-execution_count="2">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_attention_mask(size):</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create a lower triangular matrix (including the diagonal)</span></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>    mask <span class="op">=</span> torch.tril(torch.ones(size, size))</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Mask with -inf (becomes 0 after softmax)</span></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>    mask <span class="op">=</span> mask.masked_fill(mask <span class="op">==</span> <span class="dv">0</span>, <span class="bu">float</span>(<span class="st">'-inf'</span>))</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> mask</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> masked_attention(Q, K, V, mask):</span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate attention scores</span></span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>    scores <span class="op">=</span> torch.matmul(Q, K.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>))</span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Apply mask</span></span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>    scores <span class="op">=</span> scores <span class="op">+</span> mask</span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Apply softmax</span></span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a>    weights <span class="op">=</span> F.softmax(scores, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate final attention output</span></span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> torch.matmul(weights, V)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><strong>Innovación e impacto de las estrategias de enmascaramiento</strong></p>
<p>Las dos estrategias de enmascaramiento desarrolladas por el equipo de investigación (enmascaramiento de relleno, enmascaramiento causal) hicieron que el proceso de aprendizaje del transformer fuera más robusto y sentaron las bases para modelos autoregresivos posteriores como GPT. En particular, el enmascaramiento causal indujo a los modelos de lenguaje a comprender el contexto de manera secuencial, similar al proceso de comprensión lingüística humano.</p>
<p><strong>Eficiencia en la implementación</strong></p>
<p>El enmascaramiento se realiza inmediatamente después del cálculo de las puntuaciones de atención y <em>antes</em> de aplicar la función softmax. Las posiciones enmascaradas con el valor <span class="math inline">\(-\infty\)</span> se convierten en 0 al pasar a través de la función softmax, lo que bloquea completamente la información en esas posiciones. Este es un enfoque optimizado tanto desde el punto de vista de la eficiencia computacional como del uso de memoria.</p>
<p>La introducción de estas estrategias de enmascaramiento permitió que los transformers pudieran realizar aprendizaje paralelo en su verdadero sentido, lo cual tuvo un gran impacto en el desarrollo de los modelos de lenguaje modernos.</p>
</section>
<section id="evolución-del-significado-de-head-de-cabeza-a-cerebro" class="level3">
<h3 class="anchored" data-anchor-id="evolución-del-significado-de-head-de-cabeza-a-cerebro">8.2.6 Evolución del significado de “head”: de “cabeza” a “cerebro”</h3>
<p>En el deep learning, el término “head” ha evolucionado gradualmente y fundamentalmente en su significado junto con el desarrollo de las arquitecturas de redes neuronales. Inicialmente se usaba principalmente para referirse a una parte “cercana a la capa de salida” de forma relativamente simple, pero recientemente se ha expandido hacia un significado más abstracto y complejo que implica un “módulo independiente” que asume funciones específicas dentro del modelo.</p>
<ol type="1">
<li><p><strong>Inicial: “cerca de la capa de salida”</strong></p>
<p>En los primeros modelos de deep learning (por ejemplo, perceptrones multicapa simples (MLP)), “head” se refería generalmente a la última parte de la red, que recibía un vector de características procesado por el extractor de características (backbone) y realizaba la predicción final (clasificación, regresión, etc.). En este caso, la head estaba compuesta principalmente por capas completamente conectadas (fully connected layers) y funciones de activación (activation functions).</p></li>
</ol>
<div id="cell-37" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SimpleModel(nn.Module):</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, num_classes):</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.backbone <span class="op">=</span> nn.Sequential( <span class="co"># Feature extractor</span></span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">784</span>, <span class="dv">128</span>),</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">128</span>, <span class="dv">64</span>),</span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>            nn.ReLU()</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.head <span class="op">=</span> nn.Linear(<span class="dv">64</span>, num_classes)  <span class="co"># Head (output layer)</span></span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a>        features <span class="op">=</span> <span class="va">self</span>.backbone(x)</span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> <span class="va">self</span>.head(features)</span>
<span id="cb21-15"><a href="#cb21-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> output</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<ol start="2" type="1">
<li><strong>Aprendizaje multi-tarea: “ramificaciones por tarea”</strong></li>
</ol>
<p>Con el avance de los modelos de aprendizaje profundo que utilizan conjuntos de datos a gran escala como ImageNet, ha surgido el aprendizaje multi-tarea (multi-task learning), en el cual múltiples cabezas ramificadas desde un único extractor de características realizan tareas diferentes. Por ejemplo, en los modelos de detección de objetos (object detection), se utilizan simultáneamente una cabeza que clasifica el tipo de objeto a partir de la imagen y otra cabeza que predice la caja delimitadora (bounding box) que indica la ubicación del objeto.</p>
<div id="cell-39" class="cell" data-execution_count="3">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MultiTaskModel(nn.Module):</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, num_classes):</span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.backbone <span class="op">=</span> ResNet50()  <span class="co"># Feature extractor (ResNet)</span></span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.classification_head <span class="op">=</span> nn.Linear(<span class="dv">2048</span>, num_classes)  <span class="co"># Classification head</span></span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bbox_head <span class="op">=</span> nn.Linear(<span class="dv">2048</span>, <span class="dv">4</span>)  <span class="co"># Bounding box regression head</span></span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a>        features <span class="op">=</span> <span class="va">self</span>.backbone(x)</span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a>        class_output <span class="op">=</span> <span class="va">self</span>.classification_head(features)</span>
<span id="cb22-15"><a href="#cb22-15" aria-hidden="true" tabindex="-1"></a>        bbox_output <span class="op">=</span> <span class="va">self</span>.bbox_head(features)</span>
<span id="cb22-16"><a href="#cb22-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> class_output, bbox_output</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<ol start="3" type="1">
<li><p><strong>El concepto de “cabeza” en el paper Attention is All You Need (Transformers)</strong>:</p>
<p>La atención multi-cabeza en los transformers dio un paso más allá. En los transformers, ya no se sigue la idea preconcebida de que “cabeza = parte cercana a la salida”.</p></li>
</ol>
<div id="cell-41" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MultiHeadAttention(nn.Module):</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, num_heads):</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.heads <span class="op">=</span> nn.ModuleList([</span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>            AttentionHead() <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_heads)  <span class="co"># num_heads개의 독립적인 어텐션 헤드</span></span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>        ])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<ul>
<li><strong>Módulos independientes:</strong> aquí cada “cabeza” es un <em>módulo independiente</em> que recibe la entrada y realiza el mecanismo de atención de manera independiente. Cada cabeza tiene diferentes pesos y presta atención a aspectos diferentes de la secuencia de entrada.</li>
<li><strong>Procesamiento en paralelo:</strong> varias cabezas funcionan <em>en paralelo</em> para procesar diferentes tipos de información simultáneamente.</li>
<li><strong>Fase intermedia de procesamiento:</strong> las cabezas ya no se limitan a la capa de salida. Los codificadores y decodificadores del transformador están compuestos por <em>varias capas de atención multi-cabeza</em>, y cada capa de cabeza aprende diferentes representaciones (representación) de la secuencia de entrada.</li>
</ul>
<ol start="4" type="1">
<li><p><strong>Tendencias recientes: “módulos funcionales”</strong></p>
<p>En los modelos de deep learning recientes, el término “cabeza” se usa de manera más flexible. Es común referirse a un módulo independiente que realiza una función específica como “cabeza”, incluso si no está cerca de la capa de salida.</p>
<ul>
<li><strong>Modelos de lenguaje (Language Models):</strong> en modelos de lenguaje a gran escala como BERT, GPT, se utilizan varios tipos de cabezas, como “language modeling head”, “masked language modeling head”, “next sentence prediction head”.</li>
<li><strong>Transformadores visuales (Vision Transformers):</strong> en ViT, las imágenes se dividen en parches (patch) y se procesa cada parche como si fuera un token utilizando una “patch embedding head”.</li>
</ul></li>
</ol>
<p><strong>Conclusión</strong></p>
<p>El significado de “cabeza” en deep learning ha evolucionado de “una parte cercana a la salida” a “un módulo independiente que realiza una función específica (en paralelo, incluyendo procesamiento intermedio)”. Este cambio refleja la tendencia hacia un mayor grado de división y especialización de las partes del modelo a medida que las arquitecturas de deep learning se vuelven más complejas y sofisticadas. La atención multi-cabeza en los transformadores es un ejemplo representativo de este cambio de significado, mostrando cómo el término “cabeza” ya no se refiere a una “cabeza”, sino que funciona como varios “cerebros”.</p>
</section>
</section>
<section id="procesamiento-de-la-información-de-posición" class="level2">
<h2 class="anchored" data-anchor-id="procesamiento-de-la-información-de-posición">8.3 Procesamiento de la información de posición</h2>
<p><strong>Desafío:</strong> ¿Cómo se puede expresar eficazmente la información del orden de las palabras sin usar RNN?</p>
<p><strong>Penalidades del investigador:</strong> Dado que el transformer no procesa los datos secuencialmente como lo hace un RNN, era necesario informar explícitamente sobre la información de posición de las palabras. Aunque los investigadores intentaron diversos métodos (índices de posición, embeddings aprendibles, etc.), no lograron resultados satisfactorios. Era necesario encontrar una nueva forma de expresar eficazmente la información de posición, como descifrar un texto encriptado.</p>
<p>A diferencia del RNN, el transformer no utiliza estructuras recurrentes ni operaciones de convolución, por lo que era necesario proporcionar la información de orden de la secuencia por separado. “dog bites man” y “man bites dog” tienen las mismas palabras pero significados completamente diferentes debido a su orden. La operación de atención (<span class="math inline">\(QK^T\)</span>) solo calcula la similitud entre los vectores de palabras, sin considerar la información de posición, por lo que el equipo de investigación tuvo que pensar en cómo inyectar la información de posición al modelo. Este era el <strong>desafío</strong> de cómo expresar eficazmente la información del orden de las palabras sin RNN.</p>
<section id="importancia-de-la-información-secuencial" class="level3">
<h3 class="anchored" data-anchor-id="importancia-de-la-información-secuencial">8.3.1 Importancia de la información secuencial</h3>
<p>El equipo de investigación consideró diversos métodos de codificación posicional.</p>
<ol type="1">
<li><strong>Uso directo de los índices de posición:</strong> El enfoque más simple consiste en sumar el índice de posición (0, 1, 2, …) de cada palabra al vector de embedding.</li>
</ol>
<div id="cell-44" class="cell" data-execution_count="15">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_08.visualize_positional_embedding <span class="im">import</span> visualize_position_embedding</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>visualize_position_embedding()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>1. Original embedding matrix:
                dim1      dim2      dim3      dim4
I         [    0.20][    0.30][    0.10][    0.40]
love      [    0.50][    0.20][    0.80][    0.10]
deep      [    0.30][    0.70][    0.20][    0.50]
learning  [    0.60][    0.40][    0.30][    0.20]

Each row is the embedding vector of a word
--------------------------------------------------

2. Position indices:
[0 1 2 3]

Indices representing the position of each word (starting from 0)
--------------------------------------------------

3. Embeddings with position information added:
                dim1      dim2      dim3      dim4
I         [    0.20][    0.30][    0.10][    0.40]
love      [    1.50][    1.20][    1.80][    1.10]
deep      [    2.30][    2.70][    2.20][    2.50]
learning  [    3.60][    3.40][    3.30][    3.20]

Result of adding position indices to each embedding vector (broadcasting)
--------------------------------------------------

4. Changes due to adding position information:

I (0):
  Original:     [0.2 0.3 0.1 0.4]
  Pos. Added: [0.2 0.3 0.1 0.4]
  Difference:     [0. 0. 0. 0.]

love (1):
  Original:     [0.5 0.2 0.8 0.1]
  Pos. Added: [1.5 1.2 1.8 1.1]
  Difference:     [1. 1. 1. 1.]

deep (2):
  Original:     [0.3 0.7 0.2 0.5]
  Pos. Added: [2.3 2.7 2.2 2.5]
  Difference:     [2. 2. 2. 2.]

learning (3):
  Original:     [0.6 0.4 0.3 0.2]
  Pos. Added: [3.6 3.4 3.3 3.2]
  Difference:     [3. 3. 3. 3.]</code></pre>
</div>
</div>
<p>Sin embargo, este método presentaba dos problemas.</p>
<ul>
<li><strong>Imposibilidad de procesar secuencias más largas que los datos de entrenamiento:</strong> si se introduce una posición no vista durante el entrenamiento (por ejemplo, la 100ª), no se puede encontrar una representación adecuada.</li>
<li><strong>Dificultad para expresar información de distancia relativa:</strong> es difícil expresar que la distancia entre las posiciones 2 y 4 es la misma que entre las posiciones 102 y 104.</li>
</ul>
<ol start="2" type="1">
<li><strong>Embebidos de posición aprendibles:</strong> también se consideró el uso de vectores de embebido aprendibles para cada posición.</li>
</ol>
<div id="cell-46" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Conceptual code</span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>    positional_embeddings <span class="op">=</span> nn.Embedding(max_seq_length, embedding_dim)</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>    positions <span class="op">=</span> torch.arange(seq_length)</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>    positional_encoding <span class="op">=</span> positional_embeddings(positions)</span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>    final_embedding <span class="op">=</span> word_embedding <span class="op">+</span> positional_encoding</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Este método puede aprender representaciones únicas por posición, pero aún tiene la limitación fundamental de no poder procesar secuencias más largas que los datos de entrenamiento.</p>
<p><strong>Condiciones clave para la representación de información posicional</strong></p>
<p>El equipo de investigación descubrió a través de ensayos y errores que la representación de información posicional debe cumplir las siguientes tres condiciones clave:</p>
<ol type="1">
<li><strong>Sin límite de longitud de secuencia:</strong> Debe poder representar adecuadamente posiciones no vistas durante el entrenamiento (por ejemplo, la posición 1000).</li>
<li><strong>Representación de relaciones de distancia relativa:</strong> La distancia entre las posiciones 2 y 4 debe ser representada de la misma manera que la distancia entre las posiciones 102 y 104. Es decir, se deben preservar las distancias relativas entre posiciones.</li>
<li><strong>Compatibilidad con el cálculo de atención:</strong> La información posicional debe transmitir eficazmente la información de orden sin interferir en el cálculo de los pesos de atención.</li>
</ol>
</section>
<section id="diseño-del-codificador-posicional" class="level3">
<h3 class="anchored" data-anchor-id="diseño-del-codificador-posicional">8.3.2 Diseño del codificador posicional</h3>
<p>Tras estas reflexiones, el equipo de investigación encontró una solución innovadora llamada <strong>codificación posicional (Positional Encoding)</strong> que aprovecha las propiedades periódicas de las funciones seno (sin) y coseno (cos).</p>
<p><strong>Principio de la codificación posicional basada en funciones seno-coseno</strong></p>
<p>Codificando cada posición con funciones seno y coseno de diferentes frecuencias, se representa naturalmente la distancia relativa entre las posiciones.</p>
<div id="cell-48" class="cell" data-execution_count="17">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_08.positional_encoding_utils <span class="im">import</span> visualize_sinusoidal_features</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>visualize_sinusoidal_features()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="08_El nacimiento del transformer_files/figure-html/cell-21-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>3 es una ilustración que visualiza el movimiento de la posición. Muestra cómo se expresan las relaciones de posición mediante funciones senoidales. Satisface la segunda condición, “expresión de relaciones de distancia relativa”. Todas las curvas desplazadas mantienen la misma forma que la curva original mientras mantienen una separación constante. Esto significa que si la distancia entre las posiciones es la misma (por ejemplo, 2→7 y 102→107), su relación también se expresa de manera idéntica.</p>
<p>4 es un mapa de calor de codificación posicional (Positional Encoding Matrix). Muestra cómo cada posición (eje vertical) tiene un patrón único (eje horizontal). Las columnas del eje horizontal representan funciones senoidales y cosenoidales de diferentes períodos, con períodos más largos hacia la derecha. Cada fila (posición) genera un patrón único a partir de las combinaciones de rojo (positivo) y azul (negativo). Al usar una variedad de frecuencias, desde períodos cortos hasta largos, se crea un patrón único para cada posición. Este enfoque satisface la primera condición, “sin límite de longitud de secuencia”. Al combinar funciones senoidales y cosenoidales de diferentes períodos, se pueden generar valores únicos de manera matemática hasta posiciones infinitas.</p>
<p>Utilizando esta característica matemática, el equipo de investigación implementó el algoritmo de codificación posicional de la siguiente manera.</p>
<p><strong>Implementación de Codificación Posicional</strong></p>
<div id="cell-50" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> positional_encoding(seq_length, d_model):</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 1. 위치별 인코딩 행렬 생성</span></span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>    position <span class="op">=</span> np.arange(seq_length)[:, np.newaxis]  <span class="co"># [0, 1, 2, ..., seq_length-1]</span></span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 2. 각 차원별 주기 계산</span></span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a>    div_term <span class="op">=</span> np.exp(np.arange(<span class="dv">0</span>, d_model, <span class="dv">2</span>) <span class="op">*</span> <span class="op">-</span>(np.log(<span class="fl">10000.0</span>) <span class="op">/</span> d_model))</span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 예: d_model=512일 때</span></span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># div_term[0] ≈ 1.0        (가장 짧은 주기)</span></span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># div_term[256] ≈ 0.0001   (가장 긴 주기)</span></span>
<span id="cb28-10"><a href="#cb28-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb28-11"><a href="#cb28-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 3. 짝수/홀수 차원에 사인/코사인 적용</span></span>
<span id="cb28-12"><a href="#cb28-12" aria-hidden="true" tabindex="-1"></a>    pe <span class="op">=</span> np.zeros((seq_length, d_model))</span>
<span id="cb28-13"><a href="#cb28-13" aria-hidden="true" tabindex="-1"></a>    pe[:, <span class="dv">0</span>::<span class="dv">2</span>] <span class="op">=</span> np.sin(position <span class="op">*</span> div_term)  <span class="co"># 짝수 차원</span></span>
<span id="cb28-14"><a href="#cb28-14" aria-hidden="true" tabindex="-1"></a>    pe[:, <span class="dv">1</span>::<span class="dv">2</span>] <span class="op">=</span> np.cos(position <span class="op">*</span> div_term)  <span class="co"># 홀수 차원</span></span>
<span id="cb28-15"><a href="#cb28-15" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb28-16"><a href="#cb28-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> pe</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<ul>
<li><code>position</code>: <code>[0, 1, 2, ..., seq_length-1]</code> forma de array. Representa el índice de posición de cada palabra.</li>
<li><code>div_term</code>: valor que determina el período para cada dimensión. A medida que <code>d_model</code> aumenta, el período se alarga.</li>
<li><code>pe[:, 0::2] = np.sin(position * div_term)</code>: se aplica la función seno a las dimensiones con índice par.</li>
<li><code>pe[:, 1::2] = np.cos(position * div_term)</code>: se aplica la función coseno a las dimensiones con índice impar.</li>
</ul>
<p><strong>Expresión matemática</strong></p>
<p>Cada dimensión de la codificación posicional se calcula según la siguiente fórmula.</p>
<ul>
<li><span class="math inline">\(PE_{(pos, 2i)} = \sin(pos / 10000^{2i/d_{\text{model}}})\)</span></li>
<li><span class="math inline">\(PE_{(pos, 2i+1)} = \cos(pos / 10000^{2i/d_{\text{model}}})\)</span></li>
</ul>
<p>donde</p>
<ul>
<li><span class="math inline">\(pos\)</span>: posición de la palabra (0, 1, 2, …)</li>
<li><span class="math inline">\(i\)</span>: índice de dimensión (0, 1, 2, …, <span class="math inline">\(d_{model}\)</span>-1)</li>
<li><span class="math inline">\(d_{model}\)</span>: dimensión del embedding (y de la codificación posicional)</li>
</ul>
<p><strong>Verificación del cambio de período</strong></p>
<div id="cell-52" class="cell" data-execution_count="18">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_08.positional_encoding_utils <span class="im">import</span> show_positional_periods</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>show_positional_periods()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>1. Periods of positional encoding:
First dimension (i=0): 1.00
Middle dimension (i=128): 100.00
Last dimension (i=255): 9646.62

2. Positional encoding formula values (10000^(2i/d_model)):
i=  0: 1.0000000000
i=128: 100.0000000000
i=255: 9646.6161991120

3. Actual div_term values (first/middle/last):
First (i=0): 1.0000000000
Middle (i=128): 0.0100000000
Last (i=255): 0.0001036633</code></pre>
</div>
</div>
<p>Aquí, lo importante es el paso 3.</p>
<div id="cell-54" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 3. 짝수/홀수 차원에 사인/코사인 적용</span></span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>    pe <span class="op">=</span> np.zeros((seq_length, d_model))</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>    pe[:, <span class="dv">0</span>::<span class="dv">2</span>] <span class="op">=</span> np.sin(position <span class="op">*</span> div_term)  <span class="co"># 짝수 차원</span></span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>    pe[:, <span class="dv">1</span>::<span class="dv">2</span>] <span class="op">=</span> np.cos(position <span class="op">*</span> div_term)  <span class="co"># 홀수 차원</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>El resultado muestra la variación del período según las dimensiones.</p>
<p><strong>Embedding final</strong></p>
<p>La codificación posicional generada <code>pe</code> tiene forma (seq_length, d_model), y se suma a la matriz de embeddings de palabras originales (sentence_embedding) para crear el embedding final.</p>
<div id="cell-56" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>final_embedding <span class="op">=</span> sentence_embedding <span class="op">+</span> positional_encoding</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Así, el embedding final agregado contiene tanto la información semántica como posicional de la palabra. Por ejemplo, la palabra “bank” puede tener diferentes valores vectoriales finales dependiendo de su posición en la oración, lo que ayuda a distinguir entre los significados de “banco” y “orilla del río”.</p>
<p>De esta manera, el transformer es capaz de procesar eficazmente la información secuencial sin necesidad de RNN, sentando las bases para aprovechar al máximo las ventajas del procesamiento paralelo.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Haga clic para ver el contenido (deep dive: la evolución de la codificación posicional, las técnicas más recientes y su base matemática)">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Haga clic para ver el contenido (deep dive: la evolución de la codificación posicional, las técnicas más recientes y su base matemática)
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<section id="evolución-del-codificado-posicional-técnicas-más-recientes-y-fundamentos-matemáticos" class="level3 callout-body-container callout-body">
<h3 class="anchored" data-anchor-id="evolución-del-codificado-posicional-técnicas-más-recientes-y-fundamentos-matemáticos">Evolución del codificado posicional, técnicas más recientes y fundamentos matemáticos</h3>
<p>En la sección 8.3.2 revisamos el codificado posicional basado en funciones seno-coseno que es fundamental para los modelos Transformer. Sin embargo, desde la publicación del artículo “Attention is All You Need”, el codificado posicional ha evolucionado en múltiples direcciones. En esta sección de profundización abordaremos exhaustivamente el codificado posicional aprendible, el codificado posicional relativo y las tendencias más recientes en investigación, analizando detalladamente la representación matemática y los pros y contras de cada técnica.</p>
<section id="codificado-posicional-aprendible-learnable-positional-encoding" class="level4">
<h4 class="anchored" data-anchor-id="codificado-posicional-aprendible-learnable-positional-encoding">1. Codificado posicional aprendible (Learnable Positional Encoding)</h4>
<ul>
<li><p><strong>Concepto</strong>: En lugar de funciones fijas, el modelo aprende directamente incrustaciones que expresan información de posición.</p></li>
<li><p><strong>1.1 Representación matemática</strong>: El codificado posicional aprendible se representa por la siguiente matriz.</p>
<p><span class="math inline">\(P \in \mathbb{R}^{L_{max} \times d}\)</span></p>
<p>Donde <span class="math inline">\(L_{max}\)</span> es la longitud máxima de secuencia y <span class="math inline">\(d\)</span> es la dimensión de incrustación. La incrustación para la posición <span class="math inline">\(i\)</span> se da por la <span class="math inline">\(i\)</span>-ésima fila de la matriz <span class="math inline">\(P\)</span>, es decir, <span class="math inline">\(P[i,:]\)</span>.</p></li>
<li><p><strong>1.2 Técnicas para resolver el problema de extrapolación</strong>: Al tratar secuencias más largas que los datos de entrenamiento, surge un problema debido a la falta de información para posiciones fuera del rango aprendido. Se han investigado técnicas para abordar este problema.</p>
<ul>
<li><p><strong>Interpolación de posición (Chen et al., 2023)</strong>: Se genera una nueva incrustación interpolando linealmente entre las incrustaciones aprendidas.</p>
<p><span class="math inline">\(P_{ext}(i) = P[\lfloor \alpha i \rfloor] + (\alpha i - \lfloor \alpha i \rfloor)(P[\lfloor \alpha i \rfloor +1] - P[\lfloor \alpha i \rfloor])\)</span></p>
<p>Donde <span class="math inline">\(\alpha = \frac{\text{longitud de secuencia de entrenamiento}}{\text{longitud de secuencia de inferencia}}\)</span>.</p></li>
<li><p><strong>Escalado NTK-aware (2023)</strong>: Basado en la teoría del Kernel Tangente Neural (NTK), este método introduce un efecto suavizante aumentando gradualmente las frecuencias.</p></li>
</ul></li>
<li><p><strong>1.3 Aplicaciones más recientes</strong>:</p>
<ul>
<li><strong>BERT</strong>: Inicialmente limitado a 512 tokens, se expandió a 1024 tokens en RoBERTa.</li>
<li><strong>GPT-3</strong>: Tiene un límite de 2048 tokens y utiliza una técnica para aumentar gradualmente la longitud de secuencia durante el entrenamiento.</li>
</ul></li>
<li><p><strong>Ventajas</strong>:</p>
<ul>
<li><strong>Flexibilidad</strong>: Puede aprender información de posición especializada en los datos.</li>
<li><strong>Potencial mejora en rendimiento</strong>: En ciertas tareas, puede mostrar un mejor rendimiento que las funciones fijas.</li>
</ul></li>
<li><p><strong>Desventajas</strong>:</p>
<ul>
<li><strong>Riesgo de sobreajuste</strong>: El rendimiento general puede disminuir para secuencias de longitudes no presentes en los datos de entrenamiento.</li>
<li><strong>Dificultad en el procesamiento de secuencias largas</strong>: Se requieren técnicas adicionales para resolver el problema de extrapolación.</li>
</ul></li>
</ul>
</section>
<section id="codificado-posicional-relativo-relative-positional-encoding" class="level4">
<h4 class="anchored" data-anchor-id="codificado-posicional-relativo-relative-positional-encoding">2. Codificado posicional relativo (Relative Positional Encoding)</h4>
<ul>
<li><p><strong>Idea clave</strong>: En lugar de centrarse en la posición absoluta, se enfoca en la distancia relativa entre las palabras.</p></li>
<li><p><strong>Fondo</strong>: El significado de una palabra en el lenguaje natural a menudo se ve más influenciado por su relación con las palabras cercanas que por su posición absoluta. Además, el codificado posicional absoluto tiene la desventaja de no capturar eficazmente las relaciones entre palabras distantes.</p></li>
<li><p><strong>2.1 Extensión matemática</strong>:</p>
<ul>
<li><strong>Fórmula de Shaw et al.&nbsp;(2018)</strong>: En el mecanismo de atención, se añade una incrustación aprendible (<span class="math inline">\(a_{i-j}\)</span>) que representa la distancia relativa entre los vectores Query y Key. <span class="math inline">\(e_{ij} = \frac{x_iW^Q(x_jW^K + a_{i-j})^T}{\sqrt{d}}\)</span></li>
</ul></li>
</ul>
<p>aquí <span class="math inline">\(a_{i-j} \in \mathbb{R}^d\)</span> es un vector aprendible para la posición relativa <span class="math inline">\(i-j\)</span>.</p>
<ul>
<li><p><strong>Rotary Positional Encoding (RoPE)</strong>: utiliza matrices de rotación para codificar posiciones relativas.</p>
<p><span class="math inline">\(\text{RoPE}(x, m) = x \odot e^{im\theta}\)</span></p>
<p>aquí <span class="math inline">\(\theta\)</span> es un hiperparámetro que controla la frecuencia, y <span class="math inline">\(\odot\)</span> denota multiplicación compleja (o la matriz de rotación correspondiente).</p></li>
<li><p><strong>Versión simplificada de T5</strong>: utiliza un sesgo aprendible <span class="math inline">\(b\)</span> para posiciones relativas, y recorta (clipping) los valores si la distancia relativa excede un rango determinado.</p>
<p><span class="math inline">\(e_{ij} = \frac{x_iW^Q(x_jW^K)^T + b_{\text{clip}(i-j)}}{\sqrt{d}}\)</span></p>
<p><span class="math inline">\(b \in \mathbb{R}^{2k+1}\)</span> es un vector de sesgo para posiciones relativas recortadas [-k, k].</p></li>
<li><p><strong>Ventajas</strong>:</p>
<ul>
<li><strong>Mejora en la capacidad de generalización</strong>: se generaliza mejor a secuencias de longitudes no vistas durante el entrenamiento.</li>
<li><strong>Mejora en la capacidad de capturar dependencias a largo plazo</strong>: modela más eficazmente las relaciones entre palabras distantes.</li>
</ul></li>
<li><p><strong>Desventajas</strong>:</p>
<ul>
<li><strong>Aumento de la complejidad computacional</strong>: al considerar la distancia relativa, los cálculos de atención pueden volverse más complejos. (especialmente cuando se consideran las distancias relativas para todas las pares de palabras)</li>
</ul></li>
</ul>
</section>
<section id="optimización-del-codificado-posicional-basado-en-cnn" class="level4">
<h4 class="anchored" data-anchor-id="optimización-del-codificado-posicional-basado-en-cnn">3. Optimización del codificado posicional basado en CNN</h4>
<ul>
<li><p><strong>3.1 Aplicación de convolución por profundidad</strong>: realiza convoluciones independientes en cada canal para reducir el número de parámetros y mejorar la eficiencia computacional. <span class="math inline">\(P(i) = \sum_{k=-K}^K w_k \cdot x_{i+k}\)</span></p>
<p>aquí <span class="math inline">\(K\)</span> es el tamaño del kernel, y <span class="math inline">\(w_k\)</span> son pesos aprendibles.</p></li>
<li><p><strong>3.2 Convoluciones multi-escala</strong>: similar a ResNet, utiliza canales de convolución paralelos para capturar información posicional en diferentes rangos.</p>
<p><span class="math inline">\(P(i) = \text{Concat}(\text{Conv}_{3x1}(x), \text{Conv}_{5x1}(x))\)</span></p></li>
</ul>
</section>
<section id="dinámica-del-codificado-posicional-recursivo" class="level4">
<h4 class="anchored" data-anchor-id="dinámica-del-codificado-posicional-recursivo">4. Dinámica del codificado posicional recursivo</h4>
<ul>
<li><p><strong>4.1 Codificación basada en LSTM</strong>: utiliza una red LSTM para codificar información de posición secuencial.</p>
<p><span class="math inline">\(h_t = \text{LSTM}(x_t, h_{t-1})\)</span> <span class="math inline">\(P(t) = W_ph_t\)</span></p></li>
<li><p><strong>4.2 Variante más reciente: Neural ODE</strong>: modela dinámicas en tiempo continuo para superar las limitaciones de la LSTM discreta.</p>
<p><span class="math inline">\(\frac{dh(t)}{dt} = f_\theta(h(t), t)\)</span> <span class="math inline">\(P(t) = \int_0^t f_\theta(h(\tau), \tau)d\tau\)</span></p></li>
</ul>
</section>
<section id="interpretación-cuántica-del-codificado-posicional-complejo" class="level4">
<h4 class="anchored" data-anchor-id="interpretación-cuántica-del-codificado-posicional-complejo">5. Interpretación cuántica del codificado posicional complejo</h4>
<ul>
<li><p><strong>5.1 Representación de embeddings complejos</strong>: representa la información de posición en forma compleja.</p>
<p><span class="math inline">\(z(i) = r(i)e^{i\phi(i)}\)</span></p>
<p>aquí <span class="math inline">\(r\)</span> es la magnitud de la posición, y <span class="math inline">\(\phi\)</span> es el ángulo de fase.</p></li>
<li><p><strong>5.2 Teorema de desplazamiento de fase</strong>: representa el desplazamiento de posición como una rotación en el plano complejo.</p>
<p><span class="math inline">\(z(i+j) = z(i) \cdot e^{i\omega j}\)</span></p>
<p>aquí <span class="math inline">\(\omega\)</span> es un parámetro de frecuencia aprendible.</p></li>
</ul>
</section>
<section id="enfoque-híbrido" class="level4">
<h4 class="anchored" data-anchor-id="enfoque-híbrido">6. Enfoque híbrido</h4>
<ul>
<li><p><strong>6.1 Codificado posicional compuesto:</strong> <span class="math inline">\(P(i)=αP_{abs}(i)+βP_{rel}(i)\)</span></p>
<p><span class="math inline">\(P(i)=αP_{abs} (i)+βP_{rel}(i)\)</span> α, β = pesos de aprendizaje</p></li>
<li><p><strong>6.2 Codificación Posicional Dinámica:</strong></p>
<p><span class="math inline">\(P(i) = \text{MLP}(i, \text{Context})\)</span> Aprendizaje de representaciones posicionales dependientes del contexto</p></li>
</ul>
</section>
<section id="comparación-de-rendimiento-experimental-glue-benchmark" class="level4">
<h4 class="anchored" data-anchor-id="comparación-de-rendimiento-experimental-glue-benchmark">7. Comparación de rendimiento experimental (GLUE Benchmark)</h4>
<p>A continuación se presentan los resultados de la comparación de rendimiento experimental de diferentes enfoques de codificación posicional en el benchmark GLUE. (El rendimiento real puede variar según la estructura del modelo, los datos y la configuración de los hiperparámetros).</p>
<table class="caption-top table">
<colgroup>
<col style="width: 27%">
<col style="width: 12%">
<col style="width: 34%">
<col style="width: 25%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Método</th>
<th style="text-align: left;">Precisión</th>
<th style="text-align: left;">Tiempo de inferencia (ms)</th>
<th style="text-align: left;">Uso de memoria (GB)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Absoluto (Senoidal)</td>
<td style="text-align: left;">88.2</td>
<td style="text-align: left;">12.3</td>
<td style="text-align: left;">2.1</td>
</tr>
<tr class="even">
<td style="text-align: left;">Relativo (RoPE)</td>
<td style="text-align: left;">89.7</td>
<td style="text-align: left;">14.5</td>
<td style="text-align: left;">2.4</td>
</tr>
<tr class="odd">
<td style="text-align: left;">CNN Multiescala</td>
<td style="text-align: left;">87.9</td>
<td style="text-align: left;">13.8</td>
<td style="text-align: left;">3.2</td>
</tr>
<tr class="even">
<td style="text-align: left;">Complejo (CLEX)</td>
<td style="text-align: left;">90.1</td>
<td style="text-align: left;">15.2</td>
<td style="text-align: left;">2.8</td>
</tr>
<tr class="odd">
<td style="text-align: left;">PE Dinámico</td>
<td style="text-align: left;">90.3</td>
<td style="text-align: left;">17.1</td>
<td style="text-align: left;">3.5</td>
</tr>
</tbody>
</table>
</section>
<section id="tendencias-de-investigación-recientes-2024" class="level4">
<h4 class="anchored" data-anchor-id="tendencias-de-investigación-recientes-2024">8. Tendencias de investigación recientes (2024)</h4>
<p>Recientemente, se han estado investigando nuevas técnicas de codificación posicional inspiradas en sistemas cuánticos y biológicos.</p>
<ul>
<li><strong>Codificación Posicional Cuántica</strong>:
<ul>
<li>Uso de puertas de rotación de qubits: <span class="math inline">\(R_z(\theta_i)|x\rangle\)</span></li>
<li>Búsqueda de posición basada en el algoritmo de Grover</li>
</ul></li>
<li><strong>Codificación Bioinspirada</strong>:
<ul>
<li>Aplicación de la regla STDP (Spike-Timing-Dependent Plasticity) de plasticidad sináptica: <span class="math inline">\(\Delta w_{ij} \propto e^{-\frac{|i-j|}{\tau}}\)</span></li>
</ul></li>
<li><strong>Integración con Redes Neuronales de Grafos</strong>:
<ul>
<li>Representación de posiciones como nodos y relaciones como aristas: <span class="math inline">\(P(i) = \sum_{j \in \mathcal{N}(i)} \alpha_{ij}Wx_j\)</span></li>
</ul></li>
</ul>
</section>
<section id="guía-de-selección" class="level4">
<h4 class="anchored" data-anchor-id="guía-de-selección">9. Guía de selección</h4>
<ul>
<li><strong>Secuencias de longitud fija</strong>: PE aprendible. Bajo riesgo de sobreajuste y fácil optimización.</li>
<li><strong>Longitud variable/extrapolación necesaria</strong>: RoPE. Excelente escalabilidad de longitud debido a la invarianza rotacional.</li>
<li><strong>Procesamiento en tiempo real de baja latencia</strong>: Basado en CNN. Optimal para procesamiento paralelo y aceleración por hardware.</li>
<li><strong>Procesamiento de señales físicas</strong>: PE complejo. Preserva información de frecuencia y compatibilidad con transformadas de Fourier.</li>
<li><strong>Datos multimodales</strong>: PE dinámico. Adaptación receptiva al contexto cruzado modal.</li>
</ul>
</section>
<section id="apéndice-matemático" class="level4">
<h4 class="anchored" data-anchor-id="apéndice-matemático">Apéndice matemático</h4>
<ul>
<li><p><strong>Características de teoría de grupos de RoPE</strong>:</p>
<p>Representación del grupo de rotación SO(2): <span class="math inline">\(R(\theta) = \begin{bmatrix} \cos\theta &amp; -\sin\theta \\ \sin\theta &amp; \cos\theta \end{bmatrix}\)</span></p>
<p>Esta propiedad garantiza la preservación de la posición relativa en las puntuaciones de atención.</p></li>
<li><p><strong>Cálculo eficiente del sesgo de posición relativa</strong>:</p>
<p>Uso de la estructura de matriz Toeplitz: <span class="math inline">\(B = [b_{i-j}]_{i,j}\)</span></p>
<p>Implementación posible con complejidad <span class="math inline">\(O(n\log n)\)</span> utilizando FFT.</p></li>
<li><p><strong>Flujo de gradiente de PE complejo</strong>:</p>
<p>Aplicación de las reglas de diferenciación de Wirtinger: <span class="math inline">\(\frac{\partial L}{\partial z} = \frac{1}{2}\left(\frac{\partial L}{\partial \text{Re}(z)} - i\frac{\partial L}{\partial \text{Im}(z)}\right)\)</span></p></li>
</ul>
<hr>
<p><strong>Conclusión</strong>: La codificación posicional es un elemento clave que tiene un gran impacto en el rendimiento del modelo de transformadores y ha evolucionado de diversas maneras más allá de las funciones simples de seno-coseno. Cada método tiene sus propias ventajas y desventajas, así como fundamentos matemáticos, y es importante seleccionar el método adecuado según las características y requisitos del problema. Recientemente, se han estado investigando nuevas técnicas de codificación posicional inspiradas en diversos campos, como la computación cuántica y la biología, lo que sugiere un desarrollo continuo esperado en el futuro.</p>
</section>
</section>
</div>
</div>
</section>
</section>
<section id="arquitectura-completa-del-transformer" class="level2">
<h2 class="anchored" data-anchor-id="arquitectura-completa-del-transformer">8.4 Arquitectura completa del transformer</h2>
<p>Hasta ahora hemos examinado cómo se han desarrollado los componentes clave del transformer. Ahora veamos cómo estos elementos se integran en una arquitectura completa. Esta es la arquitectura completa del transformer.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../../assets/images/transformer/tr_architecture.png" class="img-fluid figure-img"></p>
<figcaption>Arquitectura del transformer</figcaption>
</figure>
</div>
<p><em>Fuente de la figura: The Illustrated Transformer (Jay Alammar, 2018) Licencia CC BY 4.0</em></p>
<p>El código de implementación del transformer para fines educativos se encuentra en chapter_08/transformer. Esta implementación se basa y modifica The Annotated Transformer del grupo Harvard NLP. Los principales cambios son los siguientes.</p>
<ol type="1">
<li><strong>Modularización:</strong> La implementación que estaba en un solo archivo se ha dividido en varios módulos para mejorar la legibilidad y reutilizabilidad.</li>
<li><strong>Adopción de la estructura Pre-LN:</strong> A diferencia del artículo original, utilizamos una estructura Pre-LN donde la normalización de capa se aplica <em>antes</em> de las operaciones de atención/feeds-forward (recientes estudios han reportado que Pre-LN es más favorable para la estabilidad y el rendimiento del entrenamiento).</li>
<li><strong>Añadida clase <code>TransformerConfig</code>:</strong> Se ha introducido una clase separada para configurar el modelo, facilitando la gestión de hiperparámetros.</li>
<li><strong>Implementación estilo PyTorch:</strong> Se han utilizado funciones de PyTorch como <code>nn.ModuleList</code> para hacer que el código sea más conciso e intuitivo.</li>
<li>El optimizador Noam se implementó pero no se utilizó.</li>
</ol>
<section id="integración-de-componentes-básicos" class="level3">
<h3 class="anchored" data-anchor-id="integración-de-componentes-básicos">8.4.1 Integración de componentes básicos</h3>
<p>El transformer está compuesto principalmente por un <strong>codificador (Encoder)</strong> y un <strong>decodificador (Decoder)</strong>, con los siguientes componentes:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 19%">
<col style="width: 38%">
<col style="width: 42%">
</colgroup>
<thead>
<tr class="header">
<th>Componente</th>
<th>Codificador</th>
<th>Decodificador</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Atención multi-cabeza</strong></td>
<td>Atención a sí misma (Self-Attention)</td>
<td>Atención a sí misma enmascarada (Masked Self-Attention)</td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td>Atención codificador-decodificador (Encoder-Decoder Attention)</td>
</tr>
<tr class="odd">
<td><strong>Red de alimentación hacia adelante</strong></td>
<td>Aplicado independientemente en cada posición</td>
<td>Aplicado independientemente en cada posición</td>
</tr>
<tr class="even">
<td><strong>Conexión residual</strong></td>
<td>Suma la entrada y salida de cada subcapa (atención, feed-forward)</td>
<td>Suma la entrada y salida de cada subcapa (atención, feed-forward)</td>
</tr>
<tr class="odd">
<td><strong>Normalización de capa</strong></td>
<td>Aplicada a la entrada de cada subcapa (Pre-LN)</td>
<td>Aplicada a la entrada de cada subcapa (Pre-LN)</td>
</tr>
</tbody>
</table>
<p><strong>Capa del codificador - código</strong></p>
<div id="cell-60" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> TransformerEncoderLayer(nn.Module):</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config):</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.attention <span class="op">=</span> MultiHeadAttention(config)</span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.feed_forward <span class="op">=</span> FeedForward(config)</span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a>        <span class="co"># SublayerConnection for Pre-LN structure</span></span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.sublayer <span class="op">=</span> nn.ModuleList([</span>
<span id="cb33-8"><a href="#cb33-8" aria-hidden="true" tabindex="-1"></a>            SublayerConnection(config) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>)</span>
<span id="cb33-9"><a href="#cb33-9" aria-hidden="true" tabindex="-1"></a>        ])</span>
<span id="cb33-10"><a href="#cb33-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-11"><a href="#cb33-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x, attention_mask<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb33-12"><a href="#cb33-12" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.sublayer[<span class="dv">0</span>](x, <span class="kw">lambda</span> x: <span class="va">self</span>.attention(x, x, x, attention_mask))</span>
<span id="cb33-13"><a href="#cb33-13" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.sublayer[<span class="dv">1</span>](x, <span class="va">self</span>.feed_forward)</span>
<span id="cb33-14"><a href="#cb33-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<ul>
<li><p><strong>Atención Multi-Cabeza (Multi-Head Attention):</strong> calcula las relaciones entre todas las posiciones de la secuencia de entrada en paralelo. Cada cabeza analiza la secuencia desde una perspectiva diferente y sintetiza los resultados para capturar información contextual rica. (“The cat sits on the mat” ejemplo donde diferentes cabezas aprenden relaciones sujeto-verbo, frase preposicional, artículo-sustantivo, etc.)</p>
<ul>
<li><strong>Red Feed-Forward:</strong> es una red compuesta por dos transformaciones lineales y la función de activación GELU que se aplica <em>independientemente</em> a cada posición.</li>
</ul></li>
</ul>
<div id="cell-62" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> FeedForward(nn.Module):</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config):</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.linear1 <span class="op">=</span> nn.Linear(config.hidden_size, config.intermediate_size)</span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.linear2 <span class="op">=</span> nn.Linear(config.intermediate_size, config.hidden_size)</span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.activation <span class="op">=</span> nn.GELU()</span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.linear1(x)</span>
<span id="cb34-10"><a href="#cb34-10" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.activation(x)</span>
<span id="cb34-11"><a href="#cb34-11" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.linear2(x)</span>
<span id="cb34-12"><a href="#cb34-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>La razón por la que se necesita una red feedforward está relacionada con la densidad de información de las salidas de atención. El resultado de la operación de atención (<span class="math inline">\(\text{Attention}(Q, K, V) = \text{softmax}(\frac{QK^T}{\sqrt{d\_k}})V\)</span>) es una suma ponderada de los vectores <span class="math inline">\(V\)</span>, donde la información contextual está <em>concentrada</em> en <span class="math inline">\(d\_{model}\)</span> dimensiones (512 en el artículo). <strong>Si se aplica directamente la función de activación ReLU, gran parte de esta información concentrada puede perderse (ReLU convierte los valores negativos en 0)</strong>. Por lo tanto, la red feedforward primero expande las <span class="math inline">\(d\_{model}\)</span> dimensiones a una dimensión mayor (<span class="math inline">\(4 \times d\_{model}\)</span>, 2048 en el artículo) para ampliar el espacio de representación, luego aplica ReLU (o GELU) y vuelve a reducir la dimensionalidad a la original, añadiendo no linealidad.</p>
<div id="cell-64" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> W1(x)    <span class="co"># hidden_size -&gt; intermediate_size (512 -&gt; 2048)</span></span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> ReLU(x)  <span class="co"># or GELU</span></span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> W2(x)    <span class="co"># intermediate_size -&gt; hidden_size (2048 -&gt; 512)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<ul>
<li><p><strong>Conexión residual (Residual Connection):</strong> Se trata de sumar la entrada y la salida de cada subcapa (atención multi-cabeza o red feedforward). Esto ayuda a mitigar el problema de desvanecimiento/explotación del gradiente y facilita el aprendizaje en redes profundas. (Consulte el Capítulo 7 sobre conexiones residuales).</p></li>
<li><p><strong>Normalización de capa (Layer Normalization):</strong> Se aplica a la <em>entrada</em> de cada subcapa (Pre-LN).</p></li>
</ul>
<div id="cell-66" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LayerNorm(nn.Module):</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config):</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.gamma <span class="op">=</span> nn.Parameter(torch.ones(config.hidden_size))</span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.beta <span class="op">=</span> nn.Parameter(torch.zeros(config.hidden_size))</span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.eps <span class="op">=</span> config.layer_norm_eps</span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-8"><a href="#cb36-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb36-9"><a href="#cb36-9" aria-hidden="true" tabindex="-1"></a>        mean <span class="op">=</span> x.mean(<span class="op">-</span><span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb36-10"><a href="#cb36-10" aria-hidden="true" tabindex="-1"></a>        std <span class="op">=</span> (x <span class="op">-</span> mean).<span class="bu">pow</span>(<span class="dv">2</span>).mean(<span class="op">-</span><span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>).sqrt()</span>
<span id="cb36-11"><a href="#cb36-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.gamma <span class="op">*</span> (x <span class="op">-</span> mean) <span class="op">/</span> (std <span class="op">+</span> <span class="va">self</span>.eps) <span class="op">+</span> <span class="va">self</span>.beta</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>La normalización de capas es una técnica propuesta en el paper “<a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://arxiv.org/abs/1607.06450">Layer Normalization</a>” de Ba, Kiros y Hinton en 2016. Mientras que la normalización por lotes (Batch Normalization) realiza la normalización a lo largo de la dimensión del lote, la normalización de capas calcula la media y la varianza a lo largo de la dimensión de características de cada muestra para realizar la normalización.</p>
<p><strong>Ventajas de la normalización de capas</strong></p>
<ol type="1">
<li><strong>Independencia del tamaño del lote:</strong> No se ve afectada por el tamaño del lote, funcionando de manera estable incluso con lotes pequeños o en entornos de aprendizaje en línea (online learning).</li>
<li><strong>Irrelevancia de la longitud de secuencia:</strong> Es adecuada para modelos que manejan secuencias de longitud variable, como RNN y Transformer.</li>
<li><strong>Estabilización y aceleración del aprendizaje:</strong> Estabiliza la distribución de entrada de cada capa, mitigando problemas de desvanecimiento o explosión de gradientes y aumentando la velocidad de aprendizaje.</li>
</ol>
<p>En el caso del Transformer, se utiliza el método Pre-LN para aplicar la normalización de capas <em>antes</em> de pasar por cada subcapa (atención multi-cabeza, red feed-forward).</p>
<p><strong>Visualización de la normalización de capas</strong></p>
<div id="cell-68" class="cell" data-execution_count="19">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_08.visualize_layer_norm <span class="im">import</span> visualize_layer_normalization</span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>visualize_layer_normalization()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="08_El nacimiento del transformer_files/figure-html/cell-30-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>========================================
Input Data Shape: (2, 5, 6)
Mean Shape: (2, 5, 1)
Standard Deviation Shape: (2, 5, 1)
Normalized Data Shape: (2, 5, 6)
Gamma (Scale) Values:
 [0.95208258 0.9814341  0.8893665  0.88037934 1.08125258 1.135624  ]
Beta (Shift) Values:
 [-0.00720101  0.10035329  0.0361636  -0.06451198  0.03613956  0.15380366]
Scaled &amp; Shifted Data Shape: (2, 5, 6)
========================================</code></pre>
</div>
</div>
<p>La imagen anterior muestra cómo funciona la normalización de capa (Layer Normalization) paso a paso.</p>
<ul>
<li><strong>Datos originales (superior izquierda):</strong> Los datos antes de la normalización están ampliamente dispersos, con una media y desviación estándar variables.</li>
<li><strong>Después de la normalización (superior derecha):</strong> Los datos se agrupan alrededor de una media de 0 y una desviación estándar cercana a 1.</li>
<li><strong>Escala y desplazamiento (centro):</strong> Se aplican parámetros aprendibles γ (gamma, escala) y β (beta, desplazamiento) para introducir pequeñas variaciones en la distribución de los datos. Esto ajusta la capacidad expresiva del modelo.</li>
<li><strong>Mapa de calor (inferior):</strong> Muestra el cambio individual de valores antes y después de la normalización, así como después de aplicar la escala/desplazamiento, basándose en los datos del primer lote.</li>
<li><strong>Valores γ/β (inferior derecha):</strong> Representa los valores de γ y β para cada dimensión oculta mediante gráficos de barras.</li>
</ul>
<p>De esta manera, la normalización de capa normaliza las entradas de cada capa para mejorar la estabilidad y velocidad del aprendizaje.</p>
<p><strong>Clave:</strong></p>
<ul>
<li>Normalización de entrada de cada capa (media 0, desviación estándar 1)</li>
<li>Ajuste de capacidad expresiva con escalas (γ) y desplazamientos (β) aprendibles</li>
<li>Mantenimiento de independencia entre muestras, a diferencia de la normalización por lotes</li>
</ul>
<p>Esta combinación de componentes (atención multi-cabeza, red feedforward, conexión residual, normalización de capa) maximiza las ventajas de cada elemento. La atención multi-cabeza captura diversos aspectos de la secuencia de entrada, la red feedforward añade no linealidad, y las conexiones residuales y la normalización de capa permiten un aprendizaje estable incluso en redes profundas.</p>
</section>
<section id="composición-del-codificador" class="level3">
<h3 class="anchored" data-anchor-id="composición-del-codificador">8.4.2 Composición del codificador</h3>
<p>El transformador tiene una estructura de codificador-decodificador para la traducción automática. El codificador comprende el idioma de origen (por ejemplo, inglés) y el decodificador genera el idioma objetivo (por ejemplo, francés). Aunque el codificador y el decodificador comparten componentes básicos como la atención multi-cabeza y las redes feedforward, están configurados de manera diferente para adaptarse a sus respectivos propósitos.</p>
<p><strong>Comparación de la composición del codificador vs.&nbsp;decodificador</strong></p>
<table class="caption-top table">
<colgroup>
<col style="width: 19%">
<col style="width: 33%">
<col style="width: 47%">
</colgroup>
<thead>
<tr class="header">
<th>Componente</th>
<th>Codificador</th>
<th>Decodificador</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Número de capas de atención</td>
<td>1 (autoatención)</td>
<td>2 (autoatención enmascarada, atención codificador-decodificador)</td>
</tr>
<tr class="even">
<td>Estrategia de máscara</td>
<td>Solo utiliza máscaras de relleno</td>
<td>Máscaras de relleno + máscaras de causalidad</td>
</tr>
<tr class="odd">
<td>Procesamiento contextual</td>
<td>Procesamiento de contexto bidireccional</td>
<td>Procesamiento de contexto unidireccional (autoregresivo)</td>
</tr>
<tr class="even">
<td>Referencia de entrada</td>
<td>Solo se refiere a su propia entrada</td>
<td>Se refiere a su propia entrada + salida del codificador</td>
</tr>
</tbody>
</table>
<p>A continuación, se resumen varios términos de atención.</p>
<p><strong>Resumen de conceptos de atención</strong></p>
<table class="caption-top table">
<colgroup>
<col style="width: 14%">
<col style="width: 25%">
<col style="width: 11%">
<col style="width: 48%">
</colgroup>
<thead>
<tr class="header">
<th>Tipo de atención</th>
<th>Características</th>
<th>Ubicación de descripción</th>
<th>Concepto clave</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Atención (básica)</td>
<td>- Cálculo de similitud mediante vectores de palabras idénticos<br>- Generación de información contextual a través de una simple suma ponderada<br>- Versión simplificada para aplicar en modelos seq2seq</td>
<td>8.2.2</td>
<td>- Cálculo de similitud mediante producto interno de vectores de palabras<br>- Conversión de pesos con softmax<br>- Aplicación de máscaras de relleno a toda la atención</td>
</tr>
<tr class="even">
<td>Autoatención (Self-Attention)</td>
<td>- Separación del espacio Q, K, V<br>- Optimización independiente en cada espacio<br>- La secuencia de entrada se refiere a sí misma<br>- Utilizado en el codificador</td>
<td>8.2.3</td>
<td>- Separación de roles para cálculo de similitud y transmisión de información<br>- Transformaciones aprendibles Q, K, V<br>- Posibilidad de procesamiento contextual bidireccional</td>
</tr>
<tr class="odd">
<td>Autoatención enmascarada</td>
<td>- Bloqueo de información futura<br>- Uso de máscaras causales<br>- Utilizado en el decodificador</td>
<td>8.2.5</td>
<td>- Máscara triangular superior para bloquear la información futura<br>- Posibilidad de generación autoregresiva<br>- Procesamiento contextual unidireccional</td>
</tr>
<tr class="even">
<td>Atención cruzada (codificador-decodificador)</td>
<td>- Query: estado del decodificador<br>- Key, Value: salida del codificador<br>- También llamada atención cruzada<br>- Utilizado en el decodificador</td>
<td>8.4.3</td>
<td>- El decodificador hace referencia a la información del codificador<br>- Cálculo de relaciones entre dos secuencias<br>- Refleja el contexto durante la traducción/generación</td>
</tr>
</tbody>
</table>
<p>En el transformador, se utilizan los nombres autoatención, enmascarada y cruzada. Mecanismos de atención son idénticos y se distinguen según la fuente de Q, K, V.</p>
<p><strong>Componentes del codificador</strong> | Componente | Descripción | | —————————- | ——————————————————————————————— | | Embeddings | Convierte los tokens de entrada en vectores y agrega información de posición para codificar el significado y el orden de la secuencia de entrada. | | TransformerEncoderLayer (x N) | Apila múltiples capas de la misma capa para extraer características más abstractas y complejas de forma jerárquica a partir de la secuencia de entrada. | | LayerNorm | Normaliza la distribución de las características de la salida final para estabilizarla y prepararla en un formato útil para el decodificador. |</p>
<div id="cell-71" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> TransformerEncoder(nn.Module):</span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config):</span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.embeddings <span class="op">=</span> Embeddings(config)</span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.layers <span class="op">=</span> nn.ModuleList([</span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a>            TransformerEncoderLayer(config) </span>
<span id="cb39-7"><a href="#cb39-7" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(config.num_hidden_layers)</span>
<span id="cb39-8"><a href="#cb39-8" aria-hidden="true" tabindex="-1"></a>        ])</span>
<span id="cb39-9"><a href="#cb39-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.norm <span class="op">=</span> LayerNorm(config)</span>
<span id="cb39-10"><a href="#cb39-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb39-11"><a href="#cb39-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, input_ids, attention_mask<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb39-12"><a href="#cb39-12" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.embeddings(input_ids)</span>
<span id="cb39-13"><a href="#cb39-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-14"><a href="#cb39-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i, layer <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="va">self</span>.layers):</span>
<span id="cb39-15"><a href="#cb39-15" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> layer(x, attention_mask)</span>
<span id="cb39-16"><a href="#cb39-16" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb39-17"><a href="#cb39-17" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> <span class="va">self</span>.norm(x)</span>
<span id="cb39-18"><a href="#cb39-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> output</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>El codificador está compuesto por una capa de incrustación, múltiples capas de codificador y una capa de normalización final.</p>
<p><strong>1. Mecanismo de autoatención (ejemplo)</strong></p>
<p>La autoatención del codificador calcula las relaciones entre todas las pares de palabras en la secuencia de entrada para enriquecer la información contextual de cada palabra.</p>
<ul>
<li><strong>Ejemplo:</strong> “The patient bear can bear the pain no longer.”</li>
<li><strong>Función:</strong> Al determinar el significado del segundo ‘bear’, la autoatención considera la relación con <em>todas</em> las palabras en la oración, como ‘patient’ (paciente), ‘bear’ (oso) y ‘pain’ (dolor). De esta manera, se puede determinar con precisión que ‘bear’ se usa en el sentido de ‘soportar’, ‘aguantar’ (procesamiento contextual bidireccional).</li>
</ul>
<p><strong>2. Importancia de la posición del dropout</strong></p>
<p>El dropout desempeña un papel crucial para prevenir el sobreajuste y mejorar la estabilidad del aprendizaje. En el codificador de transformers, se aplica dropout en las siguientes posiciones.</p>
<ul>
<li><strong>Después de la salida de incrustación:</strong> Inmediatamente después de que los embeddings de tokens y la información de posición se combinan.</li>
<li><strong>Después de cada subcapa (atención, FFN):</strong> Sigue una estructura Pre-LN (normalización → subcapa → dropout → conexión residual).</li>
<li><strong>En el interior del FFN:</strong> Después de aplicar la primera transformación lineal y la función de activación ReLU.</li>
</ul>
<p>Este esquema de aplicación de dropout regula el flujo de información, evita que el modelo dependa excesivamente de ciertas características y mejora el rendimiento generalización.</p>
<p><strong>3. Estructura de pila del codificador</strong></p>
<p>El codificador de transformers tiene una estructura en la que se apilan (stack) varias capas de codificador con la misma arquitectura.</p>
<ul>
<li><strong>Papel original:</strong> Se utilizan 6 capas de codificador.</li>
<li><strong>División de funciones</strong>:
<ul>
<li><strong>Capas inferiores:</strong> Aprendizaje de patrones lingüísticos superficiales, como palabras adyacentes y puntuación.</li>
<li><strong>Capas intermedias:</strong> Aprendizaje de estructuras gramaticales.</li>
<li><strong>Capas superiores:</strong> Aprendizaje de relaciones semánticas de nivel superior, como la referencia cruzada (coreference).</li>
</ul></li>
</ul>
<p>Cuanto más profundas son las capas, más características abstractas y complejas pueden aprender. Investigaciones posteriores han permitido el desarrollo de modelos con muchas más capas gracias a los avances en hardware y técnicas de aprendizaje (Pre-LayerNorm, recorte de gradientes, calentamiento del tasa de aprendizaje, aprendizaje de precisión mixta, acumulación de gradientes), como BERT-base: 12 capas, GPT-3: 96 capas, PaLM: 118 capas.</p>
<p><strong>4. Salida final del codificador y uso en el decodificador</strong></p>
<p>La salida final del codificador es una representación vectorial que contiene información contextual rica para cada token de entrada. Esta salida se utiliza como <strong>Key</strong> y <strong>Value</strong> en la <strong>atención codificador-decodificador (Cross-Attention)</strong> del decodificador. El decodificador genera cada token de la secuencia de salida consultando la salida del codificador, lo que permite una traducción/creación precisa considerando el contexto de la oración original.</p>
</section>
<section id="estructura-del-decodificador" class="level3">
<h3 class="anchored" data-anchor-id="estructura-del-decodificador">8.4.3 Estructura del decodificador</h3>
<p>El decodificador es similar al codificador, pero difiere en que genera la salida de manera autoregresiva (autoregressive).</p>
<p><strong>Código completo de la capa del decodificador</strong></p>
<div id="cell-74" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> TransformerDecoderLayer(nn.Module):</span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config):</span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.self_attn <span class="op">=</span> MultiHeadAttention(config)</span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.cross_attn <span class="op">=</span> MultiHeadAttention(config)</span>
<span id="cb40-6"><a href="#cb40-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.feed_forward <span class="op">=</span> FeedForward(config)</span>
<span id="cb40-7"><a href="#cb40-7" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb40-8"><a href="#cb40-8" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Pre-LN을 위한 레이어 정규화</span></span>
<span id="cb40-9"><a href="#cb40-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.norm1 <span class="op">=</span> LayerNorm(config)</span>
<span id="cb40-10"><a href="#cb40-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.norm2 <span class="op">=</span> LayerNorm(config)</span>
<span id="cb40-11"><a href="#cb40-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.norm3 <span class="op">=</span> LayerNorm(config)</span>
<span id="cb40-12"><a href="#cb40-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(config.dropout_prob)</span>
<span id="cb40-13"><a href="#cb40-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-14"><a href="#cb40-14" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x, memory, src_mask<span class="op">=</span><span class="va">None</span>, tgt_mask<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb40-15"><a href="#cb40-15" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Pre-LN 구조</span></span>
<span id="cb40-16"><a href="#cb40-16" aria-hidden="true" tabindex="-1"></a>        m <span class="op">=</span> <span class="va">self</span>.norm1(x)</span>
<span id="cb40-17"><a href="#cb40-17" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.dropout(<span class="va">self</span>.self_attn(m, m, m, tgt_mask))</span>
<span id="cb40-18"><a href="#cb40-18" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb40-19"><a href="#cb40-19" aria-hidden="true" tabindex="-1"></a>        m <span class="op">=</span> <span class="va">self</span>.norm2(x)</span>
<span id="cb40-20"><a href="#cb40-20" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.dropout(<span class="va">self</span>.cross_attn(m, memory, memory, src_mask))</span>
<span id="cb40-21"><a href="#cb40-21" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb40-22"><a href="#cb40-22" aria-hidden="true" tabindex="-1"></a>        m <span class="op">=</span> <span class="va">self</span>.norm3(x)</span>
<span id="cb40-23"><a href="#cb40-23" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.dropout(<span class="va">self</span>.feed_forward(m))</span>
<span id="cb40-24"><a href="#cb40-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><strong>Componentes principales del decodificador y sus roles</strong></p>
<table class="caption-top table">
<colgroup>
<col style="width: 9%">
<col style="width: 43%">
<col style="width: 46%">
</colgroup>
<thead>
<tr class="header">
<th>Subcapa</th>
<th>Rol</th>
<th>Características de implementación</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Atención propia enmascarada</td>
<td>Identificar las relaciones entre las palabras dentro de la secuencia de salida generada hasta el momento, evitar la referencia a información futura (generación autoregresiva)</td>
<td><code>tgt_mask</code> (máscara causal + máscara de padding) usada, <code>self.self_attn</code></td>
</tr>
<tr class="even">
<td>Atención codificador-decodificador (atención cruzada)</td>
<td>El decodificador se refiere a la salida del codificador (información contextual de la oración de entrada) para obtener información relevante al palabra actual que se está generando</td>
<td><code>Q</code>: decodificador, <code>K</code>, <code>V</code>: codificador, <code>src_mask</code> (máscara de padding) usada, <code>self.cross_attn</code></td>
</tr>
<tr class="odd">
<td>Red de alimentación hacia adelante</td>
<td>Transformar independientemente las representaciones en cada posición para generar representaciones más ricas</td>
<td>Estructura idéntica a la del codificador, <code>self.feed_forward</code></td>
</tr>
<tr class="even">
<td>Normalización de capa (LayerNorm)</td>
<td>Normalizar la entrada de cada subcapa (Pre-LN), mejorar la estabilidad y rendimiento del aprendizaje</td>
<td><code>self.norm1</code>, <code>self.norm2</code>, <code>self.norm3</code></td>
</tr>
<tr class="odd">
<td>Apagado aleatorio (Dropout)</td>
<td>Prevenir el sobreajuste, mejorar el rendimiento generalización</td>
<td>Aplicada a la salida de cada subcapa, <code>self.dropout</code></td>
</tr>
<tr class="even">
<td>Conexión residual (Residual Connection)</td>
<td>Mitigar problemas de desvanecimiento/explotación del gradiente en redes profundas, mejorar el flujo de información</td>
<td>Sumar la entrada y la salida de cada subcapa</td>
</tr>
</tbody>
</table>
<p><strong>1. Atención propia enmascarada (Masked Self-Attention)</strong> * <strong>Rol:</strong> El decodificador genera la salida de manera autoregresiva (autoregressive), es decir, no puede hacer referencia a las palabras que <em>futuras</em> aparecerán después de la palabra actualmente en generación. Por ejemplo, al traducir “I love you”, una vez generado “yo” y al generar “te”, no se puede acceder al token “amo” que aún no ha sido generado. * <strong>Implementación:</strong> Se utiliza <code>tgt_mask</code>, que combina una máscara de causalidad (causal mask) y una máscara de padding. La máscara de causalidad llena la parte superior triangular de la matriz con <code>-inf</code> para hacer que los pesos de atención a tokens futuros sean 0 (ver sección 8.2.5). En el método <code>forward</code> de <code>TransformerDecoderLayer</code>, esta máscara se aplica en la parte <code>self.self_attn(m, m, m, tgt_mask)</code>.</p>
<p><strong>2. Atención codificador-decodificador (Cross-Attention)</strong></p>
<ul>
<li><strong>Rol:</strong> El decodificador se refiere a la salida del codificador (la información contextual de la oración de entrada) para obtener información relevante al generar la palabra actual. Esto es crucial en tareas de traducción, ya que permite que el decodificador comprenda con precisión el significado de la oración original y seleccione las palabras de traducción adecuadas.</li>
<li><strong>Implementación:</strong>
<ul>
<li><strong>Query (Q):</strong> El estado actual del decodificador (la salida de la autoatención enmascarada)</li>
<li><strong>Key (K):</strong> La salida del codificador (<code>memory</code>)</li>
<li><strong>Value (V):</strong> La salida del codificador (<code>memory</code>)</li>
<li>Se utiliza <code>src_mask</code> (máscara de padding) para ignorar los tokens de padding en la salida del codificador.</li>
<li>En el método <code>forward</code> de <code>TransformerDecoderLayer</code>, esta atención se realiza en la parte <code>self.cross_attn(m, memory, memory, src_mask)</code>. <code>memory</code> representa la salida del codificador.</li>
</ul></li>
</ul>
<p><strong>3. Estructura de pila del decodificador</strong></p>
<div id="cell-76" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> TransformerDecoder(nn.Module):</span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config):</span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.embeddings <span class="op">=</span> Embeddings(config)</span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.layers <span class="op">=</span> nn.ModuleList([</span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a>            TransformerDecoderLayer(config)</span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(config.num_hidden_layers)</span>
<span id="cb41-8"><a href="#cb41-8" aria-hidden="true" tabindex="-1"></a>        ])</span>
<span id="cb41-9"><a href="#cb41-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.norm <span class="op">=</span> LayerNorm(config)</span>
<span id="cb41-10"><a href="#cb41-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-11"><a href="#cb41-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x, memory, src_mask<span class="op">=</span><span class="va">None</span>, tgt_mask<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb41-12"><a href="#cb41-12" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.embeddings(x)</span>
<span id="cb41-13"><a href="#cb41-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> layer <span class="kw">in</span> <span class="va">self</span>.layers:</span>
<span id="cb41-14"><a href="#cb41-14" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> layer(x, memory, src_mask, tgt_mask)</span>
<span id="cb41-15"><a href="#cb41-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.norm(x)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<ul>
<li>El decodificador está compuesto por varias capas de <code>TransformerDecoderLayer</code> (6 capas en el artículo original).</li>
<li>Cada capa ejecuta secuencialmente la autoatención mascara, la atención codificador-decodificador y la red feedforward.</li>
<li>La estructura Pre-LN y las conexiones residuales se aplican a cada subcapa. Esto permite un aprendizaje estable incluso en redes profundas.</li>
<li>El método <code>forward</code> de la clase <code>TransformerDecoder</code> recibe como entrada <code>x</code> (entrada del decodificador), <code>memory</code> (salida del codificador), <code>src_mask</code> (máscara de padding del codificador) y <code>tgt_mask</code> (máscara del decodificador), pasa secuencialmente a través de las capas del decodificador y devuelve la salida final.</li>
</ul>
<p><strong>Número de capas de codificador/decodificador por modelo</strong></p>
<table class="caption-top table">
<colgroup>
<col style="width: 17%">
<col style="width: 4%">
<col style="width: 18%">
<col style="width: 19%">
<col style="width: 21%">
<col style="width: 17%">
</colgroup>
<thead>
<tr class="header">
<th>Modelo</th>
<th>Año</th>
<th>Estructura</th>
<th>Capas de codificador</th>
<th>Capas de decodificador</th>
<th>Parámetros totales</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Transformer original</td>
<td>2017</td>
<td>Codificador-decodificador</td>
<td>6</td>
<td>6</td>
<td>65M</td>
</tr>
<tr class="even">
<td>BERT-base</td>
<td>2018</td>
<td>Solo codificador</td>
<td>12</td>
<td>-</td>
<td>110M</td>
</tr>
<tr class="odd">
<td>GPT-2</td>
<td>2019</td>
<td>Solo decodificador</td>
<td>-</td>
<td>48</td>
<td>1.5B</td>
</tr>
<tr class="even">
<td>T5-base</td>
<td>2020</td>
<td>Codificador-decodificador</td>
<td>12</td>
<td>12</td>
<td>220M</td>
</tr>
<tr class="odd">
<td>GPT-3</td>
<td>2020</td>
<td>Solo decodificador</td>
<td>-</td>
<td>96</td>
<td>175B</td>
</tr>
<tr class="even">
<td>PaLM</td>
<td>2022</td>
<td>Solo decodificador</td>
<td>-</td>
<td>118</td>
<td>540B</td>
</tr>
<tr class="odd">
<td>Gemma-2</td>
<td>2024</td>
<td>Solo decodificador</td>
<td>-</td>
<td>18-36</td>
<td>2B-27B</td>
</tr>
</tbody>
</table>
<p>Los modelos recientes pueden aprender eficazmente un mayor número de capas gracias a técnicas avanzadas de aprendizaje como Pre-LN. Los decodificadores más profundos pueden aprender patrones lingüísticos más abstractos y complejos, lo que ha llevado a mejoras en el rendimiento en tareas de procesamiento de lenguaje natural como la traducción y la generación de texto.</p>
<p><strong>4. Generación de salida del decodificador y condiciones de finalización</strong></p>
<ul>
<li><strong>Generación de salida:</strong> La capa <code>generator</code> (capa lineal) de la clase <code>Transformer</code> convierte la salida final del decodificador en un vector de logit de tamaño de vocabulario (<code>vocab_size</code>) y aplica <code>log_softmax</code> para obtener la distribución de probabilidad de cada token. Se predice el siguiente token basado en esta distribución de probabilidad.</li>
</ul>
<div id="cell-78" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 최종 출력 생성 (설명용)</span></span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> <span class="va">self</span>.generator(decoder_output)</span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a><span class="cf">return</span> F.log_softmax(output, dim<span class="op">=-</span><span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<ul>
<li><strong>Condiciones de terminación</strong>
<ol type="1">
<li><strong>Alcanzar la longitud máxima:</strong> cuando se alcanza la longitud de salida predeterminada.</li>
<li><strong>Condiciones de terminación definidas por el usuario:</strong> cuando se cumple una condición específica (por ejemplo, un signo de puntuación).</li>
<li><strong>Generación de tokens especiales:</strong> cuando se genera un token especial que indica el final de la frase (<code>&lt;eos&gt;</code>, <code>&lt;/s&gt;</code> etc.). El decodificador aprende durante el proceso de entrenamiento a agregar este token especial al final de las frases.</li>
</ol></li>
<li><strong>Estrategias de generación de tokens</strong></li>
</ul>
<p>En general, aunque no están incluidas en el decodificador, las estrategias de generación de tokens pueden influir en los resultados de la salida generada.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 16%">
<col style="width: 13%">
<col style="width: 32%">
<col style="width: 17%">
<col style="width: 20%">
</colgroup>
<thead>
<tr class="header">
<th>Estrategia de generación</th>
<th>Cómo funciona</th>
<th>Ventajas</th>
<th>Desventajas</th>
<th>Ejemplo</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Greedy Search</strong></td>
<td>En cada paso, selecciona el token con la probabilidad más alta.</td>
<td>Rápido, implementación simple</td>
<td>Posibilidad de solución subóptima local, falta de diversidad</td>
<td>“Yo” siguiente → “voy a la escuela” (máxima probabilidad)</td>
</tr>
<tr class="even">
<td><strong>Beam Search</strong></td>
<td>Seguimiento simultáneo de <code>k</code> caminos.</td>
<td>Búsqueda amplia, posibilidad de mejores resultados</td>
<td>Costo computacional alto, diversidad limitada</td>
<td><code>k=2</code>: mantener “Yo voy a la escuela”, “Yo voy a casa” y proceder al siguiente paso</td>
</tr>
<tr class="odd">
<td><strong>Top-k Sampling</strong></td>
<td>Selecciona proporcionalmente según la probabilidad entre los <code>k</code> tokens más probables.</td>
<td>Diversidad adecuada, evita tokens extraños</td>
<td>Dificultad en establecer el valor de <code>k</code>, rendimiento dependiente del contexto</td>
<td><code>k=3</code>: “Yo” siguiente → {“voy a la escuela”, “voy a casa”, “voy al parque”} seleccionado según la probabilidad</td>
</tr>
<tr class="even">
<td><strong>Nucleus Sampling</strong></td>
<td>Selecciona entre los tokens cuya probabilidad acumulada no excede <code>p</code>.</td>
<td>Grupo de candidatos dinámico, flexible con el contexto</td>
<td>Necesidad de ajustar el valor de <code>p</code>, aumento de complejidad computacional</td>
<td><code>p=0.9</code>: “Yo” siguiente → {“voy a la escuela”, “voy a casa”, “voy al parque”, “como”} seleccionado sin exceder una probabilidad acumulada de 0.9</td>
</tr>
<tr class="odd">
<td><strong>Temperature Sampling</strong></td>
<td>Ajuste de la distribución de probabilidad (baja para ser más seguro, alta para ser más diverso).</td>
<td>Regulación de la creatividad de la salida, implementación simple</td>
<td>Demasiado alto puede resultar inapropiado, demasiado bajo puede generar texto repetitivo</td>
<td><code>T=0.5</code>: enfatiza las altas probabilidades, <code>T=1.5</code>: aumenta la posibilidad de seleccionar bajas probabilidades</td>
</tr>
</tbody>
</table>
<p>Estas estrategias de generación de tokens se implementan generalmente como clases o funciones separadas del decodificador.</p>
</section>
<section id="explicación-de-la-estructura-completa" class="level3">
<h3 class="anchored" data-anchor-id="explicación-de-la-estructura-completa">8.4.4 Explicación de la estructura completa</h3>
<p>Hasta ahora hemos entendido el propósito del diseño y el principio de funcionamiento del transformer. Con base en lo explicado hasta el 8.4.3, examinaremos la estructura completa del transformer. La implementación se ha modificado estructuralmente, incluyendo modularización, basándose en el contenido de Havard NLP, y se ha redactado de manera lo más concisa posible para fines educativos. En un entorno de producción real, se necesitarían adiciones como tipado de tipos para la estabilidad del código, procesamiento eficiente de tensores multidimensionales, validación de entrada y manejo de errores, optimización de memoria, y escalabilidad para soportar diversas configuraciones.</p>
<p>El código está en el directorio <code>chapter_08/transformer</code>.</p>
<p><strong>Función e implementación de la capa de incrustación</strong></p>
<p>La primera etapa del transformer es la capa de incrustación, que convierte los tokens de entrada en un espacio vectorial. La entrada es una secuencia de IDs de tokens enteros (por ejemplo: [101, 2045, 3012, …]), donde cada ID de token es un índice único del diccionario léxico. La capa de incrustación mapea estos IDs a vectores de alta dimensión (vectores de incrustación).</p>
<p>La dimensión de incrustación tiene un gran impacto en el rendimiento del modelo. Una dimensión grande puede representar información semántica rica, pero aumenta el costo computacional, mientras que una dimensión pequeña tiene el efecto contrario.</p>
<p>Tras pasar por la capa de incrustación, las dimensiones del tensor cambian de la siguiente manera:</p>
<ul>
<li>Entrada: (batch_size, seq_length) → Salida: (batch_size, seq_length, hidden_size)</li>
<li>Ejemplo: (32, 50) → (32, 50, 768)</li>
</ul>
<p>A continuación se muestra un ejemplo de código para realizar incrustaciones en el transformer.</p>
<div id="cell-81" class="cell" data-execution_count="20">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_08.transformer.config <span class="im">import</span> TransformerConfig</span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_08.transformer.embeddings <span class="im">import</span> Embeddings</span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a configuration object</span></span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a>config <span class="op">=</span> TransformerConfig()</span>
<span id="cb43-7"><a href="#cb43-7" aria-hidden="true" tabindex="-1"></a>config.vocab_size <span class="op">=</span> <span class="dv">1000</span>  <span class="co"># Vocabulary size</span></span>
<span id="cb43-8"><a href="#cb43-8" aria-hidden="true" tabindex="-1"></a>config.hidden_size <span class="op">=</span> <span class="dv">768</span>  <span class="co"># Embedding dimension</span></span>
<span id="cb43-9"><a href="#cb43-9" aria-hidden="true" tabindex="-1"></a>config.max_position_embeddings <span class="op">=</span> <span class="dv">512</span>  <span class="co"># Maximum sequence length</span></span>
<span id="cb43-10"><a href="#cb43-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-11"><a href="#cb43-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Create an embedding layer</span></span>
<span id="cb43-12"><a href="#cb43-12" aria-hidden="true" tabindex="-1"></a>embedding_layer <span class="op">=</span> Embeddings(config)</span>
<span id="cb43-13"><a href="#cb43-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-14"><a href="#cb43-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate random input tokens</span></span>
<span id="cb43-15"><a href="#cb43-15" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb43-16"><a href="#cb43-16" aria-hidden="true" tabindex="-1"></a>seq_length <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb43-17"><a href="#cb43-17" aria-hidden="true" tabindex="-1"></a>input_ids <span class="op">=</span> torch.tensor([</span>
<span id="cb43-18"><a href="#cb43-18" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">1</span>, <span class="dv">5</span>, <span class="dv">9</span>, <span class="dv">2</span>],  <span class="co"># First sequence</span></span>
<span id="cb43-19"><a href="#cb43-19" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">6</span>, <span class="dv">3</span>, <span class="dv">7</span>, <span class="dv">4</span>]   <span class="co"># Second sequence</span></span>
<span id="cb43-20"><a href="#cb43-20" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb43-21"><a href="#cb43-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-22"><a href="#cb43-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform embedding</span></span>
<span id="cb43-23"><a href="#cb43-23" aria-hidden="true" tabindex="-1"></a>embedded <span class="op">=</span> embedding_layer(input_ids)</span>
<span id="cb43-24"><a href="#cb43-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-25"><a href="#cb43-25" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Input shape: </span><span class="sc">{</span>input_ids<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb43-26"><a href="#cb43-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Output: Input shape: torch.Size([2, 4])</span></span>
<span id="cb43-27"><a href="#cb43-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-28"><a href="#cb43-28" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Shape after embedding: </span><span class="sc">{</span>embedded<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb43-29"><a href="#cb43-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Output: Shape after embedding: torch.Size([2, 4, 768])</span></span>
<span id="cb43-30"><a href="#cb43-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-31"><a href="#cb43-31" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Part of the embedding vector for the first token of the first sequence:"</span>)</span>
<span id="cb43-32"><a href="#cb43-32" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(embedded[<span class="dv">0</span>, <span class="dv">0</span>, :<span class="dv">10</span>])  <span class="co"># Print only the first 10 dimensions</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Input shape: torch.Size([2, 4])
Shape after embedding: torch.Size([2, 4, 768])

Part of the embedding vector for the first token of the first sequence:
tensor([-0.7838, -0.9194,  0.4240, -0.8408, -0.0876,  2.0239,  1.3892, -0.4484,
        -0.6902,  1.1443], grad_fn=&lt;SliceBackward0&gt;)</code></pre>
</div>
</div>
<p><strong>Clase de configuración</strong></p>
<p>La clase <code>TransformerConfig</code> define todos los hiperparámetros del modelo.</p>
<div id="cell-83" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> TransformerConfig:</span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.vocab_size <span class="op">=</span> <span class="dv">30000</span>          <span class="co"># Vocabulary size</span></span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hidden_size <span class="op">=</span> <span class="dv">768</span>           <span class="co"># Hidden layer dimension</span></span>
<span id="cb45-5"><a href="#cb45-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.num_hidden_layers <span class="op">=</span> <span class="dv">12</span>      <span class="co"># Number of encoder/decoder layers</span></span>
<span id="cb45-6"><a href="#cb45-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.num_attention_heads <span class="op">=</span> <span class="dv">12</span>    <span class="co"># Number of attention heads</span></span>
<span id="cb45-7"><a href="#cb45-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.intermediate_size <span class="op">=</span> <span class="dv">3072</span>    <span class="co"># FFN intermediate layer dimension</span></span>
<span id="cb45-8"><a href="#cb45-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hidden_dropout_prob <span class="op">=</span> <span class="fl">0.1</span>   <span class="co"># Hidden layer dropout probability</span></span>
<span id="cb45-9"><a href="#cb45-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.attention_probs_dropout_prob <span class="op">=</span> <span class="fl">0.1</span>  <span class="co"># Attention dropout probability</span></span>
<span id="cb45-10"><a href="#cb45-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.max_position_embeddings <span class="op">=</span> <span class="dv">512</span>  <span class="co"># Maximum sequence length</span></span>
<span id="cb45-11"><a href="#cb45-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.layer_norm_eps <span class="op">=</span> <span class="fl">1e-12</span>      <span class="co"># Layer normalization epsilon</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><code>vocab_size</code> es el número total de tokens únicos que el modelo puede manejar. Aquí, para una implementación simple, asumimos la tokenización a nivel de palabras y lo establecemos en 30,000. En modelos de lenguaje reales, se utilizan diversos tokenizadores subpalabra como BPE (Byte Pair Encoding), Unigram, WordPiece, etc., y en este caso, <code>vocab_size</code> puede ser menor. Por ejemplo, la palabra ‘playing’ puede descomponerse en ‘play’ e ‘ing’, lo que permite representarla con solo dos subpalabras.</p>
<p><strong>Cambio de dimensiones del tensor de atención</strong></p>
<p>En la atención multi-cabeza, cada cabeza reorganiza las dimensiones del tensor de entrada para calcular la atención de manera independiente.</p>
<div id="cell-85" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MultiHeadAttention(nn.Module):</span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, query, key, value, mask<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a>        batch_size <span class="op">=</span> query.size(<span class="dv">0</span>)</span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb46-5"><a href="#cb46-5" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Linear transformations and head splitting</span></span>
<span id="cb46-6"><a href="#cb46-6" aria-hidden="true" tabindex="-1"></a>        query <span class="op">=</span> <span class="va">self</span>.linears[<span class="dv">0</span>](query).view(batch_size, <span class="op">-</span><span class="dv">1</span>, <span class="va">self</span>.h, <span class="va">self</span>.d_k).transpose(<span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb46-7"><a href="#cb46-7" aria-hidden="true" tabindex="-1"></a>        key <span class="op">=</span> <span class="va">self</span>.linears[<span class="dv">1</span>](key).view(batch_size, <span class="op">-</span><span class="dv">1</span>, <span class="va">self</span>.h, <span class="va">self</span>.d_k).transpose(<span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb46-8"><a href="#cb46-8" aria-hidden="true" tabindex="-1"></a>        value <span class="op">=</span> <span class="va">self</span>.linears[<span class="dv">2</span>](value).view(batch_size, <span class="op">-</span><span class="dv">1</span>, <span class="va">self</span>.h, <span class="va">self</span>.d_k).transpose(<span class="dv">1</span>, <span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>El proceso de transformación dimensional es el siguiente:</p>
<ol type="1">
<li>Entrada: (batch_size, seq_len, d_model)</li>
<li>Transformación lineal: (batch_size, seq_len, d_model)</li>
<li><code>view</code>: (batch_size, seq_len, h, d_k)</li>
<li><code>transpose</code>: (batch_size, h, seq_len, d_k)</li>
</ol>
<p>Aquí, h es el número de cabezas y d_k es la dimensión de cada cabeza (d_model / h). A través de este reordenamiento dimensional, cada cabeza calcula la atención de manera independiente.</p>
<p><strong>Estructura integrada del transformador</strong></p>
<p>Finalmente, examinemos la clase <code>Transformer</code> que integra todos los componentes.</p>
<div id="cell-87" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Transformer(nn.Module):</span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config: TransformerConfig):</span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb47-4"><a href="#cb47-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.encoder <span class="op">=</span> TransformerEncoder(config)</span>
<span id="cb47-5"><a href="#cb47-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.decoder <span class="op">=</span> TransformerDecoder(config)</span>
<span id="cb47-6"><a href="#cb47-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.generator <span class="op">=</span> nn.Linear(config.hidden_size, config.vocab_size)</span>
<span id="cb47-7"><a href="#cb47-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._init_weights()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>El transformador consta de tres componentes principales.</p>
<ol type="1">
<li>Codificador: procesa la secuencia de entrada.</li>
<li>Decodificador: genera la secuencia de salida.</li>
<li>Generador: convierte la salida del decodificador en probabilidades léxicas.</li>
</ol>
<p>El método <code>forward</code> procesa los datos en el siguiente orden.</p>
<div id="cell-89" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> forward(<span class="va">self</span>, src, tgt, src_mask<span class="op">=</span><span class="va">None</span>, tgt_mask<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Encoder-decoder processing</span></span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a>    encoder_output <span class="op">=</span> <span class="va">self</span>.encode(src, src_mask)</span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a>    decoder_output <span class="op">=</span> <span class="va">self</span>.decode(encoder_output, src_mask, tgt, tgt_mask)</span>
<span id="cb48-5"><a href="#cb48-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb48-6"><a href="#cb48-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Generate final output</span></span>
<span id="cb48-7"><a href="#cb48-7" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> <span class="va">self</span>.generator(decoder_output)</span>
<span id="cb48-8"><a href="#cb48-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> F.log_softmax(output, dim<span class="op">=-</span><span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>El cambio de dimensiones del tensor es el siguiente:</p>
<ol type="1">
<li>Entrada (<code>src</code>, <code>tgt</code>): (batch_size, seq_len)</li>
<li>Salida del codificador: (batch_size, src_len, hidden_size)</li>
<li>Salida del decodificador: (batch_size, tgt_len, hidden_size)</li>
<li>Salida final: (batch_size, tgt_len, vocab_size)</li>
</ol>
<p>En la siguiente sección aplicaremos esta estructura a un ejemplo práctico.</p>
</section>
</section>
<section id="ejemplos-de-transformer" class="level2">
<h2 class="anchored" data-anchor-id="ejemplos-de-transformer">8.5 Ejemplos de transformer</h2>
<p>Hasta ahora hemos examinado la estructura y el principio de funcionamiento del transformer. Ahora, a través de ejemplos prácticos, veremos cómo funciona el transformer. Los ejemplos están organizados en orden de dificultad, y cada uno está diseñado para ayudar a comprender una función específica del transformer. Estos ejemplos muestran métodos para resolver gradualmente diversos problemas de procesamiento de datos y diseño de modelos que pueden encontrarse en proyectos reales. En particular, abordan temas prácticos como el preprocesamiento de datos, el diseño de funciones de pérdida y la configuración de métricas de evaluación. La ubicación de los ejemplos es <code>transformer/examples</code>.</p>
<pre class="text"><code>examples
├── addition_task.py  # 8.5.2 Tarea de suma
├── copy_task.py      # 8.5.1 Tarea de copia simple
└── parser_task.py    # 8.5.3 Tarea de análisis sintáctico</code></pre>
<p>Lo que se aprende en cada ejemplo es lo siguiente.</p>
<p><strong>Tarea de copia simple</strong>: permite comprender las funciones básicas del transformer. A través de la visualización de patrones de atención, se puede entender claramente el principio de funcionamiento del modelo. Además, se pueden aprender métodos básicos de procesamiento de datos secuenciales, diseño de dimensiones de tensores para procesamiento por lotes, estrategias básicas de padding y masking, y diseño de funciones de pérdida especializadas en la tarea.</p>
<p><strong>Problema de suma posicional</strong>: muestra cómo es posible la generación autoregresiva. Se puede observar claramente el proceso de generación secuencial del decodificador y el papel de la atención cruzada. Además, proporciona experiencia práctica en tokenización de datos numéricos, métodos para generar conjuntos de datos válidos, evaluación de precisión parcial/completa, y pruebas de rendimiento generalizado según la expansión posicional.</p>
<p><strong>Tarea de análisis sintáctico</strong>: muestra cómo el transformer aprende y expresa relaciones estructurales. Se puede entender cómo el mecanismo de atención captura la estructura jerárquica de secuencias de entrada. Además, se pueden adquirir diversas técnicas necesarias para problemas de análisis sintáctico prácticos, como la transformación de datos estructurados en secuencias, diseño de vocabulario de tokens, estrategias de linealización de estructuras de árboles y métodos de evaluación de precisión estructural.</p>
<p>A continuación se muestra una tabla que resume lo que se aprende en cada ejemplo:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 71%">
<col style="width: 28%">
</colgroup>
<thead>
<tr class="header">
<th>Ejemplo</th>
<th>Contenido de aprendizaje</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>8.5.1 Tarea de copia simple (copy_task.py)</td>
<td>- Comprensión de las funciones básicas y el principio de funcionamiento del transformer<br> - Comprensión intuitiva a través de la visualización de patrones de atención<br> - Métodos de procesamiento de datos secuenciales y diseño de dimensiones de tensores para procesamiento por lotes<br> - Estrategias básicas de padding y masking, y diseño de funciones de pérdida especializadas en la tarea</td>
</tr>
<tr class="even">
<td>8.5.2 Tarea de suma posicional (addition_task.py)</td>
<td>- Comprensión de cómo es posible la generación autoregresiva<br> - Observación del proceso de generación secuencial del decodificador y el papel de la atención cruzada<br> - Tokenización de datos numéricos, métodos para generar conjuntos de datos válidos<br> - Evaluación de precisión parcial/completa, pruebas de rendimiento generalizado según la expansión posicional</td>
</tr>
<tr class="odd">
<td>8.5.3 Tarea de análisis sintáctico (parser_task.py)</td>
<td>- Comprensión de cómo el transformer aprende y expresa relaciones estructurales<br> - Comprensión del mecanismo de atención que captura la estructura jerárquica de secuencias de entrada<br> - Transformación de datos estructurados en secuencias, diseño de vocabulario de tokens<br> - Estrategias de linealización de estructuras de árboles, métodos de evaluación de precisión estructural</td>
</tr>
</tbody>
</table>
<section id="tarea-de-copia-simple" class="level3">
<h3 class="anchored" data-anchor-id="tarea-de-copia-simple">8.5.1 Tarea de copia simple</h3>
<p>El primer ejemplo es una tarea de copia que reproduce la secuencia de entrada tal cual. Esta tarea es adecuada para verificar el funcionamiento básico del transformer y visualizar los patrones de atención, aunque parezca sencilla, es muy útil para comprender los mecanismos centrales del transformer.</p>
<p><strong>Preparación de datos</strong></p>
<p>Los datos para la tarea de copia consisten en secuencias idénticas de entrada y salida. A continuación se muestra un ejemplo de generación de datos:</p>
</section>
<section id="tarea-de-copia-simple-1" class="level3">
<h3 class="anchored" data-anchor-id="tarea-de-copia-simple-1">8.5.1 Tarea de copia simple</h3>
<p>El primer ejemplo es una tarea de copia que reproduce la secuencia de entrada tal cual. Esta tarea es adecuada para verificar el funcionamiento básico del transformer y visualizar los patrones de atención, aunque parezca sencilla, es muy útil para comprender los mecanismos centrales del transformer.</p>
<p><strong>Preparación de datos</strong></p>
<p>Los datos para la tarea de copia consisten en secuencias idénticas de entrada y salida. A continuación se muestra un ejemplo de generación de datos:</p>
<div id="cell-92" class="cell" data-execution_count="1">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_08.transformer.examples.copy_task <span class="im">import</span> explain_copy_data</span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a>explain_copy_data(seq_length<span class="op">=</span><span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
=== Copy Task Data Explanation ===
Sequence Length: 5

1. Input Sequence:
Original Tensor Shape: torch.Size([1, 5])
Input Sequence: [7, 15, 2, 3, 12]

2. Target Sequence:
Original Tensor Shape: torch.Size([1, 5])
Target Sequence: [7, 15, 2, 3, 12]

3. Task Description:
- Basic task of copying the input sequence as is
- Tokens at each position are integer values between 1-19
- Input and output have the same sequence length
- Current Example: [7, 15, 2, 3, 12] → [7, 15, 2, 3, 12]</code></pre>
</div>
</div>
<p><code>create_copy_data</code> genera un tensor de salida idéntico al tensor de entrada para el aprendizaje. Crea un tensor bidimensional (batch_size, seq_length) para el procesamiento por lotes, donde cada elemento es un valor entero entre 1 y 19.</p>
<div id="cell-94" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_copy_data(batch_size: <span class="bu">int</span> <span class="op">=</span> <span class="dv">32</span>, seq_length: <span class="bu">int</span> <span class="op">=</span> <span class="dv">5</span>) <span class="op">-&gt;</span> torch.Tensor:</span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""복사 태스크용 데이터 생성"""</span></span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a>    sequences <span class="op">=</span> torch.randint(<span class="dv">1</span>, <span class="dv">20</span>, (batch_size, seq_length))</span>
<span id="cb52-4"><a href="#cb52-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> sequences, sequences</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Este ejemplo de datos es idéntico a los datos de entrada tokenizados utilizados en el procesamiento de lenguaje natural y modelado de secuencias. En el procesamiento de lenguaje, cada token se convierte en un valor entero único antes de ser ingresado al modelo.</p>
<p><strong>Entrenamiento del modelo</strong></p>
<p>Se entrena el modelo con el siguiente código.</p>
<div id="cell-96" class="cell" data-execution_count="1">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_08.transformer.config <span class="im">import</span> TransformerConfig</span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_08.transformer.examples.copy_task <span class="im">import</span> train_copy_task</span>
<span id="cb53-3"><a href="#cb53-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-4"><a href="#cb53-4" aria-hidden="true" tabindex="-1"></a>seq_length <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb53-5"><a href="#cb53-5" aria-hidden="true" tabindex="-1"></a>config <span class="op">=</span> TransformerConfig()</span>
<span id="cb53-6"><a href="#cb53-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Modify default values</span></span>
<span id="cb53-7"><a href="#cb53-7" aria-hidden="true" tabindex="-1"></a>config.vocab_size <span class="op">=</span> <span class="dv">20</span>           <span class="co"># Small vocabulary size (minimum size to represent integers 1-19)</span></span>
<span id="cb53-8"><a href="#cb53-8" aria-hidden="true" tabindex="-1"></a>config.hidden_size <span class="op">=</span> <span class="dv">64</span>          <span class="co"># Small hidden dimension (enough representation for a simple task)</span></span>
<span id="cb53-9"><a href="#cb53-9" aria-hidden="true" tabindex="-1"></a>config.num_hidden_layers <span class="op">=</span> <span class="dv">2</span>     <span class="co"># Minimum number of layers (considering the low complexity of the copy task)</span></span>
<span id="cb53-10"><a href="#cb53-10" aria-hidden="true" tabindex="-1"></a>config.num_attention_heads <span class="op">=</span> <span class="dv">2</span>   <span class="co"># Minimum number of heads (minimum configuration for attention from various perspectives)</span></span>
<span id="cb53-11"><a href="#cb53-11" aria-hidden="true" tabindex="-1"></a>config.intermediate_size <span class="op">=</span> <span class="dv">128</span>   <span class="co"># Small FFN dimension (set to twice the hidden dimension to ensure adequate transformation capacity)</span></span>
<span id="cb53-12"><a href="#cb53-12" aria-hidden="true" tabindex="-1"></a>config.max_position_embeddings <span class="op">=</span> seq_length  <span class="co"># Short sequence length (set to the same length as the input sequence)</span></span>
<span id="cb53-13"><a href="#cb53-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-14"><a href="#cb53-14" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> train_copy_task(config, num_epochs<span class="op">=</span><span class="dv">50</span>, batch_size<span class="op">=</span><span class="dv">40</span>, steps_per_epoch<span class="op">=</span><span class="dv">100</span>, seq_length<span class="op">=</span>seq_length)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
=== Start Training ==== 
Device: cuda:0
Model saved to saved_models/transformer_copy_task.pth</code></pre>
</div>
</div>
<p><strong>Prueba del modelo</strong></p>
<p>Se lee el modelo de entrenamiento almacenado y se realiza la prueba.</p>
<div id="cell-98" class="cell" data-execution_count="1">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_08.transformer.examples.copy_task <span class="im">import</span> test_copy</span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-3"><a href="#cb55-3" aria-hidden="true" tabindex="-1"></a>test_copy(seq_length<span class="op">=</span><span class="dv">20</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
=== Copy Test ===
Input: [10, 10, 2, 12, 1, 5, 3, 1, 8, 18, 2, 19, 2, 2, 8, 14, 7, 19, 5, 4]
Output: [10, 10, 2, 12, 1, 5, 3, 1, 8, 18, 2, 19, 2, 2, 8, 14, 7, 19, 5, 4]
Accuracy: True</code></pre>
</div>
</div>
<p><strong>Configuración del modelo</strong></p>
<ul>
<li><code>hidden_size</code>: 64 (dimensión de diseño del modelo, d_model).
<ul>
<li>En el transformer, la dimensión de diseño (d_model) es igual a:
<ol type="1">
<li>Dimensión de los embeddings de palabras.</li>
<li>Dimensión de los embeddings posicionales.</li>
<li>Dimensión de los vectores Q, K, V en la atención.</li>
<li>Dimensión de las salidas de cada subcapa del codificador/decodificador.</li>
</ol></li>
</ul></li>
<li><code>intermediate_size</code>: Tamaño del FFN, que debe ser suficientemente grande en comparación con d_model.</li>
</ul>
<p><strong>Implementación de máscaras</strong></p>
<p>El transformer utiliza dos tipos de máscaras.</p>
<ol type="1">
<li><strong>Máscara de relleno (Padding Mask)</strong>: Ignora los tokens de relleno añadidos para el procesamiento por lotes.
<ul>
<li>En este ejemplo, las secuencias tienen la misma longitud (<code>seq_length</code>) y no requieren relleno, pero se incluye para ilustrar el uso general del transformer.</li>
<li>Implementa la función <code>create_pad_mask</code> (en PyTorch, <code>nn.Transformer</code> o en la biblioteca <code>transformers</code> de Hugging Face, esta función está implementada internamente).</li>
</ul></li>
</ol>
<div id="cell-100" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a>src_mask <span class="op">=</span> create_pad_mask(src).to(device)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<ol start="2" type="1">
<li><strong>Máscara subsiguiente (Subsequent Mask)</strong>: Se utiliza para la generación autoregresiva del decodificador.</li>
</ol>
<ul>
<li>La función <code>create_subsequent_mask</code> genera una máscara en forma de matriz triangular superior que oculta los tokens posteriores a la posición actual.</li>
<li>Esto hace que el decodificador solo pueda referirse a los tokens previamente generados para predecir el siguiente token.</li>
</ul>
<div id="cell-102" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb58"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a>tgt_mask <span class="op">=</span> create_subsequent_mask(decoder_input.size(<span class="dv">1</span>)).to(device)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Esta máscara garantiza la eficiencia del procesamiento por lotes y la causalidad de las secuencias.</p>
<p><strong>Diseño de la función de pérdida</strong></p>
<p>La clase <code>CopyLoss</code> implementa una función de pérdida para la tarea de copia.</p>
<ul>
<li>Considera tanto la precisión en cada posición de token como si toda la secuencia coincide completamente.</li>
<li>Monitorea detalladamente la precisión, la pérdida y los valores predichos/reales para comprender finamente el progreso del entrenamiento.</li>
</ul>
<div id="cell-104" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb59"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> CopyLoss(nn.Module):</span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, outputs: torch.Tensor, target: torch.Tensor, </span>
<span id="cb59-3"><a href="#cb59-3" aria-hidden="true" tabindex="-1"></a>                print_details: <span class="bu">bool</span> <span class="op">=</span> <span class="va">False</span>) <span class="op">-&gt;</span> Tuple[torch.Tensor, <span class="bu">float</span>]:</span>
<span id="cb59-4"><a href="#cb59-4" aria-hidden="true" tabindex="-1"></a>        batch_size <span class="op">=</span> outputs.size(<span class="dv">0</span>)</span>
<span id="cb59-5"><a href="#cb59-5" aria-hidden="true" tabindex="-1"></a>        predictions <span class="op">=</span> F.softmax(outputs, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb59-6"><a href="#cb59-6" aria-hidden="true" tabindex="-1"></a>        target_one_hot <span class="op">=</span> F.one_hot(target, num_classes<span class="op">=</span>outputs.size(<span class="op">-</span><span class="dv">1</span>)).<span class="bu">float</span>()</span>
<span id="cb59-7"><a href="#cb59-7" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb59-8"><a href="#cb59-8" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> <span class="op">-</span>torch.<span class="bu">sum</span>(target_one_hot <span class="op">*</span> torch.log(predictions <span class="op">+</span> <span class="fl">1e-10</span>)) <span class="op">/</span> batch_size</span>
<span id="cb59-9"><a href="#cb59-9" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb59-10"><a href="#cb59-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb59-11"><a href="#cb59-11" aria-hidden="true" tabindex="-1"></a>            pred_tokens <span class="op">=</span> predictions.argmax(dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb59-12"><a href="#cb59-12" aria-hidden="true" tabindex="-1"></a>            exact_match <span class="op">=</span> (pred_tokens <span class="op">==</span> target).<span class="bu">all</span>(dim<span class="op">=</span><span class="dv">1</span>).<span class="bu">float</span>()</span>
<span id="cb59-13"><a href="#cb59-13" aria-hidden="true" tabindex="-1"></a>            match_rate <span class="op">=</span> exact_match.mean().item()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<ul>
<li>Entropía cruzada sola no es suficiente: precisión de token individual + evaluación de coincidencia de secuencia completa.</li>
<li>Fomentar que el modelo aprenda el orden correctamente.</li>
</ul>
<p><strong>Ejemplo de funcionamiento</strong> (<code>batch_size=2</code>, <code>sequence_length=3</code>, <code>vocab_size=5</code>):</p>
<ol type="1">
<li><strong>Salida del modelo (logits)</strong></li>
</ol>
<div id="cell-106" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb60"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Example: batch_size=2, sequence_length=3, vocab_size=5 (example is vocab_size=20)</span></span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-3"><a href="#cb60-3" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Model Output (logits)</span></span>
<span id="cb60-4"><a href="#cb60-4" aria-hidden="true" tabindex="-1"></a>outputs <span class="op">=</span> [</span>
<span id="cb60-5"><a href="#cb60-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># First batch</span></span>
<span id="cb60-6"><a href="#cb60-6" aria-hidden="true" tabindex="-1"></a>    [[<span class="fl">0.9</span>, <span class="fl">0.1</span>, <span class="fl">0.0</span>, <span class="fl">0.0</span>, <span class="fl">0.0</span>],  <span class="co"># First position: token 0 has the highest probability</span></span>
<span id="cb60-7"><a href="#cb60-7" aria-hidden="true" tabindex="-1"></a>     [<span class="fl">0.1</span>, <span class="fl">0.8</span>, <span class="fl">0.1</span>, <span class="fl">0.0</span>, <span class="fl">0.0</span>],  <span class="co"># Second position: token 1 has the highest probability</span></span>
<span id="cb60-8"><a href="#cb60-8" aria-hidden="true" tabindex="-1"></a>     [<span class="fl">0.0</span>, <span class="fl">0.1</span>, <span class="fl">0.9</span>, <span class="fl">0.0</span>, <span class="fl">0.0</span>]], <span class="co"># Third position: token 2 has the highest probability</span></span>
<span id="cb60-9"><a href="#cb60-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Second batch</span></span>
<span id="cb60-10"><a href="#cb60-10" aria-hidden="true" tabindex="-1"></a>    [[<span class="fl">0.8</span>, <span class="fl">0.2</span>, <span class="fl">0.0</span>, <span class="fl">0.0</span>, <span class="fl">0.0</span>],</span>
<span id="cb60-11"><a href="#cb60-11" aria-hidden="true" tabindex="-1"></a>     [<span class="fl">0.1</span>, <span class="fl">0.7</span>, <span class="fl">0.2</span>, <span class="fl">0.0</span>, <span class="fl">0.0</span>],</span>
<span id="cb60-12"><a href="#cb60-12" aria-hidden="true" tabindex="-1"></a>     [<span class="fl">0.1</span>, <span class="fl">0.1</span>, <span class="fl">0.8</span>, <span class="fl">0.0</span>, <span class="fl">0.0</span>]]</span>
<span id="cb60-13"><a href="#cb60-13" aria-hidden="true" tabindex="-1"></a>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<ol start="2" type="1">
<li><strong>Objetivo real</strong></li>
</ol>
<div id="cell-108" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb61"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Actual Target</span></span>
<span id="cb61-2"><a href="#cb61-2" aria-hidden="true" tabindex="-1"></a>target <span class="op">=</span> [</span>
<span id="cb61-3"><a href="#cb61-3" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">2</span>],  <span class="co"># Correct sequence for the first batch</span></span>
<span id="cb61-4"><a href="#cb61-4" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">2</span>]   <span class="co"># Correct sequence for the second batch</span></span>
<span id="cb61-5"><a href="#cb61-5" aria-hidden="true" tabindex="-1"></a>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<ol start="3" type="1">
<li><strong>Proceso de cálculo de la pérdida</strong>
<ul>
<li><code>predictions = softmax(outputs)</code> (ya convertido a probabilidades anteriormente)</li>
<li>Convertir <code>target</code> a vector one-hot</li>
</ul></li>
</ol>
<div id="cell-110" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb62"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. Loss Calculation Process</span></span>
<span id="cb62-2"><a href="#cb62-2" aria-hidden="true" tabindex="-1"></a><span class="co"># predictions = softmax(outputs) (already converted to probabilities above)</span></span>
<span id="cb62-3"><a href="#cb62-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert target to one-hot vectors:</span></span>
<span id="cb62-4"><a href="#cb62-4" aria-hidden="true" tabindex="-1"></a>target_one_hot <span class="op">=</span> [</span>
<span id="cb62-5"><a href="#cb62-5" aria-hidden="true" tabindex="-1"></a>    [[<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>], [<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>], [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>]],  <span class="co"># First batch</span></span>
<span id="cb62-6"><a href="#cb62-6" aria-hidden="true" tabindex="-1"></a>    [[<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>], [<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>], [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>]]   <span class="co"># Second batch</span></span>
<span id="cb62-7"><a href="#cb62-7" aria-hidden="true" tabindex="-1"></a>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<ol start="4" type="1">
<li><strong>cálculo de precisión</strong></li>
</ol>
<div id="cell-112" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb63"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 4. Accuracy Calculation</span></span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a>pred_tokens <span class="op">=</span> [</span>
<span id="cb63-3"><a href="#cb63-3" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">2</span>],  <span class="co"># First batch prediction</span></span>
<span id="cb63-4"><a href="#cb63-4" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">2</span>]   <span class="co"># Second batch prediction</span></span>
<span id="cb63-5"><a href="#cb63-5" aria-hidden="true" tabindex="-1"></a>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<ul>
<li>Coincidencia exacta de la secuencia completa: <code>exact_match = [True, True]</code> (ambos lotes son exactos)</li>
<li>Precisión promedio: <code>match_rate = 1.0</code> (100%)</li>
</ul>
<ol start="5" type="1">
<li><strong>Valor de pérdida final</strong>: media de la entropía cruzada</li>
</ol>
<div id="cell-114" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb64"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Exact sequence match</span></span>
<span id="cb64-2"><a href="#cb64-2" aria-hidden="true" tabindex="-1"></a>exact_match <span class="op">=</span> [<span class="va">True</span>, <span class="va">True</span>]  <span class="co"># Both batches match exactly</span></span>
<span id="cb64-3"><a href="#cb64-3" aria-hidden="true" tabindex="-1"></a>match_rate <span class="op">=</span> <span class="fl">1.0</span>  <span class="co"># Average accuracy 100%</span></span>
<span id="cb64-4"><a href="#cb64-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-5"><a href="#cb64-5" aria-hidden="true" tabindex="-1"></a><span class="co"># The final loss value is the average of the cross-entropy</span></span>
<span id="cb64-6"><a href="#cb64-6" aria-hidden="true" tabindex="-1"></a><span class="co"># loss = -1/2 * (log(0.9) + log(0.8) + log(0.9) + log(0.8) + log(0.7) + log(0.8))</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><strong>Visualización de atención</strong></p>
<p>A través de la visualización de atención, se puede entender de manera intuitiva el funcionamiento del transformador.</p>
<div id="cell-116" class="cell" data-execution_count="1">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb65"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_08.transformer.examples.copy_task <span class="im">import</span> visualize_attention</span>
<span id="cb65-2"><a href="#cb65-2" aria-hidden="true" tabindex="-1"></a>visualize_attention(seq_length<span class="op">=</span><span class="dv">20</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="08_El nacimiento del transformer_files/figure-html/cell-52-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Verifica cómo cada token de entrada interactúa con tokens en otras posiciones.</p>
<p>A través de este ejemplo de tarea de copia, hemos examinado el mecanismo central del transformer. En el siguiente ejemplo (problema de adición), veremos cómo el transformer aprende reglas aritméticas, como las relaciones entre números y el acarreo.</p>
</section>
<section id="problema-de-suma-de-dígitos" class="level3">
<h3 class="anchored" data-anchor-id="problema-de-suma-de-dígitos">8.5.2 Problema de suma de dígitos</h3>
<p>El segundo ejemplo es una tarea de suma que involucra dos números. Esta tarea es adecuada para comprender la capacidad generativa autoregresiva del transformer y el proceso de cálculo secuencial del decodificador. A través del cálculo con acarreo, se puede observar cómo el transformer aprende las relaciones entre los números.</p>
<p><strong>Preparación de datos</strong></p>
<p>Los datos para la tarea de suma se generan en <code>create_addition_data()</code>.</p>
<div id="cell-119" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb66"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_addition_data(batch_size: <span class="bu">int</span> <span class="op">=</span> <span class="dv">32</span>, max_digits: <span class="bu">int</span> <span class="op">=</span> <span class="dv">3</span>) <span class="op">-&gt;</span> Tuple[torch.Tensor, torch.Tensor]:</span>
<span id="cb66-2"><a href="#cb66-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Create addition dataset"""</span></span>
<span id="cb66-3"><a href="#cb66-3" aria-hidden="true" tabindex="-1"></a>    max_value <span class="op">=</span> <span class="dv">10</span> <span class="op">**</span> max_digits <span class="op">-</span> <span class="dv">1</span></span>
<span id="cb66-4"><a href="#cb66-4" aria-hidden="true" tabindex="-1"></a>    num1 <span class="op">=</span> torch.randint(<span class="dv">0</span>, max_value <span class="op">//</span> <span class="dv">2</span> <span class="op">+</span> <span class="dv">1</span>, (batch_size,))</span>
<span id="cb66-5"><a href="#cb66-5" aria-hidden="true" tabindex="-1"></a>    num2 <span class="op">=</span> torch.randint(<span class="dv">0</span>, max_value <span class="op">//</span> <span class="dv">2</span> <span class="op">+</span> <span class="dv">1</span>, (batch_size,))</span>
<span id="cb66-6"><a href="#cb66-6" aria-hidden="true" tabindex="-1"></a>    result <span class="op">=</span> num1 <span class="op">+</span> num2</span>
<span id="cb66-7"><a href="#cb66-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-8"><a href="#cb66-8" aria-hidden="true" tabindex="-1"></a>    [See source below]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<ul>
<li>Generar dos números cuya suma no supere el número de dígitos especificado.</li>
<li>Entrada: dos números + ‘+’ símbolo.</li>
<li>Incluir validación de límite de dígitos.</li>
</ul>
<p><strong>Descripción de los datos de entrenamiento</strong></p>
<div id="cell-121" class="cell" data-execution_count="4">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb67"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_08.transformer.config <span class="im">import</span> TransformerConfig</span>
<span id="cb67-2"><a href="#cb67-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_08.transformer.examples.addition_task <span class="im">import</span> explain_addition_data</span>
<span id="cb67-3"><a href="#cb67-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-4"><a href="#cb67-4" aria-hidden="true" tabindex="-1"></a>explain_addition_data()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
=== Addition Data Explanation ====
Maximum Digits: 3

1. Input Sequence:
Original Tensor Shape: torch.Size([1, 7])
First Number: 153 (Indices [np.int64(1), np.int64(5), np.int64(3)])
Plus Sign: '+' (Index 10)
Second Number: 391 (Indices [np.int64(3), np.int64(9), np.int64(1)])
Full Input: [1, 5, 3, 10, 3, 9, 1]

2. Target Sequence:
Original Tensor Shape: torch.Size([1, 3])
Actual Sum: 544
Target Sequence: [5, 4, 4]</code></pre>
</div>
</div>
<p><strong>Entrenamiento y prueba del modelo</strong></p>
<div id="cell-123" class="cell" data-execution_count="1">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb69"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_08.transformer.config <span class="im">import</span> TransformerConfig</span>
<span id="cb69-2"><a href="#cb69-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_08.transformer.examples.addition_task <span class="im">import</span> train_addition_task</span>
<span id="cb69-3"><a href="#cb69-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-4"><a href="#cb69-4" aria-hidden="true" tabindex="-1"></a>config <span class="op">=</span> TransformerConfig()</span>
<span id="cb69-5"><a href="#cb69-5" aria-hidden="true" tabindex="-1"></a>config.vocab_size <span class="op">=</span> <span class="dv">11</span>       </span>
<span id="cb69-6"><a href="#cb69-6" aria-hidden="true" tabindex="-1"></a>config.hidden_size <span class="op">=</span> <span class="dv">256</span></span>
<span id="cb69-7"><a href="#cb69-7" aria-hidden="true" tabindex="-1"></a>config.num_hidden_layers <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb69-8"><a href="#cb69-8" aria-hidden="true" tabindex="-1"></a>config.num_attention_heads <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb69-9"><a href="#cb69-9" aria-hidden="true" tabindex="-1"></a>config.intermediate_size <span class="op">=</span> <span class="dv">512</span></span>
<span id="cb69-10"><a href="#cb69-10" aria-hidden="true" tabindex="-1"></a>config.max_position_embeddings <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb69-11"><a href="#cb69-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-12"><a href="#cb69-12" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> train_addition_task(config, num_epochs<span class="op">=</span><span class="dv">10</span>, batch_size<span class="op">=</span><span class="dv">128</span>, steps_per_epoch<span class="op">=</span><span class="dv">300</span>, max_digits<span class="op">=</span><span class="dv">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 0, Average Loss: 6.1352, Final Accuracy: 0.0073, Learning Rate: 0.000100
Epoch 5, Average Loss: 0.0552, Final Accuracy: 0.9852, Learning Rate: 0.000100

=== Loss Calculation Details (Step: 3000) ===
Predicted Sequences (First 10): tensor([[6, 5, 4],
        [5, 3, 3],
        [1, 7, 5],
        [6, 0, 6],
        [7, 5, 9],
        [5, 2, 8],
        [2, 8, 1],
        [3, 5, 8],
        [0, 7, 1],
        [6, 2, 1]], device='cuda:0')

Actual Target Sequences (First 10): tensor([[6, 5, 4],
        [5, 3, 3],
        [1, 7, 5],
        [6, 0, 6],
        [7, 5, 9],
        [5, 2, 8],
        [2, 8, 1],
        [3, 5, 8],
        [0, 7, 1],
        [6, 2, 1]], device='cuda:0')

Exact Match per Sequence (First 10): tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')

Calculated Loss: 0.0106
Calculated Accuracy: 1.0000
========================================
Model saved to saved_models/transformer_addition_task.pth</code></pre>
</div>
</div>
<p>Cuando se complete el aprendizaje, cargue el modelo almacenado y realice las pruebas.</p>
<div id="cell-125" class="cell" data-execution_count="3">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb71"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_08.transformer.examples.addition_task <span class="im">import</span> test_addition</span>
<span id="cb71-2"><a href="#cb71-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-3"><a href="#cb71-3" aria-hidden="true" tabindex="-1"></a>test_addition(max_digits<span class="op">=</span><span class="dv">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
Addition Test (Digits: 3):
310 + 98 = 408 (Actual Answer: 408)
Correct: True</code></pre>
</div>
</div>
<p><strong>Configuración del modelo</strong></p>
<p>La configuración del transformer para la tarea de adición es la siguiente.</p>
<div id="cell-127" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb73"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a>config <span class="op">=</span> TransformerConfig()</span>
<span id="cb73-2"><a href="#cb73-2" aria-hidden="true" tabindex="-1"></a>config.vocab_size <span class="op">=</span> <span class="dv">11</span>          <span class="co"># 0-9 digits + '+' symbol</span></span>
<span id="cb73-3"><a href="#cb73-3" aria-hidden="true" tabindex="-1"></a>config.hidden_size <span class="op">=</span> <span class="dv">256</span>        <span class="co"># Larger hidden dimension than copy task (sufficient capacity for learning arithmetic operations)</span></span>
<span id="cb73-4"><a href="#cb73-4" aria-hidden="true" tabindex="-1"></a>config.num_hidden_layers <span class="op">=</span> <span class="dv">3</span>    <span class="co"># Deeper layers (hierarchical feature extraction for handling carry operations)</span></span>
<span id="cb73-5"><a href="#cb73-5" aria-hidden="true" tabindex="-1"></a>config.num_attention_heads <span class="op">=</span> <span class="dv">4</span>  <span class="co"># Increased number of heads (learning relationships between different digit positions)</span></span>
<span id="cb73-6"><a href="#cb73-6" aria-hidden="true" tabindex="-1"></a>config.intermediate_size <span class="op">=</span> <span class="dv">512</span>  <span class="co">#  FFN dimension: should be larger than hidden_size.</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><strong>Implementación de enmascaramiento</strong></p>
<p>En la tarea de adición, la máscara de relleno es <em>esencial</em>. Dado que los dígitos de los números de entrada pueden variar, es necesario ignorar las posiciones de relleno para realizar cálculos precisos.</p>
<div id="cell-129" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb74"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb74-1"><a href="#cb74-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> _number_to_digits(number: torch.Tensor, max_digits: <span class="bu">int</span>) <span class="op">-&gt;</span> torch.Tensor:</span>
<span id="cb74-2"><a href="#cb74-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""숫자를 자릿수 시퀀스로 변환하며 패딩 적용"""</span></span>
<span id="cb74-3"><a href="#cb74-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> torch.tensor([[<span class="bu">int</span>(d) <span class="cf">for</span> d <span class="kw">in</span> <span class="bu">str</span>(n.item()).zfill(max_digits)] </span>
<span id="cb74-4"><a href="#cb74-4" aria-hidden="true" tabindex="-1"></a>                        <span class="cf">for</span> n <span class="kw">in</span> number])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>El funcionamiento de este método es específicamente como sigue.</p>
<div id="cell-131" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb75"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb75-1"><a href="#cb75-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb75-2"><a href="#cb75-2" aria-hidden="true" tabindex="-1"></a>number <span class="op">=</span> torch.tensor([<span class="dv">7</span>, <span class="dv">25</span>, <span class="dv">348</span>])</span>
<span id="cb75-3"><a href="#cb75-3" aria-hidden="true" tabindex="-1"></a>max_digits <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb75-4"><a href="#cb75-4" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> _number_to_digits(number, max_digits)</span>
<span id="cb75-5"><a href="#cb75-5" aria-hidden="true" tabindex="-1"></a><span class="co"># 입력: [7, 25, 348]</span></span>
<span id="cb75-6"><a href="#cb75-6" aria-hidden="true" tabindex="-1"></a><span class="co"># 과정: </span></span>
<span id="cb75-7"><a href="#cb75-7" aria-hidden="true" tabindex="-1"></a><span class="co">#   7   -&gt; "7"   -&gt; "007" -&gt; [0,0,7]</span></span>
<span id="cb75-8"><a href="#cb75-8" aria-hidden="true" tabindex="-1"></a><span class="co">#   25  -&gt; "25"  -&gt; "025" -&gt; [0,2,5]</span></span>
<span id="cb75-9"><a href="#cb75-9" aria-hidden="true" tabindex="-1"></a><span class="co">#   348 -&gt; "348" -&gt; "348" -&gt; [3,4,8]</span></span>
<span id="cb75-10"><a href="#cb75-10" aria-hidden="true" tabindex="-1"></a><span class="co"># 결과: tensor([[0, 0, 7],</span></span>
<span id="cb75-11"><a href="#cb75-11" aria-hidden="true" tabindex="-1"></a><span class="co">#               [0, 2, 5],</span></span>
<span id="cb75-12"><a href="#cb75-12" aria-hidden="true" tabindex="-1"></a><span class="co">#               [3, 4, 8]])</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><strong>Diseño de la función de pérdida</strong></p>
<p>La clase <code>AdditionLoss</code> implementa la función de pérdida para la tarea de suma.</p>
<ul>
<li>A diferencia de la tarea de copia, se evalúa <em>la precisión por dígito</em> y <em>la precisión del resultado completo</em> de manera distinta.</li>
</ul>
<div id="cell-133" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb76"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb76-1"><a href="#cb76-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> AdditionLoss(nn.Module):</span>
<span id="cb76-2"><a href="#cb76-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, outputs: torch.Tensor, target: torch.Tensor, </span>
<span id="cb76-3"><a href="#cb76-3" aria-hidden="true" tabindex="-1"></a>                print_details: <span class="bu">bool</span> <span class="op">=</span> <span class="va">False</span>) <span class="op">-&gt;</span> Tuple[torch.Tensor, <span class="bu">float</span>]:</span>
<span id="cb76-4"><a href="#cb76-4" aria-hidden="true" tabindex="-1"></a>        batch_size <span class="op">=</span> outputs.size(<span class="dv">0</span>)</span>
<span id="cb76-5"><a href="#cb76-5" aria-hidden="true" tabindex="-1"></a>        predictions <span class="op">=</span> F.softmax(outputs, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb76-6"><a href="#cb76-6" aria-hidden="true" tabindex="-1"></a>        target_one_hot <span class="op">=</span> F.one_hot(target, num_classes<span class="op">=</span>outputs.size(<span class="op">-</span><span class="dv">1</span>)).<span class="bu">float</span>()</span>
<span id="cb76-7"><a href="#cb76-7" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb76-8"><a href="#cb76-8" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> <span class="op">-</span>torch.<span class="bu">sum</span>(target_one_hot <span class="op">*</span> torch.log(predictions <span class="op">+</span> <span class="fl">1e-10</span>)) <span class="op">/</span> batch_size</span>
<span id="cb76-9"><a href="#cb76-9" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb76-10"><a href="#cb76-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb76-11"><a href="#cb76-11" aria-hidden="true" tabindex="-1"></a>            pred_digits <span class="op">=</span> predictions.argmax(dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb76-12"><a href="#cb76-12" aria-hidden="true" tabindex="-1"></a>            exact_match <span class="op">=</span> (pred_digits <span class="op">==</span> target).<span class="bu">all</span>(dim<span class="op">=</span><span class="dv">1</span>).<span class="bu">float</span>()</span>
<span id="cb76-13"><a href="#cb76-13" aria-hidden="true" tabindex="-1"></a>            match_rate <span class="op">=</span> exact_match.mean().item()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<ul>
<li>Cálculo de pérdida: precisión de la predicción de cada dígito + verificación de la exactitud del <em>acarreo</em>.</li>
<li>En lugar de un simple mapeo de dígitos, se induce al aprendizaje de las reglas de suma.</li>
</ul>
<p>Ejemplo de funcionamiento de <code>AdditionLoss</code> (<code>batch_size=2</code>, <code>sequence_length=3</code>, <code>vocab_size=10</code>)</p>
<div id="cell-135" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb77"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb77-1"><a href="#cb77-1" aria-hidden="true" tabindex="-1"></a>outputs <span class="op">=</span> [</span>
<span id="cb77-2"><a href="#cb77-2" aria-hidden="true" tabindex="-1"></a>    [[<span class="fl">0.1</span>, <span class="fl">0.8</span>, <span class="fl">0.1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>],  <span class="co"># 첫 번째 자리</span></span>
<span id="cb77-3"><a href="#cb77-3" aria-hidden="true" tabindex="-1"></a>     [<span class="fl">0.1</span>, <span class="fl">0.1</span>, <span class="fl">0.7</span>, <span class="fl">0.1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>], <span class="co"># 두 번째 자리</span></span>
<span id="cb77-4"><a href="#cb77-4" aria-hidden="true" tabindex="-1"></a>     [<span class="fl">0.8</span>, <span class="fl">0.1</span>, <span class="fl">0.1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>]]   <span class="co"># 세 번째 자리</span></span>
<span id="cb77-5"><a href="#cb77-5" aria-hidden="true" tabindex="-1"></a>]  <span class="co"># 첫 번째 배치</span></span>
<span id="cb77-6"><a href="#cb77-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-7"><a href="#cb77-7" aria-hidden="true" tabindex="-1"></a>target <span class="op">=</span> [</span>
<span id="cb77-8"><a href="#cb77-8" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">0</span>]  <span class="co"># 실제 정답: "120"</span></span>
<span id="cb77-9"><a href="#cb77-9" aria-hidden="true" tabindex="-1"></a>]  <span class="co"># 첫 번째 배치</span></span>
<span id="cb77-10"><a href="#cb77-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-11"><a href="#cb77-11" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. softmax는 이미 적용되어 있다고 가정 (outputs)</span></span>
<span id="cb77-12"><a href="#cb77-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-13"><a href="#cb77-13" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. target을 원-핫 인코딩으로 변환</span></span>
<span id="cb77-14"><a href="#cb77-14" aria-hidden="true" tabindex="-1"></a>target_one_hot <span class="op">=</span> [</span>
<span id="cb77-15"><a href="#cb77-15" aria-hidden="true" tabindex="-1"></a>    [[<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>],  <span class="co"># 1</span></span>
<span id="cb77-16"><a href="#cb77-16" aria-hidden="true" tabindex="-1"></a>     [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>],  <span class="co"># 2</span></span>
<span id="cb77-17"><a href="#cb77-17" aria-hidden="true" tabindex="-1"></a>     [<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>]]  <span class="co"># 0</span></span>
<span id="cb77-18"><a href="#cb77-18" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb77-19"><a href="#cb77-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-20"><a href="#cb77-20" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. 손실 계산</span></span>
<span id="cb77-21"><a href="#cb77-21" aria-hidden="true" tabindex="-1"></a><span class="co"># -log(0.8) - log(0.7) - log(0.8) = 0.223 + 0.357 + 0.223 = 0.803</span></span>
<span id="cb77-22"><a href="#cb77-22" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> <span class="fl">0.803</span> <span class="op">/</span> batch_size</span>
<span id="cb77-23"><a href="#cb77-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-24"><a href="#cb77-24" aria-hidden="true" tabindex="-1"></a><span class="co"># 4. 정확도 계산</span></span>
<span id="cb77-25"><a href="#cb77-25" aria-hidden="true" tabindex="-1"></a>pred_digits <span class="op">=</span> [<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">0</span>]  <span class="co"># argmax 적용</span></span>
<span id="cb77-26"><a href="#cb77-26" aria-hidden="true" tabindex="-1"></a>exact_match <span class="op">=</span> <span class="va">True</span>  <span class="co"># 모든 자릿수가 일치</span></span>
<span id="cb77-27"><a href="#cb77-27" aria-hidden="true" tabindex="-1"></a>match_rate <span class="op">=</span> <span class="fl">1.0</span>  <span class="co"># 배치의 평균 정확도</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>La salida del decodificador de transformers se transforma linealmente a <code>vocab_size</code> en la última capa, por lo que los logit tienen una dimensión de <code>vocab_size</code>.</p>
<p>En la siguiente sección, examinaremos cómo los transformers aprenden relaciones estructurales más complejas a través de la tarea de parsing.</p>
</section>
<section id="tarea-de-análisis-sintáctico" class="level3">
<h3 class="anchored" data-anchor-id="tarea-de-análisis-sintáctico">8.5.3 Tarea de análisis sintáctico</h3>
<p>El último ejemplo es una implementación de la tarea de análisis sintáctico (Parser). Esta tarea consiste en recibir expresiones matemáticas y convertirlas en árboles de análisis, lo que permite verificar cuán bien el transformer procesa información estructurada.</p>
<p><strong>Explicación del proceso de preparación de datos</strong></p>
<p>Los datos de entrenamiento para la tarea de análisis sintáctico se generan siguiendo los siguientes pasos:</p>
<ol type="1">
<li><strong>Generación de expresiones</strong>:
<ul>
<li>Se utilizan funciones como <code>generate_random_expression()</code> para combinar variables (x, y, z), operadores (+, -, *, /) y números (0-9) y crear expresiones simples como “x=1+2”.</li>
</ul></li>
<li><strong>Conversión a árbol de análisis</strong>:
<ul>
<li>Se utiliza la función <code>parse_to_tree()</code> para convertir las expresiones generadas en árboles de análisis de forma anidada, como <code>['ASSIGN', 'x', ['ADD', '1', '2']]</code>. Este árbol representa la estructura jerárquica de la expresión.</li>
</ul></li>
<li><strong>Procesamiento de tokenización</strong>:
<ul>
<li>Las expresiones y los árboles de análisis se convierten en secuencias de enteros.</li>
<li>Cada token se mapea a un ID de entero único según el diccionario predefinido <code>TOKEN_DICT</code>.</li>
</ul></li>
</ol>
<div id="cell-138" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb78"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb78-1"><a href="#cb78-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_addition_data(batch_size: <span class="bu">int</span> <span class="op">=</span> <span class="dv">32</span>, max_digits: <span class="bu">int</span> <span class="op">=</span> <span class="dv">3</span>) <span class="op">-&gt;</span> Tuple[torch.Tensor, torch.Tensor]:</span>
<span id="cb78-2"><a href="#cb78-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Create addition dataset"""</span></span>
<span id="cb78-3"><a href="#cb78-3" aria-hidden="true" tabindex="-1"></a>    max_value <span class="op">=</span> <span class="dv">10</span> <span class="op">**</span> max_digits <span class="op">-</span> <span class="dv">1</span></span>
<span id="cb78-4"><a href="#cb78-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb78-5"><a href="#cb78-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Generate input numbers</span></span>
<span id="cb78-6"><a href="#cb78-6" aria-hidden="true" tabindex="-1"></a>    num1 <span class="op">=</span> torch.randint(<span class="dv">0</span>, max_value <span class="op">//</span> <span class="dv">2</span> <span class="op">+</span> <span class="dv">1</span>, (batch_size,))</span>
<span id="cb78-7"><a href="#cb78-7" aria-hidden="true" tabindex="-1"></a>    num2 <span class="op">=</span> torch.randint(<span class="dv">0</span>, max_value <span class="op">//</span> <span class="dv">2</span> <span class="op">+</span> <span class="dv">1</span>, (batch_size,))</span>
<span id="cb78-8"><a href="#cb78-8" aria-hidden="true" tabindex="-1"></a>    result <span class="op">=</span> num1 <span class="op">+</span> num2</span>
<span id="cb78-9"><a href="#cb78-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-10"><a href="#cb78-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># [이하 생략]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<ul>
<li>Generar dos números cuya suma no supere el número de dígitos especificado.</li>
<li>Entrada: dos números + ‘+’ símbolo.</li>
<li>Incluir validación de límite de dígitos.</li>
</ul>
<p><strong>Descripción de los datos de aprendizaje</strong> Lo siguiente explica la estructura de los datos de aprendizaje. Muestra cómo cambian las expresiones y tokenización en valores.</p>
<div id="cell-140" class="cell" data-execution_count="2">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb79"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb79-1"><a href="#cb79-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_08.transformer.examples.parser_task <span class="im">import</span> explain_parser_data</span>
<span id="cb79-2"><a href="#cb79-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-3"><a href="#cb79-3" aria-hidden="true" tabindex="-1"></a>explain_parser_data()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
=== Parsing Data Explanation ===
Max Tokens: 5

1. Input Sequence:
Original Tensor Shape: torch.Size([1, 5])
Expression: x = 4 + 9
Tokenized Input: [11, 1, 17, 2, 22]

2. Target Sequence:
Original Tensor Shape: torch.Size([1, 5])
Parse Tree: ['ASSIGN', 'x', 'ADD', '4', '9']
Tokenized Output: [6, 11, 7, 17, 22]</code></pre>
</div>
</div>
<p>Cuando se ejecuta el siguiente código, se muestran explicaciones en orden para facilitar la comprensión de cómo se estructuran los datos del ejemplo de análisis.</p>
<div id="cell-142" class="cell" data-execution_count="4">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb81"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb81-1"><a href="#cb81-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_08.transformer.examples.parser_task <span class="im">import</span> show_parser_examples</span>
<span id="cb81-2"><a href="#cb81-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-3"><a href="#cb81-3" aria-hidden="true" tabindex="-1"></a>show_parser_examples(num_examples<span class="op">=</span><span class="dv">3</span> )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
=== Generating 3 Parsing Examples ===

Example 1:
Generated Expression: y=7/7
Parse Tree: ['ASSIGN', 'y', ['DIV', '7', '7']]
Expression Tokens: [12, 1, 21, 5, 21]
Tree Tokens: [6, 12, 10, 21, 21]
Padded Expression Tokens: [12, 1, 21, 5, 21]
Padded Tree Tokens: [6, 12, 10, 21, 21]

Example 2:
Generated Expression: x=4/3
Parse Tree: ['ASSIGN', 'x', ['DIV', '4', '3']]
Expression Tokens: [11, 1, 18, 5, 17]
Tree Tokens: [6, 11, 10, 18, 17]
Padded Expression Tokens: [11, 1, 18, 5, 17]
Padded Tree Tokens: [6, 11, 10, 18, 17]

Example 3:
Generated Expression: x=1*4
Parse Tree: ['ASSIGN', 'x', ['MUL', '1', '4']]
Expression Tokens: [11, 1, 15, 4, 18]
Tree Tokens: [6, 11, 9, 15, 18]
Padded Expression Tokens: [11, 1, 15, 4, 18]
Padded Tree Tokens: [6, 11, 9, 15, 18]
</code></pre>
</div>
</div>
<p><strong>Modelo de aprendizaje y prueba</strong></p>
<div id="cell-144" class="cell" data-execution_count="2">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb83"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb83-1"><a href="#cb83-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_08.transformer.config <span class="im">import</span> TransformerConfig</span>
<span id="cb83-2"><a href="#cb83-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_08.transformer.examples.parser_task <span class="im">import</span> train_parser_task</span>
<span id="cb83-3"><a href="#cb83-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-4"><a href="#cb83-4" aria-hidden="true" tabindex="-1"></a>config <span class="op">=</span> TransformerConfig()</span>
<span id="cb83-5"><a href="#cb83-5" aria-hidden="true" tabindex="-1"></a>config.vocab_size <span class="op">=</span> <span class="dv">25</span>  <span class="co"># Adjusted to match the token dictionary size</span></span>
<span id="cb83-6"><a href="#cb83-6" aria-hidden="true" tabindex="-1"></a>config.hidden_size <span class="op">=</span> <span class="dv">128</span></span>
<span id="cb83-7"><a href="#cb83-7" aria-hidden="true" tabindex="-1"></a>config.num_hidden_layers <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb83-8"><a href="#cb83-8" aria-hidden="true" tabindex="-1"></a>config.num_attention_heads <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb83-9"><a href="#cb83-9" aria-hidden="true" tabindex="-1"></a>config.intermediate_size <span class="op">=</span> <span class="dv">512</span></span>
<span id="cb83-10"><a href="#cb83-10" aria-hidden="true" tabindex="-1"></a>config.max_position_embeddings <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb83-11"><a href="#cb83-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-12"><a href="#cb83-12" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> train_parser_task(config, num_epochs<span class="op">=</span><span class="dv">6</span>, batch_size<span class="op">=</span><span class="dv">64</span>, steps_per_epoch<span class="op">=</span><span class="dv">100</span>, max_tokens<span class="op">=</span><span class="dv">5</span>, print_progress<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
=== Start Training ===
Device: cuda:0
Batch Size: 64
Steps per Epoch: 100
Max Tokens: 5

Epoch 0, Average Loss: 6.3280, Final Accuracy: 0.2309, Learning Rate: 0.000100

=== Prediction Result Samples ===
Input: y = 8 * 8
Prediction: ['ASSIGN', 'y', 'MUL', '8', '8']
Truth: ['ASSIGN', 'y', 'MUL', '8', '8']
Result: Correct

Input: z = 6 / 5
Prediction: ['ASSIGN', 'z', 'DIV', '8', 'a']
Truth: ['ASSIGN', 'z', 'DIV', '6', '5']
Result: Incorrect

Epoch 5, Average Loss: 0.0030, Final Accuracy: 1.0000, Learning Rate: 0.000100

=== Prediction Result Samples ===
Input: z = 5 - 6
Prediction: ['ASSIGN', 'z', 'SUB', '5', '6']
Truth: ['ASSIGN', 'z', 'SUB', '5', '6']
Result: Correct

Input: y = 9 + 9
Prediction: ['ASSIGN', 'y', 'ADD', '9', '9']
Truth: ['ASSIGN', 'y', 'ADD', '9', '9']
Result: Correct

Model saved to saved_models/transformer_parser_task.pth</code></pre>
</div>
</div>
<p>Realiza la prueba.</p>
<div id="cell-146" class="cell" data-execution_count="3">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb85"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb85-1"><a href="#cb85-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_08.transformer.config <span class="im">import</span> TransformerConfig</span>
<span id="cb85-2"><a href="#cb85-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_08.transformer.examples.parser_task <span class="im">import</span> test_parser</span>
<span id="cb85-3"><a href="#cb85-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-4"><a href="#cb85-4" aria-hidden="true" tabindex="-1"></a>test_parser()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
=== Parser Test ===
Input Expression: x = 8 * 3
Predicted Parse Tree: ['ASSIGN', 'x', 'MUL', '8', '3']
Actual Parse Tree: ['ASSIGN', 'x', 'MUL', '8', '3']
Correct: True

=== Additional Tests ===

Input: x=1+2
Predicted Parse Tree: ['ASSIGN', 'x', 'ADD', '2', '3']

Input: y=3*4
Predicted Parse Tree: ['ASSIGN', 'y', 'MUL', '4', '5']

Input: z=5-1
Predicted Parse Tree: ['ASSIGN', 'z', 'SUB', '6', '2']

Input: x=2/3
Predicted Parse Tree: ['ASSIGN', 'x', 'DIV', '3', '4']</code></pre>
</div>
</div>
<p><strong>Configuración del modelo</strong> - <code>vocab_size</code>: 25 (tamaño del diccionario de tokens) - <code>hidden_size</code>: 128 - <code>num_hidden_layers</code>: 3 - <code>num_attention_heads</code>: 4 - <code>intermediate_size</code>: 512 - <code>max_position_embeddings</code>: 10 (número máximo de tokens)</p>
<p><strong>Diseño de la función de pérdida</strong></p>
<p>La función de pérdida para la tarea de parsing utiliza la entropía cruzada.</p>
<ol type="1">
<li><strong>Transformación de salida</strong>: se convierte la salida del modelo en probabilidades usando la función softmax.</li>
<li><strong>Transformación de objetivo</strong>: se codifica el secuencia objetivo (correcta) en one-hot.</li>
<li><strong>Cálculo de la pérdida</strong>: se calcula la media de los logaritmos de las probabilidades negativas para obtener la pérdida.</li>
<li><strong>Precisión</strong>: se calcula la precisión verificando si la secuencia predicha coincide exactamente con la secuencia correcta, reflejando la característica de esta tarea en la que el árbol de parsing debe generarse correctamente.</li>
</ol>
<p><strong>Ejemplo de funcionamiento de la función de pérdida</strong></p>
<div id="cell-148" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb87"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb87-1"><a href="#cb87-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Example input values (batch_size=2, sequence_length=4, vocab_size=5)</span></span>
<span id="cb87-2"><a href="#cb87-2" aria-hidden="true" tabindex="-1"></a><span class="co"># vocab = {'=':0, 'x':1, '+':2, '1':3, '2':4}</span></span>
<span id="cb87-3"><a href="#cb87-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-4"><a href="#cb87-4" aria-hidden="true" tabindex="-1"></a>outputs <span class="op">=</span> [</span>
<span id="cb87-5"><a href="#cb87-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># First batch: prediction probabilities for "x=1+2"</span></span>
<span id="cb87-6"><a href="#cb87-6" aria-hidden="true" tabindex="-1"></a>    [[<span class="fl">0.1</span>, <span class="fl">0.7</span>, <span class="fl">0.1</span>, <span class="fl">0.1</span>, <span class="fl">0.0</span>],  <span class="co"># predicting x</span></span>
<span id="cb87-7"><a href="#cb87-7" aria-hidden="true" tabindex="-1"></a>     [<span class="fl">0.8</span>, <span class="fl">0.1</span>, <span class="fl">0.0</span>, <span class="fl">0.1</span>, <span class="fl">0.0</span>],  <span class="co"># predicting =</span></span>
<span id="cb87-8"><a href="#cb87-8" aria-hidden="true" tabindex="-1"></a>     [<span class="fl">0.1</span>, <span class="fl">0.0</span>, <span class="fl">0.1</span>, <span class="fl">0.7</span>, <span class="fl">0.1</span>],  <span class="co"># predicting 1</span></span>
<span id="cb87-9"><a href="#cb87-9" aria-hidden="true" tabindex="-1"></a>     [<span class="fl">0.0</span>, <span class="fl">0.1</span>, <span class="fl">0.8</span>, <span class="fl">0.0</span>, <span class="fl">0.1</span>]], <span class="co"># predicting +</span></span>
<span id="cb87-10"><a href="#cb87-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb87-11"><a href="#cb87-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Second batch: prediction probabilities for "x=2+1"</span></span>
<span id="cb87-12"><a href="#cb87-12" aria-hidden="true" tabindex="-1"></a>    [[<span class="fl">0.1</span>, <span class="fl">0.8</span>, <span class="fl">0.0</span>, <span class="fl">0.1</span>, <span class="fl">0.0</span>],  <span class="co"># predicting x</span></span>
<span id="cb87-13"><a href="#cb87-13" aria-hidden="true" tabindex="-1"></a>     [<span class="fl">0.7</span>, <span class="fl">0.1</span>, <span class="fl">0.1</span>, <span class="fl">0.0</span>, <span class="fl">0.1</span>],  <span class="co"># predicting =</span></span>
<span id="cb87-14"><a href="#cb87-14" aria-hidden="true" tabindex="-1"></a>     [<span class="fl">0.1</span>, <span class="fl">0.0</span>, <span class="fl">0.1</span>, <span class="fl">0.1</span>, <span class="fl">0.7</span>],  <span class="co"># predicting 2</span></span>
<span id="cb87-15"><a href="#cb87-15" aria-hidden="true" tabindex="-1"></a>     [<span class="fl">0.0</span>, <span class="fl">0.0</span>, <span class="fl">0.9</span>, <span class="fl">0.1</span>, <span class="fl">0.0</span>]]  <span class="co"># predicting +</span></span>
<span id="cb87-16"><a href="#cb87-16" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb87-17"><a href="#cb87-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-18"><a href="#cb87-18" aria-hidden="true" tabindex="-1"></a>target <span class="op">=</span> [</span>
<span id="cb87-19"><a href="#cb87-19" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">3</span>, <span class="dv">2</span>],  <span class="co"># Actual answer: "x=1+"</span></span>
<span id="cb87-20"><a href="#cb87-20" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">4</span>, <span class="dv">2</span>]   <span class="co"># Actual answer: "x=2+"</span></span>
<span id="cb87-21"><a href="#cb87-21" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb87-22"><a href="#cb87-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-23"><a href="#cb87-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert target to one-hot encoding</span></span>
<span id="cb87-24"><a href="#cb87-24" aria-hidden="true" tabindex="-1"></a>target_one_hot <span class="op">=</span> [</span>
<span id="cb87-25"><a href="#cb87-25" aria-hidden="true" tabindex="-1"></a>    [[<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>],  <span class="co"># x</span></span>
<span id="cb87-26"><a href="#cb87-26" aria-hidden="true" tabindex="-1"></a>     [<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>],  <span class="co"># =</span></span>
<span id="cb87-27"><a href="#cb87-27" aria-hidden="true" tabindex="-1"></a>     [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>],  <span class="co"># 1</span></span>
<span id="cb87-28"><a href="#cb87-28" aria-hidden="true" tabindex="-1"></a>     [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>]], <span class="co"># +</span></span>
<span id="cb87-29"><a href="#cb87-29" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb87-30"><a href="#cb87-30" aria-hidden="true" tabindex="-1"></a>    [[<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>],  <span class="co"># x</span></span>
<span id="cb87-31"><a href="#cb87-31" aria-hidden="true" tabindex="-1"></a>     [<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>],  <span class="co"># =</span></span>
<span id="cb87-32"><a href="#cb87-32" aria-hidden="true" tabindex="-1"></a>     [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>],  <span class="co"># 2</span></span>
<span id="cb87-33"><a href="#cb87-33" aria-hidden="true" tabindex="-1"></a>     [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>]]  <span class="co"># +</span></span>
<span id="cb87-34"><a href="#cb87-34" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb87-35"><a href="#cb87-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-36"><a href="#cb87-36" aria-hidden="true" tabindex="-1"></a><span class="co"># Loss calculation (first batch)</span></span>
<span id="cb87-37"><a href="#cb87-37" aria-hidden="true" tabindex="-1"></a><span class="co"># -log(0.7) - log(0.8) - log(0.7) - log(0.8) = 0.357 + 0.223 + 0.357 + 0.223 = 1.16</span></span>
<span id="cb87-38"><a href="#cb87-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-39"><a href="#cb87-39" aria-hidden="true" tabindex="-1"></a><span class="co"># Loss calculation (second batch)</span></span>
<span id="cb87-40"><a href="#cb87-40" aria-hidden="true" tabindex="-1"></a><span class="co"># -log(0.8) - log(0.7) - log(0.7) - log(0.9) = 0.223 + 0.357 + 0.357 + 0.105 = 1.042</span></span>
<span id="cb87-41"><a href="#cb87-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-42"><a href="#cb87-42" aria-hidden="true" tabindex="-1"></a><span class="co"># Total loss</span></span>
<span id="cb87-43"><a href="#cb87-43" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> (<span class="fl">1.16</span> <span class="op">+</span> <span class="fl">1.042</span>) <span class="op">/</span> <span class="dv">2</span> <span class="op">=</span> <span class="fl">1.101</span></span>
<span id="cb87-44"><a href="#cb87-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-45"><a href="#cb87-45" aria-hidden="true" tabindex="-1"></a><span class="co"># Accuracy calculation</span></span>
<span id="cb87-46"><a href="#cb87-46" aria-hidden="true" tabindex="-1"></a>pred_tokens <span class="op">=</span> [</span>
<span id="cb87-47"><a href="#cb87-47" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">3</span>, <span class="dv">2</span>],  <span class="co"># First batch prediction</span></span>
<span id="cb87-48"><a href="#cb87-48" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">4</span>, <span class="dv">2</span>]   <span class="co"># Second batch prediction</span></span>
<span id="cb87-49"><a href="#cb87-49" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb87-50"><a href="#cb87-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-51"><a href="#cb87-51" aria-hidden="true" tabindex="-1"></a>exact_match <span class="op">=</span> [<span class="va">True</span>, <span class="va">True</span>]  <span class="co"># Both batches match exactly</span></span>
<span id="cb87-52"><a href="#cb87-52" aria-hidden="true" tabindex="-1"></a>match_rate <span class="op">=</span> <span class="fl">1.0</span>  <span class="co"># Overall accuracy</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Hasta ahora, a través de los ejemplos, hemos podido ver que el transformer puede procesar eficazmente la información estructural.</p>
</section>
</section>
<section id="conclusión" class="level2">
<h2 class="anchored" data-anchor-id="conclusión">Conclusión</h2>
<p>En el Capítulo 8, exploramos en profundidad el contexto del nacimiento del transformer y sus componentes clave. Examinamos las dificultades que los investigadores enfrentaron para superar las limitaciones de los modelos basados en RNN, el descubrimiento y desarrollo del mecanismo de atención, y cómo las ideas centrales del transformer se concretizaron gradualmente a través de la separación de espacios vectoriales Q, K, V y la atención multi-cabeza para procesamiento paralelo y captura de información contextual desde diferentes perspectivas. También analizamos en detalle la codificación posicional para representar eficazmente la información de posición, las sofisticadas estrategias de máscara para evitar fugas de información, y la estructura codificador-decodificador junto con el rol y funcionamiento de cada componente.</p>
<p>A través de tres ejemplos (copia simple, suma de dígitos, analizador), pudimos entender intuitivamente cómo funciona el transformer en la práctica y qué roles desempeñan sus componentes. Estos ejemplos muestran las capacidades básicas del transformer, su habilidad para generar de manera autoregresiva, y su capacidad para procesar información estructural, proporcionando una base de conocimientos para aplicar el transformer a problemas reales de procesamiento de lenguaje natural.</p>
<p>En el Capítulo 9, seguiremos la evolución del transformer desde la publicación del artículo “Attention is All You Need”. Examinaremos cómo surgieron modelos basados en transformers como BERT y GPT, y cómo estos modelos han llevado innovaciones más allá del procesamiento de lenguaje natural, incluyendo visión por computadora y reconocimiento de voz.</p>
</section>
<section id="ejercicios-de-práctica" class="level2">
<h2 class="anchored" data-anchor-id="ejercicios-de-práctica">Ejercicios de Práctica</h2>
<section id="problemas-básicos" class="level3">
<h3 class="anchored" data-anchor-id="problemas-básicos">Problemas Básicos</h3>
<ol type="1">
<li>¿Cuáles son las dos principales ventajas que tienen los Transformers en comparación con RNN?</li>
<li>¿Cuál es la idea clave del mecanismo de atención y qué efectos se pueden obtener a través de este?</li>
<li>¿Qué ventaja proporciona la atención multi-cabeza en comparación con la autoatención?</li>
<li>¿Por qué es necesaria la codificación posicional y cómo expresa la información de posición?</li>
<li>¿Cuáles son las funciones que desempeñan el codificador y el decodificador en un Transformer?</li>
</ol>
</section>
<section id="problemas-de-aplicación" class="level3">
<h3 class="anchored" data-anchor-id="problemas-de-aplicación">Problemas de Aplicación</h3>
<ol type="1">
<li><strong>Tarea de Resumen de Texto</strong>: Diseña un modelo de Transformer que tome como entrada un texto largo y genere un resumen corto que contenga los puntos clave, y explica qué métricas de evaluación se pueden usar para medir el rendimiento del modelo.</li>
<li><strong>Análisis del Sistema de Pregunta-Respuesta</strong>: Explica paso a paso cómo un sistema de pregunta-respuesta basado en Transformers encuentra la respuesta correcta a una pregunta dada, y analiza qué papel clave juega el mecanismo de atención en este proceso.</li>
<li><strong>Investigación de Casos de Aplicación en Diferentes Dominios</strong>: Investiga al menos dos ejemplos en los que se hayan aplicado con éxito Transformers en dominios diferentes al procesamiento del lenguaje natural, como imágenes, voz y grafos, y explica cómo se han utilizado los Transformers en cada caso y qué ventajas han proporcionado.</li>
</ol>
</section>
<section id="problemas-avanzados" class="level3">
<h3 class="anchored">Problemas Avanzados</h3>
<ol type="1">
<li><strong>Análisis Comparativo de Métodos para Mejorar la Complejidad Computacional</strong>: Investiga al menos dos métodos propuestos para mejorar la complejidad computacional de los Transformers (por ejemplo: Reformer, Performer, Longformer), y realiza un análisis comparativo de las ideas clave, ventajas y desventajas de cada método, así como de los escenarios en los que se pueden aplicar.</li>
<li><strong>Propuesta y Evaluación de una Nueva Arquitectura</strong>: Propón una nueva arquitectura basada en Transformers especializada para un problema específico (por ejemplo: clasificación de texto largo, traducción multilingüe) y explica teóricamente qué ventajas tiene frente a los modelos de Transformer existentes, así como propone métodos experimentales para verificarlo.</li>
<li><strong>Análisis e Intervención Éticos y Sociales</strong>: Analiza los impactos positivos y negativos que el desarrollo de grandes modelos de lenguaje basados en Transformers (por ejemplo: GPT-3, BERT) puede tener en la sociedad, especialmente en términos de sesgo, generación de noticias falsas y reducción de empleo, y propone soluciones técnicas y políticas para mitigar los impactos negativos.</li>
</ol>
<div class="callout callout-style-default callout-note callout-titled" title="Haz clic para ver el contenido (soluciones de los ejercicios)">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Haz clic para ver el contenido (soluciones de los ejercicios)
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<section id="soluciones-a-problemas-de-práctica" class="level2 callout-body-container callout-body">
<h2 class="anchored" data-anchor-id="soluciones-a-problemas-de-práctica">Soluciones a problemas de práctica</h2>
<section id="problemas-básicos-1" class="level3">
<h3 class="anchored" data-anchor-id="problemas-básicos-1">Problemas básicos</h3>
<ol type="1">
<li><p><strong>Ventajas del Transformer frente al RNN:</strong> El Transformer tiene dos grandes ventajas sobre el RNN: la <strong>procesamiento en paralelo</strong> y la <strong>resolución del problema de dependencias a largo plazo</strong>. El RNN procesa secuencialmente, lo que hace que sea lento, mientras que el Transformer utiliza atención para procesar todas las palabras simultáneamente, permitiendo cálculos paralelos en GPU y acelerando el aprendizaje. Además, el RNN sufre una pérdida de información en secuencias largas, pero el Transformer conserva la información importante independientemente de la distancia mediante la atención propia (self-attention).</p></li>
<li><p><strong>Núcleo y efectos del mecanismo de atención:</strong> La atención calcula <strong>qué tan importantes son las diferentes partes de la secuencia de entrada para la generación de la secuencia de salida</strong>. El decodificador no trata todas las entradas por igual al predecir palabras de salida, sino que “presta atención” a las partes más relevantes, mejorando así su comprensión del contexto y permitiendo predicciones más precisas.</p></li>
<li><p><strong>Ventajas de la atención multi-cabezal:</strong> La atención multi-cabezal realiza <strong>múltiples operaciones de autoatención en paralelo</strong>. Cada cabeza aprende las relaciones entre palabras dentro de la secuencia desde perspectivas diferentes, lo que ayuda al modelo a capturar información contextual más rica y diversa. (Similar a cómo varios detectives colaboran con sus propias áreas de especialización)</p></li>
<li><p><strong>Necesidad y método de codificación posicional:</strong> Dado que el Transformer no procesa secuencias en orden, es necesario proporcionarle la <strong>información sobre la posición</strong> de las palabras. La codificación posicional funciona agregando un vector con información de posición a cada embedding de palabra. De esta manera, el Transformer puede considerar tanto el significado como la ubicación de las palabras dentro de una oración para comprender el contexto. Se utilizan principalmente funciones seno-coseno para representar la información de posición.</p></li>
<li><p><strong>Roles del codificador y decodificador:</strong> El Transformer utiliza una estructura de codificador-decodificador. El <strong>codificador</strong> toma la secuencia de entrada y genera representaciones (vectores contextuales) que reflejan el contexto de cada palabra. El <strong>decodificador</strong> repite el proceso de predecir la siguiente palabra basándose en los vectores contextuales generados por el codificador y las palabras de salida generadas en pasos anteriores, hasta generar la secuencia de salida final.</p></li>
</ol>
</section>
<section id="problemas-aplicativos" class="level3">
<h3 class="anchored" data-anchor-id="problemas-aplicativos">Problemas aplicativos</h3>
<ol type="1">
<li><strong>Tarea de resumen de texto:</strong>
<ul>
<li><strong>Diseño del modelo:</strong> Se utiliza un modelo Transformer con estructura de codificador-decodificador. El codificador toma un texto largo como entrada y genera vectores contextuales, mientras que el decodificador predice la secuencia de palabras del resumen basándose en estos vectores contextuales.</li>
<li><strong>Evaluación:</strong> Se puede evaluar principalmente usando la puntuación ROUGE (Recall-Oriented Understudy for Gisting Evaluation). ROUGE mide la similitud entre el resumen generado y el resumen de referencia basándose en el número de n-grams que coinciden, con variantes como ROUGE-N, ROUGE-L y ROUGE-S. Además, también se puede considerar la puntuación BLEU (Bilingual Evaluation Understudy).</li>
</ul></li>
<li><strong>Análisis del sistema de preguntas y respuestas:</strong> Un sistema de preguntas y respuestas basado en Transformer realiza el siguiente proceso para encontrar la respuesta correcta a una pregunta dada:
<ol type="1">
<li>Se ingresan la pregunta y el documento al codificador Transformer para obtener vectores de embedding.</li>
<li>Se calculan los pesos de atención entre los embeddings de la pregunta y del documento (para determinar qué palabras del documento están relacionadas con cada palabra de la pregunta).</li>
<li>Se utiliza el peso de atención para calcular un promedio ponderado de los embeddings del documento, que se utiliza como vector contextual para la pregunta.</li>
<li>Se predice la posición inicial y final de la respuesta en el documento basándose en el vector contextual, y se extrae la respuesta final. En este proceso, el <strong>mecanismo de atención</strong> juega un papel crucial identificando las partes más relevantes del documento para responder a la pregunta al comprender la relación semántica entre la pregunta y el documento.</li>
</ol></li>
<li><strong>Casos de uso en otros dominios:</strong>
<ul>
<li><strong>Imágenes:</strong> Vision Transformer (ViT) divide las imágenes en varios parches y procesa cada parche como una secuencia de entrada del transformador, mostrando un rendimiento sobresaliente en tareas como la clasificación de imágenes y la detección de objetos. Esto demuestra que el transformador puede aplicarse eficazmente no solo a datos secuenciales sino también a datos bidimensionales como las imágenes.</li>
<li><strong>Audio:</strong> Conformer combina CNN y transformadores para lograr una alta precisión en reconocimiento de voz. Modela eficazmente tanto las características locales (local features) como las globales (global features) de las señales de audio, mejorando el rendimiento del reconocimiento de voz.</li>
</ul></li>
</ol>
</section>
<section id="ejercicios-avanzados" class="level3">
<h3 class="anchored" data-anchor-id="ejercicios-avanzados">Ejercicios avanzados</h3>
<ol type="1">
<li><p><strong>Análisis comparativo de métodos para mejorar la complejidad computacional:</strong></p>
<p>El transformador tiene una complejidad computacional cuadrática con respecto a la longitud de la secuencia de entrada debido a la atención propia. Se han propuesto varios métodos para mejorar esto.</p>
<ul>
<li><strong>Reformer:</strong> Utiliza la atención de Hashing Sensible a la Localidad (Locality-Sensitive Hashing, LSH) para calcular aproximadamente la similitud entre consultas y claves. LSH es una técnica de hashing que asigna vectores similares al mismo cubo, lo que permite evitar el cálculo de atención sobre toda la secuencia y concentrarse solo en tokens cercanos, reduciendo así la complejidad computacional. Reformer puede reducir significativamente el uso de memoria y el tiempo de cálculo, pero debido a la naturaleza aproximada del cálculo con LSH, la precisión puede disminuir ligeramente.</li>
<li><strong>Longformer:</strong> Combina atención de ventana deslizante (sliding window) y atención global para procesar eficientemente secuencias largas. Cada token realiza atención solo sobre tokens dentro de una ventana fija en su entorno, mientras que algunos tokens (por ejemplo, el token de inicio de oración) realizan atención sobre toda la secuencia. Longformer es rápido en el procesamiento de secuencias largas y consume menos memoria, pero el rendimiento puede variar según el tamaño de la ventana.</li>
</ul></li>
<li><p><strong>Propuesta y evaluación de nuevas arquitecturas:</strong></p>
<ul>
<li><strong>Definición del problema:</strong> Al clasificar textos largos, los transformadores existentes tienen una alta complejidad computacional y dificultad para capturar dependencias a largo plazo.</li>
<li><strong>Propuesta de arquitectura:</strong> Divide el texto en varios segmentos y aplica un codificador de transformador a cada segmento para obtener embeddings de segmentos. Luego, introduce estos embeddings de segmentos nuevamente en un codificador de transformador para obtener una representación del texto completo y clasificarlo basándose en esto.</li>
<li><strong>Ventajas teóricas:</strong> A través de una estructura jerárquica, puede capturar eficazmente dependencias a largo plazo y reducir la complejidad computacional.</li>
<li><strong>Diseño experimental:</strong> Utiliza conjuntos de datos de clasificación de texto largo como el conjunto de datos de reseñas de películas IMDB para comparar el rendimiento (precisión, F1-score) del arquitectura propuesta con modelos de transformador existentes (por ejemplo, BERT). Además, analiza los cambios en el rendimiento al variar la longitud del texto y el tamaño del segmento entre otros hiperparámetros para validar la eficacia de la arquitectura propuesta.</li>
</ul></li>
<li><p><strong>Análisis e iniciativas de respuesta a impactos éticos y sociales:</strong> El desarrollo de grandes modelos de lenguaje basados en transformers (por ejemplo, GPT-3, BERT) puede tener diversos impactos positivos y negativos en la sociedad.</p></li>
</ol>
<ul>
<li><strong>Impacto positivo:</strong> Puede reducir las barreras de comunicación y mejorar el acceso a la información a través de la traducción automática, chatbots, asistentes virtuales, etc. Además, puede aumentar la productividad mediante la generación de contenido, la creación de código, el resumen automático, etc., y acelerar la innovación al aplicarse en nuevos campos como la investigación científica (por ejemplo, predicción de estructuras de proteínas), diagnóstico médico, entre otros.</li>
<li><strong>Impacto negativo:</strong> Puede aprender sesgos presentes en los datos de entrenamiento (de género, raza, religión, etc.) y producir resultados discriminatorios. Los usuarios malintencionados pueden generar grandes cantidades de noticias falsas para manipular la opinión pública o dañar la reputación de individuos/grupos específicos. Además, el empleo en ciertas industrias relacionadas con la escritura automatizada, traducción, atención al cliente, etc., puede disminuir, y pueden surgir problemas como violaciones de privacidad y derechos de autor.</li>
<li><strong>Medidas de respuesta:</strong> Para mitigar estos impactos negativos, se necesitan esfuerzos técnicos y políticos, como la eliminación de sesgos en los datos, el desarrollo de tecnologías para detectar noticias falsas, debates sociales sobre los cambios laborales debido a la automatización y programas de reeducación, fortalecimiento de la transparencia y responsabilidad de los algoritmos, y establecimiento de directrices éticas.</li>
</ul>
</section>
</section>
</div>
</div>
</section>
</section>
<section id="referencia" class="level2">
<h2 class="anchored" data-anchor-id="referencia">Referencia</h2>
<ol type="1">
<li><a href="https://arxiv.org/abs/1706.03762">Attention is All You Need</a> (Vaswani et al., 2017) - Artículo original de Transformers</li>
<li><a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html">The Annotated Transformer</a> (Harvard NLP) - Explicación detallada de Transformers con implementación en PyTorch</li>
<li><a href="http://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a> (Jay Alammar) - Explicación visual de Transformers</li>
<li><a href="https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html">Transformer: A Novel Neural Network Architecture for Language Understanding</a> (Google AI Blog) - Introducción a Transformers</li>
<li><a href="https://lilianweng.github.io/posts/2020-04-07-the-transformer-family/">The Transformer Family</a> (Lilian Weng) - Introducción a las diversas variantes de Transformers</li>
<li><a href="https://arxiv.org/abs/1810.04805">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a> (Devlin et al., 2018) - Introducción a BERT</li>
<li><a href="https://arxiv.org/abs/2005.14165">GPT-3: Language Models are Few-Shot Learners</a> (Brown et al., 2020) - Introducción a GPT-3</li>
<li><a href="https://huggingface.co/transformers/">Hugging Face Transformers</a> - Proporciona diversos modelos y herramientas de Transformers</li>
<li><a href="https://www.tensorflow.org/text/tutorials/transformer">TensorFlow Transformer Tutorial</a> - Tutorial de implementación de Transformers con TensorFlow</li>
<li><a href="https://pytorch.org/docs/stable/nn.html#transformer-layers">PyTorch Transformer Documentation</a> - Documentación del módulo de Transformers en PyTorch</li>
<li><a href="https://arxiv.org/abs/1904.02679">Visualizing Attention in Transformer-Based Language Representation Models</a> - Visualización de atención en modelos de representación lingüística basados en Transformers</li>
<li><a href="https://arxiv.org/abs/2107.03789">A Survey of Long-Term Context in Transformers</a> - Tendencias en investigación para el manejo de contexto a largo plazo en Transformers</li>
<li><a href="https://arxiv.org/abs/2001.04451">Reformer: The Efficient Transformer</a> - Modelo Reformer que mejora la eficiencia de los Transformers</li>
<li><a href="https://arxiv.org/abs/2009.06732">Efficient Transformers: A Survey</a> - Tendencias en investigación de modelos Transformers eficientes</li>
<li><a href="https://arxiv.org/abs/2011.04006">Long Range Arena: A Benchmark for Efficient Transformers</a> - Benchmarks para Transformers eficientes que manejan contexto a largo plazo</li>
</ol>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>