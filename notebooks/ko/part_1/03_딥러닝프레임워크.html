<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>quarto-input2571f48dfeba7e03 – Deep Learning DNA: Surviving Architectures and Essential Principles</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap-f507c7d0488cb7630e20aad62ad8c2aa.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>
<script src="https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js" crossorigin="anonymous"></script>
<script>window.MathJax = {loader: {load: ['[tex]/boldsymbol']},tex: {packages: {'[+]': ['boldsymbol']}}};</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../../notebooks/ko/part_1/01_딥러닝의 시작.html">part_1</a></li><li class="breadcrumb-item"><a href="../../../notebooks/ko/part_1/03_딥러닝프레임워크.html">3. 딥러닝프레임워크</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../../../">한국어</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Language</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index_de.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Deutsch</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index_en.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">English</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index_es.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Español</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">한국어</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index_zh.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">中文</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/ko/part_1/00_서론.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">서론</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">part_1</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/ko/part_1/01_딥러닝의 시작.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">1. 딥러닝의 시작</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/ko/part_1/02_딥러닝의 수학.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">2. 딥러닝의 수학</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/ko/part_1/03_딥러닝프레임워크.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">3. 딥러닝프레임워크</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/ko/part_1/04_활성화함수.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">4. 활성화함수</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/ko/part_1/05_최적화와 시각화.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">5. 최적화와 시각화</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/ko/part_1/06_과적합과 해결 기법의 발전.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">6. 과적합과 해결 기법의 발전</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/ko/part_1/07_합성곱 신경망의 진화.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">7. 합성곱 신경망의 진화</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/ko/part_1/08_트랜스포머의 탄생.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">8. 트랜스포머의 탄생</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/ko/part_1/09_트랜스포머의 진화.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">9. 트랜스포머의 진화</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/ko/part_1/10_멀티모달 딥러닝: 다중 감각 융합의 시작.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">10. 멀티모달 딥러닝: 다중 감각 융합의 시작</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/ko/part_1/11_멀티모달 딥러닝: 한계를 넘어선 지능.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">11. 멀티모달 딥러닝: 한계를 넘어선 지능</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">딥러닝의 최전선</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/ko/딥러닝의 최전선/01_SLM: 작지만 강력한 언어모델.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">1. SLM: 작지만 강력한 언어모델</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/ko/딥러닝의 최전선/02_자율주행.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">2. 자율주행</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#딥러닝-프레임워크" id="toc-딥러닝-프레임워크" class="nav-link active" data-scroll-target="#딥러닝-프레임워크">3. 딥러닝 프레임워크</a>
  <ul class="collapse">
  <li><a href="#파이토치" id="toc-파이토치" class="nav-link" data-scroll-target="#파이토치">3.1 파이토치</a>
  <ul class="collapse">
  <li><a href="#텐서-객체" id="toc-텐서-객체" class="nav-link" data-scroll-target="#텐서-객체">3.1.1 텐서 객체</a></li>
  <li><a href="#오퍼레이션" id="toc-오퍼레이션" class="nav-link" data-scroll-target="#오퍼레이션">3.1.2 오퍼레이션</a></li>
  <li><a href="#그래디언트-연산을-위한-계산-그래프" id="toc-그래디언트-연산을-위한-계산-그래프" class="nav-link" data-scroll-target="#그래디언트-연산을-위한-계산-그래프">3.1.3 그래디언트 연산을 위한 계산 그래프</a></li>
  <li><a href="#데이터-로딩" id="toc-데이터-로딩" class="nav-link" data-scroll-target="#데이터-로딩">3.1.4 데이터 로딩</a></li>
  <li><a href="#데이터-변환transform" id="toc-데이터-변환transform" class="nav-link" data-scroll-target="#데이터-변환transform">3.1.5 데이터 변환(Transform)</a></li>
  <li><a href="#모델" id="toc-모델" class="nav-link" data-scroll-target="#모델">3.1.6 모델</a></li>
  <li><a href="#훈련" id="toc-훈련" class="nav-link" data-scroll-target="#훈련">3.1.7 훈련</a></li>
  <li><a href="#모델-저장-읽기" id="toc-모델-저장-읽기" class="nav-link" data-scroll-target="#모델-저장-읽기">3.1.8 모델 저장, 읽기</a></li>
  </ul></li>
  <li><a href="#텐서보드" id="toc-텐서보드" class="nav-link" data-scroll-target="#텐서보드">3.2 텐서보드</a>
  <ul class="collapse">
  <li><a href="#텐서보드-기본-사용법" id="toc-텐서보드-기본-사용법" class="nav-link" data-scroll-target="#텐서보드-기본-사용법">3.2.1 텐서보드 기본 사용법</a></li>
  <li><a href="#텐서보드의-주요-시각화-기능" id="toc-텐서보드의-주요-시각화-기능" class="nav-link" data-scroll-target="#텐서보드의-주요-시각화-기능">3.2.2 텐서보드의 주요 시각화 기능</a></li>
  <li><a href="#텐서보드-예제" id="toc-텐서보드-예제" class="nav-link" data-scroll-target="#텐서보드-예제">3.2.3 텐서보드 예제</a></li>
  </ul></li>
  <li><a href="#허깅페이스-트랜스포머즈" id="toc-허깅페이스-트랜스포머즈" class="nav-link" data-scroll-target="#허깅페이스-트랜스포머즈">3.3 허깅페이스 트랜스포머즈</a>
  <ul class="collapse">
  <li><a href="#트랜스포머즈-라이브러리-소개" id="toc-트랜스포머즈-라이브러리-소개" class="nav-link" data-scroll-target="#트랜스포머즈-라이브러리-소개">3.3.1 트랜스포머즈 라이브러리 소개</a></li>
  <li><a href="#주요-활용-사례" id="toc-주요-활용-사례" class="nav-link" data-scroll-target="#주요-활용-사례">3.3.2 주요 활용 사례</a></li>
  </ul></li>
  <li><a href="#연습문제" id="toc-연습문제" class="nav-link" data-scroll-target="#연습문제">연습문제</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../../notebooks/ko/part_1/01_딥러닝의 시작.html">part_1</a></li><li class="breadcrumb-item"><a href="../../../notebooks/ko/part_1/03_딥러닝프레임워크.html">3. 딥러닝프레임워크</a></li></ol></nav></header>




<p><a href="https://colab.research.google.com/github/Quantum-Intelligence-Frontier/dldna/blob/main/notebooks/ko/part_1/03_딥러닝프레임워크.ipynb" target="_parent"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"> </a></p>
<section id="딥러닝-프레임워크" class="level1">
<h1>3. 딥러닝 프레임워크</h1>
<blockquote class="blockquote">
<p>“도구는, 도구를 만드는 자만큼 훌륭하다.” - <em>익명, 그러나 폰 노이만으로 자주 인용됨</em></p>
</blockquote>
<p>딥러닝의 역사에서 프레임워크의 발전은 매우 중요합니다. 2012년 AlexNet의 성공 이후 다양한 프레임워크가 등장했습니다. Caffe, Theano, Torch7 등을 거쳐 현재는 PyTorch와 TensorFlow가 주류를 이루고 있습니다.</p>
<p>2010년대 초반, 딥러닝은 이미지 인식, 음성 인식 등 여러 분야에서 기존 기술을 뛰어넘는 놀라운 성과를 보이기 시작했습니다. 하지만 딥러닝 모델을 훈련하고 배포하는 것은 여전히 어려운 일이었습니다. 신경망 구성, 기울기 계산, GPU 가속 등을 직접 구현해야 했기 때문입니다. 이러한 복잡성은 딥러닝 연구의 진입 장벽을 높이고, 연구 속도를 늦추는 요인이었습니다. 이러한 문제를 해결하기 위해 딥러닝 프레임워크가 등장했습니다. 딥러닝 프레임워크는 신경망 모델 구축, 훈련, 배포를 위한 고수준 API와 도구를 제공하여 개발 과정을 단순화하고 가속화했습니다. 초기에는 Theano, Caffe, Torch와 같은 프레임워크가 등장하여 학계와 산업계에서 널리 사용되었습니다.</p>
<p>2015년, 구글은 텐서플로우(TensorFlow)를 오픈소스로 공개하면서 딥러닝 프레임워크 생태계에 큰 변화를 가져왔습니다. 텐서플로우는 유연한 아키텍처, 강력한 시각화 도구, 대규모 분산 학습 지원 등을 통해 빠르게 인기를 얻었습니다. 2017년, 페이스북은 파이토치(PyTorch)를 공개하며 또 다른 중요한 이정표를 세웠습니다. 파이토치는 동적 계산 그래프, 직관적인 인터페이스, 뛰어난 디버깅 기능을 제공하여 연구자들 사이에서 빠르게 확산되었습니다.</p>
<p>현재 딥러닝 프레임워크는 단순한 도구를 넘어, 딥러닝 연구와 개발의 핵심 인프라로 자리 잡았습니다. 자동 미분, GPU 가속, 모델 병렬화, 분산 학습 등 핵심 기능을 제공하며, 새로운 모델과 알고리즘 개발을 가속화하고 있습니다. 또한, 프레임워크 간의 경쟁과 협력은 딥러닝 생태계를 더욱 발전시키고 있습니다.</p>
<section id="파이토치" class="level2">
<h2 class="anchored" data-anchor-id="파이토치">3.1 파이토치</h2>
<p>PyTorch는 Torch 라이브러리를 기반으로 하는 오픈 소스 머신 러닝 프레임워크로, 컴퓨터 비전 및 자연어 처리와 같은 응용 프로그램에 사용됩니다. 2016년 Facebook의 AI Research lab (FAIR)에서 Torch7을 Python으로 재구현하면서 핵심 프레임워크로 개발되었습니다. 동적 계산 그래프와 직관적인 디버깅 기능 덕분에 연구자들 사이에서 빠르게 인기를 얻었습니다. PyTorch 외에도 TensorFlow, JAX, Caffe와 같은 다른 프레임워크가 있지만, PyTorch는 연구 분야에서 사실상의 표준이 되었습니다. 많은 새로운 모델들이 종종 PyTorch 구현과 함께 출시됩니다.</p>
<p>하나의 프레임워크에 능숙해진 후 다른 프레임워크의 장점을 활용하는 것도 좋은 전략입니다. 예를 들어, TensorFlow의 데이터 전처리 파이프라인이나 JAX의 함수형 변환 기능을 PyTorch와 함께 사용할 수 있습니다.</p>
<div id="cell-2" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip install dldna[colab] <span class="co"># in Colab</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="co"># !pip install dldna[all] # in your local</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="cell-3" class="cell" data-execution_count="1">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Print PyTorch version</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"PyTorch version: </span><span class="sc">{</span>torch<span class="sc">.</span>__version__<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Set the random seed for reproducibility</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">7</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>PyTorch version: 2.6.0+cu124</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="1">
<pre><code>&lt;torch._C.Generator at 0x7352f02b33f0&gt;</code></pre>
</div>
</div>
<p>난수를 생성할 때 초기 시드 값을 설정하면 매번 동일한 난수를 얻을 수 있습니다. 이는 반복되는 훈련에서 일관된 결과를 보장하기 위해 연구에서 흔히 사용됩니다.</p>
<section id="텐서-객체" class="level3">
<h3 class="anchored" data-anchor-id="텐서-객체">3.1.1 텐서 객체</h3>
<blockquote class="blockquote">
<p><strong>도전과제</strong>: GPU를 활용하여 대규모 행렬 연산을 어떻게 효율적으로 수행할 수 있을까?</p>
<p><strong>연구자의 고뇌</strong>: 딥러닝 모델의 크기가 커지면서, CPU만으로는 학습과 추론에 너무 많은 시간이 걸렸습니다. GPU는 병렬 연산에 특화되어 딥러닝에 적합했지만, GPU 프로그래밍은 복잡하고 어려웠습니다. 딥러닝 연구자들이 GPU를 쉽게 활용할 수 있도록, GPU 연산을 추상화하고 자동화하는 도구가 필요했습니다.</p>
</blockquote>
<p>텐서는 PyTorch의 기본 데이터 구조입니다. 2006년 CUDA가 등장한 이후 GPU 연산이 딥러닝의 핵심이 되었고, 텐서는 이러한 GPU 연산을 효율적으로 수행할 수 있도록 설계되었습니다. 텐서는 스칼라, 벡터, 행렬을 일반화한 다차원 배열입니다. 딥러닝에서는 데이터의 차원(텐서 랭크)이 매우 다양합니다. 예를 들어, 이미지는 (배치, 채널, 높이, 너비)의 4차원 텐서로, 자연어는 (배치, 시퀀스 길이, 임베딩 차원)의 3차원 텐서로 표현됩니다. 2장에서 살펴본 것처럼 이러한 차원을 자유자재로 변형하고 가공하는 것이 중요합니다.</p>
<p>다음과 같이 텐서를 선언할 수 있습니다.</p>
<div id="cell-6" class="cell" data-execution_count="2">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a 3x2x4 tensor with random values</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> torch.Tensor(<span class="dv">3</span>, <span class="dv">2</span>, <span class="dv">4</span>)</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(a)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>tensor([[[ 1.1210e-44,  0.0000e+00,  0.0000e+00,  4.1369e-41],
         [ 1.8796e-17,  0.0000e+00,  2.8026e-45,  0.0000e+00]],

        [[ 0.0000e+00,  0.0000e+00,         nan,         nan],
         [ 6.3058e-44,  4.7424e+30,  1.4013e-45,  1.3563e-19]],

        [[ 1.0089e-43,  0.0000e+00,  1.1210e-44,  0.0000e+00],
         [-8.8105e+09,  4.1369e-41,  1.8796e-17,  0.0000e+00]]])</code></pre>
</div>
</div>
<p>기존 데이터로부터도 텐서를 초기화할 수 있습니다.</p>
<div id="cell-8" class="cell" data-execution_count="4">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># From a Python list</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>d <span class="op">=</span> [[<span class="dv">1</span>, <span class="dv">2</span>], [<span class="dv">3</span>, <span class="dv">4</span>]]</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Type of d: </span><span class="sc">{</span><span class="bu">type</span>(d)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> torch.Tensor(d)  <span class="co"># Creates a *copy*</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Tensor a:</span><span class="ch">\n</span><span class="sc">{</span>a<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Type of a: </span><span class="sc">{</span><span class="bu">type</span>(a)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a><span class="co"># From a NumPy array</span></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>d_np <span class="op">=</span> np.array(d)</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Type of d_np: </span><span class="sc">{</span><span class="bu">type</span>(d_np)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> torch.from_numpy(d_np) <span class="co"># Shares memory with d_np (zero-copy)</span></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Tensor b (from_numpy):</span><span class="ch">\n</span><span class="sc">{</span>b<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>c <span class="op">=</span> torch.Tensor(d_np)  <span class="co"># Creates a *copy*</span></span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Tensor c (from np array using torch.Tensor):</span><span class="ch">\n</span><span class="sc">{</span>c<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Example of memory sharing with torch.from_numpy</span></span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>d_np[<span class="dv">0</span>, <span class="dv">0</span>] <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Modified d_np:</span><span class="ch">\n</span><span class="sc">{</span>d_np<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Tensor b (from_numpy) after modifying d_np:</span><span class="ch">\n</span><span class="sc">{</span>b<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Tensor c (copy) after modifying d_np:</span><span class="ch">\n</span><span class="sc">{</span>c<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Type of d: &lt;class 'list'&gt;
Tensor a:
tensor([[1., 2.],
        [3., 4.]])
Type of a: &lt;class 'torch.Tensor'&gt;
Type of d_np: &lt;class 'numpy.ndarray'&gt;
Tensor b (from_numpy):
tensor([[1, 2],
        [3, 4]])
Tensor c (from np array using torch.Tensor):
tensor([[1., 2.],
        [3., 4.]])
Modified d_np:
[[100   2]
 [  3   4]]
Tensor b (from_numpy) after modifying d_np:
tensor([[100,   2],
        [  3,   4]])
Tensor c (copy) after modifying d_np:
tensor([[1., 2.],
        [3., 4.]])</code></pre>
</div>
</div>
<p>출력했을 때 같아 보인다고 해서 같은 객체인 것은 아닙니다. <code>d</code>는 파이썬 리스트 객체이고, 텐서는 다양한 데이터 구조로부터 생성될 수 있습니다. 특히 NumPy 배열과의 상호 작용은 매우 효율적입니다. 그러나 리스트 객체와 NumPy 배열은 GPU를 지원하지 않으므로 대규모 연산에는 텐서로의 변환이 필수적입니다. <em>중요한</em> 점은 <code>torch.Tensor(data)</code>와 <code>torch.from_numpy(data)</code>의 차이점을 이해하는 것입니다. 전자는 <em>항상</em> 복사본을 생성하는 반면, 후자는 원본 NumPy 배열과 메모리를 공유하는 <em>뷰</em>를 생성합니다(가능한 경우 - 제로 카피). NumPy 배열을 수정하면 <code>from_numpy</code>로 생성된 텐서도 변경되며, 그 반대도 마찬가지입니다.</p>
<p>텐서를 초기화하는 방법은 매우 다양합니다. 2006년 Hinton의 논문 이후 초기화 방법의 중요성이 부각되었고 다양한 초기화 전략이 개발되었습니다. 가장 기본적인 초기화 함수는 다음과 같습니다.</p>
<ul>
<li><code>torch.zeros</code>: 0으로 초기화합니다.</li>
<li><code>torch.ones</code>: 1로 초기화합니다.</li>
<li><code>torch.rand</code>: 0과 1 사이의 균일 분포에서 난수로 초기화합니다.</li>
<li><code>torch.randn</code>: 표준 정규 분포(평균 0, 분산 1)에서 난수로 초기화합니다.</li>
<li><code>torch.arange</code>: n, n+1, n+2, … 와 같이 순차적으로 초기화합니다.</li>
</ul>
<div id="cell-10" class="cell" data-execution_count="5">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>shape <span class="op">=</span> (<span class="dv">2</span>, <span class="dv">3</span>)</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>rand_t <span class="op">=</span> torch.rand(shape)     <span class="co"># Uniform distribution [0, 1)</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>randn_t <span class="op">=</span> torch.randn(shape)   <span class="co"># Standard normal distribution</span></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>ones_t <span class="op">=</span> torch.ones(shape)</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>zeros_t <span class="op">=</span> torch.zeros(shape)</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Random tensor (uniform):</span><span class="ch">\n</span><span class="sc">{</span>rand_t<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Random tensor (normal):</span><span class="ch">\n</span><span class="sc">{</span>randn_t<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Ones tensor:</span><span class="ch">\n</span><span class="sc">{</span>ones_t<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Zeros tensor:</span><span class="ch">\n</span><span class="sc">{</span>zeros_t<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Random tensor (uniform):
tensor([[0.5349, 0.1988, 0.6592],
        [0.6569, 0.2328, 0.4251]])
Random tensor (normal):
tensor([[-1.2514, -1.8841,  0.4457],
        [-0.7068, -1.5750, -0.6318]])
Ones tensor:
tensor([[1., 1., 1.],
        [1., 1., 1.]])
Zeros tensor:
tensor([[0., 0., 0.],
        [0., 0., 0.]])</code></pre>
</div>
</div>
<p>PyTorch는 100개 이상의 텐서 연산을 지원하며, 이들 모두 GPU에서 실행할 수 있습니다. 텐서는 기본적으로 CPU 메모리에 생성되므로, GPU를 사용하려면 <code>to()</code> 함수를 사용하여 명시적으로 이동해야 합니다. 대규모 텐서를 CPU와 GPU 간에 이동하는 데는 상당한 비용이 발생하므로 신중한 메모리 관리가 필수적입니다. 실제 딥러닝 훈련에서 GPU의 메모리 대역폭은 성능에 결정적인 영향을 미칩니다. 예를 들어, 트랜스포머 모델을 훈련할 때 GPU 메모리가 클수록 더 큰 배치 크기를 사용할 수 있어 훈련 효율성이 높아집니다. 그러나 고대역폭 메모리는 생산 비용이 매우 높아 GPU 가격의 상당 부분을 차지합니다. CPU와 GPU 텐서 연산의 성능 차이는 특히 행렬 곱셈과 같이 병렬화 가능한 연산에서 두드러집니다. 이러한 이유로 현대 딥러닝에서는 GPU, TPU, NPU와 같은 전용 가속기가 필수적입니다.</p>
<div id="cell-12" class="cell" data-execution_count="6">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Device setting</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> torch.cuda.is_available():</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>    tensor <span class="op">=</span> zeros_t.to(<span class="st">"cuda"</span>)</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>    device <span class="op">=</span> <span class="st">"cuda:0"</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>    device <span class="op">=</span> <span class="st">"cpu"</span></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'GPU not available'</span>)</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a><span class="co"># CPU/GPU performance comparison</span></span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> time</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a><span class="co"># CPU operation</span></span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.rand(<span class="dv">10000</span>, <span class="dv">10000</span>)</span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>start <span class="op">=</span> time.time()</span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a>torch.matmul(x, x)</span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a>cpu_time <span class="op">=</span> time.time() <span class="op">-</span> start</span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"CPU computation time = </span><span class="sc">{</span>cpu_time<span class="sc">:3.2f}</span><span class="ss"> seconds"</span>)</span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a><span class="co"># GPU operation</span></span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> device <span class="op">!=</span> <span class="st">"cpu"</span>:</span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> x.to(device)</span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a>    start <span class="op">=</span> torch.cuda.Event(enable_timing<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb11-23"><a href="#cb11-23" aria-hidden="true" tabindex="-1"></a>    end <span class="op">=</span> torch.cuda.Event(enable_timing<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb11-24"><a href="#cb11-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-25"><a href="#cb11-25" aria-hidden="true" tabindex="-1"></a>    start.record()</span>
<span id="cb11-26"><a href="#cb11-26" aria-hidden="true" tabindex="-1"></a>    torch.matmul(x, x)</span>
<span id="cb11-27"><a href="#cb11-27" aria-hidden="true" tabindex="-1"></a>    end.record()</span>
<span id="cb11-28"><a href="#cb11-28" aria-hidden="true" tabindex="-1"></a>    torch.cuda.synchronize()  <span class="co"># Wait for all operations to complete</span></span>
<span id="cb11-29"><a href="#cb11-29" aria-hidden="true" tabindex="-1"></a>    gpu_time <span class="op">=</span> start.elapsed_time(end) <span class="op">/</span> <span class="dv">1000</span>  <span class="co"># Convert milliseconds to seconds</span></span>
<span id="cb11-30"><a href="#cb11-30" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"GPU computation time = </span><span class="sc">{</span>gpu_time<span class="sc">:3.2f}</span><span class="ss"> seconds"</span>)</span>
<span id="cb11-31"><a href="#cb11-31" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"GPU is </span><span class="sc">{</span>cpu_time <span class="op">/</span> gpu_time<span class="sc">:3.1f}</span><span class="ss"> times faster."</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>CPU computation time = 2.34 seconds
GPU computation time = 0.14 seconds
GPU is 16.2 times faster.</code></pre>
</div>
</div>
<p>NumPy와 텐서 간의 변환은 매우 효율적으로 구현됩니다. 특히, 위에서 보았듯이 <code>torch.from_numpy()</code>를 사용하면 메모리 복사 없이 메모리가 공유됩니다.</p>
<div id="cell-14" class="cell" data-execution_count="7">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>np_a <span class="op">=</span> np.array([[<span class="dv">1</span>, <span class="dv">1</span>], [<span class="dv">2</span>, <span class="dv">3</span>]])</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>tensor_a <span class="op">=</span> torch.from_numpy(np_a)</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>np_b <span class="op">=</span> tensor_a.numpy() <span class="co"># Shares memory.  If tensor_a is on CPU.</span></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"NumPy array: </span><span class="sc">{</span>np_a<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Tensor: </span><span class="sc">{</span>tensor_a<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"NumPy array from Tensor: </span><span class="sc">{</span>np_b<span class="sc">}</span><span class="ss">"</span>) <span class="co">#if tensor_a is on CPU.</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>NumPy array: [[1 1]
 [2 3]]
Tensor: tensor([[1, 1],
        [2, 3]])
NumPy array from Tensor: [[1 1]
 [2 3]]</code></pre>
</div>
</div>
<p>텐서를 NumPy로 변환할 때는 텐서가 반드시 CPU에 있어야 합니다. GPU에 있는 텐서는 먼저 <code>.cpu()</code>를 사용하여 CPU로 이동해야 합니다. 텐서의 기본 속성은 <code>shape</code>, <code>dtype</code>, <code>device</code>이며, 이를 통해 텐서의 모양과 저장 위치를 확인할 수 있습니다.</p>
<div id="cell-16" class="cell" data-execution_count="18">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> torch.rand(<span class="dv">2</span>, <span class="dv">3</span>)</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Shape = </span><span class="sc">{</span>a<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Data type = </span><span class="sc">{</span>a<span class="sc">.</span>dtype<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Device = </span><span class="sc">{</span>a<span class="sc">.</span>device<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Shape = torch.Size([2, 3])
Data type = torch.float32
Device = cpu</code></pre>
</div>
</div>
<p>인덱싱과 슬라이싱은 넘파이와 동일한 문법을 사용합니다.</p>
<div id="cell-18" class="cell" data-execution_count="19">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> torch.rand(<span class="dv">3</span>, <span class="dv">3</span>)</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Tensor a:</span><span class="ch">\n</span><span class="sc">{</span>a<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"First row: </span><span class="sc">{</span>a[<span class="dv">0</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"First column: </span><span class="sc">{</span>a[:, <span class="dv">0</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Last column: </span><span class="sc">{</span>a[..., <span class="op">-</span><span class="dv">1</span>]<span class="sc">}</span><span class="ss">"</span>)  <span class="co"># Equivalent to a[:, -1]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Tensor a:
tensor([[0.2069, 0.8296, 0.4973],
        [0.9265, 0.8386, 0.6611],
        [0.5329, 0.7822, 0.0975]])
First row: tensor([0.2069, 0.8296, 0.4973])
First column: tensor([0.2069, 0.9265, 0.5329])
Last column: tensor([0.4973, 0.6611, 0.0975])</code></pre>
</div>
</div>
</section>
<section id="오퍼레이션" class="level3">
<h3 class="anchored">3.1.2 오퍼레이션</h3>
<p>PyTorch는 NumPy의 거의 모든 연산을 지원합니다. 1964년 APL 언어에서 시작된 다차원 배열 연산의 전통이 NumPy를 거쳐 PyTorch에도 이어지고 있습니다. PyTorch 공식 문서(<a href="[https://pytorch.org/docs/stable/tensors.html">PyTorch documentation</a>)에서 지원하는 모든 연산 목록을 확인할 수 있습니다.</p>
<p>텐서의 형상 변경은 신경망에서 가장 빈번하게 사용되는 연산 중 하나입니다. <code>view()</code> 함수를 통해 텐서의 차원을 변경할 수 있으며, 이때 전체 요소의 개수는 유지되어야 합니다. <code>permute()</code> 함수는 차원의 순서를 재배열합니다.</p>
<div id="cell-20" class="cell" data-execution_count="20">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> torch.arange(<span class="dv">12</span>)</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"a: </span><span class="sc">{</span>a<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> a.view(<span class="dv">3</span>, <span class="dv">4</span>)  <span class="co"># Reshape to 3x4</span></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"x: </span><span class="sc">{</span>x<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> x.permute(<span class="dv">1</span>, <span class="dv">0</span>)  <span class="co"># Swap dimensions 0 and 1</span></span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"y: </span><span class="sc">{</span>y<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> torch.randn(<span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">5</span>)</span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"b shape: </span><span class="sc">{</span>b<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a>z <span class="op">=</span> b.permute(<span class="dv">2</span>, <span class="dv">0</span>, <span class="dv">1</span>)  <span class="co"># Change dimension order to (2, 0, 1)</span></span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"z shape: </span><span class="sc">{</span>z<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>a: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])
x: tensor([[ 0,  1,  2,  3],
        [ 4,  5,  6,  7],
        [ 8,  9, 10, 11]])
y: tensor([[ 0,  4,  8],
        [ 1,  5,  9],
        [ 2,  6, 10],
        [ 3,  7, 11]])
b shape: torch.Size([2, 3, 5])
z shape: torch.Size([5, 2, 3])</code></pre>
</div>
</div>
<p>행렬 연산은 딥러닝의 핵심이며, PyTorch는 다양한 행렬 연산 함수를 제공합니다.</p>
<ol type="1">
<li><code>torch.matmul</code>: 일반적인 행렬 연산을 수행합니다. 차원에 따라 다음과 같이 동작합니다.
<ul>
<li>1차원 × 1차원: 내적(dot product)</li>
<li>2차원 × 2차원: 행렬곱</li>
<li>1차원 × 2차원: 첫 텐서에 차원 1을 추가한 후 행렬곱</li>
<li>N차원 × M차원: 브로드캐스팅 후 행렬곱</li>
</ul></li>
<li><code>torch.mm</code>: 순수 행렬곱 연산(브로드캐스팅 미지원)</li>
<li><code>torch.bmm</code>: 배치 차원을 포함한 행렬곱 ((b, i, k) × (b, k, j) → (b, i, j))</li>
<li><code>torch.einsum</code>: 아인슈타인 합 표기법을 사용한 텐서 연산. 복잡한 텐서 연산을 간결하게 표현할 수 있습니다. (자세한 내용은 “이론 딥다이브” 참고)
<ul>
<li><code>torch.einsum('ij,jk-&gt;ik', a, b)</code>: 행렬 a와 b의 곱</li>
</ul></li>
</ol>
<div id="cell-22" class="cell" data-execution_count="21">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> torch.arange(<span class="dv">6</span>)</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> torch.arange(<span class="dv">12</span>)</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> a.view(<span class="dv">2</span>, <span class="dv">3</span>)</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>Y <span class="op">=</span> b.view(<span class="dv">3</span>, <span class="dv">4</span>)</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"X: </span><span class="sc">{</span>X<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Y: </span><span class="sc">{</span>Y<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a><span class="co"># matmul (2,3) X (3,4) -&gt; (2, 4)</span></span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"X @ Y = </span><span class="sc">{</span>torch<span class="sc">.</span>matmul(X, Y)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Using torch.einsum for matrix multiplication</span></span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a>einsum_result <span class="op">=</span> torch.einsum(<span class="st">'ij,jk-&gt;ik'</span>, X, Y)</span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"X @ Y (using einsum) = </span><span class="sc">{</span>einsum_result<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb21-15"><a href="#cb21-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-16"><a href="#cb21-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-17"><a href="#cb21-17" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> torch.arange(<span class="dv">2</span>)</span>
<span id="cb21-18"><a href="#cb21-18" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> torch.arange(<span class="dv">2</span>)</span>
<span id="cb21-19"><a href="#cb21-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"a: </span><span class="sc">{</span>a<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb21-20"><a href="#cb21-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"b: </span><span class="sc">{</span>b<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb21-21"><a href="#cb21-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-22"><a href="#cb21-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Vector x Vector operation</span></span>
<span id="cb21-23"><a href="#cb21-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"a @ b = </span><span class="sc">{</span>torch<span class="sc">.</span>matmul(a, b)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb21-24"><a href="#cb21-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-25"><a href="#cb21-25" aria-hidden="true" tabindex="-1"></a><span class="co"># 1D tensor (vector), 2D tensor (matrix) operation</span></span>
<span id="cb21-26"><a href="#cb21-26" aria-hidden="true" tabindex="-1"></a><span class="co"># (2) x (2,2) is treated as (1,2) x (2,2) for matrix multiplication.</span></span>
<span id="cb21-27"><a href="#cb21-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Result: (1,2) x (2,2) -&gt; (1,2)</span></span>
<span id="cb21-28"><a href="#cb21-28" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> torch.arange(<span class="dv">4</span>)</span>
<span id="cb21-29"><a href="#cb21-29" aria-hidden="true" tabindex="-1"></a>B <span class="op">=</span> b.view(<span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb21-30"><a href="#cb21-30" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"a: </span><span class="sc">{</span>a<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb21-31"><a href="#cb21-31" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"B: </span><span class="sc">{</span>B<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb21-32"><a href="#cb21-32" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"a @ B = </span><span class="sc">{</span>torch<span class="sc">.</span>matmul(a, B)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb21-33"><a href="#cb21-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-34"><a href="#cb21-34" aria-hidden="true" tabindex="-1"></a><span class="co"># Matrix x Vector operation</span></span>
<span id="cb21-35"><a href="#cb21-35" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> torch.randn(<span class="dv">3</span>, <span class="dv">4</span>)</span>
<span id="cb21-36"><a href="#cb21-36" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> torch.randn(<span class="dv">4</span>)</span>
<span id="cb21-37"><a href="#cb21-37" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"X @ b shape = </span><span class="sc">{</span>torch<span class="sc">.</span>matmul(X, b)<span class="sc">.</span>size()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb21-38"><a href="#cb21-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-39"><a href="#cb21-39" aria-hidden="true" tabindex="-1"></a><span class="co"># Batched matrix x Batched matrix</span></span>
<span id="cb21-40"><a href="#cb21-40" aria-hidden="true" tabindex="-1"></a><span class="co"># The leading batch dimension is maintained.</span></span>
<span id="cb21-41"><a href="#cb21-41" aria-hidden="true" tabindex="-1"></a><span class="co"># The 2nd and 3rd dimensions are treated as matrices for multiplication.</span></span>
<span id="cb21-42"><a href="#cb21-42" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> torch.arange(<span class="dv">18</span>).view(<span class="dv">3</span>, <span class="dv">2</span>, <span class="dv">3</span>)</span>
<span id="cb21-43"><a href="#cb21-43" aria-hidden="true" tabindex="-1"></a>Y <span class="op">=</span> torch.arange(<span class="dv">18</span>).view(<span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">2</span>)</span>
<span id="cb21-44"><a href="#cb21-44" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"X: </span><span class="sc">{</span>X<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb21-45"><a href="#cb21-45" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Y: </span><span class="sc">{</span>Y<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb21-46"><a href="#cb21-46" aria-hidden="true" tabindex="-1"></a><span class="co"># Batch dimension remains the same, and (2,3)x(3,2) -&gt; (2,2)</span></span>
<span id="cb21-47"><a href="#cb21-47" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"X @ Y shape: </span><span class="sc">{</span>torch<span class="sc">.</span>matmul(X, Y)<span class="sc">.</span>size()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb21-48"><a href="#cb21-48" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"X @ Y: </span><span class="sc">{</span>torch<span class="sc">.</span>matmul(X, Y)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb21-49"><a href="#cb21-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-50"><a href="#cb21-50" aria-hidden="true" tabindex="-1"></a><span class="co"># Batched matrix x Broadcasted matrix</span></span>
<span id="cb21-51"><a href="#cb21-51" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> torch.arange(<span class="dv">18</span>).view(<span class="dv">3</span>, <span class="dv">2</span>, <span class="dv">3</span>)</span>
<span id="cb21-52"><a href="#cb21-52" aria-hidden="true" tabindex="-1"></a>Y <span class="op">=</span> torch.arange(<span class="dv">6</span>).view(<span class="dv">3</span>, <span class="dv">2</span>)</span>
<span id="cb21-53"><a href="#cb21-53" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"X: </span><span class="sc">{</span>X<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb21-54"><a href="#cb21-54" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Y: </span><span class="sc">{</span>Y<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb21-55"><a href="#cb21-55" aria-hidden="true" tabindex="-1"></a><span class="co"># The second matrix lacks a batch dimension.</span></span>
<span id="cb21-56"><a href="#cb21-56" aria-hidden="true" tabindex="-1"></a><span class="co"># It's broadcasted to match the batch dimension of the first matrix (repeated 3 times).</span></span>
<span id="cb21-57"><a href="#cb21-57" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"X @ Y shape: </span><span class="sc">{</span>torch<span class="sc">.</span>matmul(X, Y)<span class="sc">.</span>size()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb21-58"><a href="#cb21-58" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"X @ Y: </span><span class="sc">{</span>torch<span class="sc">.</span>matmul(X, Y)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb21-59"><a href="#cb21-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-60"><a href="#cb21-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-61"><a href="#cb21-61" aria-hidden="true" tabindex="-1"></a><span class="co"># Using torch.einsum for matrix multiplication</span></span>
<span id="cb21-62"><a href="#cb21-62" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> torch.arange(<span class="dv">6</span>).view(<span class="dv">2</span>, <span class="dv">3</span>)</span>
<span id="cb21-63"><a href="#cb21-63" aria-hidden="true" tabindex="-1"></a>Y <span class="op">=</span> torch.arange(<span class="dv">12</span>).view(<span class="dv">3</span>, <span class="dv">4</span>)</span>
<span id="cb21-64"><a href="#cb21-64" aria-hidden="true" tabindex="-1"></a>einsum_result <span class="op">=</span> torch.einsum(<span class="st">'ij,jk-&gt;ik'</span>, X, Y)  <span class="co"># Equivalent to torch.matmul(X, Y)</span></span>
<span id="cb21-65"><a href="#cb21-65" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"X @ Y (using einsum) = </span><span class="sc">{</span>einsum_result<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>X: tensor([[0, 1, 2],
        [3, 4, 5]])
Y: tensor([[ 0,  1,  2,  3],
        [ 4,  5,  6,  7],
        [ 8,  9, 10, 11]])
X @ Y = tensor([[20, 23, 26, 29],
        [56, 68, 80, 92]])
X @ Y (using einsum) = tensor([[20, 23, 26, 29],
        [56, 68, 80, 92]])
a: tensor([0, 1])
b: tensor([0, 1])
a @ b = 1
a: tensor([0, 1])
B: tensor([[0, 1],
        [2, 3]])
a @ B = tensor([2, 3])
X @ b shape = torch.Size([3])
X: tensor([[[ 0,  1,  2],
         [ 3,  4,  5]],

        [[ 6,  7,  8],
         [ 9, 10, 11]],

        [[12, 13, 14],
         [15, 16, 17]]])
Y: tensor([[[ 0,  1],
         [ 2,  3],
         [ 4,  5]],

        [[ 6,  7],
         [ 8,  9],
         [10, 11]],

        [[12, 13],
         [14, 15],
         [16, 17]]])
X @ Y shape: torch.Size([3, 2, 2])
X @ Y: tensor([[[ 10,  13],
         [ 28,  40]],

        [[172, 193],
         [244, 274]],

        [[550, 589],
         [676, 724]]])
X: tensor([[[ 0,  1,  2],
         [ 3,  4,  5]],

        [[ 6,  7,  8],
         [ 9, 10, 11]],

        [[12, 13, 14],
         [15, 16, 17]]])
Y: tensor([[0, 1],
        [2, 3],
        [4, 5]])
X @ Y shape: torch.Size([3, 2, 2])
X @ Y: tensor([[[ 10,  13],
         [ 28,  40]],

        [[ 46,  67],
         [ 64,  94]],

        [[ 82, 121],
         [100, 148]]])
X @ Y (using einsum) = tensor([[20, 23, 26, 29],
        [56, 68, 80, 92]])</code></pre>
</div>
</div>
<p><code>torch.einsum</code>은 아인슈타인 표기법을 사용하여 텐서 연산을 표현합니다. <code>'ij,jk-&gt;ik'</code>는 <code>X</code> 텐서의 <code>(i, j)</code> 차원과 <code>Y</code> 텐서의 <code>(j, k)</code> 차원을 곱하여 <code>(i, k)</code> 차원의 결과를 생성하라는 의미입니다. 이는 행렬 곱셈 <code>torch.matmul(X, Y)</code>와 동일한 결과를 나타냅니다. <code>einsum</code>은 이 외에도 전치, 합, 내적, 외적, 배치 행렬 곱 등 다양한 연산을 지원합니다. 더 자세한 내용은 PyTorch 문서를 참조하시기 바랍니다.</p>
<div id="cell-24" class="cell" data-execution_count="22">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Other einsum examples</span></span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Transpose</span></span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> torch.randn(<span class="dv">2</span>, <span class="dv">3</span>)</span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> torch.einsum(<span class="st">'ij-&gt;ji'</span>, a)  <span class="co"># Swap dimensions</span></span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Sum of all elements</span></span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> torch.randn(<span class="dv">2</span>, <span class="dv">3</span>)</span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> torch.einsum(<span class="st">'ij-&gt;'</span>, a)  <span class="co"># Sum all elements</span></span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Batch matrix multiplication</span></span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> torch.randn(<span class="dv">3</span>, <span class="dv">2</span>, <span class="dv">5</span>)</span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> torch.randn(<span class="dv">3</span>, <span class="dv">5</span>, <span class="dv">3</span>)</span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true" tabindex="-1"></a>c <span class="op">=</span> torch.einsum(<span class="st">'bij,bjk-&gt;bik'</span>, a, b) <span class="co"># Batch matrix multiplication</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="callout callout-style-default callout-note callout-titled" title="클릭하여 내용 보기(딥다이브 : 아인슈타인 표기법과 torch.einsum)">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
클릭하여 내용 보기(딥다이브 : 아인슈타인 표기법과 torch.einsum)
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<section id="아인슈타인-표기법과-torch.einsum" class="level2 callout-body-container callout-body">
<h2 class="anchored" data-anchor-id="아인슈타인-표기법과-torch.einsum">아인슈타인 표기법과 torch.einsum</h2>
<section id="아인슈타인-표기법-einstein-notation" class="level3">
<h3 class="anchored" data-anchor-id="아인슈타인-표기법-einstein-notation">아인슈타인 표기법 (Einstein Notation)</h3>
<p>아인슈타인 표기법(Einstein Notation, 또는 Einstein Summation Convention)은 1916년 알베르트 아인슈타인이 일반 상대성 이론을 기술하면서 도입한 표기법입니다. 원래는 물리학, 특히 상대성 이론의 수식을 간결하게 표현하기 위해 고안되었지만, 그 편리함과 표현력 덕분에 텐서 연산을 다루는 다양한 분야에서 널리 사용되고 있습니다.</p>
<p><strong>핵심 아이디어:</strong></p>
<ul>
<li><strong>반복되는 인덱스는 합을 의미한다:</strong> 하나의 항(term)에서 같은 인덱스가 두 번 나타나면, 해당 인덱스에 대해 <em>모든 가능한 값</em>을 더한다(summation)는 것을 암시합니다. 명시적인 합 기호(<span class="math inline">\(\\sum\)</span>)를 생략하여 표기를 간결하게 만듭니다.</li>
<li><strong>자유 인덱스와 더미 인덱스:</strong>
<ul>
<li><strong>자유 인덱스 (free index):</strong> 결과 텐서에 나타나는 인덱스. 각 항에서 한 번만 나타납니다.</li>
<li><strong>더미 인덱스 (dummy index):</strong> 합산되는 인덱스. 하나의 항에서 두 번 나타납니다. (summation index, bound index)</li>
</ul></li>
</ul>
<p><strong>기본 규칙</strong></p>
<ol type="1">
<li><strong>같은 인덱스가 한 항에 두 번 나타나면, 해당 인덱스에 대해 합산합니다.</strong></li>
<li><strong>자유 인덱스는 결과 텐서의 차원을 결정합니다.</strong></li>
<li><strong>더미 인덱스는 내부 계산에만 사용되고 결과에는 나타나지 않습니다.</strong></li>
<li><strong>인덱스 문자는 임의로 선택할 수 있지만, 혼동을 피하기 위해 일관성을 유지하는 것이 좋습니다.</strong> (관례적으로 <span class="math inline">\(i, j, k, l, m, n\)</span> 등을 사용)</li>
<li><strong>화살표(<span class="math inline">\(\\rightarrow\)</span>)의 왼쪽</strong>은 입력 텐서, <strong>오른쪽</strong>은 출력 텐서를 나타냅니다.</li>
</ol>
<p><strong>예시</strong></p>
<ul>
<li><strong>벡터 내적 (dot product):</strong> <span class="math inline">\(a\_i b\_i\)</span> (<span class="math inline">\(\\sum\_i a\_i b\_i\)</span> 와 동일)</li>
<li><strong>행렬 곱셈 (matrix multiplication):</strong> <span class="math inline">\(A\_{ij} B\_{jk} = C\_{ik}\)</span> (<span class="math inline">\(\\sum\_j A\_{ij}B\_{jk}\)</span> 와 동일)</li>
<li><strong>전치 (transpose):</strong> <span class="math inline">\(A\_{ij} = B\_{ji}\)</span> (B는 A의 전치 행렬)</li>
<li><strong>대각합 (trace):</strong> <span class="math inline">\(A\_{ii}\)</span> (<span class="math inline">\(\\sum\_i A\_{ii}\)</span> 와 동일)</li>
<li><strong>외적 (outer product):</strong> <span class="math inline">\(a\_i b\_j = C\_{ij}\)</span></li>
<li><strong>원소별 곱(element-wise multiplication):</strong> <span class="math inline">\(A\_{ij}B\_{ij} = C\_{ij}\)</span> (Hadamard product)</li>
</ul>
<p><strong>딥러닝에서의 활용 예시</strong></p>
<ul>
<li><strong>배치 행렬 곱셈 (batched matrix multiplication):</strong> <span class="math inline">\(A\_{bij} B\_{bjk} = C\_{bik}\)</span> (<span class="math inline">\(b\)</span>: 배치 차원)</li>
<li><strong>어텐션 메커니즘 (attention mechanism):</strong> <span class="math inline">\(e\_{ij} = Q\_{ik} K\_{jk}\)</span>, <span class="math inline">\(a\_{ij} = \\text{softmax}(e\_{ij})\)</span>, <span class="math inline">\(v\_{i} = a\_{ij} V\_{j}\)</span> (<span class="math inline">\(Q\)</span>: 쿼리, <span class="math inline">\(K\)</span>: 키, <span class="math inline">\(V\)</span>: 값)</li>
<li><strong>양선형 변환 (bilinear transformation):</strong> <span class="math inline">\(x\_i W\_{ijk} y\_j = z\_k\)</span></li>
<li><strong>다차원 컨볼루션 (convolution):</strong> <span class="math inline">\(I\_{b,c,i,j} \* F\_{o,c,k,l} = O\_{b,o,i',j'}\)</span> (<span class="math inline">\(b\)</span>: 배치, <span class="math inline">\(c\)</span>: 입력 채널, <span class="math inline">\(o\)</span>: 출력 채널, <span class="math inline">\(i, j\)</span>: 입력 공간 차원, <span class="math inline">\(k, l\)</span>: 필터 공간 차원)</li>
<li><strong>배치 정규화 (Batch Normalization):</strong> <span class="math inline">\(\\gamma\_c \* \\frac{x\_{b,c,h,w} - \\mu\_c}{\\sigma\_c} + \\beta\_c\)</span> (<span class="math inline">\(c\)</span>: 채널 차원, <span class="math inline">\(b\)</span>:배치, <span class="math inline">\(h\)</span>:높이, <span class="math inline">\(w\)</span>:너비)</li>
<li><strong>RNN hidden state update</strong>: <span class="math inline">\(h\_t = \\tanh(W\_{ih}x\_t + b\_{ih} + W\_{hh}h\_{t-1} + b\_{hh})\)</span> (<span class="math inline">\(h\)</span>: hidden, <span class="math inline">\(x\)</span>: input, <span class="math inline">\(W\)</span>: weight, <span class="math inline">\(b\)</span>: bias)</li>
<li><strong>LSTM cell state update</strong>: <span class="math inline">\(c\_t = f\_t \* c\_{t-1} + i\_t \* \\tilde{c}\_t\)</span> (<span class="math inline">\(c\)</span>: cell state, <span class="math inline">\(f\)</span>: forget gate, <span class="math inline">\(i\)</span>: input gate, <span class="math inline">\(\\tilde{c}\_t\)</span>: candidate cell state)</li>
</ul>
</section>
<section id="torch.einsum" class="level3">
<h3 class="anchored" data-anchor-id="torch.einsum">torch.einsum</h3>
<p><code>torch.einsum</code>은 PyTorch에서 아인슈타인 표기법을 사용하여 텐서 연산을 수행하는 함수입니다. <code>einsum</code>은 “Einstein summation”의 약자입니다.</p>
<p><strong>사용법:</strong></p>
<div class="sourceCode" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>torch.einsum(equation, <span class="op">*</span>operands)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li><code>equation</code>: 아인슈타인 표기법 문자열. <code>'ij,jk-&gt;ik'</code> 와 같은 형태입니다.</li>
<li><code>*operands</code>: 연산에 참여하는 텐서들 (가변 인자).</li>
</ul>
<p><strong>장점</strong></p>
<ul>
<li><strong>간결성:</strong> 복잡한 텐서 연산을 한 줄의 코드로 표현할 수 있습니다.</li>
<li><strong>가독성:</strong> 아인슈타인 표기법은 텐서 연산의 의미를 명확하게 드러냅니다.</li>
<li><strong>유연성:</strong> 다양한 텐서 연산을 조합하여 새로운 연산을 쉽게 정의할 수 있습니다.</li>
<li><strong>최적화:</strong> PyTorch는 <code>einsum</code> 연산을 자동으로 최적화하여 효율적으로 계산합니다. (경우에 따라) 수동으로 구현한 연산보다 빠를 수 있습니다. BLAS, cuBLAS와 같은 라이브러리의 최적화된 루틴을 활용하거나, 연산 순서를 최적화합니다.</li>
<li><strong>자동 미분 지원</strong>: <code>einsum</code>으로 정의된 연산은 PyTorch의 자동 미분 시스템과 완벽하게 호환됩니다.</li>
</ul>
<p><strong>torch.einsum 예:</strong></p>
<div class="sourceCode" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a><span class="co"># 행렬 곱셈</span></span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> torch.randn(<span class="dv">3</span>, <span class="dv">4</span>)</span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>B <span class="op">=</span> torch.randn(<span class="dv">4</span>, <span class="dv">5</span>)</span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a>C <span class="op">=</span> torch.einsum(<span class="st">'ij,jk-&gt;ik'</span>, A, B)  <span class="co"># C = A @ B</span></span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a><span class="co"># 전치</span></span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> torch.randn(<span class="dv">3</span>, <span class="dv">4</span>)</span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a>B <span class="op">=</span> torch.einsum(<span class="st">'ij-&gt;ji'</span>, A)  <span class="co"># B = A.T</span></span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a><span class="co"># 대각합</span></span>
<span id="cb25-13"><a href="#cb25-13" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> torch.randn(<span class="dv">3</span>, <span class="dv">3</span>)</span>
<span id="cb25-14"><a href="#cb25-14" aria-hidden="true" tabindex="-1"></a>trace <span class="op">=</span> torch.einsum(<span class="st">'ii-&gt;'</span>, A)  <span class="co"># trace = torch.trace(A)</span></span>
<span id="cb25-15"><a href="#cb25-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-16"><a href="#cb25-16" aria-hidden="true" tabindex="-1"></a><span class="co"># 배치 행렬 곱셈</span></span>
<span id="cb25-17"><a href="#cb25-17" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> torch.randn(<span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>)</span>
<span id="cb25-18"><a href="#cb25-18" aria-hidden="true" tabindex="-1"></a>B <span class="op">=</span> torch.randn(<span class="dv">2</span>, <span class="dv">4</span>, <span class="dv">5</span>)</span>
<span id="cb25-19"><a href="#cb25-19" aria-hidden="true" tabindex="-1"></a>C <span class="op">=</span> torch.einsum(<span class="st">'bij,bjk-&gt;bik'</span>, A, B) <span class="co"># C = torch.bmm(A, B)</span></span>
<span id="cb25-20"><a href="#cb25-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-21"><a href="#cb25-21" aria-hidden="true" tabindex="-1"></a><span class="co"># 외적</span></span>
<span id="cb25-22"><a href="#cb25-22" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> torch.randn(<span class="dv">3</span>)</span>
<span id="cb25-23"><a href="#cb25-23" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> torch.randn(<span class="dv">4</span>)</span>
<span id="cb25-24"><a href="#cb25-24" aria-hidden="true" tabindex="-1"></a>C <span class="op">=</span> torch.einsum(<span class="st">'i,j-&gt;ij'</span>, a, b) <span class="co"># C = torch.outer(a, b)</span></span>
<span id="cb25-25"><a href="#cb25-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-26"><a href="#cb25-26" aria-hidden="true" tabindex="-1"></a><span class="co"># 원소별 곱</span></span>
<span id="cb25-27"><a href="#cb25-27" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> torch.randn(<span class="dv">2</span>,<span class="dv">3</span>)</span>
<span id="cb25-28"><a href="#cb25-28" aria-hidden="true" tabindex="-1"></a>B <span class="op">=</span> torch.randn(<span class="dv">2</span>,<span class="dv">3</span>)</span>
<span id="cb25-29"><a href="#cb25-29" aria-hidden="true" tabindex="-1"></a>C <span class="op">=</span> torch.einsum(<span class="st">'ij,ij-&gt;ij'</span>, A, B) <span class="co"># C = A * B</span></span>
<span id="cb25-30"><a href="#cb25-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-31"><a href="#cb25-31" aria-hidden="true" tabindex="-1"></a><span class="co"># 양선형 변환</span></span>
<span id="cb25-32"><a href="#cb25-32" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.randn(<span class="dv">3</span>)</span>
<span id="cb25-33"><a href="#cb25-33" aria-hidden="true" tabindex="-1"></a>W <span class="op">=</span> torch.randn(<span class="dv">5</span>, <span class="dv">3</span>, <span class="dv">4</span>)</span>
<span id="cb25-34"><a href="#cb25-34" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> torch.randn(<span class="dv">4</span>)</span>
<span id="cb25-35"><a href="#cb25-35" aria-hidden="true" tabindex="-1"></a>z <span class="op">=</span> torch.einsum(<span class="st">'i,ijk,j-&gt;k'</span>, x, W, y) <span class="co"># z_k = sum_i sum_j x_i * W_{ijk} * y_j</span></span>
<span id="cb25-36"><a href="#cb25-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-37"><a href="#cb25-37" aria-hidden="true" tabindex="-1"></a><span class="co"># 다차원 텐서 축소</span></span>
<span id="cb25-38"><a href="#cb25-38" aria-hidden="true" tabindex="-1"></a>tensor <span class="op">=</span> torch.randn(<span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>, <span class="dv">6</span>)</span>
<span id="cb25-39"><a href="#cb25-39" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> torch.einsum(<span class="st">'...ij-&gt;...i'</span>, tensor)  <span class="co"># 마지막 두 차원에 대해 합산</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong><code>torch.einsum</code> vs.&nbsp;다른 연산:</strong></p>
<table class="caption-top table">
<colgroup>
<col style="width: 25%">
<col style="width: 26%">
<col style="width: 47%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">연산</th>
<th style="text-align: left;"><code>torch.einsum</code></th>
<th style="text-align: left;">다른 방법</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">행렬 곱셈</td>
<td style="text-align: left;"><code>'ij,jk-&gt;ik'</code></td>
<td style="text-align: left;"><code>torch.matmul(A, B)</code> 또는 <code>A @ B</code></td>
</tr>
<tr class="even">
<td style="text-align: left;">전치</td>
<td style="text-align: left;"><code>'ij-&gt;ji'</code></td>
<td style="text-align: left;"><code>torch.transpose(A, 0, 1)</code> 또는 <code>A.T</code></td>
</tr>
<tr class="odd">
<td style="text-align: left;">대각합</td>
<td style="text-align: left;"><code>'ii-&gt;'</code></td>
<td style="text-align: left;"><code>torch.trace(A)</code></td>
</tr>
<tr class="even">
<td style="text-align: left;">배치 행렬 곱셈</td>
<td style="text-align: left;"><code>'bij,bjk-&gt;bik'</code></td>
<td style="text-align: left;"><code>torch.bmm(A, B)</code></td>
</tr>
<tr class="odd">
<td style="text-align: left;">내적</td>
<td style="text-align: left;"><code>'i,i-&gt;'</code></td>
<td style="text-align: left;"><code>torch.dot(a, b)</code></td>
</tr>
<tr class="even">
<td style="text-align: left;">외적</td>
<td style="text-align: left;"><code>'i,j-&gt;ij'</code></td>
<td style="text-align: left;"><code>torch.outer(a, b)</code></td>
</tr>
<tr class="odd">
<td style="text-align: left;">원소별 곱</td>
<td style="text-align: left;"><code>'ij,ij-&gt;ij'</code></td>
<td style="text-align: left;"><code>A * B</code></td>
</tr>
<tr class="even">
<td style="text-align: left;">텐서 축소 (sum, mean 등)</td>
<td style="text-align: left;"><code>'ijk-&gt;i'</code> (예시)</td>
<td style="text-align: left;"><code>torch.sum(A, dim=(1, 2))</code></td>
</tr>
</tbody>
</table>
<p><strong><code>torch.einsum</code>의 한계</strong></p>
<ul>
<li><strong>초기 학습 곡선:</strong> 아인슈타인 표기법에 익숙하지 않은 사용자에게는 처음에는 다소 어려울 수 있습니다.</li>
<li><strong>복잡한 연산의 가독성:</strong> 매우 복잡한 연산의 경우, <code>einsum</code> 문자열이 길어져 오히려 가독성이 떨어질 수 있습니다. 이 경우, 연산을 여러 단계로 나누거나, 주석을 활용하는 것이 좋습니다.</li>
<li><strong>모든 연산 표현 불가:</strong> <code>einsum</code>은 선형 대수 연산을 기반으로 하기 때문에, 비선형 연산(예: <code>max</code>, <code>min</code>, <code>sort</code>)이나 조건부 연산은 직접 표현할 수 없습니다. 이 경우, 다른 PyTorch 함수와 함께 사용해야 합니다.</li>
</ul>
<p><strong><code>einsum</code> 최적화 (<code>torch.compile</code>)</strong></p>
<p><code>torch.compile</code> (PyTorch 2.0 이상)은 <code>einsum</code> 연산을 더욱 최적화할 수 있습니다. <code>compile</code>은 JIT(Just-In-Time) 컴파일을 통해 코드를 분석하고, 텐서 연산을 융합하거나, 메모리 접근 패턴을 최적화하는 등 다양한 최적화를 수행합니다.</p>
<div class="sourceCode" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a><span class="co"># PyTorch 2.0 이상에서 사용 가능</span></span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a><span class="at">@torch.compile</span></span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> my_einsum_function(a, b):</span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> torch.einsum(<span class="st">'ij,jk-&gt;ik'</span>, a, b)</span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a><span class="co"># 처음 호출 시 컴파일, 이후 호출 시 최적화된 코드 실행</span></span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> my_einsum_function(torch.randn(<span class="dv">10</span>, <span class="dv">20</span>), torch.randn(<span class="dv">20</span>, <span class="dv">30</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>결론:</strong></p>
<p>아인슈타인 표기법과 <code>torch.einsum</code>은 딥러닝에서 복잡한 텐서 연산을 간결하고 효율적으로 표현하고 계산하는 강력한 도구입니다. 처음에는 다소 낯설 수 있지만, 익숙해지면 코드의 가독성과 효율성을 크게 향상시킬 수 있습니다. 특히, 트랜스포머 모델과 같이 복잡한 텐서 연산이 많은 딥러닝 모델을 다룰 때 그 진가를 발휘합니다. <code>torch.compile</code>과 함께 사용하면 성능을 더욱 향상시킬 수 있습니다.</p>
<p><strong>레퍼런스:</strong></p>
<ol type="1">
<li><strong>Einstein Notation:</strong> <a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://en.wikipedia.org/wiki/Einstein_notation">https://en.wikipedia.org/wiki/Einstein_notation</a></li>
<li><strong><code>torch.einsum</code> documentation:</strong> <a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://pytorch.org/docs/stable/generated/torch.einsum.html">https://pytorch.org/docs/stable/generated/torch.einsum.html</a></li>
<li><strong>A basic introduction to NumPy’s einsum:</strong> <a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://ajcr.net/Basic-guide-to-einsum/">https://ajcr.net/Basic-guide-to-einsum/</a></li>
<li><strong>Einsum is All You Need - Einstein Summation in Deep Learning:</strong> <a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://rockt.github.io/2018/04/30/einsum">https://rockt.github.io/2018/04/30/einsum</a></li>
</ol>
</section>
</section>
</div>
</div>
</section>
<section id="그래디언트-연산을-위한-계산-그래프" class="level3">
<h3 class="anchored" data-anchor-id="그래디언트-연산을-위한-계산-그래프">3.1.3 그래디언트 연산을 위한 계산 그래프</h3>
<p>자동 미분(Automatic Differentiation)은 1970년대부터 연구되었지만, 2015년 이후 딥러닝의 발전과 함께 크게 주목받게 되었습니다. PyTorch는 동적 계산 그래프(dynamic computation graph)를 통해 자동 미분을 구현하며, 이는 2장에서 살펴본 연쇄 법칙(chain rule)의 실제 구현체입니다.</p>
<p>PyTorch의 자동 미분은 각 연산 단계마다 그래디언트를 추적하고 저장할 수 있습니다. 이를 위해서는 텐서에 그래디언트 추적을 명시적으로 선언해야 합니다.</p>
<div id="cell-27" class="cell" data-execution_count="23">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> torch.randn((<span class="dv">2</span>,))</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"a.requires_grad (default): </span><span class="sc">{</span>a<span class="sc">.</span>requires_grad<span class="sc">}</span><span class="ss">"</span>)  <span class="co"># False (default)</span></span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>a.requires_grad_(<span class="va">True</span>)  <span class="co"># In-place modification</span></span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"a.requires_grad (after setting to True): </span><span class="sc">{</span>a<span class="sc">.</span>requires_grad<span class="sc">}</span><span class="ss">"</span>)  <span class="co"># True</span></span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Declare during creation</span></span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.arange(<span class="dv">2</span>, dtype<span class="op">=</span>torch.float32, requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"x.requires_grad (declared at creation): </span><span class="sc">{</span>x<span class="sc">.</span>requires_grad<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>a.requires_grad (default): False
a.requires_grad (after setting to True): True
x.requires_grad (declared at creation): True</code></pre>
</div>
</div>
<p>예를 들어, 다음과 같은 간단한 손실 함수를 생각해 봅시다. (그림 3-1, 이전 버전 참조)</p>
<p><span class="math display">\[y = \frac {1}{N}\displaystyle\sum_{i}^{N} \{(x_i - 1)^2 + 4) \}\]</span></p>
<p><span class="math inline">\(x_i\)</span>에 대한 연산은 순차적으로 <span class="math inline">\(a_i = x_i - 1\)</span>, <span class="math inline">\(b_i = a_i^2\)</span>, <span class="math inline">\(c_i = b_i + 4\)</span>, <span class="math inline">\(y = \frac{1}{N}\sum_{i=1}^{N} c_i\)</span> 와 같이 표현할 수 있습니다.</p>
<p>이 식에 대해 순방향(forward), 역방향(backward) 연산을 수행해 보겠습니다.</p>
<div id="cell-29" class="cell" data-execution_count="24">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> x <span class="op">-</span> <span class="dv">1</span></span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> a<span class="op">**</span><span class="dv">2</span></span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>c <span class="op">=</span> b <span class="op">+</span> <span class="dv">4</span></span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> c.mean()</span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"y = </span><span class="sc">{</span>y<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform backward operation</span></span>
<span id="cb29-9"><a href="#cb29-9" aria-hidden="true" tabindex="-1"></a>y.backward()</span>
<span id="cb29-10"><a href="#cb29-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-11"><a href="#cb29-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the gradient of x (x.grad)</span></span>
<span id="cb29-12"><a href="#cb29-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"x.grad = </span><span class="sc">{</span>x<span class="sc">.</span>grad<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>y = 4.5
x.grad = tensor([-1.,  0.])</code></pre>
</div>
</div>
<p>각 단계의 그래디언트를 수식으로 계산하면 다음과 같습니다.</p>
<p><span class="math inline">\(\frac{\partial a_i}{\partial x_i} = 1, \frac{\partial b_i}{\partial a_i} = 2 \cdot a_i, \frac{\partial c_i}{\partial b_i} = 1,  \frac{\partial y}{\partial c_i} = \frac{1}{N}\)</span></p>
<p>따라서 연쇄 법칙에 의해</p>
<p><span class="math inline">\(\frac{\partial y}{\partial x_i} = \frac{\partial y}{\partial  c_i}\frac{\partial c_i}{\partial b_i}\frac{\partial b_i}{\partial a_i}\frac{\partial a_i}{\partial x_i} =  \frac{1}{N} \cdot 1 \cdot 2 \cdot a_i \cdot 1 = \frac{2}{N}a_i = \frac{2}{N}(x_i - 1)\)</span></p>
<p><span class="math inline">\(x_i\)</span>가 [0, 1]이고, N=2 (x의 원소 개수) 이므로, <span class="math inline">\(\frac{\partial y}{\partial x_i}  = [-0.5, 0.5]\)</span> 가 됩니다. 이는 PyTorch의 자동 미분 결과와 일치합니다.</p>
<p>PyTorch는 1970년대부터 연구된 자동 미분의 개념을 현대적으로 구현했습니다. 특히 계산 그래프의 동적 생성과 그래디언트 추적 기능은 매우 유용합니다. 그러나 때로는 이러한 자동 미분 기능을 비활성화해야 할 필요가 있습니다.</p>
<div id="cell-31" class="cell" data-execution_count="8">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.randn(<span class="dv">3</span>, <span class="dv">4</span>)</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>w <span class="op">=</span> torch.randn(<span class="dv">4</span>, <span class="dv">2</span>)</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> torch.randn(<span class="dv">2</span>)</span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a><span class="co"># If gradient tracking is needed</span></span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a>z <span class="op">=</span> torch.matmul(x, w) <span class="op">+</span> b</span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a>z.requires_grad_(<span class="va">True</span>)  <span class="co"># Can also be set using requires_grad_()</span></span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"z.requires_grad: </span><span class="sc">{</span>z<span class="sc">.</span>requires_grad<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-10"><a href="#cb31-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Disable gradient tracking method 1: Using 'with' statement</span></span>
<span id="cb31-11"><a href="#cb31-11" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb31-12"><a href="#cb31-12" aria-hidden="true" tabindex="-1"></a>    z <span class="op">=</span> torch.matmul(x, w) <span class="op">+</span> b</span>
<span id="cb31-13"><a href="#cb31-13" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"z.requires_grad (inside no_grad): </span><span class="sc">{</span>z<span class="sc">.</span>requires_grad<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb31-14"><a href="#cb31-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-15"><a href="#cb31-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Disable gradient tracking method 2: Using detach()</span></span>
<span id="cb31-16"><a href="#cb31-16" aria-hidden="true" tabindex="-1"></a>z_det <span class="op">=</span> z.detach()</span>
<span id="cb31-17"><a href="#cb31-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"z_det.requires_grad: </span><span class="sc">{</span>z_det<span class="sc">.</span>requires_grad<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>z.requires_grad: True
z.requires_grad (inside no_grad): False
z_det.requires_grad: False</code></pre>
</div>
</div>
<p>그래디언트 추적 비활성화는 다음과 같은 경우에 특히 유용합니다.</p>
<ol type="1">
<li><strong>추론(Inference) 시</strong>: 순전파만 필요한 경우 메모리와 계산 비용을 절감합니다.</li>
<li><strong>파인튜닝(Fine-tuning)</strong>: 특정 파라미터만 업데이트하고 나머지는 고정할 때 사용합니다.</li>
<li><strong>성능 최적화</strong>: 역전파는 추가 메모리와 계산 비용이 발생하므로, 필요 없는 경우 비활성화합니다.</li>
</ol>
<p>특히 대규모 언어 모델의 파인튜닝에서는 대부분의 파라미터를 고정하고 일부만 업데이트하는 것이 일반적이므로, 그래디언트 추적의 선택적 활성화는 매우 중요한 기능입니다.</p>
</section>
<section id="데이터-로딩" class="level3">
<h3 class="anchored" data-anchor-id="데이터-로딩">3.1.4 데이터 로딩</h3>
<p>데이터 로딩은 딥러닝의 핵심 요소입니다. 2000년대 초반까지는 각 연구팀이 독자적인 데이터 처리 방식을 사용했으나, 2009년 ImageNet과 같은 대규모 데이터셋의 등장으로 표준화된 데이터 로딩 시스템의 필요성이 대두되었습니다.</p>
<p>PyTorch는 데이터 처리와 학습 로직을 분리하기 위해 두 가지 핵심 클래스를 제공합니다.</p>
<ol type="1">
<li><code>torch.utils.data.Dataset</code>: 데이터와 레이블에 대한 일관된 접근 인터페이스를 제공합니다. <code>__len__</code> 과 <code>__getitem__</code> 메서드를 구현해야 합니다.</li>
<li><code>torch.utils.data.DataLoader</code>: 배치(batch) 단위의 효율적인 데이터 로딩 메커니즘을 제공합니다. <code>Dataset</code>을 감싸서 미니배치 생성, 셔플링, 병렬 데이터 로딩 등을 자동화합니다.</li>
</ol>
<p>다음은 디리클레(Dirichlet) 분포를 사용한 랜덤 데이터 생성 예제입니다.</p>
<div id="cell-34" class="cell" data-execution_count="9">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.utils.data <span class="im">as</span> data</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize with Dirichlet distribution</span></span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> np.random.dirichlet(np.ones(<span class="dv">5</span>), size<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> np.zeros_like(a)</span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate label values</span></span>
<span id="cb33-8"><a href="#cb33-8" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> (a <span class="op">==</span> a.<span class="bu">max</span>(axis<span class="op">=</span><span class="dv">1</span>)[:, <span class="va">None</span>]).astype(<span class="bu">int</span>)</span>
<span id="cb33-9"><a href="#cb33-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-10"><a href="#cb33-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Data (a):</span><span class="ch">\n</span><span class="sc">{</span>a<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb33-11"><a href="#cb33-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Labels (b):</span><span class="ch">\n</span><span class="sc">{</span>b<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb33-12"><a href="#cb33-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-13"><a href="#cb33-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-14"><a href="#cb33-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a custom Dataset class by inheriting from PyTorch's Dataset.</span></span>
<span id="cb33-15"><a href="#cb33-15" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> RandomData(data.Dataset):</span>
<span id="cb33-16"><a href="#cb33-16" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, feature, length):</span>
<span id="cb33-17"><a href="#cb33-17" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb33-18"><a href="#cb33-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.feature <span class="op">=</span> feature</span>
<span id="cb33-19"><a href="#cb33-19" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.length <span class="op">=</span> length</span>
<span id="cb33-20"><a href="#cb33-20" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.generate_data()</span>
<span id="cb33-21"><a href="#cb33-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-22"><a href="#cb33-22" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> generate_data(<span class="va">self</span>):</span>
<span id="cb33-23"><a href="#cb33-23" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> np.random.dirichlet(np.ones(<span class="va">self</span>.feature), size<span class="op">=</span><span class="va">self</span>.length)</span>
<span id="cb33-24"><a href="#cb33-24" aria-hidden="true" tabindex="-1"></a>        y <span class="op">=</span> (x <span class="op">==</span> x.<span class="bu">max</span>(axis<span class="op">=</span><span class="dv">1</span>)[:, <span class="va">None</span>]).astype(<span class="bu">int</span>)  <span class="co"># One-hot encoding</span></span>
<span id="cb33-25"><a href="#cb33-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-26"><a href="#cb33-26" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.data <span class="op">=</span> x  <span class="co"># numpy object</span></span>
<span id="cb33-27"><a href="#cb33-27" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.label <span class="op">=</span> y</span>
<span id="cb33-28"><a href="#cb33-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-29"><a href="#cb33-29" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__len__</span>(<span class="va">self</span>):</span>
<span id="cb33-30"><a href="#cb33-30" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.length</span>
<span id="cb33-31"><a href="#cb33-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-32"><a href="#cb33-32" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__getitem__</span>(<span class="va">self</span>, index):</span>
<span id="cb33-33"><a href="#cb33-33" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Return data and label as torch tensors</span></span>
<span id="cb33-34"><a href="#cb33-34" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.tensor(<span class="va">self</span>.data[index], dtype<span class="op">=</span>torch.float32), torch.tensor(<span class="va">self</span>.label[index], dtype<span class="op">=</span>torch.int64)</span>
<span id="cb33-35"><a href="#cb33-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-36"><a href="#cb33-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-37"><a href="#cb33-37" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> RandomData(feature<span class="op">=</span><span class="dv">10</span>, length<span class="op">=</span><span class="dv">100</span>)</span>
<span id="cb33-38"><a href="#cb33-38" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Number of data samples = </span><span class="sc">{</span><span class="bu">len</span>(dataset)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb33-39"><a href="#cb33-39" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Data at index 0 = </span><span class="sc">{</span>dataset[<span class="dv">0</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb33-40"><a href="#cb33-40" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Data type = </span><span class="sc">{</span><span class="bu">type</span>(dataset[<span class="dv">0</span>][<span class="dv">0</span>])<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Data (a):
[[0.46073711 0.01119455 0.28991657 0.11259078 0.12556099]
 [0.07331166 0.43554042 0.1243009  0.13339224 0.23345478]]
Labels (b):
[[1 0 0 0 0]
 [0 1 0 0 0]]
Number of data samples = 100
Data at index 0 = (tensor([1.4867e-01, 1.6088e-01, 1.2207e-02, 3.6049e-02, 1.1054e-04, 8.1160e-02,
        2.9811e-02, 1.9398e-01, 4.9448e-02, 2.8769e-01]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1]))
Data type = &lt;class 'torch.Tensor'&gt;</code></pre>
</div>
</div>
<p><code>DataLoader</code>는 배치 처리를 위한 다양한 기능을 제공합니다. 주요 매개변수는 다음과 같습니다.</p>
<ul>
<li><code>batch_size</code>: 배치당 샘플 수</li>
<li><code>shuffle</code>: 데이터 순서 무작위화 (훈련 시 일반적으로 <code>True</code>로 설정)</li>
<li><code>num_workers</code>: 데이터 로딩 병렬화를 위한 프로세스 수</li>
<li><code>drop_last</code>: 마지막 불완전 배치(incomplete batch) 처리 여부 (True면 버림)</li>
</ul>
<p><code>Dataset</code>으로부터 <code>__getitem__</code>을 사용하여 데이터를 읽어들이면서 결과를 텐서 객체로 변환합니다. 특히 <code>num_workers</code> 설정은 대규모 이미지나 비디오 데이터셋 처리 시 중요합니다. 그러나 작은 데이터셋의 경우 단일 프로세스가 더 효율적일 수 있습니다. <code>num_workers</code> 값을 너무 크게 설정하면 오히려 오버헤드가 발생할 수 있으므로, 적절한 값을 찾는 것이 중요합니다. (일반적으로 코어 수 또는 코어 수 * 2 정도를 시도해 봅니다.)</p>
<div id="cell-36" class="cell" data-execution_count="10">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>data_loader <span class="op">=</span> data.DataLoader(dataset, batch_size<span class="op">=</span><span class="dv">4</span>, shuffle<span class="op">=</span><span class="va">True</span>, num_workers<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Read one batch.</span></span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a>train_x, train_y <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(data_loader))</span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"1st batch training data = </span><span class="sc">{</span>train_x<span class="sc">}</span><span class="ss">, </span><span class="ch">\n</span><span class="ss"> Data shape = </span><span class="sc">{</span>train_x<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"1st batch label data = </span><span class="sc">{</span>train_y<span class="sc">}</span><span class="ss">, </span><span class="ch">\n</span><span class="ss"> Data shape = </span><span class="sc">{</span>train_y<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"1st batch label data type = </span><span class="sc">{</span><span class="bu">type</span>(train_y)<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>1st batch training data = tensor([[3.3120e-02, 1.4274e-01, 9.7984e-02, 1.9628e-03, 6.8926e-02, 3.4525e-01,
         4.6966e-02, 6.0947e-02, 4.2738e-02, 1.5937e-01],
        [8.0707e-02, 4.9181e-02, 3.1863e-02, 1.4238e-02, 1.6089e-02, 1.7980e-01,
         1.7544e-01, 1.3465e-01, 1.6361e-01, 1.5442e-01],
        [4.2364e-02, 3.3635e-02, 2.0840e-01, 1.6919e-02, 4.5977e-02, 6.5791e-02,
         1.8726e-01, 1.0325e-01, 2.2029e-01, 7.6117e-02],
        [1.4867e-01, 1.6088e-01, 1.2207e-02, 3.6049e-02, 1.1054e-04, 8.1160e-02,
         2.9811e-02, 1.9398e-01, 4.9448e-02, 2.8769e-01]]), 
 Data shape = torch.Size([4, 10])
1st batch label data = tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]]), 
 Data shape = torch.Size([4, 10])
1st batch label data type = &lt;class 'torch.Tensor'&gt;</code></pre>
</div>
</div>
<p>PyTorch는 도메인별 데이터 처리를 위한 특화된 패키지들을 제공합니다. 2016년 이후 딥러닝이 다양한 분야로 확장되면서, 각 도메인에 특화된 데이터 처리의 필요성이 대두되었습니다.</p>
<ul>
<li><code>torchvision</code>: 컴퓨터 비전</li>
<li><code>torchaudio</code>: 오디오 처리</li>
<li><code>torchtext</code>: 자연어 처리</li>
</ul>
<p>Fashion-MNIST는 2017년 Zalando Research에서 공개한 데이터셋으로, MNIST를 대체하기 위해 설계되었습니다. 데이터셋의 구성은 다음과 같습니다.</p>
<ul>
<li>훈련 데이터: 60,000개</li>
<li>테스트 데이터: 10,000개</li>
<li>이미지 크기: 28x28 그레이스케일</li>
</ul>
<div id="cell-38" class="cell" data-execution_count="11">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> Dataset, DataLoader</span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision <span class="im">import</span> datasets</span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision.transforms <span class="im">import</span> ToTensor, Normalize, Compose</span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn_image <span class="im">as</span> isns</span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt <span class="co"># Added for visualization</span></span>
<span id="cb37-7"><a href="#cb37-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-8"><a href="#cb37-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-9"><a href="#cb37-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Function to calculate mean and std of the dataset</span></span>
<span id="cb37-10"><a href="#cb37-10" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> calculate_mean_std(dataset):</span>
<span id="cb37-11"><a href="#cb37-11" aria-hidden="true" tabindex="-1"></a>    dataloader <span class="op">=</span> DataLoader(dataset, batch_size<span class="op">=</span><span class="bu">len</span>(dataset), shuffle<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb37-12"><a href="#cb37-12" aria-hidden="true" tabindex="-1"></a>    data, _ <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(dataloader))</span>
<span id="cb37-13"><a href="#cb37-13" aria-hidden="true" tabindex="-1"></a>    mean <span class="op">=</span> data.mean(axis<span class="op">=</span>(<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">3</span>))  <span class="co"># Calculate mean across channel dimension</span></span>
<span id="cb37-14"><a href="#cb37-14" aria-hidden="true" tabindex="-1"></a>    std <span class="op">=</span> data.std(axis<span class="op">=</span>(<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">3</span>))    <span class="co"># Calculate std across channel dimension</span></span>
<span id="cb37-15"><a href="#cb37-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> mean, std</span>
<span id="cb37-16"><a href="#cb37-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-17"><a href="#cb37-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-18"><a href="#cb37-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Datasets.  Note:  We *don't* apply Normalize here yet.</span></span>
<span id="cb37-19"><a href="#cb37-19" aria-hidden="true" tabindex="-1"></a>train_dataset <span class="op">=</span> datasets.FashionMNIST(</span>
<span id="cb37-20"><a href="#cb37-20" aria-hidden="true" tabindex="-1"></a>    root<span class="op">=</span><span class="st">"data"</span>, train<span class="op">=</span><span class="va">True</span>, download<span class="op">=</span><span class="va">True</span>, transform<span class="op">=</span>ToTensor()</span>
<span id="cb37-21"><a href="#cb37-21" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb37-22"><a href="#cb37-22" aria-hidden="true" tabindex="-1"></a>test_dataset <span class="op">=</span> datasets.FashionMNIST(</span>
<span id="cb37-23"><a href="#cb37-23" aria-hidden="true" tabindex="-1"></a>    root<span class="op">=</span><span class="st">"data"</span>, train<span class="op">=</span><span class="va">False</span>, download<span class="op">=</span><span class="va">True</span>, transform<span class="op">=</span>ToTensor()</span>
<span id="cb37-24"><a href="#cb37-24" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb37-25"><a href="#cb37-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-26"><a href="#cb37-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate mean and std for normalization</span></span>
<span id="cb37-27"><a href="#cb37-27" aria-hidden="true" tabindex="-1"></a>train_mean, train_std <span class="op">=</span> calculate_mean_std(train_dataset)</span>
<span id="cb37-28"><a href="#cb37-28" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Train data mean: </span><span class="sc">{</span>train_mean<span class="sc">}</span><span class="ss">, std: </span><span class="sc">{</span>train_std<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb37-29"><a href="#cb37-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-30"><a href="#cb37-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Now define transforms *with* normalization</span></span>
<span id="cb37-31"><a href="#cb37-31" aria-hidden="true" tabindex="-1"></a>transform <span class="op">=</span> Compose([</span>
<span id="cb37-32"><a href="#cb37-32" aria-hidden="true" tabindex="-1"></a>    ToTensor(),</span>
<span id="cb37-33"><a href="#cb37-33" aria-hidden="true" tabindex="-1"></a>    Normalize(train_mean, train_std)  <span class="co"># Use calculated mean and std</span></span>
<span id="cb37-34"><a href="#cb37-34" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb37-35"><a href="#cb37-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-36"><a href="#cb37-36" aria-hidden="true" tabindex="-1"></a><span class="co"># Re-create datasets with the normalization transform</span></span>
<span id="cb37-37"><a href="#cb37-37" aria-hidden="true" tabindex="-1"></a>train_dataset <span class="op">=</span> datasets.FashionMNIST(</span>
<span id="cb37-38"><a href="#cb37-38" aria-hidden="true" tabindex="-1"></a>    root<span class="op">=</span><span class="st">"data"</span>, train<span class="op">=</span><span class="va">True</span>, download<span class="op">=</span><span class="va">True</span>, transform<span class="op">=</span>transform</span>
<span id="cb37-39"><a href="#cb37-39" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb37-40"><a href="#cb37-40" aria-hidden="true" tabindex="-1"></a>test_dataset <span class="op">=</span> datasets.FashionMNIST(</span>
<span id="cb37-41"><a href="#cb37-41" aria-hidden="true" tabindex="-1"></a>    root<span class="op">=</span><span class="st">"data"</span>, train<span class="op">=</span><span class="va">False</span>, download<span class="op">=</span><span class="va">True</span>, transform<span class="op">=</span>transform</span>
<span id="cb37-42"><a href="#cb37-42" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb37-43"><a href="#cb37-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-44"><a href="#cb37-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-45"><a href="#cb37-45" aria-hidden="true" tabindex="-1"></a><span class="co"># Check one training data sample.</span></span>
<span id="cb37-46"><a href="#cb37-46" aria-hidden="true" tabindex="-1"></a>sample_idx <span class="op">=</span> torch.randint(<span class="bu">len</span>(train_dataset), size<span class="op">=</span>(<span class="dv">1</span>,)).item()</span>
<span id="cb37-47"><a href="#cb37-47" aria-hidden="true" tabindex="-1"></a>img, label <span class="op">=</span> train_dataset[sample_idx]  <span class="co"># Use a random index</span></span>
<span id="cb37-48"><a href="#cb37-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-49"><a href="#cb37-49" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Label: </span><span class="sc">{</span>label<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb37-50"><a href="#cb37-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-51"><a href="#cb37-51" aria-hidden="true" tabindex="-1"></a><span class="co"># Manually create a label map</span></span>
<span id="cb37-52"><a href="#cb37-52" aria-hidden="true" tabindex="-1"></a>labels_map <span class="op">=</span> {</span>
<span id="cb37-53"><a href="#cb37-53" aria-hidden="true" tabindex="-1"></a>    <span class="dv">0</span>: <span class="st">"T-shirt"</span>,</span>
<span id="cb37-54"><a href="#cb37-54" aria-hidden="true" tabindex="-1"></a>    <span class="dv">1</span>: <span class="st">"Trouser"</span>,</span>
<span id="cb37-55"><a href="#cb37-55" aria-hidden="true" tabindex="-1"></a>    <span class="dv">2</span>: <span class="st">"Pullover"</span>,</span>
<span id="cb37-56"><a href="#cb37-56" aria-hidden="true" tabindex="-1"></a>    <span class="dv">3</span>: <span class="st">"Dress"</span>,</span>
<span id="cb37-57"><a href="#cb37-57" aria-hidden="true" tabindex="-1"></a>    <span class="dv">4</span>: <span class="st">"Coat"</span>,</span>
<span id="cb37-58"><a href="#cb37-58" aria-hidden="true" tabindex="-1"></a>    <span class="dv">5</span>: <span class="st">"Sandal"</span>,</span>
<span id="cb37-59"><a href="#cb37-59" aria-hidden="true" tabindex="-1"></a>    <span class="dv">6</span>: <span class="st">"Shirt"</span>,</span>
<span id="cb37-60"><a href="#cb37-60" aria-hidden="true" tabindex="-1"></a>    <span class="dv">7</span>: <span class="st">"Sneaker"</span>,</span>
<span id="cb37-61"><a href="#cb37-61" aria-hidden="true" tabindex="-1"></a>    <span class="dv">8</span>: <span class="st">"Bag"</span>,</span>
<span id="cb37-62"><a href="#cb37-62" aria-hidden="true" tabindex="-1"></a>    <span class="dv">9</span>: <span class="st">"Ankle Boot"</span>,</span>
<span id="cb37-63"><a href="#cb37-63" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb37-64"><a href="#cb37-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-65"><a href="#cb37-65" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Label map: </span><span class="sc">{</span>labels_map[label]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb37-66"><a href="#cb37-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-67"><a href="#cb37-67" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot using seaborn-image.</span></span>
<span id="cb37-68"><a href="#cb37-68" aria-hidden="true" tabindex="-1"></a>isns.imgplot(img.squeeze())  <span class="co"># Squeeze to remove channel dimension for grayscale</span></span>
<span id="cb37-69"><a href="#cb37-69" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="ss">f"Label: </span><span class="sc">{</span>labels_map[label]<span class="sc">}</span><span class="ss">"</span>) <span class="co"># Add title to plot</span></span>
<span id="cb37-70"><a href="#cb37-70" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb37-71"><a href="#cb37-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-72"><a href="#cb37-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-73"><a href="#cb37-73" aria-hidden="true" tabindex="-1"></a><span class="co"># Define data loaders</span></span>
<span id="cb37-74"><a href="#cb37-74" aria-hidden="true" tabindex="-1"></a>train_dataloader <span class="op">=</span> DataLoader(train_dataset, batch_size<span class="op">=</span><span class="dv">32</span>, shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb37-75"><a href="#cb37-75" aria-hidden="true" tabindex="-1"></a>test_dataloader <span class="op">=</span> DataLoader(test_dataset, batch_size<span class="op">=</span><span class="dv">32</span>, shuffle<span class="op">=</span><span class="va">False</span>) <span class="co"># No need to shuffle test data</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Train data mean: tensor([0.2860]), std: tensor([0.3530])
Label: 5
Label map: Sandal</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="03_딥러닝프레임워크_files/figure-html/cell-19-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="데이터-변환transform" class="level3">
<h3 class="anchored" data-anchor-id="데이터-변환transform">3.1.5 데이터 변환(Transform)</h3>
<p>데이터 변환(Data Transform)은 딥러닝에서 매우 중요한 전처리 과정입니다. 2012년 AlexNet의 성공 이후, 데이터 증강(Data Augmentation)이 모델 성능 향상의 핵심 요소로 자리 잡았습니다. PyTorch는 이러한 변환을 위한 다양한 도구를 제공합니다. <code>transforms.Compose</code>를 사용하면 여러 변환을 순차적으로 적용할 수 있습니다. 또한 <code>Lambda</code> 함수를 통해 사용자 정의 변환도 쉽게 구현할 수 있습니다.</p>
<p>데이터 변환은 모델의 일반화(generalization) 성능을 향상시키는 데 매우 중요합니다. 특히 컴퓨터 비전 분야에서는 다양한 변환을 통한 데이터 증강이 표준 관행이 되었습니다. <code>Normalize</code> 변환의 경우, 모델 학습의 안정성을 위해 데이터를 표준화하는 필수적인 단계입니다.</p>
<p><code>Normalize</code> 변환을 적용하기 위해서는 데이터셋의 평균(mean)과 표준편차(standard deviation)를 알아야 합니다. 이를 계산하는 코드는 다음과 같습니다.</p>
<div id="cell-40" class="cell" data-execution_count="12">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision <span class="im">import</span> transforms</span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> PIL</span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> DataLoader</span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision <span class="im">import</span> datasets</span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-7"><a href="#cb39-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate mean and std of the dataset</span></span>
<span id="cb39-8"><a href="#cb39-8" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> calculate_mean_std(dataset):</span>
<span id="cb39-9"><a href="#cb39-9" aria-hidden="true" tabindex="-1"></a>    dataloader <span class="op">=</span> DataLoader(dataset, batch_size<span class="op">=</span><span class="bu">len</span>(dataset), shuffle<span class="op">=</span><span class="va">False</span>) <span class="co"># Load all data at once</span></span>
<span id="cb39-10"><a href="#cb39-10" aria-hidden="true" tabindex="-1"></a>    data, _ <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(dataloader))</span>
<span id="cb39-11"><a href="#cb39-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># For grayscale images, calculate mean and std over height, width dimensions (0, 2, 3)</span></span>
<span id="cb39-12"><a href="#cb39-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># For RGB images, the calculation would be over (0, 1, 2)</span></span>
<span id="cb39-13"><a href="#cb39-13" aria-hidden="true" tabindex="-1"></a>    mean <span class="op">=</span> data.mean(dim<span class="op">=</span>(<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">3</span>))  <span class="co"># Calculate mean across batch and spatial dimensions</span></span>
<span id="cb39-14"><a href="#cb39-14" aria-hidden="true" tabindex="-1"></a>    std <span class="op">=</span> data.std(dim<span class="op">=</span>(<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">3</span>))    <span class="co"># Calculate std across batch and spatial dimensions</span></span>
<span id="cb39-15"><a href="#cb39-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> mean, std</span>
<span id="cb39-16"><a href="#cb39-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-17"><a href="#cb39-17" aria-hidden="true" tabindex="-1"></a><span class="co"># --- Example usage with FashionMNIST ---</span></span>
<span id="cb39-18"><a href="#cb39-18" aria-hidden="true" tabindex="-1"></a><span class="co"># 1.  Create dataset *without* normalization first:</span></span>
<span id="cb39-19"><a href="#cb39-19" aria-hidden="true" tabindex="-1"></a>train_dataset_for_calc <span class="op">=</span> datasets.FashionMNIST(</span>
<span id="cb39-20"><a href="#cb39-20" aria-hidden="true" tabindex="-1"></a>    root<span class="op">=</span><span class="st">"data"</span>, train<span class="op">=</span><span class="va">True</span>, download<span class="op">=</span><span class="va">True</span>, transform<span class="op">=</span>transforms.ToTensor()  <span class="co"># Only ToTensor</span></span>
<span id="cb39-21"><a href="#cb39-21" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb39-22"><a href="#cb39-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-23"><a href="#cb39-23" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Calculate mean and std:</span></span>
<span id="cb39-24"><a href="#cb39-24" aria-hidden="true" tabindex="-1"></a>train_mean, train_std <span class="op">=</span> calculate_mean_std(train_dataset_for_calc)</span>
<span id="cb39-25"><a href="#cb39-25" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Train data mean: </span><span class="sc">{</span>train_mean<span class="sc">}</span><span class="ss">, std: </span><span class="sc">{</span>train_std<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb39-26"><a href="#cb39-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-27"><a href="#cb39-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-28"><a href="#cb39-28" aria-hidden="true" tabindex="-1"></a><span class="co"># 3.  *Now* create the dataset with normalization:</span></span>
<span id="cb39-29"><a href="#cb39-29" aria-hidden="true" tabindex="-1"></a>transform <span class="op">=</span> transforms.Compose([</span>
<span id="cb39-30"><a href="#cb39-30" aria-hidden="true" tabindex="-1"></a>    transforms.ToTensor(),</span>
<span id="cb39-31"><a href="#cb39-31" aria-hidden="true" tabindex="-1"></a>    transforms.Normalize(train_mean, train_std)  <span class="co"># Use calculated mean and std</span></span>
<span id="cb39-32"><a href="#cb39-32" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb39-33"><a href="#cb39-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-34"><a href="#cb39-34" aria-hidden="true" tabindex="-1"></a><span class="co"># Example of defining a custom transform using Lambda</span></span>
<span id="cb39-35"><a href="#cb39-35" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> crop_image(image: PIL.Image.Image) <span class="op">-&gt;</span> PIL.Image.Image:</span>
<span id="cb39-36"><a href="#cb39-36" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Original image is assumed to be 28x28.</span></span>
<span id="cb39-37"><a href="#cb39-37" aria-hidden="true" tabindex="-1"></a>    left, top, width, height <span class="op">=</span> <span class="dv">5</span>, <span class="dv">5</span>, <span class="dv">18</span>, <span class="dv">18</span> <span class="co"># Example crop parameters</span></span>
<span id="cb39-38"><a href="#cb39-38" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> transforms.functional.crop(image, top<span class="op">=</span>top, left<span class="op">=</span>left, width<span class="op">=</span>width, height<span class="op">=</span>height)</span>
<span id="cb39-39"><a href="#cb39-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-40"><a href="#cb39-40" aria-hidden="true" tabindex="-1"></a><span class="co"># Compose transforms, including the custom one and normalization.</span></span>
<span id="cb39-41"><a href="#cb39-41" aria-hidden="true" tabindex="-1"></a>transform_with_crop <span class="op">=</span> transforms.Compose([</span>
<span id="cb39-42"><a href="#cb39-42" aria-hidden="true" tabindex="-1"></a>    transforms.Lambda(crop_image), <span class="co"># Custom cropping</span></span>
<span id="cb39-43"><a href="#cb39-43" aria-hidden="true" tabindex="-1"></a>    transforms.ColorJitter(),</span>
<span id="cb39-44"><a href="#cb39-44" aria-hidden="true" tabindex="-1"></a>    transforms.RandomInvert(),</span>
<span id="cb39-45"><a href="#cb39-45" aria-hidden="true" tabindex="-1"></a>    transforms.ToTensor(), <span class="co"># Must be *before* Normalize</span></span>
<span id="cb39-46"><a href="#cb39-46" aria-hidden="true" tabindex="-1"></a>    transforms.Normalize(train_mean, train_std) <span class="co"># Use calculated mean and std</span></span>
<span id="cb39-47"><a href="#cb39-47" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb39-48"><a href="#cb39-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-49"><a href="#cb39-49" aria-hidden="true" tabindex="-1"></a>train_dataset_transformed <span class="op">=</span> datasets.FashionMNIST(root<span class="op">=</span><span class="st">"data"</span>, train<span class="op">=</span><span class="va">True</span>, download<span class="op">=</span><span class="va">True</span>, transform<span class="op">=</span>transform_with_crop)</span>
<span id="cb39-50"><a href="#cb39-50" aria-hidden="true" tabindex="-1"></a><span class="co"># Get one sample to check the transformation.</span></span>
<span id="cb39-51"><a href="#cb39-51" aria-hidden="true" tabindex="-1"></a>sample_img, sample_label <span class="op">=</span> train_dataset_transformed[<span class="dv">0</span>]</span>
<span id="cb39-52"><a href="#cb39-52" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Transformed image shape: </span><span class="sc">{</span>sample_img<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb39-53"><a href="#cb39-53" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Transformed image min/max: </span><span class="sc">{</span>sample_img<span class="sc">.</span><span class="bu">min</span>()<span class="sc">}</span><span class="ss">, </span><span class="sc">{</span>sample_img<span class="sc">.</span><span class="bu">max</span>()<span class="sc">}</span><span class="ss">"</span>) <span class="co"># Check normalization</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Train data mean: tensor([0.2860]), std: tensor([0.3530])
Transformed image shape: torch.Size([1, 18, 18])
Transformed image min/max: -0.8102576732635498, 2.022408962249756</code></pre>
</div>
</div>
<p>위 코드에서는 먼저 <code>ToTensor()</code> 변환만 적용된 데이터셋을 생성하여 평균과 표준편차를 계산합니다. 그 후, 계산된 값을 사용하여 <code>Normalize</code> 변환을 포함한 최종 변환을 정의합니다. <code>Lambda</code> 함수를 사용하여 사용자 정의 <code>crop_image</code> 함수를 변환 파이프라인에 추가하는 예제도 포함되어 있습니다. <code>ToTensor()</code>는 <code>Normalize</code> <em>전에</em> 와야 합니다. <code>ToTensor()</code>는 [0, 255] 범위의 이미지를 [0, 1] 범위의 텐서로 변환하고, <code>Normalize</code>는 이 [0, 1] 범위의 데이터를 평균 0, 표준편차 1이 되도록 정규화합니다. 데이터 증강은 훈련 데이터에만 적용하고, 검증/테스트 데이터에는 적용하지 않는 것이 일반적입니다.</p>
</section>
<section id="모델" class="level3">
<h3 class="anchored" data-anchor-id="모델">3.1.6 모델</h3>
<p>신경망 모델의 구현 방식은 1980년대부터 다양하게 발전해왔습니다. PyTorch는 2016년 출시 때부터 객체 지향적 모델 구현 방식을 채택했으며, 이는 <code>nn.Module</code>을 통해 구현됩니다. 이러한 방식은 모델의 재사용성과 확장성을 크게 향상시켰습니다.</p>
<p>모델 클래스는 <code>nn.Module</code>을 상속받아 구현하며, 일반적으로 다음과 같은 메서드들을 포함합니다.</p>
<ul>
<li><code>__init__()</code>: 신경망의 구성 요소(층, 활성화 함수 등)를 정의하고 초기화합니다.</li>
<li><code>forward()</code>: 입력 데이터를 받아 모델의 순전파 연산을 수행하고 출력(로짓 또는 예측값)을 반환합니다.</li>
<li>(선택 사항) <code>training_step()</code>, <code>validation_step()</code>, <code>test_step()</code>: PyTorch Lightning과 같은 라이브러리와 함께 사용할 때, 각 학습/검증/테스트 단계에서의 동작을 정의합니다.</li>
<li>(선택 사항) 기타 사용자 정의 메서드: 모델의 특정 기능을 수행하는 메서드들을 추가할 수 있습니다.</li>
</ul>
<div id="cell-43" class="cell" data-execution_count="13">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch <span class="im">import</span> nn</span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SimpleNetwork(nn.Module):</span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()  <span class="co"># Or super(SimpleNetwork, self).__init__()</span></span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.flatten <span class="op">=</span> nn.Flatten()</span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.network_stack <span class="op">=</span> nn.Sequential(</span>
<span id="cb41-8"><a href="#cb41-8" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">28</span> <span class="op">*</span> <span class="dv">28</span>, <span class="dv">512</span>),</span>
<span id="cb41-9"><a href="#cb41-9" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb41-10"><a href="#cb41-10" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">512</span>, <span class="dv">512</span>),</span>
<span id="cb41-11"><a href="#cb41-11" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb41-12"><a href="#cb41-12" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">512</span>, <span class="dv">10</span>),</span>
<span id="cb41-13"><a href="#cb41-13" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb41-14"><a href="#cb41-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-15"><a href="#cb41-15" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb41-16"><a href="#cb41-16" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.flatten(x)  <span class="co"># Flatten the image data into a 1D array</span></span>
<span id="cb41-17"><a href="#cb41-17" aria-hidden="true" tabindex="-1"></a>        logits <span class="op">=</span> <span class="va">self</span>.network_stack(x)</span>
<span id="cb41-18"><a href="#cb41-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> logits</span>
<span id="cb41-19"><a href="#cb41-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-20"><a href="#cb41-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Move model to the appropriate device (CPU or GPU)</span></span>
<span id="cb41-21"><a href="#cb41-21" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> SimpleNetwork().to(device)</span>
<span id="cb41-22"><a href="#cb41-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>SimpleNetwork(
  (flatten): Flatten(start_dim=1, end_dim=-1)
  (network_stack): Sequential(
    (0): Linear(in_features=784, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=512, bias=True)
    (3): ReLU()
    (4): Linear(in_features=512, out_features=10, bias=True)
  )
)</code></pre>
</div>
</div>
<p>로짓(logit)은 몇 가지 의미를 가집니다.</p>
<ul>
<li>수학적 의미: [0, 1] 범위의 확률을 [−∞, ∞] 범위의 실수로 변환하는 함수입니다.</li>
<li>딥러닝에서의 의미: 정규화되지 않은(unnormalized) 신경망의 원시 출력값입니다.</li>
</ul>
<p>흔히 다중 클래스 분류(multi-class classification) 문제에서는 마지막에 <code>softmax</code> 함수를 적용해서 레이블과 비교할 수 있는 확률 값으로 변환합니다. 이때 로짓은 <code>softmax</code> 함수의 입력값이 됩니다.</p>
<p>모델을 클래스로부터 생성하고 이를 <code>device</code>로 전송합니다. GPU가 존재하는 경우에는 GPU 메모리로 모델이 올라갑니다.</p>
<div id="cell-45" class="cell" data-execution_count="14">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.rand(<span class="dv">1</span>, <span class="dv">28</span>, <span class="dv">28</span>, device<span class="op">=</span>device)</span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a>logits <span class="op">=</span> model(x)  <span class="co"># Don't call forward() directly!  Call the *model* object.</span></span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a>prediction <span class="op">=</span> nn.Softmax(dim<span class="op">=</span><span class="dv">1</span>)(logits)  <span class="co"># Convert logits to probabilities</span></span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a>y_label <span class="op">=</span> prediction.argmax(<span class="dv">1</span>) <span class="co"># Get the predicted class</span></span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Logits: </span><span class="sc">{</span>logits<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb43-7"><a href="#cb43-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Prediction probabilities: </span><span class="sc">{</span>prediction<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb43-8"><a href="#cb43-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Predicted class: </span><span class="sc">{</span>y_label<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Logits: tensor([[ 0.0464, -0.0368,  0.0447, -0.0640, -0.0253,  0.0242,  0.0378, -0.1139,
          0.0005,  0.0299]], device='cuda:0', grad_fn=&lt;AddmmBackward0&gt;)
Prediction probabilities: tensor([[0.1052, 0.0968, 0.1050, 0.0942, 0.0979, 0.1029, 0.1043, 0.0896, 0.1005,
         0.1035]], device='cuda:0', grad_fn=&lt;SoftmaxBackward0&gt;)
Predicted class: tensor([0], device='cuda:0')</code></pre>
</div>
</div>
<p>주의할 점은 모델의 <code>forward()</code> 메서드를 직접 호출하지 않아야 한다는 것입니다. 대신 모델 객체를 함수처럼 호출하면(<code>model(x)</code>) 자동으로 <code>forward()</code>가 실행되며, 이는 PyTorch의 자동 미분 시스템과 통합되어 있습니다. 모델 객체의 <code>__call__</code> 메소드가 <code>forward()</code>를 호출하고, 추가적으로 필요한 작업들(hook 등)을 수행합니다.</p>
</section>
<section id="훈련" class="level3">
<h3 class="anchored" data-anchor-id="훈련">3.1.7 훈련</h3>
<blockquote class="blockquote">
<p><strong>도전 과제</strong>: 어떻게 하면 대규모 데이터셋과 복잡한 모델을 효율적으로 학습시킬 수 있을까?</p>
<p><strong>연구자의 고뇌</strong>: 딥러닝 모델의 성능은 데이터의 양과 질, 그리고 모델의 복잡도에 크게 영향을 받습니다. 하지만 대규모 데이터셋과 복잡한 모델을 학습시키려면 많은 시간과 컴퓨팅 자원이 필요했습니다. 학습 과정을 안정화하고, 과적합을 방지하며, 최적의 하이퍼파라미터를 찾는 것도 어려운 문제였습니다. 이러한 문제를 해결하기 위해, 효율적인 학습 알고리즘, 최적화 기법, 그리고 자동화된 훈련 루프가 필요했습니다.</p>
</blockquote>
<p>훈련할 데이터와 모델을 준비하고 나면 실제적인 훈련을 수행합니다. 신경망 모델을 좋은 근사기(approximator)로 만들기 위해서 파라미터를 반복적으로 업데이트 해줘야 합니다. 라벨과 예측값의 차를 계산하는 오차함수(loss function)을 정의하고 최적화기(optimizaer)를 선택해서 파라미터를 지속적으로 업데이트 하면서 오차를 줄여나갑니다.</p>
<p>훈련을 진행하는 순서는 다음과 같습니다.</p>
<ol type="1">
<li>데이터셋과 데이터로더 초기화</li>
<li>배치 단위 데이터 로딩</li>
<li>순전파를 통한 예측값 계산</li>
<li>손실 함수를 통한 오차 계산</li>
<li>역전파를 통한 그래디언트 계산</li>
<li>최적화기를 통한 파라미터 업데이트</li>
</ol>
<p>전체 데이터셋에 대한 1회 반복을 에포크(epoch)라고 하며, 이러한 과정을 여러 에포크 반복하는 것을 훈련 루프라고 합니다.</p>
<section id="초매개변수" class="level5">
<h5 class="anchored" data-anchor-id="초매개변수">초매개변수</h5>
<p>훈련에는 세 가지 핵심 초매개변수가 필요합니다.</p>
<ul>
<li>에포크(epoch) 수 : 에포크를 몇번 반복할지 정합니다. 통상 과대적합 직전까지만 반복하는 것이 좋습니다.</li>
<li>배치 크기 : 모델에 한번에 통과시킬 훈련 데이터의 갯수. 전체 데이터를 통과시키는 것은 GPU 메모리 한계와 메트릭스 연산 시간의 지수적 증가로 비현실 적인 경우가 많습니다. 일부 데이터로 점진적으로 모델 파라미터를 업데이트해서 최적값에 근접시킵니다. 배치 크기가 너무 작으면 변화량이 너무 튀어서 최소값에 근접하기 어려울 수 있습니다.</li>
<li>학습률 : 업데이트될 값의 스케일을 조정합니다. 점진적으로 찾아가는 스텝 사이즈로 비유할 수 있습니다. 통상 작은 값을 가집니다. 다음장에서 학습률과 최적화기의 관계를 살펴봅니다.</li>
</ul>
<div id="cell-48" class="cell" data-execution_count="15">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 3가지 초매개변수</span></span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a>learning_rate <span class="op">=</span> <span class="fl">1e-3</span> <span class="co"># 최적화기를 위해 앞서 지정했음.</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="훈련-루프" class="level5">
<h5 class="anchored" data-anchor-id="훈련-루프">훈련 루프</h5>
<p>훈련 루프는 에포크마다 두 단계로 진행됩니다. 1. 훈련 단계: 파라미터 최적화 2. 검증 단계: 성능 평가</p>
<p>2015년 배치 정규화의 등장으로 train()과 eval() 모드의 구분이 중요해졌습니다. eval() 모드에서는 배치 정규화나 드롭아웃과 같은 훈련 전용 연산을 비활성화하여 추론 속도를 향상시킵니다.</p>
</section>
<section id="loss-function" class="level5">
<h5 class="anchored" data-anchor-id="loss-function">Loss Function</h5>
<p>손실 함수는 신경망 학습의 핵심 요소입니다. 1943년 McCulloch-Pitts 뉴런 모델 이후, 다양한 손실 함수들이 제안되었습니다. 특히 1989년 정보 이론에서 차용한 크로스 엔트로피(Cross-Entropy)의 도입은 딥러닝 발전의 중요한 전환점이 되었습니다.</p>
</section>
<section id="binary-cross-entropy-bce" class="level5">
<h5 class="anchored" data-anchor-id="binary-cross-entropy-bce">Binary Cross-Entropy (BCE)</h5>
<p>이진 분류에서 주로 사용되는 BCE는 다음과 같이 정의됩니다.</p>
<p><span class="math display">\[\mathcal{L} = - \sum_{i} [y_i \log{x_i} + (1-y_i)\log{(1-x_i)}] \]</span></p>
<p>여기서 <span class="math inline">\(y\)</span>는 실제 레이블, <span class="math inline">\(x\)</span>는 모델의 예측값이며, 둘 다 [0, 1] 범위를 가집니다.</p>
<p>PyTorch는 다양한 손실 함수를 제공합니다.</p>
<ul>
<li><code>nn.MSELoss</code>: 회귀 문제용 (Mean Squared Error)</li>
<li><code>nn.NLLLoss</code>: 음의 로그 우도 (Negative Log-Likelihood)</li>
<li><code>nn.CrossEntropyLoss</code>: <code>LogSoftmax</code>와 <code>NLLLoss</code>의 조합</li>
<li><code>nn.BCEWithLogitsLoss</code>: 수치 안정성을 위해 시그모이드 층과 BCE를 통합</li>
</ul>
<p>특히 주목할 만한 것은 <code>nn.BCEWithLogitsLoss</code>입니다. 이는 수치 안정성을 위해 시그모이드 층과 BCE를 통합했습니다. 로그 함수 사용은 다음과 같은 장점을 가집니다 (2장에 더 자세히 기술되어 있습니다):</p>
<ol type="1">
<li>급격한 수치 변화를 완화</li>
<li>곱셈을 덧셈으로 변환하여 계산 효율성 향상</li>
</ol>
<div id="cell-50" class="cell" data-execution_count="16">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize the loss function</span></span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a>loss_fn <span class="op">=</span> nn.CrossEntropyLoss()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="최적화기optimizer" class="level5">
<h5 class="anchored" data-anchor-id="최적화기optimizer">최적화기(Optimizer)</h5>
<p>최적화 알고리즘은 1950년대의 기본적인 경사 하강법(Gradient Descent)에서 시작하여 2014년 Adam의 등장으로 큰 발전을 이루었습니다. <code>torch.optim</code>은 다양한 최적화기를 제공하며, 현재는 Adam과 AdamW가 주류를 이룹니다.</p>
<div id="cell-52" class="cell" data-execution_count="17">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Declare the optimizer.</span></span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.SGD(model.parameters(), lr<span class="op">=</span>learning_rate)</span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-4"><a href="#cb47-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Learning rate scheduler (optional, but often beneficial)</span></span>
<span id="cb47-5"><a href="#cb47-5" aria-hidden="true" tabindex="-1"></a>scheduler <span class="op">=</span> torch.optim.lr_scheduler.StepLR(optimizer, step_size<span class="op">=</span><span class="dv">30</span>, gamma<span class="op">=</span><span class="fl">0.1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>위 코드에서는 <code>torch.optim.lr_scheduler.StepLR</code>를 사용하여 학습률 스케줄러를 추가했습니다. <code>step_size</code> 에폭마다 <code>gamma</code>를 곱하여 학습률을 감소시킵니다. 학습률 스케줄링은 학습 속도와 안정성에 큰 영향을 미칠 수 있습니다.</p>
</section>
<section id="훈련-루프-training-loop" class="level5">
<h5 class="anchored" data-anchor-id="훈련-루프-training-loop">훈련 루프 (Training Loop)</h5>
<p>데이터셋에 대해 반복적으로 수행하는 훈련 루프를 구성해 보겠습니다. 하나의 에포크는 일반적으로 훈련과 검증 두 가지로 구성됩니다.</p>
<ol type="1">
<li><strong>Training Loop</strong>: 훈련 데이터셋을 사용하여 파라미터를 최적화합니다.</li>
<li><strong>Validation Loop</strong>: 테스트(검증) 데이터셋을 사용하여 모델의 성능이 어떻게 변하는지 확인합니다.</li>
</ol>
<p>훈련 시 모델의 모드를 <code>train</code>과 <code>eval</code> 두 가지로 설정할 수 있습니다. 이것은 일종의 스위치라고 할 수 있습니다. 2015년 배치 정규화의 등장으로 <code>train()</code>과 <code>eval()</code> 모드의 구분이 중요해졌습니다. <code>eval()</code> 모드에서는 배치 정규화나 드롭아웃과 같은 훈련 전용 연산을 비활성화하여 추론 속도를 향상시킵니다.</p>
<div id="cell-55" class="cell" data-execution_count="18">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.tensorboard <span class="im">import</span> SummaryWriter</span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a><span class="co"># TensorBoard writer setup</span></span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a>writer <span class="op">=</span> SummaryWriter(<span class="st">'runs/fashion_mnist_experiment_1'</span>)</span>
<span id="cb48-5"><a href="#cb48-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-6"><a href="#cb48-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-7"><a href="#cb48-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train_loop(model, data_loader, loss_fn, optimizer, epoch):  <span class="co"># Added epoch for logging</span></span>
<span id="cb48-8"><a href="#cb48-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-9"><a href="#cb48-9" aria-hidden="true" tabindex="-1"></a>    model.train()  <span class="co"># Set the model to training mode</span></span>
<span id="cb48-10"><a href="#cb48-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-11"><a href="#cb48-11" aria-hidden="true" tabindex="-1"></a>    size <span class="op">=</span> <span class="bu">len</span>(data_loader.dataset)  <span class="co"># Total number of data samples</span></span>
<span id="cb48-12"><a href="#cb48-12" aria-hidden="true" tabindex="-1"></a>    num_batches <span class="op">=</span> <span class="bu">len</span>(data_loader)</span>
<span id="cb48-13"><a href="#cb48-13" aria-hidden="true" tabindex="-1"></a>    total_loss <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb48-14"><a href="#cb48-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-15"><a href="#cb48-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> batch_count, (input_data, label_data) <span class="kw">in</span> <span class="bu">enumerate</span>(data_loader):</span>
<span id="cb48-16"><a href="#cb48-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Move data to the GPU (if available).</span></span>
<span id="cb48-17"><a href="#cb48-17" aria-hidden="true" tabindex="-1"></a>        input_data <span class="op">=</span> input_data.to(device)</span>
<span id="cb48-18"><a href="#cb48-18" aria-hidden="true" tabindex="-1"></a>        label_data <span class="op">=</span> label_data.to(device)</span>
<span id="cb48-19"><a href="#cb48-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-20"><a href="#cb48-20" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compute predictions</span></span>
<span id="cb48-21"><a href="#cb48-21" aria-hidden="true" tabindex="-1"></a>        preds <span class="op">=</span> model(input_data)</span>
<span id="cb48-22"><a href="#cb48-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-23"><a href="#cb48-23" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compute loss</span></span>
<span id="cb48-24"><a href="#cb48-24" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> loss_fn(preds, label_data)</span>
<span id="cb48-25"><a href="#cb48-25" aria-hidden="true" tabindex="-1"></a>        total_loss <span class="op">+=</span> loss.item()</span>
<span id="cb48-26"><a href="#cb48-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-27"><a href="#cb48-27" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Backpropagation</span></span>
<span id="cb48-28"><a href="#cb48-28" aria-hidden="true" tabindex="-1"></a>        loss.backward()  <span class="co"># Perform backpropagation</span></span>
<span id="cb48-29"><a href="#cb48-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-30"><a href="#cb48-30" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Update parameters</span></span>
<span id="cb48-31"><a href="#cb48-31" aria-hidden="true" tabindex="-1"></a>        optimizer.step()</span>
<span id="cb48-32"><a href="#cb48-32" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()  <span class="co"># Zero the gradients before next iteration</span></span>
<span id="cb48-33"><a href="#cb48-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-34"><a href="#cb48-34" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> batch_count <span class="op">%</span> <span class="dv">100</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb48-35"><a href="#cb48-35" aria-hidden="true" tabindex="-1"></a>            loss, current <span class="op">=</span> loss.item(), batch_count <span class="op">*</span> batch_size <span class="op">+</span> <span class="bu">len</span>(input_data)</span>
<span id="cb48-36"><a href="#cb48-36" aria-hidden="true" tabindex="-1"></a>            <span class="co"># print(f"loss: {loss:&gt;7f}  [{current:&gt;5d}/{size:&gt;5d}]")</span></span>
<span id="cb48-37"><a href="#cb48-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-38"><a href="#cb48-38" aria-hidden="true" tabindex="-1"></a>    avg_train_loss <span class="op">=</span> total_loss <span class="op">/</span> num_batches</span>
<span id="cb48-39"><a href="#cb48-39" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> avg_train_loss</span>
<span id="cb48-40"><a href="#cb48-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-41"><a href="#cb48-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-42"><a href="#cb48-42" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> eval_loop(model, data_loader, loss_fn):</span>
<span id="cb48-43"><a href="#cb48-43" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">eval</span>()  <span class="co"># Set the model to evaluation mode</span></span>
<span id="cb48-44"><a href="#cb48-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-45"><a href="#cb48-45" aria-hidden="true" tabindex="-1"></a>    correct, test_loss <span class="op">=</span> <span class="fl">0.0</span>, <span class="fl">0.0</span></span>
<span id="cb48-46"><a href="#cb48-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-47"><a href="#cb48-47" aria-hidden="true" tabindex="-1"></a>    size <span class="op">=</span> <span class="bu">len</span>(data_loader.dataset)  <span class="co"># Total data size</span></span>
<span id="cb48-48"><a href="#cb48-48" aria-hidden="true" tabindex="-1"></a>    num_batches <span class="op">=</span> <span class="bu">len</span>(data_loader)  <span class="co"># Number of batches</span></span>
<span id="cb48-49"><a href="#cb48-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-50"><a href="#cb48-50" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():  <span class="co"># Disable gradient calculation within this block</span></span>
<span id="cb48-51"><a href="#cb48-51" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> input_data, label_data <span class="kw">in</span> data_loader:  <span class="co"># No need for enumerate as count is not used</span></span>
<span id="cb48-52"><a href="#cb48-52" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Move data to GPU (if available).</span></span>
<span id="cb48-53"><a href="#cb48-53" aria-hidden="true" tabindex="-1"></a>            input_data <span class="op">=</span> input_data.to(device)</span>
<span id="cb48-54"><a href="#cb48-54" aria-hidden="true" tabindex="-1"></a>            label_data <span class="op">=</span> label_data.to(device)</span>
<span id="cb48-55"><a href="#cb48-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-56"><a href="#cb48-56" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Compute predictions</span></span>
<span id="cb48-57"><a href="#cb48-57" aria-hidden="true" tabindex="-1"></a>            preds <span class="op">=</span> model(input_data)</span>
<span id="cb48-58"><a href="#cb48-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-59"><a href="#cb48-59" aria-hidden="true" tabindex="-1"></a>            test_loss <span class="op">+=</span> loss_fn(preds, label_data).item()</span>
<span id="cb48-60"><a href="#cb48-60" aria-hidden="true" tabindex="-1"></a>            correct <span class="op">+=</span> (preds.argmax(<span class="dv">1</span>) <span class="op">==</span> label_data).<span class="bu">type</span>(torch.<span class="bu">float</span>).<span class="bu">sum</span>().item()</span>
<span id="cb48-61"><a href="#cb48-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-62"><a href="#cb48-62" aria-hidden="true" tabindex="-1"></a>    test_loss <span class="op">/=</span> num_batches</span>
<span id="cb48-63"><a href="#cb48-63" aria-hidden="true" tabindex="-1"></a>    correct <span class="op">/=</span> size</span>
<span id="cb48-64"><a href="#cb48-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-65"><a href="#cb48-65" aria-hidden="true" tabindex="-1"></a>    <span class="co"># print(f"\n Test Result \n Accuracy: {(100 * correct):&gt;0.1f}%, Average loss: {test_loss:&gt;8f} \n")</span></span>
<span id="cb48-66"><a href="#cb48-66" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> test_loss, correct</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="전체-훈련-과정" class="level5">
<h5 class="anchored" data-anchor-id="전체-훈련-과정">전체 훈련 과정</h5>
<p>전체 훈련 과정은 각 에포크마다 훈련과 검증을 반복합니다. <code>tqdm</code>을 사용하여 진행 상황을 시각적으로 표시하고, TensorBoard를 사용하여 학습률 변화를 기록합니다.</p>
<div id="cell-57" class="cell" data-execution_count="19">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Progress bar utility</span></span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tqdm.notebook <span class="im">import</span> tqdm</span>
<span id="cb49-3"><a href="#cb49-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-4"><a href="#cb49-4" aria-hidden="true" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">5</span>  <span class="co"># Reduced for demonstration</span></span>
<span id="cb49-5"><a href="#cb49-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> tqdm(<span class="bu">range</span>(epochs)):</span>
<span id="cb49-6"><a href="#cb49-6" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Epoch </span><span class="sc">{</span>epoch<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ch">\n</span><span class="ss">-------------------------------"</span>)</span>
<span id="cb49-7"><a href="#cb49-7" aria-hidden="true" tabindex="-1"></a>    train_loss <span class="op">=</span> train_loop(model, train_dataloader, loss_fn, optimizer, epoch)</span>
<span id="cb49-8"><a href="#cb49-8" aria-hidden="true" tabindex="-1"></a>    test_loss, correct <span class="op">=</span> eval_loop(model, test_dataloader, loss_fn)</span>
<span id="cb49-9"><a href="#cb49-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-10"><a href="#cb49-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Log training and validation metrics to TensorBoard</span></span>
<span id="cb49-11"><a href="#cb49-11" aria-hidden="true" tabindex="-1"></a>    writer.add_scalar(<span class="st">'Loss/train'</span>, train_loss, epoch)</span>
<span id="cb49-12"><a href="#cb49-12" aria-hidden="true" tabindex="-1"></a>    writer.add_scalar(<span class="st">'Loss/test'</span>, test_loss, epoch)</span>
<span id="cb49-13"><a href="#cb49-13" aria-hidden="true" tabindex="-1"></a>    writer.add_scalar(<span class="st">'Accuracy/test'</span>, correct, epoch)</span>
<span id="cb49-14"><a href="#cb49-14" aria-hidden="true" tabindex="-1"></a>    writer.add_scalar(<span class="st">'Learning Rate'</span>, optimizer.param_groups[<span class="dv">0</span>][<span class="st">'lr'</span>], epoch) <span class="co"># Log learning rate</span></span>
<span id="cb49-15"><a href="#cb49-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-16"><a href="#cb49-16" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'Epoch: </span><span class="sc">{</span>epoch<span class="sc">}</span><span class="ss">, Train Loss: </span><span class="sc">{</span>train_loss<span class="sc">:.4f}</span><span class="ss">, Test Loss: </span><span class="sc">{</span>test_loss<span class="sc">:.4f}</span><span class="ss">, Test Accuracy: </span><span class="sc">{</span>correct<span class="sc">:.2f}</span><span class="ss">%, LR: </span><span class="sc">{</span>optimizer<span class="sc">.</span>param_groups[<span class="dv">0</span>][<span class="st">"lr"</span>]<span class="sc">:.6f}</span><span class="ss">'</span>)</span>
<span id="cb49-17"><a href="#cb49-17" aria-hidden="true" tabindex="-1"></a>    scheduler.step()  <span class="co"># Update learning rate.  Place *after* logging.</span></span>
<span id="cb49-18"><a href="#cb49-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-19"><a href="#cb49-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Done!"</span>)</span>
<span id="cb49-20"><a href="#cb49-20" aria-hidden="true" tabindex="-1"></a>writer.close() <span class="co"># Close TensorBoard Writer</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"84baac2d3bc14a3b960d258d62b7996a","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1
-------------------------------
Epoch: 0, Train Loss: 1.5232, Test Loss: 0.9543, Test Accuracy: 0.71%, LR: 0.001000
Epoch 2
-------------------------------
Epoch: 1, Train Loss: 0.7920, Test Loss: 0.7059, Test Accuracy: 0.76%, LR: 0.001000
Epoch 3
-------------------------------
Epoch: 2, Train Loss: 0.6442, Test Loss: 0.6208, Test Accuracy: 0.78%, LR: 0.001000
Epoch 4
-------------------------------
Epoch: 3, Train Loss: 0.5790, Test Loss: 0.5757, Test Accuracy: 0.79%, LR: 0.001000
Epoch 5
-------------------------------
Epoch: 4, Train Loss: 0.5383, Test Loss: 0.5440, Test Accuracy: 0.80%, LR: 0.001000
Done!</code></pre>
</div>
</div>
<p>이러한 훈련-검증 사이클은 1990년대부터 표준적인 딥러닝 훈련 방식으로 자리잡았습니다. 특히 검증 단계는 과적합을 모니터링하고 조기 종료(early stopping)를 결정하는데 중요한 역할을 합니다.</p>
</section>
</section>
<section id="모델-저장-읽기" class="level3">
<h3 class="anchored" data-anchor-id="모델-저장-읽기">3.1.8 모델 저장, 읽기</h3>
<p>모델 저장은 딥러닝 실무에서 매우 중요한 부분입니다. 훈련된 모델을 저장하고 나중에 다시 로드하여 재사용하거나, 다른 환경(예: 서버, 모바일 장치)에 배포할 수 있습니다. PyTorch는 두 가지 주요 저장 방식을 제공합니다.</p>
<section id="가중치만-저장하기" class="level5">
<h5 class="anchored" data-anchor-id="가중치만-저장하기">가중치만 저장하기</h5>
<p>모델의 학습된 파라미터(가중치와 편향)는 <code>state_dict</code>라는 Python 딕셔너리에 저장됩니다. <code>state_dict</code>는 각 층(layer)을 해당 층의 파라미터 텐서에 매핑하는 구조입니다. 이 방식은 모델 구조가 변경되어도 가중치를 불러올 수 있다는 장점이 있어서, 일반적으로 권장됩니다.</p>
<div id="cell-60" class="cell" data-execution_count="37">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Save model weights</span></span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a>torch.save(model.state_dict(), <span class="st">'model_weights.pth'</span>)</span>
<span id="cb51-3"><a href="#cb51-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-4"><a href="#cb51-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Load weights</span></span>
<span id="cb51-5"><a href="#cb51-5" aria-hidden="true" tabindex="-1"></a>model_saved_weights <span class="op">=</span> SimpleNetwork()  <span class="co"># Create an empty model with the same architecture</span></span>
<span id="cb51-6"><a href="#cb51-6" aria-hidden="true" tabindex="-1"></a>model_saved_weights.load_state_dict(torch.load(<span class="st">'model_weights.pth'</span>))</span>
<span id="cb51-7"><a href="#cb51-7" aria-hidden="true" tabindex="-1"></a>model_saved_weights.to(device) <span class="co"># Don't forget to move to the correct device!</span></span>
<span id="cb51-8"><a href="#cb51-8" aria-hidden="true" tabindex="-1"></a>model_saved_weights.<span class="bu">eval</span>() <span class="co"># Set to evaluation mode</span></span>
<span id="cb51-9"><a href="#cb51-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-10"><a href="#cb51-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Check performance (assuming eval_loop is defined)</span></span>
<span id="cb51-11"><a href="#cb51-11" aria-hidden="true" tabindex="-1"></a>eval_loop(model_saved_weights, test_dataloader, loss_fn)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_112013/3522135054.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model_saved_weights.load_state_dict(torch.load('model_weights.pth'))</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="37">
<pre><code>(0.5459668265935331, 0.8036)</code></pre>
</div>
</div>
</section>
<section id="전체-모델-저장하기" class="level5">
<h5 class="anchored" data-anchor-id="전체-모델-저장하기">전체 모델 저장하기</h5>
<p>2018년 이후 모델 아키텍처가 더욱 복잡해지면서, 모델 구조와 가중치를 함께 저장하는 방식도 사용됩니다.</p>
<div id="cell-62" class="cell" data-execution_count="38">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb54"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a>torch.save(model, <span class="st">'model_trained.pth'</span>)</span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-3"><a href="#cb54-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the entire model</span></span>
<span id="cb54-4"><a href="#cb54-4" aria-hidden="true" tabindex="-1"></a>model_saved <span class="op">=</span> torch.load(<span class="st">'model_trained.pth'</span>)</span>
<span id="cb54-5"><a href="#cb54-5" aria-hidden="true" tabindex="-1"></a>model_saved.to(device)  <span class="co"># Move the loaded model to the correct device.</span></span>
<span id="cb54-6"><a href="#cb54-6" aria-hidden="true" tabindex="-1"></a>model_saved.<span class="bu">eval</span>() <span class="co">#  Set the loaded model to evaluation mode</span></span>
<span id="cb54-7"><a href="#cb54-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-8"><a href="#cb54-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Check performance</span></span>
<span id="cb54-9"><a href="#cb54-9" aria-hidden="true" tabindex="-1"></a>eval_loop(model_saved, test_dataloader, loss_fn)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_112013/3185686172.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model_saved = torch.load('model_trained.pth')</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="38">
<pre><code>(0.5459668265935331, 0.8036)</code></pre>
</div>
</div>
<p>전체 모델 저장 방식은 편리하지만, 모델 클래스 정의가 변경될 경우 호환성 문제가 발생할 수 있습니다. 특히, 프로덕션 환경에서는 모델 아키텍처가 자주 변경되지 않으므로 가중치만 저장하는 방식이 더 안정적일 수 있습니다. 또한, 전체 모델을 저장하는 방식은 Python의 <code>pickle</code> 모듈을 사용하는데, <code>pickle</code>은 임의의 코드를 실행할 수 있는 취약점이 있어 보안상 위험할 수 있습니다.</p>
</section>
<section id="safetensors-a-safer-alternative" class="level5">
<h5 class="anchored" data-anchor-id="safetensors-a-safer-alternative">Safetensors: A Safer Alternative</h5>
<p>최근에는 <code>pickle</code> 대신 <code>safetensors</code>와 같은 새로운 저장 형식이 등장하여 보안성과 로딩 속도를 개선하고 있습니다. <code>safetensors</code>는 텐서 데이터를 안전하고 효율적으로 저장하기 위한 형식입니다.</p>
<ul>
<li><strong>보안:</strong> <code>safetensors</code>는 임의의 코드 실행을 허용하지 않아 <code>pickle</code>보다 훨씬 안전합니다.</li>
<li><strong>Zero-copy:</strong> 데이터를 복사하지 않고 메모리에 직접 매핑하여 로딩 속도가 빠릅니다.</li>
<li><strong>Lazy loading:</strong> 필요한 부분만 로드할 수 있어 메모리 사용량이 적습니다.</li>
<li><strong>다양한 프레임워크 지원</strong>: PyTorch, TensorFlow, JAX 등</li>
</ul>
<div id="cell-64" class="cell" data-execution_count="39">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Install safetensors: pip install safetensors</span></span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-3"><a href="#cb57-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> safetensors.torch <span class="im">import</span> save_file, load_file</span>
<span id="cb57-4"><a href="#cb57-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-5"><a href="#cb57-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Save using safetensors</span></span>
<span id="cb57-6"><a href="#cb57-6" aria-hidden="true" tabindex="-1"></a>state_dict <span class="op">=</span> model.state_dict()</span>
<span id="cb57-7"><a href="#cb57-7" aria-hidden="true" tabindex="-1"></a>save_file(state_dict, <span class="st">"model_weights.safetensors"</span>)</span>
<span id="cb57-8"><a href="#cb57-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-9"><a href="#cb57-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Load using safetensors</span></span>
<span id="cb57-10"><a href="#cb57-10" aria-hidden="true" tabindex="-1"></a>loaded_state_dict <span class="op">=</span> load_file(<span class="st">"model_weights.safetensors"</span>, device<span class="op">=</span>device) <span class="co"># Load directly to the device.</span></span>
<span id="cb57-11"><a href="#cb57-11" aria-hidden="true" tabindex="-1"></a>model_new <span class="op">=</span> SimpleNetwork().to(device) <span class="co"># Create an instance of your model class</span></span>
<span id="cb57-12"><a href="#cb57-12" aria-hidden="true" tabindex="-1"></a>model_new.load_state_dict(loaded_state_dict)</span>
<span id="cb57-13"><a href="#cb57-13" aria-hidden="true" tabindex="-1"></a>model_new.<span class="bu">eval</span>()</span>
<span id="cb57-14"><a href="#cb57-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-15"><a href="#cb57-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Check performance</span></span>
<span id="cb57-16"><a href="#cb57-16" aria-hidden="true" tabindex="-1"></a>eval_loop(model_new, test_dataloader, loss_fn)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="39">
<pre><code>(0.5459668265935331, 0.8036)</code></pre>
</div>
</div>
</section>
</section>
</section>
<section id="텐서보드" class="level2">
<h2 class="anchored" data-anchor-id="텐서보드">3.2 텐서보드</h2>
<p>텐서보드는 딥러닝 훈련에서 생성되는 다양한 로그를 기록, 추적하고 효율적으로 시각화해주는 도구입니다. 흔히 대시보드라 불리는 로그 데이터 기록/시각화 도구의 한 종류입니다. 텐서플로우용으로 처음 개발되었으나 지금은 파이토치와 통합되었습니다. 텐서보드와 유사한 대시 보드 형태의 시각화 도구는 다음과 같은 것들이 있습니다.</p>
<ul>
<li>Weights &amp; Biases (WandB): 클라우드 기반의 MLOps 통합 플랫폼으로, 실험 추적, 데이터셋 버전 관리, 모델 관리 등 광범위한 기능을 제공합니다. 특히 팀 협업 기능이 뛰어나 기업 환경에서 널리 사용됩니다.</li>
<li>Vertex AI: Google Cloud의 완전 관리형 ML 도구로, BigQuery, Dataproc, Spark와의 네이티브 통합을 제공합니다. 모델 구축, 배포, 확장을 빠르게 수행할 수 있어 대규모 ML 워크플로우에 적합합니다.</li>
<li>MLflow: 실험 추적, 모델 패키징, 중앙 레지스트리 등을 제공하는 오픈소스 도구입니다. ML 모델의 추적과 배포를 단순화하여 데이터 과학 및 ML 분야에서 널리 사용되고 있습니다.</li>
</ul>
<p>위 3가지 말고도 다양한 도구들이 있습니다. 여기서는 주로 텐서보드를 사용하겠습니다.</p>
<section id="텐서보드-기본-사용법" class="level3">
<h3 class="anchored" data-anchor-id="텐서보드-기본-사용법">3.2.1 텐서보드 기본 사용법</h3>
<p>텐서보드는 2015년 텐서플로우와 함께 등장했습니다. 당시 딥러닝 모델의 복잡도가 급격히 증가하면서 훈련 과정을 효과적으로 모니터링할 필요성이 대두되었습니다.</p>
<p>텐서보드의 핵심 기능은 다음과 같습니다. 1. 스칼라 지표 추적: 손실값, 정확도 등의 수치 기록 2. 모델 구조 시각화: 계산 그래프의 도식화 3. 분포 추적: 가중치, 그래디언트의 분포 변화 관찰 4. 임베딩 투영: 고차원 벡터의 2D/3D 시각화 5. 하이퍼파라미터 최적화: 다양한 설정의 실험 결과 비교</p>
<p>텐서보드는 딥러닝 훈련 과정을 시각화하고 분석하는 강력한 도구입니다. 텐서보드의 기본 사용법은 크게 설치, 로그 디렉토리 설정, 콜백 설정의 세 단계로 구성됩니다.</p>
<section id="설치-방법" class="level5">
<h5 class="anchored" data-anchor-id="설치-방법">설치 방법</h5>
<p>텐서보드는 pip나 conda를 통해 설치할 수 있습니다.</p>
<div id="cell-66" class="cell" data-vscode="{&quot;languageId&quot;:&quot;shellscript&quot;}">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb59"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip install tensorboard</span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a><span class="co"># 또는</span></span>
<span id="cb59-3"><a href="#cb59-3" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>conda install <span class="op">-</span>c conda<span class="op">-</span>forge tensorboard</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="로그-디렉토리-설정" class="level5">
<h5 class="anchored" data-anchor-id="로그-디렉토리-설정">로그 디렉토리 설정</h5>
<p>텐서보드는 로그 디렉토리에 저장된 이벤트 파일을 읽어 시각화합니다. 주피터 노트북이나 코랩에서는 다음과 같이 설정합니다.</p>
<div id="cell-68" class="cell" data-execution_count="41">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb60"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.tensorboard <span class="im">import</span> SummaryWriter</span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-3"><a href="#cb60-3" aria-hidden="true" tabindex="-1"></a><span class="co"># 로그 디렉토리 설정</span></span>
<span id="cb60-4"><a href="#cb60-4" aria-hidden="true" tabindex="-1"></a>log_dir <span class="op">=</span> <span class="st">'logs/experiment_1'</span></span>
<span id="cb60-5"><a href="#cb60-5" aria-hidden="true" tabindex="-1"></a>writer <span class="op">=</span> SummaryWriter(log_dir)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="텐서보드-실행" class="level5">
<h5 class="anchored" data-anchor-id="텐서보드-실행">텐서보드 실행</h5>
<p>텐서보드는 다음 두 가지 방법으로 실행할 수 있습니다.</p>
<ol type="1">
<li>커맨드 라인에서 실행</li>
</ol>
<div id="cell-70" class="cell" data-vscode="{&quot;languageId&quot;:&quot;shellscript&quot;}">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb61"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a>tensorboard <span class="op">--</span>logdir<span class="op">=</span>logs</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<ol start="2" type="1">
<li>주피터 노트북에서 실행</li>
</ol>
<div id="cell-72" class="cell" data-vscode="{&quot;languageId&quot;:&quot;shellscript&quot;}">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb62"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>load_ext tensorboard</span>
<span id="cb62-2"><a href="#cb62-2" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>tensorboard <span class="op">--</span>logdir<span class="op">=</span>logs</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>실행 후 웹 브라우저에서 http://localhost:6006 으로 접속하면 텐서보드 대시보드를 확인할 수 있습니다.</p>
</section>
<section id="원격-서버에서-실행" class="level5">
<h5 class="anchored" data-anchor-id="원격-서버에서-실행">원격 서버에서 실행</h5>
<p>원격 서버에서 텐서보드를 실행할 때는 SSH 터널링을 사용합니다.</p>
<div id="cell-74" class="cell" data-vscode="{&quot;languageId&quot;:&quot;shellscript&quot;}">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb63"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a>ssh <span class="op">-</span>L <span class="dv">6006</span>:<span class="fl">127.0.0.1</span>:<span class="dv">6006</span> username<span class="op">@</span>server_ip</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><strong>주요 파라미터 (SummaryWriter)</strong></p>
<p><code>SummaryWriter</code>는 텐서보드에 기록할 데이터를 생성하는 핵심 클래스입니다. 주요 파라미터는 다음과 같습니다.</p>
<ul>
<li><code>log_dir</code>: 로그 파일이 저장될 디렉토리 경로.</li>
<li><code>comment</code>: log_dir 에 덧붙일 문자열.</li>
<li><code>flush_secs</code>: 로그를 디스크에 쓰는 주기(초).</li>
<li><code>max_queue</code>: 보류 중인 이벤트/스텝을 얼마나 보관할지 설정.</li>
</ul>
<p><strong>주요 메서드(SummaryWriter)</strong></p>
<ul>
<li><code>add_scalar(tag, scalar_value, global_step=None)</code>: 스칼라 값(예: 손실, 정확도)을 기록합니다.</li>
<li><code>add_histogram(tag, values, global_step=None, bins='tensorflow')</code>: 히스토그램(값의 분포)을 기록합니다.</li>
<li><code>add_image(tag, img_tensor, global_step=None, dataformats='CHW')</code>: 이미지를 기록합니다.</li>
<li><code>add_figure(tag, figure, global_step=None, close=True)</code>: Matplotlib figure를 기록합니다.</li>
<li><code>add_video(tag, vid_tensor, global_step=None, fps=4, dataformats='NCHW')</code>: 비디오를 기록합니다.</li>
<li><code>add_audio(tag, snd_tensor, global_step=None, sample_rate=44100)</code>: 오디오를 기록합니다.</li>
<li><code>add_text(tag, text_string, global_step=None)</code>: 텍스트를 기록합니다.</li>
<li><code>add_graph(model, input_to_model=None, verbose=False)</code>: 모델 그래프를 기록합니다.</li>
<li><code>add_embedding(mat, metadata=None, label_img=None, global_step=None, tag='default', metadata_header=None)</code>: 임베딩 프로젝터를 기록합니다.</li>
<li><code>add_hparams(hparam_dict, metric_dict, hparam_domain_discrete=None, run_name=None)</code>: 하이퍼파라미터와 해당 메트릭을 기록합니다.</li>
<li><code>flush()</code>: 보류 중인 모든 이벤트를 디스크에 기록합니다.</li>
<li><code>close()</code>: 로깅을 종료하고 리소스를 해제합니다.</li>
</ul>
<p><strong>주요 콜백 파라미터(텐서플로우/케라스)</strong></p>
<p>텐서플로우/케라스에서 텐서보드를 사용할 때는 <code>tf.keras.callbacks.TensorBoard</code> 콜백을 사용합니다. 주요 파라미터는 다음과 같습니다.</p>
<ul>
<li><code>log_dir</code>: 로그 저장 위치.</li>
<li><code>histogram_freq</code>: 히스토그램 계산 주기 (0이면 계산 안 함). 가중치, 편향, 활성화 값의 분포를 시각화하는 데 사용됩니다.</li>
<li><code>write_graph</code>: 모델 그래프 시각화 여부.</li>
<li><code>write_images</code>: 모델 가중치를 이미지로 시각화할지 여부.</li>
<li><code>update_freq</code>: 손실과 메트릭을 기록하는 주기 (‘batch’, ‘epoch’, 또는 정수).</li>
<li><code>profile_batch</code>: 프로파일링할 배치 범위를 지정 (예: <code>profile_batch='5, 8'</code>). 프로파일링은 성능 병목 현상을 찾는 데 유용합니다.</li>
<li><code>embeddings_freq</code>: 임베딩 레이어를 시각화하는 주기.</li>
<li><code>embeddings_metadata</code>: 임베딩 메타데이터 파일 경로.</li>
</ul>
</section>
</section>
<section id="텐서보드의-주요-시각화-기능" class="level3">
<h3 class="anchored" data-anchor-id="텐서보드의-주요-시각화-기능">3.2.2 텐서보드의 주요 시각화 기능</h3>
<p>텐서보드는 모델 학습 과정에서 발생하는 다양한 지표를 시각화할 수 있습니다. 주요 시각화 대시보드는 스칼라, 히스토그램, 분포, 그래프, 임베딩 등이 있습니다.</p>
<section id="스칼라-지표-시각화" class="level5">
<h5 class="anchored" data-anchor-id="스칼라-지표-시각화">스칼라 지표 시각화</h5>
<p>스칼라 대시보드는 손실값, 정확도와 같은 수치형 지표의 변화를 시각화합니다. 학습률, 그래디언트 노름, 층별 가중치의 평균/분산 등 모델 훈련 과정의 다양한 통계값을 추적할 수 있습니다. 최신 생성 모델에서 중요한 FID(Fréchet Inception Distance) 스코어나 QICE(Quantile Interval Coverage Error)와 같은 품질 평가 지표도 함께 모니터링할 수 있습니다. 이러한 지표들을 통해 모델의 학습 진행 상황을 실시간으로 모니터링하고, 과적합이나 학습 불안정성 등의 문제를 조기에 발견할 수 있습니다. 다음과 같이 스칼라 값을 기록할 수 있습니다.</p>
<div id="cell-77" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb64"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a>writer.add_scalar(<span class="st">'Loss/train'</span>, train_loss, step)</span>
<span id="cb64-2"><a href="#cb64-2" aria-hidden="true" tabindex="-1"></a>writer.add_scalar(<span class="st">'Accuracy/train'</span>, train_acc, step)</span>
<span id="cb64-3"><a href="#cb64-3" aria-hidden="true" tabindex="-1"></a>writer.add_scalar(<span class="st">'Learning/learning_rate'</span>, current_lr, step)</span>
<span id="cb64-4"><a href="#cb64-4" aria-hidden="true" tabindex="-1"></a>writer.add_scalar(<span class="st">'Gradients/norm'</span>, grad_norm, step)</span>
<span id="cb64-5"><a href="#cb64-5" aria-hidden="true" tabindex="-1"></a>writer.add_scalar(<span class="st">'Quality/fid_score'</span>, fid_score, step)</span>
<span id="cb64-6"><a href="#cb64-6" aria-hidden="true" tabindex="-1"></a>writer.add_scalar(<span class="st">'Metrics/qice'</span>, qice_value, step)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="히스토그램과-분포-시각화" class="level5">
<h5 class="anchored" data-anchor-id="히스토그램과-분포-시각화">히스토그램과 분포 시각화</h5>
<p>가중치와 편향의 분포 변화를 관찰할 수 있습니다. 히스토그램은 각 층의 가중치, 편향, 그래디언트, 활성화 값들의 분포를 시각적으로 보여주어 모델의 내부 상태를 이해하는 데 도움을 줍니다. 특히 학습 과정에서 가중치가 특정 값으로 포화되거나, 그래디언트가 소실/폭주하는 문제를 조기에 발견할 수 있어 모델 디버깅에 매우 유용합니다. 다음과 같이 히스토그램을 기록할 수 있습니다.</p>
<div id="cell-79" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb65"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> name, param <span class="kw">in</span> model.named_parameters():</span>
<span id="cb65-2"><a href="#cb65-2" aria-hidden="true" tabindex="-1"></a>    writer.add_histogram(<span class="ss">f'Parameters/</span><span class="sc">{</span>name<span class="sc">}</span><span class="ss">'</span>, param.data, global_step)</span>
<span id="cb65-3"><a href="#cb65-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> param.grad <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb65-4"><a href="#cb65-4" aria-hidden="true" tabindex="-1"></a>        writer.add_histogram(<span class="ss">f'Gradients/</span><span class="sc">{</span>name<span class="sc">}</span><span class="ss">'</span>, param.grad, global_step)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="모델-구조-시각화" class="level5">
<h5 class="anchored" data-anchor-id="모델-구조-시각화">모델 구조 시각화</h5>
<p>모델의 구조를 시각적으로 확인할 수 있습니다. 특히 복잡한 신경망의 계층 구조와 연결을 직관적으로 파악할 수 있습니다. 텐서보드는 계산 그래프를 통해 데이터의 흐름, 각 층의 입출력 형태, 연산 순서 등을 그래프 형태로 표현하며, 각 노드를 확장하여 세부 정보를 검토할 수 있습니다. 최근에는 Transformer나 Diffusion 모델의 복잡한 어텐션 메커니즘, 크로스 어텐션 레이어, 조건부 분기 구조 등을 시각화하는 데 특히 유용합니다. 이는 모델 디버깅과 최적화에 매우 유용하며, 특히 스킵 연결이나 병렬 구조가 있는 복잡한 아키텍처를 이해하는 데 도움이 됩니다. 다음과 같이 모델 그래프를 기록할 수 있습니다.</p>
<div id="cell-81" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb66"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a>writer.add_graph(model, input_to_model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="임베딩-시각화" class="level5">
<h5 class="anchored" data-anchor-id="임베딩-시각화">임베딩 시각화</h5>
<p>텐서보드의 Projector를 사용하면 고차원 임베딩을 2D나 3D 공간에 투영하여 시각화할 수 있습니다. 이는 단어 임베딩이나 이미지 특징 벡터의 관계를 분석하는 데 유용합니다. PCA나 UMAP과 같은 차원 축소 기법을 통해 복잡한 고차원 데이터의 군집 구조와 상대적 거리를 보존하면서 시각화합니다. 특히 UMAP은 국소적 구조와 전역적 구조를 모두 잘 보존하면서 빠른 시각화가 가능합니다. 이를 통해 비슷한 특성을 가진 데이터 포인트들이 어떻게 군집화되는지, 클래스 간의 구분이 잘 이루어지는지 확인할 수 있으며, 학습 과정에서 특징 공간이 어떻게 변화하는지 추적할 수 있습니다. 다음과 같이 임베딩을 기록할 수 있습니다.</p>
<div id="cell-83" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb67"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a>writer.add_embedding(</span>
<span id="cb67-2"><a href="#cb67-2" aria-hidden="true" tabindex="-1"></a>    features,</span>
<span id="cb67-3"><a href="#cb67-3" aria-hidden="true" tabindex="-1"></a>    metadata<span class="op">=</span>labels,</span>
<span id="cb67-4"><a href="#cb67-4" aria-hidden="true" tabindex="-1"></a>    label_img<span class="op">=</span>images,</span>
<span id="cb67-5"><a href="#cb67-5" aria-hidden="true" tabindex="-1"></a>    global_step<span class="op">=</span>step</span>
<span id="cb67-6"><a href="#cb67-6" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="하이퍼파라미터-시각화" class="level5">
<h5 class="anchored" data-anchor-id="하이퍼파라미터-시각화">하이퍼파라미터 시각화</h5>
<p>하이퍼파라미터 튜닝 결과를 시각화할 수 있습니다. 학습률, 배치 크기, 드롭아웃 비율뿐만 아니라, Transformer 모델의 어텐션 헤드 수, 프롬프트 길이, 토큰 임베딩 차원과 같은 구조적 파라미터들의 영향도 분석할 수 있습니다. 최신 LLM이나 Diffusion 모델에서 중요한 노이즈 스케줄링, 샘플링 스텝 수, CFG(Classifier-Free Guidance) 가중치 등의 추론 파라미터도 함께 시각화가 가능합니다. 다양한 하이퍼파라미터 조합에 따른 모델의 성능을 병렬 좌표 그래프나 산점도로 표현하여 최적의 구성을 찾는 데 도움을 줍니다. 특히 여러 실험 결과를 한눈에 비교할 수 있어 하이퍼파라미터 간의 상호작용이 모델 성능에 미치는 영향을 분석하기 용이합니다. 다음과 같이 하이퍼파라미터와 관련 메트릭을 기록할 수 있습니다.</p>
<div id="cell-85" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb68"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a>writer.add_hparams(</span>
<span id="cb68-2"><a href="#cb68-2" aria-hidden="true" tabindex="-1"></a>    {</span>
<span id="cb68-3"><a href="#cb68-3" aria-hidden="true" tabindex="-1"></a>        <span class="st">'lr'</span>: learning_rate, </span>
<span id="cb68-4"><a href="#cb68-4" aria-hidden="true" tabindex="-1"></a>        <span class="st">'batch_size'</span>: batch_size, </span>
<span id="cb68-5"><a href="#cb68-5" aria-hidden="true" tabindex="-1"></a>        <span class="st">'num_heads'</span>: n_heads,</span>
<span id="cb68-6"><a href="#cb68-6" aria-hidden="true" tabindex="-1"></a>        <span class="st">'cfg_scale'</span>: guidance_scale,</span>
<span id="cb68-7"><a href="#cb68-7" aria-hidden="true" tabindex="-1"></a>        <span class="st">'sampling_steps'</span>: num_steps,</span>
<span id="cb68-8"><a href="#cb68-8" aria-hidden="true" tabindex="-1"></a>        <span class="st">'prompt_length'</span>: max_length</span>
<span id="cb68-9"><a href="#cb68-9" aria-hidden="true" tabindex="-1"></a>    },</span>
<span id="cb68-10"><a href="#cb68-10" aria-hidden="true" tabindex="-1"></a>    {</span>
<span id="cb68-11"><a href="#cb68-11" aria-hidden="true" tabindex="-1"></a>        <span class="st">'accuracy'</span>: accuracy, </span>
<span id="cb68-12"><a href="#cb68-12" aria-hidden="true" tabindex="-1"></a>        <span class="st">'loss'</span>: final_loss,</span>
<span id="cb68-13"><a href="#cb68-13" aria-hidden="true" tabindex="-1"></a>        <span class="st">'fid_score'</span>: fid_score</span>
<span id="cb68-14"><a href="#cb68-14" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb68-15"><a href="#cb68-15" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="이미지-시각화" class="level5">
<h5 class="anchored" data-anchor-id="이미지-시각화">이미지 시각화</h5>
<p>학습 과정에서 생성된 이미지나 중간 특징맵을 시각화할 수 있습니다. 컨볼루션 레이어의 필터와 활성화 맵을 시각화하여 모델이 어떤 특징을 학습하고 있는지 직관적으로 이해할 수 있으며, 각 층에서 입력 이미지의 어떤 부분에 주목하는지 확인할 수 있습니다. 특히 Stable Diffusion이나 DALL-E와 같은 최신 생성 모델에서는 생성된 이미지의 품질 변화를 시각적으로 추적할 수 있어 매우 유용합니다. 특히 하이브리드 모델의 등장으로 더욱 정교하고 사실적인 이미지 생성이 가능해졌습니다. 다음과 같이 이미지를 기록할 수 있습니다.</p>
<div id="cell-87" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb69"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 입력 이미지나 생성된 이미지 시각화</span></span>
<span id="cb69-2"><a href="#cb69-2" aria-hidden="true" tabindex="-1"></a>writer.add_images(<span class="st">'Images/generated'</span>, generated_images, global_step)</span>
<span id="cb69-3"><a href="#cb69-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-4"><a href="#cb69-4" aria-hidden="true" tabindex="-1"></a><span class="co"># 디퓨전 모델의 중간 생성 과정 시각화</span></span>
<span id="cb69-5"><a href="#cb69-5" aria-hidden="true" tabindex="-1"></a>writer.add_images(<span class="st">'Diffusion/steps'</span>, diffusion_steps, global_step)</span>
<span id="cb69-6"><a href="#cb69-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-7"><a href="#cb69-7" aria-hidden="true" tabindex="-1"></a><span class="co"># 어텐션 맵 시각화</span></span>
<span id="cb69-8"><a href="#cb69-8" aria-hidden="true" tabindex="-1"></a>writer.add_image(<span class="st">'Attention/maps'</span>, attention_visualization, global_step)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>텐서보드의 시각화 기능을 통해 모델의 학습 과정을 직관적으로 이해하고 문제점을 빠르게 파악할 수 있습니다. 특히 실시간으로 학습 진행 상황을 모니터링할 수 있어 학습 과정의 조기 중단이나 하이퍼파라미터 조정에 유용합니다. 임베딩 시각화는 고차원 데이터의 관계를 이해하는 데 특히 유용하며, 모델이 학습한 특징 공간의 구조를 분석하는 데 도움이 됩니다.</p>
</section>
</section>
<section id="텐서보드-예제" class="level3">
<h3 class="anchored" data-anchor-id="텐서보드-예제">3.2.3 텐서보드 예제</h3>
<p>이 절에서는 앞서 살펴본 텐서보드의 다양한 기능을 실제 딥러닝 모델 훈련에 적용하는 구체적인 예제를 살펴보겠습니다. MNIST 손글씨 숫자 데이터셋을 사용하여 간단한 CNN(Convolutional Neural Network) 모델을 훈련하고, 훈련 과정에서 발생하는 주요 지표와 데이터들을 텐서보드를 통해 시각화하는 방법을 단계별로 설명합니다.</p>
<p><strong>핵심 시각화 요소:</strong></p>
<table class="caption-top table">
<colgroup>
<col style="width: 15%">
<col style="width: 76%">
<col style="width: 8%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">시각화 유형</th>
<th style="text-align: left;">시각화 내용</th>
<th style="text-align: left;">텐서보드 탭</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>스칼라 지표</strong></td>
<td style="text-align: left;">훈련/테스트 손실(loss), 훈련/테스트 정확도(accuracy), 학습률(learning rate), 그래디언트 노름(norm)</td>
<td style="text-align: left;">SCALARS</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>히스토그램/분포</strong></td>
<td style="text-align: left;">모든 레이어의 가중치(weight) 분포, 모든 레이어의 그래디언트(gradient) 분포</td>
<td style="text-align: left;">DISTRIBUTIONS, HISTOGRAMS</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>모델 구조</strong></td>
<td style="text-align: left;">MNIST CNN 모델의 계산 그래프(computational graph)</td>
<td style="text-align: left;">GRAPHS</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>특징 맵</strong></td>
<td style="text-align: left;">Conv1 레이어 특징 맵, Conv2 레이어 특징 맵, 입력 이미지 그리드, Conv1 필터 시각화</td>
<td style="text-align: left;">IMAGES</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>임베딩</strong></td>
<td style="text-align: left;">FC1 층의 32차원 특징 벡터, t-SNE를 이용한 2D 시각화, MNIST 이미지 레이블</td>
<td style="text-align: left;">PROJECTOR</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>하이퍼파라미터</strong></td>
<td style="text-align: left;">배치 크기, 학습률, 드롭아웃 비율, 옵티마이저 종류, Weight decay, Momentum, 스케줄러 스텝/감마</td>
<td style="text-align: left;">HPARAMS</td>
</tr>
</tbody>
</table>
<p><strong>시각화 주기:</strong></p>
<ul>
<li>스칼라/히스토그램: 50 배치(batch)마다</li>
<li>특징 맵/이미지: 50 배치마다</li>
<li>임베딩: 매 에포크(epoch) 종료 시</li>
<li>하이퍼파라미터: 학습 시작과 종료 시</li>
</ul>
<p><strong>코드 예제</strong></p>
<p>이 예제에서는 <code>dld</code> 패키지를 사용합니다. 필요한 모듈을 임포트하고 훈련을 시작합니다. <code>train()</code> 함수는 기본 하이퍼파라미터를 사용하여 MNIST 데이터셋에 대해 CNN 모델을 훈련하고, 훈련 과정을 텐서보드에 기록합니다. 다른 하이퍼파라미터로 실험하려면, <code>train()</code> 함수에 <code>hparams_dict</code> 인자를 전달하면 됩니다.</p>
<div id="cell-91" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb70"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a><span class="co"># In a notebook cell:</span></span>
<span id="cb70-2"><a href="#cb70-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_03.train <span class="im">import</span> train</span>
<span id="cb70-3"><a href="#cb70-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-4"><a href="#cb70-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Run with default hyperparameters</span></span>
<span id="cb70-5"><a href="#cb70-5" aria-hidden="true" tabindex="-1"></a>train()</span>
<span id="cb70-6"><a href="#cb70-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-7"><a href="#cb70-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Run with custom hyperparameters</span></span>
<span id="cb70-8"><a href="#cb70-8" aria-hidden="true" tabindex="-1"></a>my_hparams <span class="op">=</span> {</span>
<span id="cb70-9"><a href="#cb70-9" aria-hidden="true" tabindex="-1"></a>    <span class="st">'batch_size'</span>: <span class="dv">128</span>,</span>
<span id="cb70-10"><a href="#cb70-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">'learning_rate'</span>: <span class="fl">0.01</span>,</span>
<span id="cb70-11"><a href="#cb70-11" aria-hidden="true" tabindex="-1"></a>    <span class="st">'epochs'</span>: <span class="dv">8</span>,</span>
<span id="cb70-12"><a href="#cb70-12" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb70-13"><a href="#cb70-13" aria-hidden="true" tabindex="-1"></a>train(hparams_dict<span class="op">=</span>my_hparams, log_dir<span class="op">=</span><span class="st">'runs/my_custom_run'</span>)</span>
<span id="cb70-14"><a href="#cb70-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-15"><a href="#cb70-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Start TensorBoard (in a separate cell, or from the command line)</span></span>
<span id="cb70-16"><a href="#cb70-16" aria-hidden="true" tabindex="-1"></a><span class="co"># %load_ext tensorboard</span></span>
<span id="cb70-17"><a href="#cb70-17" aria-hidden="true" tabindex="-1"></a><span class="co"># %tensorboard --logdir runs</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><strong>텐서보드 실행:</strong></p>
<p>훈련이 완료되면, 쉘에서 다음 명령을 사용하여 텐서보드를 실행합니다.</p>
<div id="cell-93" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb71"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-2"><a href="#cb71-2" aria-hidden="true" tabindex="-1"></a>tensorboard <span class="op">--</span>logdir runs</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>웹 브라우저에서 <code>http://localhost:6006</code>에 접속하면 텐서보드 대시보드를 볼 수 있습니다.</p>
<p>다음과 같이 각 항목별로 여러 카드들이 생성된 것을 확인할 수 있습니다. <img src="../../../assets/images/03_01.png" class="img-fluid" alt="텐서보드"></p>
<p>각 항목에서 개별 값들의 변화, 이미지들을 확인할 수 있습니다. <img src="../../../assets/images/03_02.png" class="img-fluid" alt="텐서보드"></p>
<p><strong>텐서보드 대시보드 활용</strong></p>
<ul>
<li><strong>SCALARS 탭:</strong> 훈련/테스트 손실, 정확도, 학습률 등의 변화를 시간에 따라 추적합니다. 이를 통해 모델이 잘 학습되고 있는지, 과적합(overfitting)이 발생하고 있는지 등을 파악할 수 있습니다.</li>
<li><strong>GRAPHS 탭:</strong> 모델의 계산 그래프를 시각화하여, 데이터 흐름과 연산 과정을 한눈에 보여줍니다. 복잡한 모델의 구조를 이해하는 데 도움이 됩니다.</li>
<li><strong>DISTRIBUTIONS/HISTOGRAMS 탭:</strong> 가중치와 그래디언트의 분포를 시각화합니다. 이를 통해 가중치 초기화가 적절한지, 그래디언트 소실(vanishing gradients) 또는 폭주(exploding gradients) 문제가 발생하는지 등을 진단할 수 있습니다.</li>
<li><strong>IMAGES 탭:</strong> 입력 이미지, 특징 맵, 필터 등을 이미지 형태로 시각화합니다. 이를 통해 모델이 이미지의 어떤 부분을 보고 판단하는지, 특징 추출이 잘 되는지 등을 직관적으로 확인할 수 있습니다.</li>
<li><strong>PROJECTOR 탭:</strong> 고차원 임베딩을 2D/3D로 투영하여 시각화합니다. 이를 통해 데이터의 군집화, 이상치(outlier) 등을 파악할 수 있습니다.</li>
<li><strong>HPARAMS 탭:</strong> 다양한 하이퍼파라미터 조합으로 실험한 결과를 비교하고, 최적의 설정을 찾는 데 도움을 줍니다.</li>
</ul>
<p>이 예제에서는 텐서보드를 사용하여 딥러닝 모델 훈련 과정을 시각화하는 방법을 살펴보았습니다. 텐서보드는 단순한 시각화 도구를 넘어, 모델의 동작 방식을 이해하고, 문제점을 진단하며, 성능을 개선하는 데 필수적인 도구입니다.</p>
</section>
</section>
<section id="허깅페이스-트랜스포머즈" class="level2">
<h2 class="anchored" data-anchor-id="허깅페이스-트랜스포머즈">3.3 허깅페이스 트랜스포머즈</h2>
<p>허깅페이스는 2016년 프랑스 기업가들이 십대를 위한 챗봇 앱으로 시작했습니다. 초기에는 감정 지원과 엔터테인먼트를 제공하는 AI 친구를 목표로 했으나, 자사 챗봇의 NLP 모델을 오픈소스로 공개하면서 큰 전환점을 맞이했습니다. 이는 당시 BERT, GPT 등 고성능 언어 모델이 등장했지만, 이를 실제로 활용하기 어려웠던 시기와 맞물려 큰 반향을 일으켰습니다.2019년 트랜스포머즈 라이브러리 출시는 자연어 처리 분야에 혁신을 가져왔습니다. 파이토치가 딥러닝의 기초 연산과 학습 프레임워크를 제공한다면, 허깅페이스는 이를 기반으로 실제 언어 모델의 구현과 활용에 초점을 맞췄습니다. 특히 사전 학습된 모델의 공유와 재사용을 용이하게 만들어, 이전까지 소수 대기업의 전유물이었던 대규모 언어 모델을 누구나 활용할 수 있게 했습니다.</p>
<p>허깅페이스는 “AI의 GitHub”라 불릴 만큼 개방적인 생태계를 구축했습니다. 현재 100만 개 이상의 모델과 수십만 개의 데이터셋이 공유되어 있으며, 이는 단순한 코드 저장소를 넘어 윤리적이고 책임있는 AI 개발을 위한 플랫폼으로 발전했습니다. 특히 모델 카드 시스템을 도입해 각 모델의 한계와 편향성을 명시하고, 커뮤니티 기반의 피드백 시스템을 통해 모델의 품질과 윤리성을 지속적으로 검증합니다.이러한 노력은 AI 개발의 민주화를 넘어 책임있는 기술 발전의 새로운 패러다임을 제시했습니다. 허깅페이스의 접근 방식은 기술적 혁신과 윤리적 고려사항을 균형있게 다루며, 이는 현대 AI 개발의 모범 사례로 자리잡았습니다.</p>
<section id="트랜스포머즈-라이브러리-소개" class="level3">
<h3 class="anchored" data-anchor-id="트랜스포머즈-라이브러리-소개">3.3.1 트랜스포머즈 라이브러리 소개</h3>
<p>트랜스포머즈는 사전 훈련된 모델을 쉽게 다운로드하고 사용할 수 있는 통합 인터페이스를 제공합니다. 파이토치나 텐서플로우 같은 프레임워크 위에서 동작하며, 이는 기존 딥러닝 생태계와의 호환성을 보장합니다. 특히 JAX와 같은 새로운 프레임워크도 지원하여 연구자들의 선택의 폭을 넓혔습니다. 트랜스포머즈의 핵심 구성요소는 크게 두 가지입니다.</p>
<section id="모델-허브와-파이프라인" class="level5">
<h5 class="anchored" data-anchor-id="모델-허브와-파이프라인">모델 허브와 파이프라인</h5>
<p>모델 허브는 사전 훈련된 모델들의 중앙 저장소 역할을 합니다. 텍스트 생성, 분류, 번역, 요약, 질의응답 등 다양한 자연어 처리 작업에 특화된 모델들이 공개되어 있습니다. 각 모델은 성능 지표, 라이선스 정보, 학습 데이터 출처 등 상세한 메타데이터와 함께 제공됩니다. 특히 모델 카드(Model Card) 시스템을 통해 모델의 한계와 편향성도 명시하여 책임있는 AI 개발을 장려합니다.</p>
<p>파이프라인은 복잡한 전처리와 후처리 과정을 추상화하여 단순한 인터페이스로 제공합니다. 이는 프로덕션 환경에서 특히 유용하며, 모델 통합 비용을 크게 줄여줍니다. 파이프라인은 내부적으로 토크나이저와 모델을 자동으로 구성하고, 배치 처리나 GPU 가속과 같은 최적화도 자동으로 수행합니다.</p>
<div id="cell-96" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb72"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> pipeline</span>
<span id="cb72-2"><a href="#cb72-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-3"><a href="#cb72-3" aria-hidden="true" tabindex="-1"></a>classifier <span class="op">=</span> pipeline(<span class="st">"sentiment-analysis"</span>)</span>
<span id="cb72-4"><a href="#cb72-4" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> classifier(<span class="st">"I love this book!"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stderr">
<pre><code>No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).
Using a pipeline without specifying a model name and revision in production is not recommended.</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"c6703892f09b4ade869f16b776740536","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"b5210ba6dfe24216ab409ef41d197c2c","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"cc25443102364b8e96035a7c0218e23b","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"4e19048c8971437b82f666267ec92f21","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Device set to use cuda:0</code></pre>
</div>
</div>
</section>
<section id="토크나이저와-모델-클래스" class="level5">
<h5 class="anchored" data-anchor-id="토크나이저와-모델-클래스">토크나이저와 모델 클래스</h5>
<p>토크나이저는 입력 텍스트를 모델이 처리할 수 있는 숫자 시퀀스로 변환합니다. 각 모델은 전용 토크나이저를 가지며, 이는 학습 데이터의 특성을 반영합니다. 토크나이저는 단순한 단어 분리를 넘어 서브워드 토큰화, 특수 토큰 추가, 패딩, 트런케이션 등 복잡한 전처리를 일관되게 처리합니다. 특히 WordPiece, BPE, SentencePiece와 같은 다양한 토큰화 알고리즘을 통합적으로 지원하여, 각 언어와 도메인의 특성에 맞는 최적의 토큰화 방식을 선택할 수 있습니다.</p>
<p>모델 클래스는 실제 연산을 수행하는 신경망을 구현합니다. BERT, GPT, T5와 같은 다양한 아키텍처를 지원하며, AutoModel 계열 클래스를 통해 모델의 아키텍처를 자동으로 선택할 수 있습니다. 각 모델은 pre-trained 가중치와 함께 제공되며, 필요에 따라 특정 태스크에 맞게 파인튜닝할 수 있습니다. 또한 모델 병렬화, 양자화, 프루닝과 같은 최적화 기법도 즉시 적용 가능합니다.</p>
<div id="cell-98" class="cell" data-execution_count="43">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb75"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb75-1"><a href="#cb75-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoTokenizer, AutoModel</span>
<span id="cb75-2"><a href="#cb75-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb75-3"><a href="#cb75-3" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(<span class="st">"bert-base-uncased"</span>)</span>
<span id="cb75-4"><a href="#cb75-4" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> AutoModel.from_pretrained(<span class="st">"bert-base-uncased"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
</section>
<section id="주요-활용-사례" class="level3">
<h3 class="anchored" data-anchor-id="주요-활용-사례">3.3.2 주요 활용 사례</h3>
<p>트랜스포머즈 라이브러리는 다양한 자연어 처리 작업에 활용됩니다. 2020년 이후 GPT 계열 모델의 발전으로 텍스트 생성 능력이 비약적으로 향상되었고, 2024년에는 Llama 3와 같은 고성능 오픈소스 모델의 등장으로 활용 범위가 더욱 확대되었습니다. 특히 Llama 3의 405B 파라미터 모델은 GPT-4와 견줄만한 성능을 보이며, 다국어 처리와 코딩, 추론 능력에서 큰 발전을 이루었습니다. 이러한 발전은 실제 비즈니스 환경에서 다양한 응용을 가능하게 했습니다. 고객 지원, 콘텐츠 생성, 데이터 분석, 자동화된 작업 처리 등 광범위한 분야에서 활용되고 있습니다. 특히 코드 생성과 디버깅 능력이 크게 향상되어 개발자 생산성 향상에도 기여하고 있습니다.</p>
<p><strong>허깅 페이스 허브 활용:</strong></p>
<p>허깅 페이스 허브(<a href="https://huggingface.co/models">https://huggingface.co/models</a>)는 수많은 모델과 데이터셋을 검색하고, 필터링하고, 다운로드할 수 있는 플랫폼입니다.</p>
<ul>
<li><strong>모델 검색:</strong> 왼쪽 상단의 검색창에서 모델 이름(예: “bert”, “gpt2”, “t5”)이나 작업(예: “text-classification”, “question-answering”)으로 검색할 수 있습니다.</li>
<li><strong>필터링:</strong> 왼쪽 패널에서 작업(Task), 라이브러리(Libraries), 언어(Languages), 데이터셋(Datasets) 등 다양한 기준으로 필터링할 수 있습니다.</li>
<li><strong>모델 페이지:</strong> 각 모델 페이지에는 모델 설명, 사용 예제, 성능 지표, 모델 카드 등 유용한 정보가 제공됩니다.</li>
</ul>
<p><strong>텍스트 생성과 분류</strong></p>
<p>텍스트 생성은 주어진 프롬프트를 기반으로 자연스러운 텍스트를 생성하는 작업입니다. 최신 모델들은 다음과 같은 고급 기능을 제공합니다. - 다중 모달 생성: 텍스트와 이미지를 결합한 콘텐츠 생성 - 코드 자동 생성: 프로그래밍 언어별 최적화된 코드 작성 - 대화형 에이전트: 맥락을 이해하는 지능형 챗봇 구현 - 전문 분야 텍스트: 의료, 법률 등 특수 도메인 문서 생성</p>
<div id="cell-100" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb76"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb76-1"><a href="#cb76-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> pipeline</span>
<span id="cb76-2"><a href="#cb76-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-3"><a href="#cb76-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Text generation pipeline (using gpt2 model)</span></span>
<span id="cb76-4"><a href="#cb76-4" aria-hidden="true" tabindex="-1"></a>generator <span class="op">=</span> pipeline(<span class="st">'text-generation'</span>, model<span class="op">=</span><span class="st">'gpt2'</span>)  <span class="co"># Smaller model</span></span>
<span id="cb76-5"><a href="#cb76-5" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> generator(<span class="st">"Design a webpage that"</span>, max_length<span class="op">=</span><span class="dv">50</span>, num_return_sequences<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb76-6"><a href="#cb76-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(result[<span class="dv">0</span>][<span class="st">'generated_text'</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stderr">
<pre><code>Device set to use cuda:0
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Design a webpage that is compatible with your browser with our FREE SEO Service.

You read that right. By utilizing a web browser's default settings, your webpage should be free from advertisements and other types of spam. The best way to avoid this</code></pre>
</div>
</div>
<p>텍스트 분류는 2025년에 더욱 정교화되어, 다음과 같은 기능을 제공합니다.</p>
<ul>
<li>제로샷/퓨샷 학습: 허깅페이스의 트랜스포머 라이브러리를 통해 새로운 카테고리에 대한 즉각적 적응이 가능합니다. 특히 자연어 추론 기반의 사전 학습 모델은 8개 미만의 예제만으로도 90% 이상의 정확도를 달성하며, 다양한 도메인에 적용 가능합니다.</li>
<li>다국어 분류: 허깅페이스의 ModernBERT와 같은 최신 다국어 모델은 16개 이상의 주요 언어를 지원합니다. 특히 150M 파라미터의 base 모델도 80% 이상의 F1 점수를 달성하며, 저자원 언어에서도 우수한 성능을 보입니다.</li>
<li>계층적 분류: 허깅페이스의 HiGen 프레임워크는 계층적 레이블 분류를 위한 특화된 기능을 제공합니다. 레벨 기반 손실 함수를 통해 텍스트와 레이블 간의 의미적 관계를 효과적으로 포착하며, 특히 데이터가 부족한 클래스에서도 높은 성능을 보입니다.</li>
<li>실시간 분류: 허깅페이스 파이프라인을 통해 스트리밍 데이터의 실시간 처리가 가능합니다. Flash Attention과 같은 최적화 기술이 기본 통합되어 있어 긴 시퀀스도 효율적으로 처리할 수 있으며, 특히 실시간 응용에서 높은 처리량을 제공합니다.</li>
</ul>
<section id="파인튜닝과-모델-공유" class="level5">
<h5 class="anchored" data-anchor-id="파인튜닝과-모델-공유">파인튜닝과 모델 공유</h5>
<p>허깅페이스는 최신 파인튜닝 기술을 통합 제공하여 대규모 언어 모델의 효율적인 학습을 지원합니다. 이러한 기술들은 모델의 성능은 유지하면서도 학습 비용과 시간을 크게 절감할 수 있게 해줍니다.</p>
<ul>
<li>QLoRA (Quantized Low-Rank Adaptation): 허깅페이스의 PEFT 라이브러리를 통해 제공되며, 4비트 양자화와 저순위 적응을 결합하여 메모리 사용량을 90% 이상 줄일 수 있습니다. 특히 65B 파라미터 모델도 단일 48GB GPU에서 파인튜닝이 가능합니다.</li>
<li>Spectrum: 허깅페이스 TRL 라이브러리와 통합된 선택적 레이어 최적화 기법입니다. 각 레이어의 신호 대 잡음비를 분석하여 중요도가 높은 레이어만 선택적으로 학습함으로써 계산 효율성을 향상시킵니다.</li>
<li>Flash Attention: 허깅페이스 트랜스포머 2.2 버전부터 기본 지원되며, attn_implementation=“flash_attention_2” 파라미터로 쉽게 활성화할 수 있습니다. 특히 긴 시퀀스 처리에서 메모리 효율성이 크게 향상됩니다.</li>
<li>DeepSpeed: 허깅페이스 Accelerate 라이브러리를 통해 완벽하게 통합되어 있으며, ZeRO 옵티마이저를 통해 대규모 분산 학습을 효율적으로 지원합니다. 특히 추론 시에도 활용 가능하여 큰 모델을 여러 GPU에 분산하여 로드할 수 있습니다.</li>
</ul>
<div id="cell-102" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb79"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb79-1"><a href="#cb79-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments, Trainer, DataCollatorWithPadding</span>
<span id="cb79-2"><a href="#cb79-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> datasets <span class="im">import</span> Dataset</span>
<span id="cb79-3"><a href="#cb79-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb79-4"><a href="#cb79-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb79-5"><a href="#cb79-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-6"><a href="#cb79-6" aria-hidden="true" tabindex="-1"></a><span class="co"># --- 1. Load a pre-trained model and tokenizer ---</span></span>
<span id="cb79-7"><a href="#cb79-7" aria-hidden="true" tabindex="-1"></a>model_name <span class="op">=</span> <span class="st">"distilbert-base-uncased"</span>  <span class="co"># Use a small, fast model</span></span>
<span id="cb79-8"><a href="#cb79-8" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(model_name)</span>
<span id="cb79-9"><a href="#cb79-9" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> AutoModelForSequenceClassification.from_pretrained(model_name, num_labels<span class="op">=</span><span class="dv">2</span>)  <span class="co"># Binary classification</span></span>
<span id="cb79-10"><a href="#cb79-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-11"><a href="#cb79-11" aria-hidden="true" tabindex="-1"></a><span class="co"># --- 2. Create a simple dataset (for demonstration) ---</span></span>
<span id="cb79-12"><a href="#cb79-12" aria-hidden="true" tabindex="-1"></a>raw_data <span class="op">=</span> {</span>
<span id="cb79-13"><a href="#cb79-13" aria-hidden="true" tabindex="-1"></a>    <span class="st">"text"</span>: [</span>
<span id="cb79-14"><a href="#cb79-14" aria-hidden="true" tabindex="-1"></a>        <span class="st">"This is a positive example!"</span>,</span>
<span id="cb79-15"><a href="#cb79-15" aria-hidden="true" tabindex="-1"></a>        <span class="st">"This is a negative example."</span>,</span>
<span id="cb79-16"><a href="#cb79-16" aria-hidden="true" tabindex="-1"></a>        <span class="st">"Another positive one."</span>,</span>
<span id="cb79-17"><a href="#cb79-17" aria-hidden="true" tabindex="-1"></a>        <span class="st">"And a negative one."</span></span>
<span id="cb79-18"><a href="#cb79-18" aria-hidden="true" tabindex="-1"></a>    ],</span>
<span id="cb79-19"><a href="#cb79-19" aria-hidden="true" tabindex="-1"></a>    <span class="st">"label"</span>: [<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>],  <span class="co"># 1 for positive, 0 for negative</span></span>
<span id="cb79-20"><a href="#cb79-20" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb79-21"><a href="#cb79-21" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> Dataset.from_dict(raw_data)</span>
<span id="cb79-22"><a href="#cb79-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-23"><a href="#cb79-23" aria-hidden="true" tabindex="-1"></a><span class="co"># --- 3. Tokenize the dataset ---</span></span>
<span id="cb79-24"><a href="#cb79-24" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> tokenize_function(examples):</span>
<span id="cb79-25"><a href="#cb79-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> tokenizer(examples[<span class="st">"text"</span>], truncation<span class="op">=</span><span class="va">True</span>) <span class="co">#padding is handled by data collator</span></span>
<span id="cb79-26"><a href="#cb79-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-27"><a href="#cb79-27" aria-hidden="true" tabindex="-1"></a>tokenized_dataset <span class="op">=</span> dataset.<span class="bu">map</span>(tokenize_function, batched<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb79-28"><a href="#cb79-28" aria-hidden="true" tabindex="-1"></a>tokenized_dataset <span class="op">=</span> tokenized_dataset.remove_columns([<span class="st">"text"</span>]) <span class="co"># remove text, keep label</span></span>
<span id="cb79-29"><a href="#cb79-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-30"><a href="#cb79-30" aria-hidden="true" tabindex="-1"></a><span class="co"># --- 4. Data Collator (for dynamic padding) ---</span></span>
<span id="cb79-31"><a href="#cb79-31" aria-hidden="true" tabindex="-1"></a>data_collator <span class="op">=</span> DataCollatorWithPadding(tokenizer<span class="op">=</span>tokenizer)</span>
<span id="cb79-32"><a href="#cb79-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-33"><a href="#cb79-33" aria-hidden="true" tabindex="-1"></a><span class="co"># --- 5. Training Arguments ---</span></span>
<span id="cb79-34"><a href="#cb79-34" aria-hidden="true" tabindex="-1"></a>fp16_enabled <span class="op">=</span> <span class="va">False</span></span>
<span id="cb79-35"><a href="#cb79-35" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> torch.cuda.is_available():</span>
<span id="cb79-36"><a href="#cb79-36" aria-hidden="true" tabindex="-1"></a>    <span class="cf">try</span>:</span>
<span id="cb79-37"><a href="#cb79-37" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> torch.cuda.get_device_capability()[<span class="dv">0</span>] <span class="op">&gt;=</span> <span class="dv">7</span>:</span>
<span id="cb79-38"><a href="#cb79-38" aria-hidden="true" tabindex="-1"></a>            fp16_enabled <span class="op">=</span> <span class="va">True</span></span>
<span id="cb79-39"><a href="#cb79-39" aria-hidden="true" tabindex="-1"></a>    <span class="cf">except</span>:</span>
<span id="cb79-40"><a href="#cb79-40" aria-hidden="true" tabindex="-1"></a>        <span class="cf">pass</span></span>
<span id="cb79-41"><a href="#cb79-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-42"><a href="#cb79-42" aria-hidden="true" tabindex="-1"></a>training_args <span class="op">=</span> TrainingArguments(</span>
<span id="cb79-43"><a href="#cb79-43" aria-hidden="true" tabindex="-1"></a>    output_dir<span class="op">=</span><span class="st">"./results"</span>,</span>
<span id="cb79-44"><a href="#cb79-44" aria-hidden="true" tabindex="-1"></a>    num_train_epochs<span class="op">=</span><span class="dv">1</span>,          <span class="co"># Keep it short</span></span>
<span id="cb79-45"><a href="#cb79-45" aria-hidden="true" tabindex="-1"></a>    per_device_train_batch_size<span class="op">=</span><span class="dv">2</span>,  <span class="co"># Small batch size</span></span>
<span id="cb79-46"><a href="#cb79-46" aria-hidden="true" tabindex="-1"></a>    logging_steps<span class="op">=</span><span class="dv">1</span>,           <span class="co"># Log every step</span></span>
<span id="cb79-47"><a href="#cb79-47" aria-hidden="true" tabindex="-1"></a>    save_strategy<span class="op">=</span><span class="st">"no"</span>,         <span class="co"># No saving</span></span>
<span id="cb79-48"><a href="#cb79-48" aria-hidden="true" tabindex="-1"></a>    report_to<span class="op">=</span><span class="st">"none"</span>,          <span class="co"># No reporting</span></span>
<span id="cb79-49"><a href="#cb79-49" aria-hidden="true" tabindex="-1"></a>    fp16<span class="op">=</span>fp16_enabled,  <span class="co"># Use fp16 if avail.</span></span>
<span id="cb79-50"><a href="#cb79-50" aria-hidden="true" tabindex="-1"></a>    <span class="co"># --- Optimization techniques (demonstration) ---</span></span>
<span id="cb79-51"><a href="#cb79-51" aria-hidden="true" tabindex="-1"></a>    <span class="co"># gradient_checkpointing=True,  # Enable gradient checkpointing (if needed for large models)</span></span>
<span id="cb79-52"><a href="#cb79-52" aria-hidden="true" tabindex="-1"></a>    <span class="co"># gradient_accumulation_steps=2, # Increase effective batch size</span></span>
<span id="cb79-53"><a href="#cb79-53" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb79-54"><a href="#cb79-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-55"><a href="#cb79-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-56"><a href="#cb79-56" aria-hidden="true" tabindex="-1"></a><span class="co"># --- 6. Trainer ---</span></span>
<span id="cb79-57"><a href="#cb79-57" aria-hidden="true" tabindex="-1"></a>trainer <span class="op">=</span> Trainer(</span>
<span id="cb79-58"><a href="#cb79-58" aria-hidden="true" tabindex="-1"></a>    model<span class="op">=</span>model,</span>
<span id="cb79-59"><a href="#cb79-59" aria-hidden="true" tabindex="-1"></a>    args<span class="op">=</span>training_args,</span>
<span id="cb79-60"><a href="#cb79-60" aria-hidden="true" tabindex="-1"></a>    train_dataset<span class="op">=</span>tokenized_dataset,</span>
<span id="cb79-61"><a href="#cb79-61" aria-hidden="true" tabindex="-1"></a>    <span class="co"># eval_dataset=...,  # Add an eval dataset if you have one</span></span>
<span id="cb79-62"><a href="#cb79-62" aria-hidden="true" tabindex="-1"></a>    data_collator<span class="op">=</span>data_collator,  <span class="co"># Use the data collator</span></span>
<span id="cb79-63"><a href="#cb79-63" aria-hidden="true" tabindex="-1"></a>    <span class="co"># optimizers=(optimizer, scheduler) # you could also customize optimizer</span></span>
<span id="cb79-64"><a href="#cb79-64" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb79-65"><a href="#cb79-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-66"><a href="#cb79-66" aria-hidden="true" tabindex="-1"></a><span class="co"># --- 7. Train ---</span></span>
<span id="cb79-67"><a href="#cb79-67" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Starting training..."</span>)</span>
<span id="cb79-68"><a href="#cb79-68" aria-hidden="true" tabindex="-1"></a>trainer.train()</span>
<span id="cb79-69"><a href="#cb79-69" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Training finished!"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"ad7aa580feaf4d5fa3abcd96b1bc43e3","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"24731acb95cb4dbea5d194f768b17df3","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"ee9456452ccb4910849e2973a1765462","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"688ecbdc102a4cc6b562c911f79851c0","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"62d43521fee04e07a6ce10a5488d2b38","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"c2522a9d78974c45beeb8575e3c29e85","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Starting training...</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/home/sean/anaconda3/envs/DL/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn(</code></pre>
</div>
<div class="cell-output cell-output-display">

    <div>
      
      <progress value="1" max="1" style="width:300px; height:20px; vertical-align: middle;"></progress>
      [1/1 00:00, Epoch 1/1]
    </div>
    
<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">Step</th>
<th data-quarto-table-cell-role="th">Training Loss</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>0.667500</td>
</tr>
</tbody>
</table>
<p>
</p></div>
<div class="cell-output cell-output-stdout">
<pre><code>Training finished!</code></pre>
</div>
</div>
<p>모델 공유 생태계는 2025년 현재 다음과 같은 최신 기능을 지원합니다. - 모델 카드 자동 생성: 허깅페이스의 자동화된 모델 카드 시스템은 성능 지표와 편향성을 자동으로 분석하고 문서화합니다. 특히 Model Card Toolkit을 통해 표준화된 형식으로 모델의 특성과 한계점을 명확하게 기술할 수 있습니다. - 버전 관리: 허깅페이스 허브의 Git 기반 버전 관리 시스템을 통해 모델의 변경 이력과 성능 변화를 추적합니다. 각 버전별 성능 메트릭과 파라미터 변화를 자동으로 기록하고 비교할 수 있습니다. - 협업 도구: 허깅페이스 스페이스와 통합된 협업 환경을 제공합니다. 팀원들은 모델 개발, 테스트, 배포 과정을 실시간으로 공유하고 피드백을 주고받을 수 있으며, CI/CD 파이프라인과의 연동도 지원합니다. - 윤리적 AI: 허깅페이스의 윤리적 AI 프레임워크를 통해 모델의 편향성을 자동으로 검증하고 평가합니다. 특히 다양한 인구 통계학적 그룹에 대한 성능 차이를 분석하고, 잠재적 위험을 사전에 식별할 수 있습니다.</p>
</section>
</section>
</section>
<section id="연습문제" class="level2">
<h2 class="anchored" data-anchor-id="연습문제">연습문제</h2>
<p><strong>1. 기초 문제</strong></p>
<ul>
<li>파이토치 텐서와 넘파이 배열의 차이점과 상호 변환 방법을 설명하세요.</li>
<li><code>torch.nn.Linear</code> 레이어의 역할과 가중치 초기화 방법을 설명하세요.</li>
<li>파이토치에서 자동 미분(automatic differentiation)이 어떻게 동작하는지 설명하고, <code>requires_grad</code> 속성의 역할을 설명하세요.</li>
</ul>
<p><strong>2. 응용 문제</strong></p>
<ul>
<li>주어진 데이터셋을 <code>torch.utils.data.Dataset</code>과 <code>torch.utils.data.DataLoader</code>를 사용하여 훈련/검증/테스트용으로 분할하고, 배치 단위로 데이터를 로드하는 코드를 작성하세요.</li>
<li><code>nn.Module</code>을 상속받아 간단한 CNN 모델(예: LeNet-5)을 구현하고, <code>torchsummary</code>를 사용하여 모델의 구조와 파라미터 수를 확인하세요.</li>
<li>MNIST 또는 Fashion-MNIST 데이터셋을 사용하여 모델을 훈련하고, 텐서보드를 사용하여 훈련 과정(손실, 정확도 등)을 시각화하세요.</li>
</ul>
<p><strong>3. 심화 문제</strong></p>
<ul>
<li><code>torch.einsum</code>을 사용하여 행렬 곱셈, 전치, 배치 행렬 곱셈, 양선형 변환(bilinear transformation) 등을 구현해보세요. (각 연산에 대한 아인슈타인 표기법을 제시하고, 파이토치 코드로 구현하세요.)</li>
<li>커스텀 데이터셋을 만들고, <code>torchvision.transforms</code>를 사용하여 데이터 증강(data augmentation)을 적용하는 코드를 작성하세요. (예: 이미지 회전, 자르기, 색상 변환 등)</li>
<li><code>torch.autograd.grad</code>를 사용하여 고계도함수(higher-order derivatives)를 계산하는 방법을 설명하고, 간단한 예제 코드를 작성하세요. (예: 헤시안 행렬 계산)</li>
<li><code>torch.nn.Module</code>의 <code>forward()</code> 메서드를 직접 호출하지 않고, 모델 객체를 함수처럼 호출하는 이유를 설명하세요. (힌트: <code>__call__</code> 메서드와 자동 미분 시스템과의 관계)</li>
</ul>
<div class="callout callout-style-default callout-note callout-titled" title="클릭하여 내용 보기(해답)">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
클릭하여 내용 보기(해답)
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<section id="연습문제-해답" class="level2 callout-body-container callout-body">
<h2 class="anchored" data-anchor-id="연습문제-해답">연습문제 해답</h2>
<section id="기초-문제-해답" class="level3">
<h3 class="anchored" data-anchor-id="기초-문제-해답">1. 기초 문제 해답</h3>
<ol type="1">
<li><strong>텐서 vs.&nbsp;넘파이 배열:</strong>
<ul>
<li><strong>차이점:</strong> 텐서는 GPU 가속, 자동 미분 지원. 넘파이는 CPU 기반 범용 배열 연산.</li>
<li><strong>변환:</strong> <code>torch.from_numpy()</code>, <code>.numpy()</code> (단, GPU 텐서는 <code>.cpu()</code> 선행).</li>
</ul>
<div class="sourceCode" id="cb84"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb84-1"><a href="#cb84-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 예시</span></span>
<span id="cb84-2"><a href="#cb84-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb84-3"><a href="#cb84-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb84-4"><a href="#cb84-4" aria-hidden="true" tabindex="-1"></a>numpy_array <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>])</span>
<span id="cb84-5"><a href="#cb84-5" aria-hidden="true" tabindex="-1"></a>torch_tensor <span class="op">=</span> torch.from_numpy(numpy_array)  <span class="co"># 또는 torch.tensor()</span></span>
<span id="cb84-6"><a href="#cb84-6" aria-hidden="true" tabindex="-1"></a>numpy_back <span class="op">=</span> torch_tensor.cpu().numpy()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li><strong><code>nn.Linear</code>:</strong>
<ul>
<li><strong>역할:</strong> <code>y = xW^T + b</code> (선형 변환). 입력 <code>x</code>에 가중치 <code>W</code>를 곱하고 편향 <code>b</code>를 더함.</li>
<li><strong>초기화:</strong> 기본적으로 Kaiming He 초기화 (균등 분포). <code>torch.nn.init</code> 모듈로 변경 가능.</li>
</ul>
<div class="sourceCode" id="cb85"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb85-1"><a href="#cb85-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 예시</span></span>
<span id="cb85-2"><a href="#cb85-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb85-3"><a href="#cb85-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.init <span class="im">as</span> init</span>
<span id="cb85-4"><a href="#cb85-4" aria-hidden="true" tabindex="-1"></a>linear_layer <span class="op">=</span> nn.Linear(in_features<span class="op">=</span><span class="dv">10</span>, out_features<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb85-5"><a href="#cb85-5" aria-hidden="true" tabindex="-1"></a>init.xavier_uniform_(linear_layer.weight) <span class="co"># Xavier 초기화</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li><strong>자동 미분 (Autograd):</strong>
<ul>
<li><strong>동작:</strong> <code>requires_grad=True</code> 텐서 연산 시 계산 그래프 생성, <code>.backward()</code> 호출 시 연쇄 법칙으로 그래디언트 계산.</li>
<li><strong><code>requires_grad</code>:</strong> 그래디언트 계산 및 추적 여부 설정.</li>
</ul>
<div class="sourceCode" id="cb86"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb86-1"><a href="#cb86-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 예시</span></span>
<span id="cb86-2"><a href="#cb86-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb86-3"><a href="#cb86-3" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.tensor([<span class="fl">2.0</span>], requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb86-4"><a href="#cb86-4" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> x<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> <span class="dv">3</span><span class="op">*</span>x <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb86-5"><a href="#cb86-5" aria-hidden="true" tabindex="-1"></a>y.backward()</span>
<span id="cb86-6"><a href="#cb86-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(x.grad)  <span class="co"># 출력: tensor([7.])</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
</ol>
</section>
<section id="응용-문제-해답" class="level3">
<h3 class="anchored" data-anchor-id="응용-문제-해답">2. 응용 문제 해답</h3>
<ol start="4" type="1">
<li><p><strong><code>Dataset</code>, <code>DataLoader</code>:</strong></p>
<div class="sourceCode" id="cb87"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb87-1"><a href="#cb87-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> Dataset, DataLoader, random_split</span>
<span id="cb87-2"><a href="#cb87-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torchvision.transforms <span class="im">as</span> transforms</span>
<span id="cb87-3"><a href="#cb87-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision <span class="im">import</span> datasets</span>
<span id="cb87-4"><a href="#cb87-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-5"><a href="#cb87-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Custom Dataset (예시)</span></span>
<span id="cb87-6"><a href="#cb87-6" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> CustomDataset(Dataset):</span>
<span id="cb87-7"><a href="#cb87-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, data, targets, transform<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb87-8"><a href="#cb87-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.data <span class="op">=</span> data</span>
<span id="cb87-9"><a href="#cb87-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.targets <span class="op">=</span> targets</span>
<span id="cb87-10"><a href="#cb87-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.transform <span class="op">=</span> transform</span>
<span id="cb87-11"><a href="#cb87-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__len__</span>(<span class="va">self</span>):</span>
<span id="cb87-12"><a href="#cb87-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">len</span>(<span class="va">self</span>.data)</span>
<span id="cb87-13"><a href="#cb87-13" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__getitem__</span>(<span class="va">self</span>, idx):</span>
<span id="cb87-14"><a href="#cb87-14" aria-hidden="true" tabindex="-1"></a>        sample, label <span class="op">=</span> <span class="va">self</span>.data[idx], <span class="va">self</span>.targets[idx]</span>
<span id="cb87-15"><a href="#cb87-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.transform:</span>
<span id="cb87-16"><a href="#cb87-16" aria-hidden="true" tabindex="-1"></a>            sample <span class="op">=</span> <span class="va">self</span>.transform(sample)</span>
<span id="cb87-17"><a href="#cb87-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> sample, label</span>
<span id="cb87-18"><a href="#cb87-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-19"><a href="#cb87-19" aria-hidden="true" tabindex="-1"></a><span class="co"># MNIST DataLoader 예시 (torchvision 활용)</span></span>
<span id="cb87-20"><a href="#cb87-20" aria-hidden="true" tabindex="-1"></a>transform <span class="op">=</span> transforms.ToTensor() <span class="co"># 이미지데이터를 텐서로</span></span>
<span id="cb87-21"><a href="#cb87-21" aria-hidden="true" tabindex="-1"></a>mnist_dataset <span class="op">=</span> datasets.MNIST(root<span class="op">=</span><span class="st">'./data'</span>, train<span class="op">=</span><span class="va">True</span>, download<span class="op">=</span><span class="va">True</span>, transform<span class="op">=</span>transform)</span>
<span id="cb87-22"><a href="#cb87-22" aria-hidden="true" tabindex="-1"></a>train_size <span class="op">=</span> <span class="bu">int</span>(<span class="fl">0.8</span> <span class="op">*</span> <span class="bu">len</span>(mnist_dataset))</span>
<span id="cb87-23"><a href="#cb87-23" aria-hidden="true" tabindex="-1"></a>val_size <span class="op">=</span> <span class="bu">len</span>(mnist_dataset) <span class="op">-</span> train_size</span>
<span id="cb87-24"><a href="#cb87-24" aria-hidden="true" tabindex="-1"></a>train_dataset, val_dataset <span class="op">=</span> random_split(mnist_dataset, [train_size, val_size])</span>
<span id="cb87-25"><a href="#cb87-25" aria-hidden="true" tabindex="-1"></a>train_loader <span class="op">=</span> DataLoader(train_dataset, batch_size<span class="op">=</span><span class="dv">64</span>, shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb87-26"><a href="#cb87-26" aria-hidden="true" tabindex="-1"></a>val_loader <span class="op">=</span> DataLoader(val_dataset, batch_size<span class="op">=</span><span class="dv">32</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li><p><strong>LeNet-5, <code>torchsummary</code>, 텐서보드:</strong> (전체 코드는 이전 답변 참고, 여기서는 핵심 부분만)</p>
<div class="sourceCode" id="cb88"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb88-1"><a href="#cb88-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb88-2"><a href="#cb88-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb88-3"><a href="#cb88-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchsummary <span class="im">import</span> summary</span>
<span id="cb88-4"><a href="#cb88-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.tensorboard <span class="im">import</span> SummaryWriter</span>
<span id="cb88-5"><a href="#cb88-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-6"><a href="#cb88-6" aria-hidden="true" tabindex="-1"></a><span class="co"># LeNet-5 모델</span></span>
<span id="cb88-7"><a href="#cb88-7" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LeNet5(nn.Module):</span>
<span id="cb88-8"><a href="#cb88-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb88-9"><a href="#cb88-9" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(LeNet5, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb88-10"><a href="#cb88-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv1 <span class="op">=</span> nn.Conv2d(<span class="dv">1</span>, <span class="dv">6</span>, kernel_size<span class="op">=</span><span class="dv">5</span>, stride<span class="op">=</span><span class="dv">1</span>, padding<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb88-11"><a href="#cb88-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.pool1 <span class="op">=</span> nn.AvgPool2d(kernel_size<span class="op">=</span><span class="dv">2</span>, stride<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb88-12"><a href="#cb88-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv2 <span class="op">=</span> nn.Conv2d(<span class="dv">6</span>, <span class="dv">16</span>, kernel_size<span class="op">=</span><span class="dv">5</span>, stride<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb88-13"><a href="#cb88-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.pool2 <span class="op">=</span> nn.AvgPool2d(kernel_size<span class="op">=</span><span class="dv">2</span>, stride<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb88-14"><a href="#cb88-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc1 <span class="op">=</span> nn.Linear(<span class="dv">16</span> <span class="op">*</span> <span class="dv">5</span> <span class="op">*</span> <span class="dv">5</span>, <span class="dv">120</span>)</span>
<span id="cb88-15"><a href="#cb88-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc2 <span class="op">=</span> nn.Linear(<span class="dv">120</span>, <span class="dv">84</span>)</span>
<span id="cb88-16"><a href="#cb88-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc3 <span class="op">=</span> nn.Linear(<span class="dv">84</span>, <span class="dv">10</span>)</span>
<span id="cb88-17"><a href="#cb88-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-18"><a href="#cb88-18" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb88-19"><a href="#cb88-19" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> F.relu(<span class="va">self</span>.conv1(x))</span>
<span id="cb88-20"><a href="#cb88-20" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.pool1(x)</span>
<span id="cb88-21"><a href="#cb88-21" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> F.relu(<span class="va">self</span>.conv2(x))</span>
<span id="cb88-22"><a href="#cb88-22" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.pool2(x)</span>
<span id="cb88-23"><a href="#cb88-23" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.view(x.size(<span class="dv">0</span>), <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb88-24"><a href="#cb88-24" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> F.relu(<span class="va">self</span>.fc1(x))</span>
<span id="cb88-25"><a href="#cb88-25" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> F.relu(<span class="va">self</span>.fc2(x))</span>
<span id="cb88-26"><a href="#cb88-26" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.fc3(x)</span>
<span id="cb88-27"><a href="#cb88-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb88-28"><a href="#cb88-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-29"><a href="#cb88-29" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LeNet5()</span>
<span id="cb88-30"><a href="#cb88-30" aria-hidden="true" tabindex="-1"></a>summary(model, input_size<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">28</span>, <span class="dv">28</span>)) <span class="co"># 모델 구조 요약</span></span>
<span id="cb88-31"><a href="#cb88-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-32"><a href="#cb88-32" aria-hidden="true" tabindex="-1"></a><span class="co"># ... (훈련 코드, 이전 답변 참고) ...</span></span>
<span id="cb88-33"><a href="#cb88-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-34"><a href="#cb88-34" aria-hidden="true" tabindex="-1"></a>writer <span class="op">=</span> SummaryWriter() <span class="co"># 텐서보드</span></span>
<span id="cb88-35"><a href="#cb88-35" aria-hidden="true" tabindex="-1"></a><span class="co"># ... (훈련 중 writer.add_scalar() 등으로 로깅) ...</span></span>
<span id="cb88-36"><a href="#cb88-36" aria-hidden="true" tabindex="-1"></a>writer.close()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
</ol>
</section>
<section id="심화-문제-해답" class="level3">
<h3 class="anchored" data-anchor-id="심화-문제-해답">3. 심화 문제 해답</h3>
<ol start="6" type="1">
<li><p><strong><code>torch.einsum</code>:</strong></p>
<div class="sourceCode" id="cb89"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb89-1"><a href="#cb89-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb89-2"><a href="#cb89-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb89-3"><a href="#cb89-3" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> torch.randn(<span class="dv">3</span>, <span class="dv">4</span>)</span>
<span id="cb89-4"><a href="#cb89-4" aria-hidden="true" tabindex="-1"></a>B <span class="op">=</span> torch.randn(<span class="dv">4</span>, <span class="dv">5</span>)</span>
<span id="cb89-5"><a href="#cb89-5" aria-hidden="true" tabindex="-1"></a>C <span class="op">=</span> torch.einsum(<span class="st">"ij,jk-&gt;ik"</span>, A, B)   <span class="co"># 행렬 곱셈</span></span>
<span id="cb89-6"><a href="#cb89-6" aria-hidden="true" tabindex="-1"></a>D <span class="op">=</span> torch.einsum(<span class="st">"ij-&gt;ji"</span>, A)        <span class="co"># 전치</span></span>
<span id="cb89-7"><a href="#cb89-7" aria-hidden="true" tabindex="-1"></a>E <span class="op">=</span> torch.einsum(<span class="st">"bi,bj,ijk-&gt;bk"</span>, A, B, torch.randn(<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>))  <span class="co"># 양선형 변환</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li><p><strong>커스텀 데이터셋, 데이터 증강:</strong></p>
<div class="sourceCode" id="cb90"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb90-1"><a href="#cb90-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> Dataset</span>
<span id="cb90-2"><a href="#cb90-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision <span class="im">import</span> transforms</span>
<span id="cb90-3"><a href="#cb90-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> Image</span>
<span id="cb90-4"><a href="#cb90-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb90-5"><a href="#cb90-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb90-6"><a href="#cb90-6" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> CustomImageDataset(Dataset): <span class="co"># Dataset 상속</span></span>
<span id="cb90-7"><a href="#cb90-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, root_dir, transform<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb90-8"><a href="#cb90-8" aria-hidden="true" tabindex="-1"></a>        <span class="co"># ... (생성자 구현) ...</span></span>
<span id="cb90-9"><a href="#cb90-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">pass</span></span>
<span id="cb90-10"><a href="#cb90-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__len__</span>(<span class="va">self</span>):</span>
<span id="cb90-11"><a href="#cb90-11" aria-hidden="true" tabindex="-1"></a>        <span class="co"># ... (데이터 개수 반환) ...</span></span>
<span id="cb90-12"><a href="#cb90-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">pass</span></span>
<span id="cb90-13"><a href="#cb90-13" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__getitem__</span>(<span class="va">self</span>, idx):</span>
<span id="cb90-14"><a href="#cb90-14" aria-hidden="true" tabindex="-1"></a>        <span class="co"># ... (idx에 해당하는 샘플 반환) ...</span></span>
<span id="cb90-15"><a href="#cb90-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">pass</span></span>
<span id="cb90-16"><a href="#cb90-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb90-17"><a href="#cb90-17" aria-hidden="true" tabindex="-1"></a><span class="co"># 데이터 증강</span></span>
<span id="cb90-18"><a href="#cb90-18" aria-hidden="true" tabindex="-1"></a>transform <span class="op">=</span> transforms.Compose([</span>
<span id="cb90-19"><a href="#cb90-19" aria-hidden="true" tabindex="-1"></a>    transforms.RandomResizedCrop(<span class="dv">224</span>),  <span class="co"># 무작위 크기 및 비율로 자르기</span></span>
<span id="cb90-20"><a href="#cb90-20" aria-hidden="true" tabindex="-1"></a>    transforms.RandomHorizontalFlip(),     <span class="co"># 수평 뒤집기</span></span>
<span id="cb90-21"><a href="#cb90-21" aria-hidden="true" tabindex="-1"></a>    transforms.ToTensor(),              <span class="co"># 텐서로 변환</span></span>
<span id="cb90-22"><a href="#cb90-22" aria-hidden="true" tabindex="-1"></a>    transforms.Normalize(mean<span class="op">=</span>[<span class="fl">0.485</span>, <span class="fl">0.456</span>, <span class="fl">0.406</span>], std<span class="op">=</span>[<span class="fl">0.229</span>, <span class="fl">0.224</span>, <span class="fl">0.225</span>]) <span class="co"># 정규화</span></span>
<span id="cb90-23"><a href="#cb90-23" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb90-24"><a href="#cb90-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb90-25"><a href="#cb90-25" aria-hidden="true" tabindex="-1"></a><span class="co"># dataset = CustomImageDataset(root_dir='path/to/images', transform=transform)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li><p><strong>고계도함수:</strong></p>
<div class="sourceCode" id="cb91"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb91-1"><a href="#cb91-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb91-2"><a href="#cb91-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-3"><a href="#cb91-3" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.tensor(<span class="fl">2.0</span>, requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb91-4"><a href="#cb91-4" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> x<span class="op">**</span><span class="dv">3</span></span>
<span id="cb91-5"><a href="#cb91-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-6"><a href="#cb91-6" aria-hidden="true" tabindex="-1"></a><span class="co"># 1차 미분</span></span>
<span id="cb91-7"><a href="#cb91-7" aria-hidden="true" tabindex="-1"></a>first_derivative <span class="op">=</span> torch.autograd.grad(y, x, create_graph<span class="op">=</span><span class="va">True</span>)[<span class="dv">0</span>]  <span class="co"># create_graph=True</span></span>
<span id="cb91-8"><a href="#cb91-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(first_derivative)</span>
<span id="cb91-9"><a href="#cb91-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-10"><a href="#cb91-10" aria-hidden="true" tabindex="-1"></a><span class="co"># 2차 미분 (헤시안)</span></span>
<span id="cb91-11"><a href="#cb91-11" aria-hidden="true" tabindex="-1"></a>second_derivative <span class="op">=</span> torch.autograd.grad(first_derivative, x)[<span class="dv">0</span>]</span>
<span id="cb91-12"><a href="#cb91-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(second_derivative)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li><p><strong><code>__call__</code> 메서드:</strong></p></li>
</ol>
<p><code>nn.Module</code>의 <code>__call__</code> 메서드는 <code>forward()</code>를 호출하기 <em>전후</em>에 추가적인 작업(hook 등록, 자동 미분 관련 설정 등)을 수행합니다. 단순히 <code>forward()</code>를 직접 호출하면 이러한 기능들이 생략되어, 올바른 그래디언트 계산이 이루어지지 않거나, 모델의 다른 기능(예: <code>nn.Module</code>의 <code>training</code> 속성 설정)이 제대로 작동하지 않을 수 있습니다. 따라서 <em>반드시</em> 모델 객체를 함수처럼 호출(<code>model(input)</code>)해야 합니다.</p>
</section>
</section>
</div>
</div>
<p><strong>참고 자료</strong></p>
<ol type="1">
<li><strong>PyTorch 공식 튜토리얼:</strong> <a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://pytorch.org/tutorials/">https://pytorch.org/tutorials/</a></li>
<li><strong>Deep Learning with PyTorch (Stevens, Antiga, Viehmann, 2020):</strong> <a href="https://www.google.com/search?q=https://pytorch.org/deep-learning-with-pytorch">https://pytorch.org/deep-learning-with-pytorch</a></li>
<li><strong>Programming PyTorch for Deep Learning (Delugach, 2023):</strong> <a href="https://www.google.com/search?q=https://www.oreilly.com/library/view/programming-pytorch-for/9781098142481/">https://www.oreilly.com/library/view/programming-pytorch-for/9781098142481/</a></li>
<li><strong>PyTorch Recipes (Kalyan, 2019):</strong> <a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://pytorch.org/tutorials/recipes/recipes_index.html">https://pytorch.org/tutorials/recipes/recipes_index.html</a></li>
<li><strong>Understanding the difficulty of training deep feedforward neural networks (Glorot &amp; Bengio, 2010):</strong> <a href="http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf">http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf</a></li>
<li><strong>Fastai library:</strong> <a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://docs.fast.ai/">https://docs.fast.ai/</a></li>
<li><strong>PyTorch Lightning:</strong> <a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://www.pytorchlightning.ai/">https://www.pytorchlightning.ai/</a></li>
<li><strong>Hugging Face Transformers documentation:</strong> <a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://huggingface.co/docs/transformers/index">https://huggingface.co/docs/transformers/index</a></li>
<li><strong>TensorBoard documentation:</strong> <a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://www.tensorflow.org/tensorboard">https://www.tensorflow.org/tensorboard</a></li>
<li><strong>Weights &amp; Biases documentation:</strong> <a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://docs.wandb.ai/">https://docs.wandb.ai/</a></li>
</ol>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>