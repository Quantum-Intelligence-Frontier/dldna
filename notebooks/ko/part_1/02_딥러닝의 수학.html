<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>quarto-input347e87cbe0f7bb0c – Deep Learning DNA: Surviving Architectures and Essential Principles</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap-f507c7d0488cb7630e20aad62ad8c2aa.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script>window.MathJax = {loader: {load: ['[tex]/boldsymbol']},tex: {packages: {'[+]': ['boldsymbol']}}};</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../../notebooks/ko/part_1/01_딥러닝의 시작.html">part_1</a></li><li class="breadcrumb-item"><a href="../../../notebooks/ko/part_1/02_딥러닝의 수학.html">2. 딥러닝의 수학</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../../../">한국어</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Language</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index_de.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Deutsch</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index_en.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">English</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index_es.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Español</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">한국어</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index_zh.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">中文</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/ko/part_1/00_서론.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">서론</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">part_1</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/ko/part_1/01_딥러닝의 시작.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">1. 딥러닝의 시작</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/ko/part_1/02_딥러닝의 수학.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">2. 딥러닝의 수학</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/ko/part_1/03_딥러닝프레임워크.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">3. 딥러닝프레임워크</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/ko/part_1/04_활성화함수.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">4. 활성화함수</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/ko/part_1/05_최적화와 시각화.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">5. 최적화와 시각화</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/ko/part_1/06_과적합과 해결 기법의 발전.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">6. 과적합과 해결 기법의 발전</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/ko/part_1/07_합성곱 신경망의 진화.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">7. 합성곱 신경망의 진화</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/ko/part_1/08_트랜스포머의 탄생.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">8. 트랜스포머의 탄생</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/ko/part_1/09_트랜스포머의 진화.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">9. 트랜스포머의 진화</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/ko/part_1/10_멀티모달 딥러닝: 다중 감각 융합의 시작.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">10. 멀티모달 딥러닝: 다중 감각 융합의 시작</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/ko/part_1/11_멀티모달 딥러닝: 한계를 넘어선 지능.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">11. 멀티모달 딥러닝: 한계를 넘어선 지능</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">딥러닝의 최전선</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/ko/딥러닝의 최전선/01_SLM: 작지만 강력한 언어모델.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">1. SLM: 작지만 강력한 언어모델</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/ko/딥러닝의 최전선/02_자율주행.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">2. 자율주행</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#딥-러닝의-수학" id="toc-딥-러닝의-수학" class="nav-link active" data-scroll-target="#딥-러닝의-수학">2. 딥 러닝의 수학</a>
  <ul class="collapse">
  <li><a href="#선형대수학-기초" id="toc-선형대수학-기초" class="nav-link" data-scroll-target="#선형대수학-기초">2.1 선형대수학 기초</a>
  <ul class="collapse">
  <li><a href="#벡터" id="toc-벡터" class="nav-link" data-scroll-target="#벡터">2.1.1 벡터</a></li>
  <li><a href="#차원-랭크" id="toc-차원-랭크" class="nav-link" data-scroll-target="#차원-랭크">2.1.2 차원, 랭크</a></li>
  <li><a href="#선형변환의-기본" id="toc-선형변환의-기본" class="nav-link" data-scroll-target="#선형변환의-기본">2.1.3 선형변환의 기본</a></li>
  <li><a href="#텐서연산" id="toc-텐서연산" class="nav-link" data-scroll-target="#텐서연산">2.1.4 텐서연산</a></li>
  <li><a href="#특이값-분해와-주성분-분석" id="toc-특이값-분해와-주성분-분석" class="nav-link" data-scroll-target="#특이값-분해와-주성분-분석">2.1.5 특이값 분해와 주성분 분석</a></li>
  </ul></li>
  <li><a href="#미적분학과-최적화" id="toc-미적분학과-최적화" class="nav-link" data-scroll-target="#미적분학과-최적화">2.2 미적분학과 최적화</a>
  <ul class="collapse">
  <li><a href="#체인룰" id="toc-체인룰" class="nav-link" data-scroll-target="#체인룰">2.2.1 체인룰</a></li>
  <li><a href="#그래디언트와-야코비안" id="toc-그래디언트와-야코비안" class="nav-link" data-scroll-target="#그래디언트와-야코비안">2.2.2 그래디언트와 야코비안</a></li>
  <li><a href="#체인룰과-신경망의-역전파" id="toc-체인룰과-신경망의-역전파" class="nav-link" data-scroll-target="#체인룰과-신경망의-역전파">2.2.3 체인룰과 신경망의 역전파</a></li>
  <li><a href="#역전파를-위한-그래디언트-계산" id="toc-역전파를-위한-그래디언트-계산" class="nav-link" data-scroll-target="#역전파를-위한-그래디언트-계산">2.2.4 역전파를 위한 그래디언트 계산</a></li>
  </ul></li>
  <li><a href="#확률과-통계" id="toc-확률과-통계" class="nav-link" data-scroll-target="#확률과-통계">2.3 확률과 통계</a>
  <ul class="collapse">
  <li><a href="#확률-분포와-기댓값" id="toc-확률-분포와-기댓값" class="nav-link" data-scroll-target="#확률-분포와-기댓값">2.3.1 확률 분포와 기댓값</a></li>
  <li><a href="#베이즈-정리와-최대-우도-추정" id="toc-베이즈-정리와-최대-우도-추정" class="nav-link" data-scroll-target="#베이즈-정리와-최대-우도-추정">2.3.2 베이즈 정리와 최대 우도 추정</a></li>
  <li><a href="#정보-이론-기초" id="toc-정보-이론-기초" class="nav-link" data-scroll-target="#정보-이론-기초">2.3.3 정보 이론 기초</a></li>
  <li><a href="#손실-함수" id="toc-손실-함수" class="nav-link" data-scroll-target="#손실-함수">2.3.4 손실 함수</a></li>
  </ul></li>
  <li><a href="#연습문제" id="toc-연습문제" class="nav-link" data-scroll-target="#연습문제">연습문제</a>
  <ul class="collapse">
  <li><a href="#선형대수학" id="toc-선형대수학" class="nav-link" data-scroll-target="#선형대수학">1. 선형대수학</a></li>
  </ul></li>
  <li><a href="#연습문제-1" id="toc-연습문제-1" class="nav-link" data-scroll-target="#연습문제-1">연습문제</a>
  <ul class="collapse">
  <li><a href="#미적분학과-최적화-1" id="toc-미적분학과-최적화-1" class="nav-link" data-scroll-target="#미적분학과-최적화-1">2 미적분학과 최적화</a></li>
  </ul></li>
  <li><a href="#연습문제-2" id="toc-연습문제-2" class="nav-link" data-scroll-target="#연습문제-2">연습문제</a>
  <ul class="collapse">
  <li><a href="#확률과-통계-1" id="toc-확률과-통계-1" class="nav-link" data-scroll-target="#확률과-통계-1">3 확률과 통계</a></li>
  </ul></li>
  <li><a href="#참고-문헌" id="toc-참고-문헌" class="nav-link" data-scroll-target="#참고-문헌">참고 문헌</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../../notebooks/ko/part_1/01_딥러닝의 시작.html">part_1</a></li><li class="breadcrumb-item"><a href="../../../notebooks/ko/part_1/02_딥러닝의 수학.html">2. 딥러닝의 수학</a></li></ol></nav></header>




<p><a href="https://colab.research.google.com/github/Quantum-Intelligence-Frontier/dldna/blob/main/notebooks/ko/part_1/02_딥러닝의 수학.ipynb" target="_parent"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"> </a></p>
<section id="딥-러닝의-수학" class="level1">
<h1>2. 딥 러닝의 수학</h1>
<blockquote class="blockquote">
<p>“모든 수는 단위들로 이루어져 있고, 모든 수는 단위들로 나눌 수 있다” - 알 콰리즈미 (780-850), 페르시아의 수학자</p>
</blockquote>
<p>이 장에서는 딥러닝의 핵심을 이루는 수학적 개념들을 살펴보겠습니다. 딥러닝 모델은 복잡한 수학적 함수의 조합으로 이루어져 있습니다. 선형대수, 미적분, 확률 및 통계에 대한 깊이 있는 이해는 모델의 동작 원리를 파악하고, 성능을 개선하며, 새로운 모델을 설계하는 데 필수적입니다. 예를 들어, 행렬 연산에 대한 이해는 Convolutional Neural Network (CNN)의 동작 방식을 이해하는 데 중요하며, 미분과 최적화는 모델의 학습 과정을 이해하는 데 핵심적인 역할을 합니다.</p>
<p>이번 장이 어렵게 느껴지는 경우 다음장으로 넘어가도 됩니다. 수시로 되돌아와서 익숙해지는 것이 좋습니다.</p>
<section id="선형대수학-기초" class="level2">
<h2 class="anchored" data-anchor-id="선형대수학-기초">2.1 선형대수학 기초</h2>
<p>선형대수학은 딥러닝의 가장 기초입니다. 행렬 연산부터 고급 최적화 기법에 이르기까지 선형대수는 필수적인 도구입니다. 이 섹션에서는 벡터, 행렬, 텐서 등의 기본 개념부터 시작하여 특이값 분해와 주성분 분석과 같은 고급 주제까지 다룰 것입니다.</p>
<section id="벡터" class="level3">
<h3 class="anchored" data-anchor-id="벡터">2.1.1 벡터</h3>
<p>벡터와 행렬은 데이터를 표현하고 각 데이터를 변환하는 가장 기본이 되는 연산입니다.</p>
<p><strong>벡터의 기본</strong></p>
<p>벡터는 크기와 방향을 가진 양을 나타내는 수학적 객체입니다. 수학적 정의는 동일합니다. 대신 응용분야 마다 바라보는 관점이 약간 다릅니다.</p>
<ul>
<li>수학적 관점: 수학에서 벡터는 크기와 방향을 가진 추상적인 객체로 정의됩니다. 벡터 공간의 원소로서, 덧셈과 스칼라 곱에 대해 닫혀 있는 특성을 갖습니다.</li>
<li>물리학적 관점: 물리학에서 벡터는 주로 힘, 속도, 가속도와 같은 물리량을 표현하는 데 사용됩니다. 이 경우 벡터의 크기와 방향은 실제 물리적 의미를 갖습니다. 물리학은 모든 변화를 벡터로 다루며, 벡터의 차원도 제한적입니다. 대표적으로 공간은 3차원, 시공간의 경우 4차원을 가집니다.</li>
<li>컴퓨터 과학적 관점: 컴퓨터 과학, 특히 기계학습과 딥러닝에서 벡터는 주로 데이터의 특성(feature)을 표현하는 데 사용됩니다. 여기서 벡터의 각 요소는 데이터의 특정 속성을 나타내며, 반드시 물리적인 방향성을 갖지 않을 수 있습니다. 특성을 표현하기 위해 벡터 차원은 수십에서 수천차원을 가질 수 있습니다.</li>
</ul>
<p>이러한 다양한 관점을 이해하는 것은 딥러닝에서 벡터를 다룰 때 중요합니다. 딥러닝에서 벡터는 주로 컴퓨터 과학적 관점에서 사용되지만, 수학적 연산과 물리적 직관도 같이 활용합니다.</p>
<p>딥러닝에서 벡터는 주로 데이터의 여러 특성(features)을 동시에 표현하는 데 사용됩니다. 예를 들어, 주택 가격 예측 모델에서 사용되는 5차원 벡터는 다음과 같이 표현할 수 있습니다.</p>
<p><span class="math inline">\(\mathbf{v} = \begin{bmatrix} v_1 \ v_2 \ v_3 \ v_4 \ v_5 \end{bmatrix}\)</span></p>
<p>이 벡터의 각 요소는 주택의 다양한 특성을 나타냅니다. <span class="math inline">\(v_1\)</span>: 주택의 면적 (제곱미터), <span class="math inline">\(v_2\)</span>: 방의 개수, <span class="math inline">\(v_3\)</span>: 주택의 나이 (년), <span class="math inline">\(v_4\)</span>: 주변 학교까지의 거리 (킬로미터), <span class="math inline">\(v_5\)</span>: 범죄율 (백분율)</p>
<p>딥러닝 모델은 이러한 다차원 벡터를 입력으로 주택 가격을 예측할 수 있습니다. 이처럼 벡터는 복잡한 실제 데이터의 여러 특성을 효과적으로 표현하고 처리하는 데 사용됩니다.</p>
<p>넘파이에서 벡터는 손쉽게 생성해서 사용할 수 있습니다.</p>
<div id="cell-2" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip install dldna[colab] <span class="co"># in Colab</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co"># !pip install dldna[all] # in your local</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="cell-3" class="cell" data-execution_count="1">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Vector creation</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>v <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>])</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Vector magnitude (L2 norm)</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>magnitude <span class="op">=</span> np.linalg.norm(v)</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Vector magnitude: </span><span class="sc">{</span>magnitude<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Vector normalization</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>normalized_v <span class="op">=</span> v <span class="op">/</span> magnitude</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Normalized vector: </span><span class="sc">{</span>normalized_v<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Vector magnitude: 3.7416573867739413
Normalized vector: [0.26726124 0.53452248 0.80178373]</code></pre>
</div>
</div>
<p>벡터의 개념을 좀 더 깊이 살펴보면, 행벡터와 열벡터의 구분, 그리고 물리학과 공학에서 사용되는 공변벡터와 반변벡터의 개념이 있습니다.</p>
<p><strong>행벡터와 열벡터</strong></p>
<p>벡터는 일반적으로 열벡터를 기본으로 표현합니다. 행벡터는 열벡터의 전치(transpose)로 간주할 수 있습니다. 수학적으로 더 정확히 말하면, 행벡터는 듀얼 벡터 또는 공변벡터(covector)라고 부를 수 있습니다.</p>
<p>열벡터: <span class="math inline">\(\mathbf{v} = \begin{bmatrix} v_1 \ v_2 \ v_3 \end{bmatrix}\)</span>, 행벡터: <span class="math inline">\(\mathbf{v}^T = [v_1 \quad v_2 \quad v_3]\)</span></p>
<p>행벡터와 열벡터는 서로 다른 성질을 가지고 있습니다. 행벡터는 열벡터에 대해 선형 함수로 작용하여 스칼라를 생성합니다. 이는 내적 연산으로 표현됩니다.</p>
<p><span class="math display">\[\mathbf{u}^T\mathbf{v} = u_1v_1 + u_2v_2 + u_3v_3\]</span></p>
<p><strong>공변벡터와 반변벡터</strong></p>
<p>물리학과 공학에서는 공변벡터(covariant vector)와 반변벡터(contravariant vector)의 개념이 중요하게 다뤄집니다. 이는 좌표계 변환에 따른 벡터의 변환 특성을 나타냅니다.</p>
<ul>
<li>반변벡터: 좌표계가 변할 때 기저의 역방향으로 변환되는 벡터입니다. 일반적으로 위첨자로 표기합니다 (예: <span class="math inline">\(v^i\)</span>).</li>
<li>공변벡터: 좌표계가 변할 때 기저와 같은 방향으로 변환되는 벡터입니다. 일반적으로 아래첨자로 표기합니다 (예: <span class="math inline">\(v_i\)</span>).</li>
</ul>
<p>텐서 표기법에서 이러한 구분은 중요합니다. 예를 들어, <span class="math inline">\(T^i_j\)</span>는 상단 인덱스 <span class="math inline">\(i\)</span>가 반변성을, 하단 인덱스 <span class="math inline">\(j\)</span>가 공변성을 나타냅니다. 대표적으로 일반상대론에서 이러한 공변, 반변성은 매우 중요한 개념으로 다뤄집니다.</p>
<p><strong>딥러닝에서의 적용</strong></p>
<p>딥러닝에서는 이러한 공변성과 반변성의 구분이 명시적으로 강조되지 않는 경우가 많습니다. 그 이유는 다음과 같습니다.</p>
<ol type="1">
<li>표준화된 데이터 표현: 딥러닝에서는 대부분의 경우 데이터를 표준화된 형태(예: 열벡터)로 다루므로, 공변성과 반변성의 구분이 덜 중요해집니다.</li>
<li>유클리드 공간 가정: 많은 딥러닝 모델은 데이터가 유클리드 공간에 있다고 가정합니다. 이 공간에서는 공변성과 반변성의 구분이 명확하지 않습니다.</li>
<li>연산의 단순화: 딥러닝의 주요 연산(예: 행렬 곱, 활성화 함수 적용)은 이러한 구분 없이도 효과적으로 수행될 수 있습니다.</li>
<li>자동 미분: 현대 딥러닝 프레임워크의 자동 미분 기능은 이러한 세부적인 구분 없이도 정확한 그래디언트를 계산할 수 있습니다.</li>
</ol>
<p>그러나 특정 분야, 특히 물리 기반 머신러닝이나 기하학적 딥러닝에서는 이러한 개념이 여전히 중요할 수 있습니다. 예를 들어, 미분기하학을 활용한 딥러닝 모델에서는 공변성과 반변성의 구분이 모델의 설계와 해석에 중요한 역할을 할 수 있습니다.</p>
<p>결론적으로, 딥러닝에서 벡터의 기본 개념은 단순화되어 사용되지만, 더 복잡한 수학적 개념은 고급 모델 설계와 특수한 응용 분야에서 여전히 중요한 역할을 합니다.</p>
<div class="callout callout-style-default callout-note callout-titled" title="클릭하여 내용 보기 (딥다이브: 벡터 공간과 선형 결합)">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
클릭하여 내용 보기 (딥다이브: 벡터 공간과 선형 결합)
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<section id="벡터-공간과-선형-결합" class="level3 callout-body-container callout-body">
<h3 class="anchored" data-anchor-id="벡터-공간과-선형-결합">벡터 공간과 선형 결합</h3>
<p>선형대수학의 핵심 개념인 벡터 공간(vector space)은 딥러닝에서 데이터를 표현하고 변환하는 기본적인 틀을 제공합니다. 이 딥다이브에서는 벡터 공간의 엄밀한 정의와 관련 개념들을 살펴보고, 딥러닝에서의 응용 예시를 제시합니다.</p>
<section id="벡터-공간-vector-space" class="level4">
<h4 class="anchored" data-anchor-id="벡터-공간-vector-space">벡터 공간 (Vector Space)</h4>
<p>벡터 공간은 다음 8가지 공리(axiom)를 만족하는 집합 <span class="math inline">\(V\)</span>와, 덧셈(addition) 및 스칼라 곱(scalar multiplication) 연산으로 구성됩니다. 여기서 <span class="math inline">\(V\)</span>의 원소를 벡터(vector)라고 부르고, 스칼라(scalar)는 실수(real number) <span class="math inline">\(\mathbb{R}\)</span> 또는 복소수(complex number) <span class="math inline">\(\mathbb{C}\)</span> 집합의 원소입니다. (딥러닝에서는 주로 실수를 사용합니다.)</p>
<p><strong>벡터 덧셈 (Vector Addition):</strong> <span class="math inline">\(V\)</span>의 임의의 두 원소 <span class="math inline">\(\mathbf{u}, \mathbf{v}\)</span>에 대해, <span class="math inline">\(\mathbf{u} + \mathbf{v}\)</span>도 <span class="math inline">\(V\)</span>의 원소입니다. (덧셈에 대해 닫혀있다, closed under addition)</p>
<p><strong>스칼라 곱 (Scalar Multiplication):</strong> <span class="math inline">\(V\)</span>의 임의의 원소 <span class="math inline">\(\mathbf{u}\)</span>와 스칼라 <span class="math inline">\(c\)</span>에 대해, <span class="math inline">\(c\mathbf{u}\)</span>도 <span class="math inline">\(V\)</span>의 원소입니다. (스칼라 곱에 대해 닫혀있다, closed under scalar multiplication)</p>
<p><strong>벡터 덧셈과 스칼라 곱은 다음 8가지 공리를 만족해야 합니다.</strong> (<span class="math inline">\(\mathbf{u}, \mathbf{v}, \mathbf{w} \in V\)</span>, <span class="math inline">\(c, d\)</span>: 스칼라)</p>
<ol type="1">
<li><strong>덧셈의 교환 법칙 (Commutativity of addition):</strong> <span class="math inline">\(\mathbf{u} + \mathbf{v} = \mathbf{v} + \mathbf{u}\)</span></li>
<li><strong>덧셈의 결합 법칙 (Associativity of addition):</strong> <span class="math inline">\((\mathbf{u} + \mathbf{v}) + \mathbf{w} = \mathbf{u} + (\mathbf{v} + \mathbf{w})\)</span></li>
<li><strong>덧셈의 항등원 (Additive identity):</strong> 모든 <span class="math inline">\(\mathbf{u} \in V\)</span>에 대해 <span class="math inline">\(\mathbf{u} + \mathbf{0} = \mathbf{u}\)</span>를 만족하는 <span class="math inline">\(\mathbf{0} \in V\)</span> (영벡터, zero vector)가 존재합니다.</li>
<li><strong>덧셈의 역원 (Additive inverse):</strong> 각 <span class="math inline">\(\mathbf{u} \in V\)</span>에 대해 <span class="math inline">\(\mathbf{u} + (-\mathbf{u}) = \mathbf{0}\)</span>를 만족하는 <span class="math inline">\(-\mathbf{u} \in V\)</span> (덧셈의 역원)가 존재합니다.</li>
<li><strong>스칼라 곱의 분배 법칙 (Distributivity of scalar multiplication with respect to vector addition):</strong> <span class="math inline">\(c(\mathbf{u} + \mathbf{v}) = c\mathbf{u} + c\mathbf{v}\)</span></li>
<li><strong>스칼라 곱의 분배 법칙 (Distributivity of scalar multiplication with respect to scalar addition):</strong> <span class="math inline">\((c + d)\mathbf{u} = c\mathbf{u} + d\mathbf{u}\)</span></li>
<li><strong>스칼라 곱의 결합 법칙 (Compatibility of scalar multiplication with scalar multiplication):</strong> <span class="math inline">\(c(d\mathbf{u}) = (cd)\mathbf{u}\)</span></li>
<li><strong>스칼라 곱의 항등원 (Identity element of scalar multiplication):</strong> <span class="math inline">\(1\mathbf{u} = \mathbf{u}\)</span> (여기서 1은 스칼라 곱의 항등원)</li>
</ol>
<p><strong>예시:</strong></p>
<ul>
<li><span class="math inline">\(\mathbb{R}^n\)</span>: <span class="math inline">\(n\)</span>차원 실수 벡터 공간 (n-tuples of real numbers)</li>
<li><span class="math inline">\(\mathbb{C}^n\)</span>: <span class="math inline">\(n\)</span>차원 복소수 벡터 공간</li>
<li><span class="math inline">\(M_{m \times n}(\mathbb{R})\)</span>: <span class="math inline">\(m \times n\)</span> 실수 행렬 공간</li>
<li><span class="math inline">\(P_n\)</span>: <span class="math inline">\(n\)</span>차 이하의 실수 계수 다항식 공간</li>
<li><span class="math inline">\(C[a, b]\)</span>: 구간 <span class="math inline">\([a, b]\)</span>에서 연속인 실수값 함수들의 공간</li>
</ul>
</section>
<section id="부분-공간-subspace" class="level4">
<h4 class="anchored" data-anchor-id="부분-공간-subspace">부분 공간 (Subspace)</h4>
<p>벡터 공간 <span class="math inline">\(V\)</span>의 부분 집합 <span class="math inline">\(W\)</span>가 다음 조건을 만족하면 <span class="math inline">\(W\)</span>를 <span class="math inline">\(V\)</span>의 부분 공간이라고 합니다.</p>
<ol type="1">
<li><span class="math inline">\(\mathbf{0} \in W\)</span> (영벡터를 포함)</li>
<li><span class="math inline">\(\mathbf{u}, \mathbf{v} \in W\)</span> 이면, <span class="math inline">\(\mathbf{u} + \mathbf{v} \in W\)</span> (덧셈에 대해 닫혀있다)</li>
<li><span class="math inline">\(\mathbf{u} \in W\)</span> 이고 <span class="math inline">\(c\)</span>가 스칼라이면, <span class="math inline">\(c\mathbf{u} \in W\)</span> (스칼라 곱에 대해 닫혀있다)</li>
</ol>
<p>즉, 부분 공간은 벡터 공간의 부분 집합이면서, 그 자체로 벡터 공간의 성질을 만족하는 것입니다.</p>
</section>
<section id="선형-결합-linear-combination" class="level4">
<h4 class="anchored" data-anchor-id="선형-결합-linear-combination">선형 결합 (Linear Combination)</h4>
<p>벡터 공간 <span class="math inline">\(V\)</span>의 벡터 <span class="math inline">\(\mathbf{v}_1, \mathbf{v}_2, ..., \mathbf{v}_k\)</span>와 스칼라 <span class="math inline">\(c_1, c_2, ..., c_k\)</span>에 대해, 다음 형태의 식을 선형 결합이라고 합니다.</p>
<p><span class="math inline">\(c_1\mathbf{v}_1 + c_2\mathbf{v}_2 + ... + c_k\mathbf{v}_k\)</span></p>
</section>
<section id="선형-독립-linear-independence과-선형-종속-linear-dependence" class="level4">
<h4 class="anchored" data-anchor-id="선형-독립-linear-independence과-선형-종속-linear-dependence">선형 독립 (Linear Independence)과 선형 종속 (Linear Dependence)</h4>
<p>벡터들의 집합 {<span class="math inline">\(\mathbf{v}_1, \mathbf{v}_2, ..., \mathbf{v}_k\)</span>}이 다음 조건을 만족하면 <em>선형 독립</em>(linearly independent)이라고 합니다.</p>
<p><span class="math inline">\(c_1\mathbf{v}_1 + c_2\mathbf{v}_2 + ... + c_k\mathbf{v}_k = \mathbf{0}\)</span> 이면, 반드시 <span class="math inline">\(c_1 = c_2 = ... = c_k = 0\)</span></p>
<p>위 조건을 만족하지 않으면(즉, 모두 0이 아닌 스칼라 <span class="math inline">\(c_1, ..., c_k\)</span>가 존재하여 위 식을 만족시키면), 이 벡터들의 집합은 <em>선형 종속</em>(linearly dependent)이라고 합니다.</p>
<p><strong>직관적인 의미:</strong></p>
<ul>
<li><strong>선형 독립:</strong> 벡터들을 “서로 다른 방향”으로 뻗어 나가는 것으로 생각할 수 있습니다. 어떤 벡터도 다른 벡터들의 선형 결합으로 표현될 수 없습니다.</li>
<li><strong>선형 종속:</strong> 벡터들 중 일부가 “같은 방향” (또는 “평면”, “초평면”)에 놓여 있는 것으로 생각할 수 있습니다. 어떤 벡터가 다른 벡터들의 선형 결합으로 표현될 수 있습니다.</li>
</ul>
</section>
<section id="기저-basis와-차원-dimension" class="level4">
<h4 class="anchored" data-anchor-id="기저-basis와-차원-dimension">기저 (Basis)와 차원 (Dimension)</h4>
<ul>
<li><strong>기저 (Basis):</strong> 벡터 공간 <span class="math inline">\(V\)</span>의 <em>기저</em>는 다음 두 조건을 만족하는 벡터들의 집합입니다.
<ol type="1">
<li>선형 독립입니다.</li>
<li><span class="math inline">\(V\)</span>를 span합니다 (아래 span 설명 참조).</li>
</ol></li>
<li><strong>차원 (Dimension):</strong> 벡터 공간의 기저에 있는 벡터의 개수를 <em>차원</em>이라고 합니다. (dim <span class="math inline">\(V\)</span>)</li>
</ul>
<p><strong>핵심:</strong> 주어진 벡터 공간의 기저는 유일하지 않지만, 모든 기저는 동일한 개수의 벡터를 가집니다.</p>
</section>
<section id="span" class="level4">
<h4 class="anchored" data-anchor-id="span">Span</h4>
<p>벡터들의 집합 {<span class="math inline">\(\mathbf{v}_1, \mathbf{v}_2, ..., \mathbf{v}_k\)</span>}의 <em>span</em>은 이 벡터들의 모든 가능한 선형 결합의 집합입니다.</p>
<p>span{<span class="math inline">\(\mathbf{v}_1, \mathbf{v}_2, ..., \mathbf{v}_k\)</span>} = {<span class="math inline">\(c_1\mathbf{v}_1 + c_2\mathbf{v}_2 + ... + c_k\mathbf{v}_k\)</span> | <span class="math inline">\(c_1, c_2, ..., c_k\)</span>는 스칼라}</p>
<p>즉, 주어진 벡터들을 사용하여 만들 수 있는 모든 벡터의 집합입니다. Span은 항상 부분 공간이 됩니다.</p>
</section>
<section id="딥러닝에서의-벡터-공간-예시" class="level4">
<h4 class="anchored" data-anchor-id="딥러닝에서의-벡터-공간-예시">딥러닝에서의 벡터 공간 예시</h4>
<ul>
<li><strong>특징 벡터 (Feature Vector):</strong> 이미지, 텍스트, 오디오 등 딥러닝 모델의 입력 데이터는 종종 고차원 벡터로 표현됩니다. 예를 들어, 28x28 픽셀 크기의 흑백 이미지는 784차원 벡터로 표현될 수 있습니다. 각 차원은 이미지의 특정 픽셀의 밝기 값을 나타냅니다.</li>
<li><strong>가중치 벡터 (Weight Vector):</strong> 신경망의 각 층(layer)은 가중치 행렬(weight matrix)과 편향 벡터(bias vector)로 구성됩니다. 가중치 행렬의 각 행(또는 열)은 특정 뉴런의 가중치를 나타내는 벡터로 볼 수 있습니다.</li>
<li><strong>임베딩 벡터 (Embedding Vector):</strong> 단어, 사용자, 아이템 등을 저차원 벡터 공간에 표현하는 데 사용됩니다. Word2Vec, GloVe, BERT 등은 단어를 벡터로 표현하는 대표적인 임베딩 기법입니다.</li>
<li><strong>잠재 공간 (Latent Space):</strong> 오토인코더(Autoencoder), 변분 오토인코더(Variational Autoencoder, VAE), 생성적 적대 신경망(Generative Adversarial Network, GAN) 등은 데이터를 저차원의 잠재 공간에 매핑하는 방법을 학습합니다. 이 잠재 공간도 벡터 공간으로 볼 수 있습니다.</li>
</ul>
</section>
</section>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled" title="클릭하여 내용 보기 (딥다이브: 노름과 거리 - 딥러닝 관점)">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
클릭하여 내용 보기 (딥다이브: 노름과 거리 - 딥러닝 관점)
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<section id="딥러닝에서의-노름과-거리" class="level3 callout-body-container callout-body">
<h3 class="anchored" data-anchor-id="딥러닝에서의-노름과-거리">딥러닝에서의 노름과 거리</h3>
<p>벡터의 크기(magnitude)나 두 벡터 사이의 거리(distance)를 측정하는 것은 딥러닝에서 매우 중요합니다. 손실 함수, 정규화(regularization), 유사도 측정 등 다양한 곳에 활용됩니다.</p>
<section id="노름-norm" class="level4">
<h4 class="anchored" data-anchor-id="노름-norm">노름 (Norm)</h4>
<p>벡터 <span class="math inline">\(\mathbf{x} = [x_1, x_2, ..., x_n]\)</span>의 Lp-norm은 다음과 같이 정의됩니다 (<span class="math inline">\(p \ge 1\)</span>).</p>
<p><span class="math inline">\(||\mathbf{x}||_p = \left( \sum_{i=1}^{n} |x_i|^p \right)^{1/p}\)</span></p>
<ul>
<li><strong>L1-norm (<span class="math inline">\(p=1\)</span>):</strong> <span class="math inline">\(||\mathbf{x}||_1 = \sum_{i=1}^{n} |x_i|\)</span> (Manhattan distance, Taxicab norm)
<ul>
<li><strong>특징:</strong> 각 요소의 절댓값의 합. 특징 벡터의 각 요소의 크기가 중요할 때 유용.</li>
<li><strong>딥러닝 활용:</strong> L1 정규화 (Lasso regression)는 가중치의 절댓값 합을 제한하여 희소한(sparse) 모델(일부 가중치가 0)을 만드는 데 사용.</li>
</ul></li>
<li><strong>L2-norm (<span class="math inline">\(p=2\)</span>):</strong> <span class="math inline">\(||\mathbf{x}||_2 = \sqrt{\sum_{i=1}^{n} x_i^2}\)</span> (Euclidean norm)
<ul>
<li><strong>특징:</strong> 원점에서 벡터 좌표까지의 직선 거리 (피타고라스 정리). 가장 널리 사용되는 노름.</li>
<li><strong>딥러닝 활용:</strong> L2 정규화 (Ridge regression)는 가중치의 제곱합을 제한하여 가중치가 너무 커지는 것을 방지(overfitting 방지). 가중치 감쇠(weight decay)라고도 함.</li>
</ul></li>
<li><strong>L∞-norm (<span class="math inline">\(p \to \infty\)</span>):</strong> <span class="math inline">\(||\mathbf{x}||_\infty = \max_i |x_i|\)</span>
<ul>
<li><strong>특징:</strong> 벡터 요소 중 절댓값이 가장 큰 값.</li>
<li><strong>딥러닝 활용:</strong> (덜 일반적) 특정 요소의 값이 너무 커지는 것을 제한하고 싶을 때 사용.</li>
</ul></li>
</ul>
</section>
<section id="거리-distance" class="level4">
<h4 class="anchored" data-anchor-id="거리-distance">거리 (Distance)</h4>
<p>두 벡터 <span class="math inline">\(\mathbf{x}\)</span>와 <span class="math inline">\(\mathbf{y}\)</span> 사이의 거리는 일반적으로 두 벡터의 차이의 노름으로 정의됩니다.</p>
<p><span class="math inline">\(d(\mathbf{x}, \mathbf{y}) = ||\mathbf{x} - \mathbf{y}||\)</span></p>
<ul>
<li><strong>L1 거리:</strong> <span class="math inline">\(d(\mathbf{x}, \mathbf{y}) = ||\mathbf{x} - \mathbf{y}||_1 = \sum_{i=1}^{n} |x_i - y_i|\)</span></li>
<li><strong>L2 거리:</strong> <span class="math inline">\(d(\mathbf{x}, \mathbf{y}) = ||\mathbf{x} - \mathbf{y}||_2 = \sqrt{\sum_{i=1}^{n} (x_i - y_i)^2}\)</span></li>
</ul>
<p><strong>딥러닝에서의 활용 예시:</strong></p>
<ul>
<li><strong>손실 함수:</strong> MSE (L2 Loss), MAE (L1 Loss)</li>
<li><strong>정규화:</strong> L1 regularization, L2 regularization (weight decay)</li>
<li><strong>유사도/거리 기반 학습:</strong> k-NN, SVM, Siamese Network, Triplet Network, Contrastive Learning</li>
<li><strong>임베딩:</strong> 단어, 사용자, 아이템 등을 벡터 공간에 표현하고, 벡터 간 거리를 통해 유사도/관련성을 파악</li>
<li><strong>이상치 탐지 (Outlier Detection):</strong> 데이터 포인트 간의 거리를 기반으로 이상치 탐지.</li>
</ul>
<p><strong>참고:</strong> 딥러닝에서는 “거리”와 “유사도(similarity)”를 구분하는 것이 중요합니다. 거리는 작을수록 유사도가 높고, 유사도는 클수록 가깝습니다. 코사인 유사도(cosine similarity)는 딥러닝에서 자주 사용되는 유사도 측정 방법 중 하나입니다.</p>
</section>
</section>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled" title="클릭하여 내용 보기 (딥다이브: 아핀 공간)">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
클릭하여 내용 보기 (딥다이브: 아핀 공간)
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<section id="아핀-공간-affine-space" class="level3 callout-body-container callout-body">
<h3 class="anchored" data-anchor-id="아핀-공간-affine-space">아핀 공간 (Affine Space)</h3>
<p>아핀 공간(Affine Space)은 선형대수학의 벡터 공간(Vector Space) 개념을 일반화한 것으로, 기하학적 관점에서 딥러닝 모델을 이해하는 데 유용한 도구입니다. 특히, 아핀 변환(Affine Transformation)은 딥러닝에서 자주 사용되는 선형 변환에 편향(bias)을 추가한 형태를 나타냅니다.</p>
<section id="아핀-공간의-정의" class="level4">
<h4 class="anchored" data-anchor-id="아핀-공간의-정의">아핀 공간의 정의</h4>
<p>아핀 공간은 (점들의 집합, 벡터 공간, 점과 벡터의 덧셈)의 세 요소로 구성된 구조입니다. 좀 더 구체적으로,</p>
<ul>
<li><strong>점들의 집합 (<span class="math inline">\(\mathcal{A}\)</span>):</strong> 기하학적 객체(점)들의 집합입니다. 벡터 공간과 달리, <em>원점(origin)이 고정되어 있지 않습니다</em>.</li>
<li><strong>벡터 공간 (<span class="math inline">\(V\)</span>):</strong> 점들 간의 <em>변위(displacement)</em> 또는 <em>차이(difference)</em>를 나타내는 벡터들의 집합입니다. 벡터 공간의 모든 성질(덧셈, 스칼라 곱, 8가지 공리)을 만족합니다.</li>
<li><strong>점과 벡터의 덧셈 (<span class="math inline">\(\mathcal{A} \times V \to \mathcal{A}\)</span>):</strong> 점 <span class="math inline">\(P \in \mathcal{A}\)</span>와 벡터 <span class="math inline">\(\mathbf{v} \in V\)</span>를 더하여 새로운 점 <span class="math inline">\(Q \in \mathcal{A}\)</span>를 얻는 연산입니다. <span class="math inline">\(Q = P + \mathbf{v}\)</span>로 표기합니다.</li>
</ul>
<p>이 덧셈 연산은 다음 두 가지 성질을 만족해야 합니다.</p>
<ol type="1">
<li><strong>임의의 점 <span class="math inline">\(P \in \mathcal{A}\)</span>에 대해, <span class="math inline">\(P + \mathbf{0} = P\)</span> (여기서 <span class="math inline">\(\mathbf{0}\)</span>은 벡터 공간 <span class="math inline">\(V\)</span>의 영벡터)</strong></li>
<li><strong>임의의 점 <span class="math inline">\(P, Q, R \in \mathcal{A}\)</span>에 대해, <span class="math inline">\((P + \mathbf{u}) + \mathbf{v} = P + (\mathbf{u} + \mathbf{v})\)</span> (여기서 <span class="math inline">\(\mathbf{u}, \mathbf{v} \in V\)</span>)</strong></li>
</ol>
<p><strong>중요한 특징</strong></p>
<ul>
<li>아핀 공간에는 “원점”이라는 특별한 점이 <em>없습니다</em>. 모든 점이 동등합니다.</li>
<li>두 점 <span class="math inline">\(P, Q \in \mathcal{A}\)</span>의 “차이”는 벡터 공간 <span class="math inline">\(V\)</span>의 벡터로 표현됩니다: <span class="math inline">\(\overrightarrow{PQ} = Q - P \in V\)</span>. (하지만 두 점의 “합”은 정의되지 않습니다.)</li>
<li>점과 벡터의 덧셈을 통해, 한 점에서 다른 점으로 이동할 수 있습니다.</li>
</ul>
</section>
<section id="아핀-결합-affine-combination" class="level4">
<h4 class="anchored" data-anchor-id="아핀-결합-affine-combination">아핀 결합 (Affine Combination)</h4>
<p>아핀 공간 <span class="math inline">\(\mathcal{A}\)</span>의 점 <span class="math inline">\(P_1, P_2, ..., P_k\)</span>와 스칼라 <span class="math inline">\(c_1, c_2, ..., c_k\)</span>가 주어졌을 때, 다음 형태의 식을 아핀 결합이라고 합니다.</p>
<p><span class="math inline">\(c_1P_1 + c_2P_2 + ... + c_kP_k\)</span> (단, <span class="math inline">\(c_1 + c_2 + ... + c_k = 1\)</span>)</p>
<p><strong>중요:</strong> 일반적인 선형 결합과 달리, 아핀 결합은 <em>계수들의 합이 1</em>이어야 합니다. 이 조건은 아핀 공간의 “원점이 없다”는 성질을 반영합니다.</p>
</section>
<section id="아핀-변환-affine-transformation" class="level4">
<h4 class="anchored" data-anchor-id="아핀-변환-affine-transformation">아핀 변환 (Affine Transformation)</h4>
<p>아핀 변환은 아핀 공간에서 아핀 공간으로의 함수로, 선형 변환과 평행 이동(translation)의 조합으로 표현됩니다. 즉, 아핀 변환은 <em>선형 변환</em>과 <em>편향(bias)</em>을 포함합니다.</p>
<p><span class="math inline">\(f(P) = T(P) + \mathbf{b}\)</span></p>
<ul>
<li><span class="math inline">\(T\)</span>: 선형 변환 (벡터 공간 <span class="math inline">\(V\)</span>에서 <span class="math inline">\(V\)</span>로의 선형 변환)</li>
<li><span class="math inline">\(\mathbf{b}\)</span>: 평행 이동 벡터 (벡터 공간 <span class="math inline">\(V\)</span>의 원소)</li>
</ul>
<p><strong>행렬 표현:</strong></p>
<p>아핀 변환은 확장된 행렬(augmented matrix)을 사용하여 표현할 수 있습니다. <span class="math inline">\(n\)</span>차원 아핀 공간에서 <span class="math inline">\(n+1\)</span>차원 벡터를 사용하면, 아핀 변환을 <span class="math inline">\((n+1) \times (n+1)\)</span> 행렬로 나타낼 수 있습니다.</p>
<p><span class="math inline">\(\begin{bmatrix} \mathbf{y} \\ 1 \end{bmatrix} = \begin{bmatrix} \mathbf{A} &amp; \mathbf{b} \\ \mathbf{0}^T &amp; 1 \end{bmatrix} \begin{bmatrix} \mathbf{x} \\ 1 \end{bmatrix}\)</span></p>
<ul>
<li><span class="math inline">\(\mathbf{A}\)</span>: <span class="math inline">\(n \times n\)</span> 선형 변환 행렬</li>
<li><span class="math inline">\(\mathbf{b}\)</span>: <span class="math inline">\(n\)</span>차원 평행 이동 벡터</li>
<li><span class="math inline">\(\mathbf{x}\)</span>: <span class="math inline">\(n\)</span>차원 입력 벡터 (아핀 공간의 점)</li>
<li><span class="math inline">\(\mathbf{y}\)</span>: <span class="math inline">\(n\)</span>차원 출력 벡터 (아핀 공간의 점)</li>
</ul>
</section>
<section id="딥러닝에서의-아핀-공간과-아핀-변환" class="level4">
<h4 class="anchored" data-anchor-id="딥러닝에서의-아핀-공간과-아핀-변환">딥러닝에서의 아핀 공간과 아핀 변환</h4>
<ul>
<li><strong>Fully Connected Layer:</strong> 딥러닝의 fully connected layer (dense layer)는 아핀 변환을 수행합니다. <span class="math inline">\(\mathbf{y} = \mathbf{W}\mathbf{x} + \mathbf{b}\)</span> 에서 <span class="math inline">\(\mathbf{W}\mathbf{x}\)</span>는 선형 변환, <span class="math inline">\(\mathbf{b}\)</span>는 편향(평행 이동)을 나타냅니다.</li>
<li><strong>입력 공간:</strong> 딥러닝 모델의 입력 데이터는 종종 고차원 벡터로 표현되지만, 엄밀하게는 원점이 없는 아핀 공간의 점으로 간주할 수 있습니다. 예를 들어, 이미지 데이터는 각 픽셀의 밝기 값을 요소로 갖는 벡터로 표현되지만, 이 벡터 공간에는 특별한 원점이 없습니다.</li>
<li><strong>특징 공간 (Feature Space):</strong> 신경망의 각 층은 입력을 새로운 특징 공간으로 변환합니다. 이러한 변환은 종종 아핀 변환(선형 변환 + 편향)과 비선형 활성화 함수의 조합으로 이루어집니다.</li>
<li><strong>데이터 증강 (Data Augmentation):</strong> 이미지 데이터에 대한 회전, 이동, 크기 조정 등은 아핀 변환으로 표현될 수 있습니다.</li>
<li><strong>Affine Layer</strong>: 선형변환과 달리 bias를 고려한다.</li>
</ul>
</section>
<section id="bias-를-사용하지-않는-딥러닝-모델" class="level4">
<h4 class="anchored" data-anchor-id="bias-를-사용하지-않는-딥러닝-모델">Bias 를 사용하지 않는 딥러닝 모델</h4>
<p>최근 딥러닝 연구에서는 계산 효율성, 모델 해석 가능성, 또는 특정 이론적 배경을 바탕으로 편향(bias) 항을 제거한 모델들이 제안되기도 합니다.</p>
<ul>
<li><strong>DeepMind의 MuZero (2020):</strong> 강화 학습 모델인 MuZero는 policy network와 value network에서 bias를 사용하지 않습니다. 논문에서는 bias를 제거함으로써 representation learning에 도움이 된다고 언급합니다.</li>
<li><strong>OpenAI의 GPT (Generative Pre-trained Transformer) 계열</strong>: 일부 연구 및 구현에서, 계산 효율성을 위해 bias 항을 제거하기도 합니다. 하지만, 모든 GPT 계열 모델이 bias를 사용하지 <em>않는 것은 아닙니다.</em> GPT-3 등 대규모 모델에서는 여전히 bias를 사용하는 경우가 많습니다.</li>
<li><strong>No-Bias Networks</strong>: 일부 연구에서는 bias를 제거하는 것이 모델의 일반화 성능에 어떤 영향을 미치는지 체계적으로 분석하기도 합니다.</li>
</ul>
<p><strong>Bias를 제거하는 이유</strong></p>
<ul>
<li><strong>계산 효율성:</strong> bias 항을 제거하면 모델의 파라미터 수가 줄어들어 계산량과 메모리 사용량이 감소합니다. 특히, 대규모 모델에서는 이러한 효과가 더 커질 수 있습니다.</li>
<li><strong>표현 학습 (Representation Learning):</strong> 특정 문제에서는 bias 항이 불필요하거나 오히려 표현 학습을 방해할 수 있습니다. 예를 들어, MuZero에서는 bias가 없는 representation이 더 추상적이고 일반화된 표현을 학습하는 데 도움이 될 수 있다고 봅니다.</li>
<li><strong>이론적/수학적 근거:</strong> 일부 모델(예: 특정 종류의 생성 모델)에서는 bias 항이 없는 형태가 수학적으로 더 자연스럽거나, 특정 이론적 분석에 더 적합할 수 있습니다.</li>
<li><strong>정규화 효과</strong>: Bias가 없다면, weight matrix가 좀 더 중요한 정보를 담도록 하는 정규화 효과가 있을 수 있다는 연구 결과도 있습니다.</li>
</ul>
<p><strong>주의:</strong> bias를 제거하는 것이 <em>항상</em> 성능 향상을 보장하는 것은 아닙니다. 문제의 특성, 모델의 구조, 데이터의 양 등에 따라 bias의 유무가 성능에 미치는 영향은 달라질 수 있습니다.</p>
<p>아핀 공간과 아핀 변환의 개념은 딥러닝 모델의 기하학적 해석, 일반화 성능 분석, 새로운 아키텍처 설계 등에 활용될 수 있습니다.</p>
</section>
</section>
</div>
</div>
</section>
<section id="차원-랭크" class="level3">
<h3 class="anchored" data-anchor-id="차원-랭크">2.1.2 차원, 랭크</h3>
<p>텐서, 벡터, 행렬과 관련된 용어들은 수학, 물리학, 컴퓨터 과학 분야에서 약간씩 다르게 사용되어 혼란을 야기할 수 있습니다. 이러한 혼란을 피하기 위해 주요 개념들을 정리해보겠습니다. 텐서의 랭크와 차원에 대해 먼저 살펴보겠습니다. 텐서의 랭크는 텐서가 가진 인덱스의 개수를 의미합니다. 예를 들어, 스칼라는 랭크 0 텐서, 벡터는 랭크 1 텐서, 행렬은 랭크 2 텐서로 분류됩니다. 3차원 이상의 텐서는 일반적으로 그냥 텐서라고 부릅니다.</p>
<p>“차원”이라는 용어는 두 가지 의미로 사용될 수 있어 주의가 필요합니다. 첫 번째로, 텐서의 랭크와 동일한 의미로 사용되는 경우가 있습니다. 이 경우 벡터를 1차원 텐서, 행렬을 2차원 텐서라고 부르게 됩니다. 두 번째로, 배열의 길이 또는 크기를 나타내는 데 사용되기도 합니다. 예를 들어, 벡터 <span class="math inline">\(\mathbf{a} = [1, 2, 3, 4]\)</span>의 차원이 4라고 표현하는 경우가 이에 해당합니다.</p>
<p>분야별로 용어 사용의 차이를 아는 것도 중요합니다. 물리학에서는 요소의 개수가 물리적 의미를 가지므로 더 엄격하게 사용하는 경향이 있습니다. 반면 컴퓨터 과학에서는 벡터, 행렬, 텐서를 주로 숫자의 배열로 취급하며, “차원”이라는 용어를 데이터의 개수와 인덱스의 개수 모두를 지칭하는 데 혼용하는 경향이 있습니다.</p>
<p>이러한 용어 사용의 차이로 인한 혼란을 피하기 위해서는 몇 가지 주의해야 합니다. 용어의 의미는 맥락에 따라 달라질 수 있으므로 주의 깊게 해석해야 합니다. 논문이나 책에서 어떤 의미로 “차원”을 사용하는지 명확히 구분할 필요가 있습니다. 특히 딥러닝 분야에서는 텐서의 랭크와 배열의 크기를 모두 “차원”으로 표현하는 경우가 많으므로 일관된 해석이 중요합니다.</p>
<p>딥러닝 프레임워크에서는 텐서의 형태(shape)를 나타내는 데 ‘차원(dimension)’ 또는 ’축(axis)’이라는 용어를 사용합니다. 예를 들어, PyTorch에서는 <code>tensor.shape</code> 또는 <code>tensor.size()</code>를 통해 텐서의 각 차원의 크기를 확인할 수 있습니다. 이 책에서는 텐서의 랭크(rank)는 ’차원’으로, 배열의 길이/크기는 shape의 각 요소값 혹은 차원으로 표현하겠습니다.</p>
</section>
<section id="선형변환의-기본" class="level3">
<h3 class="anchored" data-anchor-id="선형변환의-기본">2.1.3 선형변환의 기본</h3>
<p>딥러닝 훈련에 필요한 수학을 살펴보도록 하겠습니다. 신경망의 핵심 연산인 선형변환은 순방향 계산에서 매우 간단하게 표현됩니다. 이 섹션에서는 활성화 함수를 통과하기 전까지의 기본적인 선형 연산에 초점을 맞추겠습니다.</p>
<p>순방향 연산의 기본 형태는 다음과 같습니다.</p>
<p><span class="math display">\[\boldsymbol y = \boldsymbol x \boldsymbol W + \boldsymbol b\]</span></p>
<p>여기서 <span class="math inline">\(\boldsymbol x\)</span>는 입력, <span class="math inline">\(\boldsymbol W\)</span>는 가중치, <span class="math inline">\(\boldsymbol b\)</span>는 편향, 그리고 <span class="math inline">\(\boldsymbol y\)</span>는 출력을 나타냅니다. 신경망 수학에서는 입력과 출력을 벡터로, 가중치를 행렬로 표현하는 경우가 많습니다. 편향(<span class="math inline">\(\boldsymbol b\)</span>)은 때로 스칼라값으로 표현되기도 하지만, 정확히는 출력과 동일한 형태인 벡터로 표현해야 합니다.</p>
<p><strong>행렬과 선형변환</strong></p>
<p>행렬은 선형변환을 표현하는 강력한 도구입니다. 선형변환은 벡터 공간의 한 점을 다른 점으로 매핑하는 과정으로, 이는 전체 공간의 변형으로 볼 수 있습니다. 이러한 개념을 시각적으로 이해하는 데 도움이 되는 자료로 3Blue1Brown의 “Linear transformations and matrices” 영상[1]을 추천합니다. 이 영상은 선형대수학의 기본 개념을 직관적으로 설명하며, 행렬이 어떻게 공간을 변형시키는지 명확하게 보여줍니다.</p>
<p>입력 데이터 <span class="math inline">\(\boldsymbol x\)</span>를 벡터로 표현할 때, 이는 단일 데이터 포인트를 의미하며 벡터의 길이는 특성의 개수가 됩니다. 그러나 실제 훈련 과정에서는 보통 여러 데이터를 한 번에 처리합니다. 이 경우 입력은 (n, m) 형태의 행렬 <span class="math inline">\(\boldsymbol X\)</span>가 되며, 여기서 n은 데이터의 개수, m은 특성의 개수를 나타냅니다.</p>
<p>실제 딥러닝 모델에서는 입력 데이터가 2차원 행렬을 넘어 더 높은 차원의 텐서 형태를 가질 수 있습니다.</p>
<ul>
<li>이미지 데이터: (배치 크기, 높이, 너비, 채널) 형태의 4차원 텐서</li>
<li>비디오 데이터: (배치 크기, 프레임 수, 높이, 너비, 채널) 형태의 5차원 텐서</li>
</ul>
<p>이러한 고차원 데이터를 처리하기 위해 신경망은 다양한 형태의 선형 및 비선형 변환을 사용합니다. 선형변환의 역방향 전파 과정에서는 그래디언트를 계산하고 이를 역순으로 각 층에 전달하여 파라미터를 업데이트합니다. 이 과정은 복잡할 수 있지만, 자동 미분 도구를 통해 효율적으로 수행됩니다. 선형변환은 딥러닝 모델의 기본 구성 요소이지만, 실제 모델의 성능은 비선형 활성화 함수와의 조합을 통해 얻어집니다. 다음 섹션에서는 이러한 비선형성이 어떻게 모델의 표현력을 증가시키는지 살펴보겠습니다.</p>
<div id="cell-10" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># if in Colab, plase don't run this and below code. just see the result video bleow the following cell.</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="co">#from manim import *  </span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="cell-11" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="op">%%</span>manim <span class="op">-</span>qh <span class="op">-</span>v WARNING LinearTransformations  </span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> manim <span class="im">import</span> <span class="op">*</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> manim <span class="im">import</span> config</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LinearTransformations(ThreeDScene):</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> construct(<span class="va">self</span>):</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.set_camera_orientation(phi<span class="op">=</span><span class="dv">75</span> <span class="op">*</span> DEGREES, theta<span class="op">=-</span><span class="dv">45</span> <span class="op">*</span> DEGREES)</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>        axes <span class="op">=</span> ThreeDAxes(x_range<span class="op">=</span>[<span class="op">-</span><span class="dv">6</span>, <span class="dv">6</span>, <span class="dv">1</span>], y_range<span class="op">=</span>[<span class="op">-</span><span class="dv">6</span>, <span class="dv">6</span>, <span class="dv">1</span>], z_range<span class="op">=</span>[<span class="op">-</span><span class="dv">6</span>, <span class="dv">6</span>, <span class="dv">1</span>], x_length<span class="op">=</span><span class="dv">10</span>, y_length<span class="op">=</span><span class="dv">10</span>, z_length<span class="op">=</span><span class="dv">10</span>).set_color(GRAY)</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.add(axes)</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>        <span class="co"># --- 3D Linear Transformation (Rotation and Shear) ---</span></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>        title <span class="op">=</span> Text(<span class="st">"3D Linear Transformations"</span>, color<span class="op">=</span>BLACK).to_edge(UP)</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.play(Write(title))</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.wait(<span class="dv">1</span>)</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 1. Rotation around Z-axis</span></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>        text_rotation <span class="op">=</span> Text(<span class="st">"Rotation around Z-axis"</span>, color<span class="op">=</span>BLUE).scale(<span class="fl">0.7</span>).next_to(title, DOWN, buff<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.play(Write(text_rotation))</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>        cube <span class="op">=</span> Cube(side_length<span class="op">=</span><span class="dv">2</span>, fill_color<span class="op">=</span>BLUE, fill_opacity<span class="op">=</span><span class="fl">0.5</span>, stroke_color<span class="op">=</span>WHITE, stroke_width<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.play(Create(cube))</span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.play(Rotate(cube, angle<span class="op">=</span>PI<span class="op">/</span><span class="dv">2</span>, axis<span class="op">=</span>OUT, about_point<span class="op">=</span>ORIGIN), run_time<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.wait(<span class="dv">1</span>)</span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.play(FadeOut(text_rotation))</span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 2. Shear</span></span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a>        text_shear <span class="op">=</span> Text(<span class="st">"Shear Transformation"</span>, color<span class="op">=</span>GREEN).scale(<span class="fl">0.7</span>).next_to(title, DOWN, buff<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.play(Write(text_shear))</span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Define the shear transformation matrix.  This shears in x relative to y, and in y relative to x.</span></span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a>        shear_matrix <span class="op">=</span> np.array([</span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a>            [<span class="dv">1</span>, <span class="fl">0.5</span>, <span class="dv">0</span>],</span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a>            [<span class="fl">0.5</span>, <span class="dv">1</span>, <span class="dv">0</span>],</span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a>            [<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>]</span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a>        ])</span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-36"><a href="#cb5-36" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.play(</span>
<span id="cb5-37"><a href="#cb5-37" aria-hidden="true" tabindex="-1"></a>            cube.animate.apply_matrix(shear_matrix),</span>
<span id="cb5-38"><a href="#cb5-38" aria-hidden="true" tabindex="-1"></a>            run_time<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb5-39"><a href="#cb5-39" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb5-40"><a href="#cb5-40" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.wait(<span class="dv">1</span>)</span>
<span id="cb5-41"><a href="#cb5-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-42"><a href="#cb5-42" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Add transformed axes to visualize the shear</span></span>
<span id="cb5-43"><a href="#cb5-43" aria-hidden="true" tabindex="-1"></a>        transformed_axes <span class="op">=</span> axes.copy().apply_matrix(shear_matrix)</span>
<span id="cb5-44"><a href="#cb5-44" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.play(Create(transformed_axes), run_time<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb5-45"><a href="#cb5-45" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.wait(<span class="dv">1</span>)</span>
<span id="cb5-46"><a href="#cb5-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-47"><a href="#cb5-47" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.play(FadeOut(cube), FadeOut(transformed_axes), FadeOut(text_shear))</span>
<span id="cb5-48"><a href="#cb5-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-49"><a href="#cb5-49" aria-hidden="true" tabindex="-1"></a>        <span class="co"># --- 2D to 3D Transformation (Paraboloid) ---</span></span>
<span id="cb5-50"><a href="#cb5-50" aria-hidden="true" tabindex="-1"></a>        text_2d_to_3d <span class="op">=</span> Text(<span class="st">"2D to 3D: Paraboloid"</span>, color<span class="op">=</span>MAROON).scale(<span class="fl">0.7</span>).next_to(title, DOWN, buff<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb5-51"><a href="#cb5-51" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.play(Write(text_2d_to_3d))</span>
<span id="cb5-52"><a href="#cb5-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-53"><a href="#cb5-53" aria-hidden="true" tabindex="-1"></a>        square <span class="op">=</span> Square(side_length<span class="op">=</span><span class="dv">4</span>, fill_color<span class="op">=</span>MAROON, fill_opacity<span class="op">=</span><span class="fl">0.5</span>, stroke_color<span class="op">=</span>WHITE, stroke_width<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb5-54"><a href="#cb5-54" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.play(Create(square))</span>
<span id="cb5-55"><a href="#cb5-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-56"><a href="#cb5-56" aria-hidden="true" tabindex="-1"></a>        <span class="kw">def</span> paraboloid(point):  <span class="co"># Function for the transformation</span></span>
<span id="cb5-57"><a href="#cb5-57" aria-hidden="true" tabindex="-1"></a>            x, y, _ <span class="op">=</span> point</span>
<span id="cb5-58"><a href="#cb5-58" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> [x, y, <span class="fl">0.2</span> <span class="op">*</span> (x<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> y<span class="op">**</span><span class="dv">2</span>)]  <span class="co"># Adjust scaling factor (0.2) as needed</span></span>
<span id="cb5-59"><a href="#cb5-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-60"><a href="#cb5-60" aria-hidden="true" tabindex="-1"></a>        paraboloid_surface <span class="op">=</span> always_redraw(<span class="kw">lambda</span>: Surface(</span>
<span id="cb5-61"><a href="#cb5-61" aria-hidden="true" tabindex="-1"></a>            <span class="kw">lambda</span> u, v: axes.c2p(<span class="op">*</span>paraboloid(axes.p2c(np.array([u,v,<span class="dv">0</span>])))),</span>
<span id="cb5-62"><a href="#cb5-62" aria-hidden="true" tabindex="-1"></a>            u_range<span class="op">=</span>[<span class="op">-</span><span class="dv">2</span>, <span class="dv">2</span>],</span>
<span id="cb5-63"><a href="#cb5-63" aria-hidden="true" tabindex="-1"></a>            v_range<span class="op">=</span>[<span class="op">-</span><span class="dv">2</span>, <span class="dv">2</span>],</span>
<span id="cb5-64"><a href="#cb5-64" aria-hidden="true" tabindex="-1"></a>            resolution<span class="op">=</span>(<span class="dv">15</span>, <span class="dv">15</span>), <span class="co"># Added for smoothness</span></span>
<span id="cb5-65"><a href="#cb5-65" aria-hidden="true" tabindex="-1"></a>            fill_color<span class="op">=</span>MAROON,</span>
<span id="cb5-66"><a href="#cb5-66" aria-hidden="true" tabindex="-1"></a>            fill_opacity<span class="op">=</span><span class="fl">0.7</span>,</span>
<span id="cb5-67"><a href="#cb5-67" aria-hidden="true" tabindex="-1"></a>            stroke_color<span class="op">=</span>WHITE,</span>
<span id="cb5-68"><a href="#cb5-68" aria-hidden="true" tabindex="-1"></a>            stroke_width<span class="op">=</span><span class="fl">0.5</span></span>
<span id="cb5-69"><a href="#cb5-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-70"><a href="#cb5-70" aria-hidden="true" tabindex="-1"></a>        ).set_shade_in_3d(<span class="va">True</span>))</span>
<span id="cb5-71"><a href="#cb5-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-72"><a href="#cb5-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-73"><a href="#cb5-73" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.play(</span>
<span id="cb5-74"><a href="#cb5-74" aria-hidden="true" tabindex="-1"></a>            Transform(square, paraboloid_surface),</span>
<span id="cb5-75"><a href="#cb5-75" aria-hidden="true" tabindex="-1"></a>            run_time<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb5-76"><a href="#cb5-76" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb5-77"><a href="#cb5-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-78"><a href="#cb5-78" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.wait(<span class="dv">2</span>)</span>
<span id="cb5-79"><a href="#cb5-79" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.play(FadeOut(square), FadeOut(text_2d_to_3d))</span>
<span id="cb5-80"><a href="#cb5-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-81"><a href="#cb5-81" aria-hidden="true" tabindex="-1"></a>        <span class="co"># --- 3D to 2D Transformation (Projection) ---</span></span>
<span id="cb5-82"><a href="#cb5-82" aria-hidden="true" tabindex="-1"></a>        text_3d_to_2d <span class="op">=</span> Text(<span class="st">"3D to 2D: Projection"</span>, color<span class="op">=</span>PURPLE).scale(<span class="fl">0.7</span>).next_to(title, DOWN, buff<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb5-83"><a href="#cb5-83" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.play(Write(text_3d_to_2d))</span>
<span id="cb5-84"><a href="#cb5-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-85"><a href="#cb5-85" aria-hidden="true" tabindex="-1"></a>        sphere <span class="op">=</span> Sphere(radius<span class="op">=</span><span class="fl">1.5</span>, fill_color<span class="op">=</span>PURPLE, fill_opacity<span class="op">=</span><span class="fl">0.7</span>, stroke_color<span class="op">=</span>WHITE, stroke_width<span class="op">=</span><span class="dv">1</span>, resolution<span class="op">=</span>(<span class="dv">20</span>,<span class="dv">20</span>)).set_shade_in_3d(<span class="va">True</span>)</span>
<span id="cb5-86"><a href="#cb5-86" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.play(Create(sphere))</span>
<span id="cb5-87"><a href="#cb5-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-88"><a href="#cb5-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-89"><a href="#cb5-89" aria-hidden="true" tabindex="-1"></a>        <span class="kw">def</span> project_to_2d(mob, alpha):</span>
<span id="cb5-90"><a href="#cb5-90" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> p <span class="kw">in</span> mob.points:</span>
<span id="cb5-91"><a href="#cb5-91" aria-hidden="true" tabindex="-1"></a>                p[<span class="dv">2</span>] <span class="op">*=</span> (<span class="dv">1</span><span class="op">-</span>alpha)</span>
<span id="cb5-92"><a href="#cb5-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-93"><a href="#cb5-93" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.play(</span>
<span id="cb5-94"><a href="#cb5-94" aria-hidden="true" tabindex="-1"></a>            UpdateFromAlphaFunc(sphere, project_to_2d),</span>
<span id="cb5-95"><a href="#cb5-95" aria-hidden="true" tabindex="-1"></a>            run_time<span class="op">=</span><span class="dv">2</span></span>
<span id="cb5-96"><a href="#cb5-96" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb5-97"><a href="#cb5-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-98"><a href="#cb5-98" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.wait(<span class="dv">1</span>)</span>
<span id="cb5-99"><a href="#cb5-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-100"><a href="#cb5-100" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Show a circle representing the final projection </span></span>
<span id="cb5-101"><a href="#cb5-101" aria-hidden="true" tabindex="-1"></a>        circle <span class="op">=</span> Circle(radius<span class="op">=</span><span class="fl">1.5</span>, color<span class="op">=</span>PURPLE, fill_opacity<span class="op">=</span><span class="fl">0.7</span>, stroke_color <span class="op">=</span> WHITE, stroke_width<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb5-102"><a href="#cb5-102" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.add(circle)</span>
<span id="cb5-103"><a href="#cb5-103" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.wait(<span class="dv">1</span>)</span>
<span id="cb5-104"><a href="#cb5-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-105"><a href="#cb5-105" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.play(FadeOut(sphere), FadeOut(text_3d_to_2d), FadeOut(circle), FadeOut(title))</span>
<span id="cb5-106"><a href="#cb5-106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-107"><a href="#cb5-107" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.wait(<span class="dv">1</span>)</span>
<span id="cb5-108"><a href="#cb5-108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-109"><a href="#cb5-109" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> logging</span>
<span id="cb5-110"><a href="#cb5-110" aria-hidden="true" tabindex="-1"></a>logging.getLogger(<span class="st">"manim"</span>).setLevel(logging.WARNING)</span>
<span id="cb5-111"><a href="#cb5-111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-112"><a href="#cb5-112" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="va">__name__</span> <span class="op">==</span> <span class="st">"__main__"</span>:</span>
<span id="cb5-113"><a href="#cb5-113" aria-hidden="true" tabindex="-1"></a>    config.video_dir <span class="op">=</span> <span class="st">"./"</span></span>
<span id="cb5-114"><a href="#cb5-114" aria-hidden="true" tabindex="-1"></a>    scene <span class="op">=</span> LinearTransformations()</span>
<span id="cb5-115"><a href="#cb5-115" aria-hidden="true" tabindex="-1"></a>    scene.render()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<video width="640" height="480" controls="">
<source src="../../../assets/videos/LinearTransformations.mp4" type="video/mp4">
<p>Your browser does not support the video tag. </p>
<p>선형변환은 벡터 공간의 구조를 보존하면서 한 벡터 공간을 다른 벡터 공간으로 매핑하는 함수입니다. 이러한 변환은 행렬 연산으로 표현될 수 있으며, 이는 딥러닝에서 핵심적인 역할을 합니다. 위의 애니메이션은 선형변환을 시각화합니다.</p>
<p>선형변환의 이해는 신경망의 동작 원리를 파악하는 중요합니다. 예를 들어 과적합된 모델은 입력 공간을 지나치게 왜곡할 수 있으며, 반면 잘 일반화된 모델은 더 부드러운 변환을 수행할 수 있습니다. 딥러닝 모델을 설계하고 최적화할 때 기하학적 직관은 매우 유용할 수 있습니다.</p>
<div class="callout callout-style-default callout-note callout-titled" title="클릭하여 내용 보기 (딥다이브: 선형 변환의 엄밀한 정의와 추가적인 성질)">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
클릭하여 내용 보기 (딥다이브: 선형 변환의 엄밀한 정의와 추가적인 성질)
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<section id="선형-변환의-엄밀한-정의와-추가적인-성질" class="level3 callout-body-container callout-body">
<h3 class="anchored" data-anchor-id="선형-변환의-엄밀한-정의와-추가적인-성질">선형 변환의 엄밀한 정의와 추가적인 성질</h3>
<p>선형 변환(Linear Transformation)은 벡터 공간 사이의 함수로, 벡터 공간의 선형 구조(덧셈과 스칼라 곱)를 보존하는 변환입니다. 딥러닝에서 fully connected layer는 선형 변환의 대표적인 예시입니다.</p>
<section id="선형-변환의-엄밀한-정의" class="level4">
<h4 class="anchored" data-anchor-id="선형-변환의-엄밀한-정의">선형 변환의 엄밀한 정의</h4>
<p><span class="math inline">\(V\)</span>와 <span class="math inline">\(W\)</span>를 벡터 공간이라고 할 때, 함수 <span class="math inline">\(T: V \to W\)</span>가 다음 두 조건을 만족하면 <em>선형 변환</em>이라고 합니다.</p>
<ol type="1">
<li><strong>덧셈 보존:</strong> 임의의 <span class="math inline">\(\mathbf{u}, \mathbf{v} \in V\)</span>에 대해, <span class="math inline">\(T(\mathbf{u} + \mathbf{v}) = T(\mathbf{u}) + T(\mathbf{v})\)</span></li>
<li><strong>스칼라 곱 보존:</strong> 임의의 <span class="math inline">\(\mathbf{u} \in V\)</span>와 스칼라 <span class="math inline">\(c\)</span>에 대해, <span class="math inline">\(T(c\mathbf{u}) = cT(\mathbf{u})\)</span></li>
</ol>
<p>이 두 조건을 만족하는 함수만이 선형 변환이라고 불릴 수 있습니다.</p>
</section>
<section id="딥러닝에서의-선형-변환-예시-fully-connected-layer" class="level4">
<h4 class="anchored" data-anchor-id="딥러닝에서의-선형-변환-예시-fully-connected-layer">딥러닝에서의 선형 변환 예시: Fully Connected Layer</h4>
<p>딥러닝의 fully connected layer (dense layer)는 선형 변환의 대표적인 예시입니다. 입력 벡터 <span class="math inline">\(\mathbf{x} \in \mathbb{R}^m\)</span>와 가중치 행렬 <span class="math inline">\(\mathbf{W} \in \mathbb{R}^{n \times m}\)</span>, 편향 벡터 <span class="math inline">\(\mathbf{b} \in \mathbb{R}^n\)</span>에 대해, fully connected layer의 연산은 다음과 같이 표현됩니다.</p>
<p><span class="math inline">\(\mathbf{y} = \mathbf{W}\mathbf{x} + \mathbf{b}\)</span></p>
<p>여기서 <span class="math inline">\(\mathbf{y} \in \mathbb{R}^n\)</span>는 출력 벡터입니다. <span class="math inline">\(\mathbf{W}\mathbf{x}\)</span> 부분은 선형 변환에 해당하며, <span class="math inline">\(\mathbf{b}\)</span>는 아핀 변환을 위한 평행 이동(translation)을 나타냅니다. (엄밀하게는, bias를 포함하면 <em>아핀 변환</em>이지만, 딥러닝에서는 <em>선형 변환</em>이라고 부르는 경우가 많습니다.)</p>
</section>
<section id="핵-kernel과-치역-range" class="level4">
<h4 class="anchored" data-anchor-id="핵-kernel과-치역-range">핵 (Kernel)과 치역 (Range)</h4>
<p>선형 변환 <span class="math inline">\(T: V \to W\)</span>에 대해,</p>
<ul>
<li><strong>핵 (Kernel, 또는 영 공간 Null Space):</strong> <span class="math inline">\(V\)</span>에서 <span class="math inline">\(\mathbf{0}_W\)</span> (W의 영벡터)로 mapping 되는 모든 벡터의 집합.
<ul>
<li><span class="math inline">\(\text{ker}(T) = \{\mathbf{v} \in V | T(\mathbf{v}) = \mathbf{0}_W \}\)</span></li>
<li><span class="math inline">\(\text{ker}(T)\)</span>는 <span class="math inline">\(V\)</span>의 부분 공간(subspace)입니다.</li>
</ul></li>
<li><strong>치역 (Range, 또는 상 Image):</strong> <span class="math inline">\(V\)</span>의 모든 벡터가 <span class="math inline">\(T\)</span>에 의해 mapping 되는 <span class="math inline">\(W\)</span>의 부분 집합.
<ul>
<li><span class="math inline">\(\text{range}(T) = \{T(\mathbf{v}) | \mathbf{v} \in V \}\)</span></li>
<li><span class="math inline">\(\text{range}(T)\)</span>는 <span class="math inline">\(W\)</span>의 부분 공간입니다.</li>
</ul></li>
</ul>
</section>
<section id="rank-nullity-theorem-차원-정리" class="level4">
<h4 class="anchored" data-anchor-id="rank-nullity-theorem-차원-정리">Rank-Nullity Theorem (차원 정리)</h4>
<p>선형 변환 <span class="math inline">\(T: V \to W\)</span>에서 <span class="math inline">\(V\)</span>가 유한 차원 벡터 공간일 때, 다음이 성립합니다.</p>
<p><span class="math inline">\(\text{dim}(\text{ker}(T)) + \text{dim}(\text{range}(T)) = \text{dim}(V)\)</span></p>
<ul>
<li><span class="math inline">\(\text{dim}(\text{ker}(T))\)</span>: <span class="math inline">\(T\)</span>의 <em>nullity</em> (퇴화 차수)</li>
<li><span class="math inline">\(\text{dim}(\text{range}(T))\)</span>: <span class="math inline">\(T\)</span>의 <em>rank</em> (계수)</li>
</ul>
<p>즉, 입력 공간의 차원은 핵의 차원(nullity)과 치역의 차원(rank)의 합과 같습니다.</p>
</section>
<section id="선형-변환의-행렬-표현-matrix-representation" class="level4">
<h4 class="anchored" data-anchor-id="선형-변환의-행렬-표현-matrix-representation">선형 변환의 행렬 표현 (Matrix Representation)</h4>
<p>유한 차원 벡터 공간 사이의 선형 변환은 항상 행렬로 표현할 수 있습니다. <span class="math inline">\(V\)</span>의 기저(basis)를 {<span class="math inline">\(\mathbf{v}_1, \mathbf{v}_2, ..., \mathbf{v}_m\)</span>}, <span class="math inline">\(W\)</span>의 기저를 {<span class="math inline">\(\mathbf{w}_1, \mathbf{w}_2, ..., \mathbf{w}_n\)</span>}이라고 할 때, 선형 변환 <span class="math inline">\(T: V \to W\)</span>는 다음과 같이 <span class="math inline">\(n \times m\)</span> 행렬 <span class="math inline">\(\mathbf{A}\)</span>로 표현됩니다.</p>
<p><span class="math inline">\(T(\mathbf{v}_j) = \sum_{i=1}^{n} a_{ij}\mathbf{w}_i\)</span> (for <span class="math inline">\(j = 1, 2, ..., m\)</span>)</p>
<p>여기서 <span class="math inline">\(a_{ij}\)</span>는 행렬 <span class="math inline">\(\mathbf{A}\)</span>의 <span class="math inline">\(i\)</span>행 <span class="math inline">\(j\)</span>열 원소입니다. 즉, 행렬 <span class="math inline">\(\mathbf{A}\)</span>의 <span class="math inline">\(j\)</span>번째 열은 <span class="math inline">\(T(\mathbf{v}_j)\)</span>를 <span class="math inline">\(W\)</span>의 기저로 표현했을 때의 계수들입니다.</p>
<p>벡터 <span class="math inline">\(\mathbf{v} \in V\)</span>를 기저 {<span class="math inline">\(\mathbf{v}_1, \mathbf{v}_2, ..., \mathbf{v}_m\)</span>}의 선형 결합으로 표현하면 <span class="math inline">\(\mathbf{v} = c_1\mathbf{v}_1 + c_2\mathbf{v}_2 + ... + c_m\mathbf{v}_m\)</span> 이고, 이 벡터의 좌표를 열벡터 <span class="math inline">\(\begin{bmatrix} c_1 \\ c_2 \\ \vdots \\ c_m \end{bmatrix}\)</span>로 나타낼 수 있습니다. 그러면,</p>
<p><span class="math inline">\(T(\mathbf{v}) = \mathbf{A} \begin{bmatrix} c_1 \\ c_2 \\ \vdots \\ c_m \end{bmatrix}\)</span></p>
<p><strong>딥러닝에서의 행렬 표현:</strong> 딥러닝에서 fully connected layer의 연산 <span class="math inline">\(\mathbf{y} = \mathbf{W}\mathbf{x}\)</span>는 선형 변환의 행렬 표현과 정확히 일치합니다.</p>
</section>
</section>
</div>
</div>
</video></section>
<section id="텐서연산" class="level3">
<h3 class="anchored" data-anchor-id="텐서연산">2.1.4 텐서연산</h3>
<blockquote class="blockquote">
<p><strong>도전과제:</strong> 어떻게 하면 다차원 데이터를 효율적으로 표현하고 연산할 수 있을까?</p>
<p><strong>연구자의 고뇌:</strong> 딥러닝 초기, 연구자들은 이미지, 텍스트, 오디오 등 다양한 형태의 데이터를 다뤄야 했습니다. 이러한 데이터는 단순한 벡터나 행렬로 표현하기 어려웠고, 복잡한 데이터 구조를 효과적으로 처리할 수 있는 방법이 필요했습니다. 또한, 대규모 데이터를 빠르게 처리하기 위한 효율적인 연산 방법도 중요한 과제였습니다.</p>
</blockquote>
<p>텐서는 딥러닝에서 데이터와 모델 파라미터를 표현하는 기본적인 수학적 객체입니다. 스칼라, 벡터, 행렬을 일반화한 개념으로, 다차원 배열로 생각할 수 있습니다. 텐서는 그 차원(dimension, rank) 에 따라 다음과 같이 분류됩니다.</p>
<ul>
<li>0차원 텐서: 스칼라 (예: 3.14)</li>
<li>1차원 텐서: 벡터 (예: [1, 2, 3])</li>
<li>2차원 텐서: 행렬 (예: [[1, 2], [3, 4]])</li>
<li>3차원 이상: 고차원 텐서</li>
</ul>
<p>딥러닝에서는 주로 다음과 같은 형태의 텐서를 다룹니다.</p>
<ul>
<li><strong>입력 데이터:</strong>
<ul>
<li><strong>일반:</strong> (배치 크기, 특성 수)</li>
<li><strong>시계열/텍스트:</strong> (배치 크기, 시퀀스 길이, 특성 수/임베딩 차원)</li>
<li><strong>이미지:</strong> (배치 크기, 높이, 너비, 채널)</li>
</ul></li>
<li><strong>가중치 (Weights):</strong>
<ul>
<li><strong>Fully-connected:</strong> (입력 특성 수, 출력 특성 수)</li>
<li><strong>Convolutional:</strong> (출력 채널 수, 입력 채널 수, 커널 높이, 커널 너비)</li>
</ul></li>
<li><strong>출력 데이터 (Output/Prediction):</strong>
<ul>
<li><strong>분류 (Classification):</strong> (배치 크기, 클래스 수)</li>
<li><strong>회귀 (Regression):</strong> (배치 크기, 출력 차원)</li>
</ul></li>
<li><strong>편향(Bias):</strong>
<ul>
<li><strong>Fully connected:</strong> (출력 특성 수,)</li>
<li><strong>Convolutional:</strong> (출력 채널 수,)</li>
</ul></li>
<li><strong>Feature maps (Convolutional layers의 출력):</strong> (배치 크기, 출력 채널 수, 높이, 너비)</li>
</ul>
<p>신경망의 기본적인 선형변환은 다음과 같습니다.</p>
<p><span class="math inline">\(y_j = \sum\limits_{i} x_i w_{ij} + b_j\)</span></p>
<p>여기서 <span class="math inline">\(i\)</span>는 입력의 인덱스, <span class="math inline">\(j\)</span>는 출력의 인덱스입니다. 이를 벡터와 행렬 형태로 표현하면 다음과 같습니다.</p>
<p><span class="math inline">\(\boldsymbol x = \begin{bmatrix}x_{1} &amp; x_{2} &amp; \cdots &amp; x_{i} \end{bmatrix}\)</span></p>
<p><span class="math inline">\(\boldsymbol W = \begin{bmatrix}
w_{11} &amp; \cdots &amp; w_{1j} \
\vdots &amp; \ddots &amp; \vdots \
w_{i1} &amp; \cdots &amp; w_{ij}
\end{bmatrix}\)</span></p>
<p><span class="math inline">\(\boldsymbol b = \begin{bmatrix}b_{1} &amp; b_{2} &amp; \cdots &amp; b_{j} \end{bmatrix}\)</span></p>
<p><span class="math inline">\(\boldsymbol y = \boldsymbol x \boldsymbol W + \boldsymbol b\)</span></p>
<p>텐서 연산의 주요 특징은 다음과 같습니다.</p>
<ol type="1">
<li><p>브로드캐스팅: 크기가 다른 텐서 간의 연산을 가능하게 합니다.</p></li>
<li><p>차원 축소: sum(), mean() 등의 연산으로 텐서의 특정 차원을 축소할 수 있습니다.</p></li>
<li><p>재형성(reshape): 텐서의 형태를 변경하여 다른 차원의 텐서로 변환할 수 있습니다.</p></li>
</ol>
<p>신경망 학습에서 가장 중요한 연산 중 하나는 그래디언트 계산입니다. 주요 그래디언트 계산은 다음과 같습니다.</p>
<ol type="1">
<li><p>입력에 대한 그래디언트: <span class="math inline">\(\frac{\partial \boldsymbol y}{\partial \boldsymbol{x}}\)</span></p></li>
<li><p>가중치에 대한 그래디언트: <span class="math inline">\(\frac{\partial \boldsymbol y}{\partial \boldsymbol W}\)</span></p></li>
</ol>
<p>이러한 그래디언트는 각각 입력과 가중치의 변화에 따른 출력의 변화를 나타내며, 역전파 알고리즘의 핵심입니다.</p>
<p>텐서 연산은 현대 딥러닝의 근간을 이루며, GPU를 활용한 고도의 병렬 처리를 통해 대규모 모델의 효율적인 학습과 추론을 가능케 합니다. 또한 텐서 연산의 자동 미분(automatic differentiation)은 효율적인 그래디언트 계산을 가능하게 하여 현대 딥러닝 연구의 주요한 돌파구가 되었습니다. 이는 단순한 수치 계산을 넘어, 모델의 구조와 학습 과정 자체를 프로그래밍 가능한 대상으로 만들었습니다. 텐서 연산의 실제적 예는 3장 파이토치에서 추가적으로 살펴보겠습니다.</p>
</section>
<section id="특이값-분해와-주성분-분석" class="level3">
<h3 class="anchored" data-anchor-id="특이값-분해와-주성분-분석">2.1.5 특이값 분해와 주성분 분석</h3>
<p>특이값 분해(Singular Value Decomposition, SVD)와 주성분 분석(Principal Component Analysis, PCA)은 고차원 데이터의 차원을 축소하고, 데이터에 내재된 주요 특징을 추출하는 데 사용되는 강력한 수학적 도구입니다.</p>
<section id="특이값-분해-svd" class="level4">
<h4 class="anchored" data-anchor-id="특이값-분해-svd">특이값 분해 (SVD)</h4>
<p>SVD는 임의의 <span class="math inline">\(m \times n\)</span> 행렬 <span class="math inline">\(\mathbf{A}\)</span>를 다음과 같이 세 행렬의 곱으로 분해하는 방법입니다.</p>
<p><span class="math inline">\(\mathbf{A} = \mathbf{U\Sigma V^T}\)</span></p>
<p>여기서,</p>
<ul>
<li><span class="math inline">\(\mathbf{U}\)</span>: <span class="math inline">\(m \times m\)</span> 크기의 직교 행렬 (left singular vectors)</li>
<li><span class="math inline">\(\mathbf{\Sigma}\)</span>: <span class="math inline">\(m \times n\)</span> 크기의 대각 행렬 (singular values, 특이값)</li>
<li><span class="math inline">\(\mathbf{V}\)</span>: <span class="math inline">\(n \times n\)</span> 크기의 직교 행렬 (right singular vectors)</li>
</ul>
<p><strong>핵심 아이디어:</strong></p>
<ul>
<li><strong>특이값:</strong> <span class="math inline">\(\mathbf{\Sigma}\)</span>의 대각 원소(<span class="math inline">\(\sigma_1 \ge \sigma_2 \ge \dots \ge \sigma_r &gt; 0\)</span>)는 행렬 <span class="math inline">\(\mathbf{A}\)</span>의 특이값(singular value)이며, 해당 축(방향)으로의 데이터 분산(variation) 정도를 나타냅니다. 큰 특이값은 데이터의 중요한 특징을, 작은 특이값은 노이즈나 덜 중요한 정보를 나타냅니다.</li>
<li><strong>차원 축소:</strong> <span class="math inline">\(k\)</span>개의 가장 큰 특이값과 이에 대응하는 특이 벡터들만 사용하여 행렬 <span class="math inline">\(\mathbf{A}\)</span>를 근사(<span class="math inline">\(\mathbf{A} \approx \mathbf{U}_k \mathbf{\Sigma}_k \mathbf{V}_k^T\)</span>)하면, 원본 데이터의 주요 특징을 유지하면서 차원을 <span class="math inline">\(m \times n\)</span> 에서 <span class="math inline">\(k\)</span>로 줄일 수 있습니다.</li>
</ul>
<p><strong>딥러닝에서의 활용:</strong></p>
<ul>
<li><p><strong>모델 압축:</strong> 신경망의 가중치 행렬에 SVD를 적용하여 저차원 행렬로 근사하면, 모델의 크기를 줄이고 추론 속도를 향상시킬 수 있습니다. 특히, Transformer 기반 언어 모델(예: BERT)에서 임베딩 행렬의 크기를 줄이는 데 효과적입니다.</p></li>
<li><p><strong>추천 시스템</strong>: SVD를 활용하여 사용자와 아이템 간의 잠재 요인(latent factor)을 추출 할 수 있습니다.</p>
<ul>
<li><strong>잠재 요인</strong>: SVD를 통해 사용자와 아이템 행렬을 분해하면, 사용자와 아이템을 저차원 공간에 표현할수 있게 됩니다.
<ul>
<li>사용자의 잠재 요인: 사용자의 숨겨진 선호도를 나타냅니다. (예: 영화광, 액션을 선호함, 로맨틱 코미디를 선호함, …)</li>
<li>아이템의 잠재 요인: 아이템의 숨겨진 특징을 나타냅니다. (예: 블록버스터 영화, 배우 A가 출연함, 해피엔딩, …)</li>
</ul></li>
<li><strong>저차원 표현</strong>: SVD를 통해 원래는 매우 컸던 사용자-아이템 행렬을 저차원의 행렬 곱으로 근사할 수 있습니다.</li>
<li><strong>추천</strong>: 저차원 공간에서 사용자와 아이템의 유사도를 계산하거나, 내적을 통해 사용자가 특정 아이템을 선호할 확률을 예측할 수 있습니다.</li>
</ul></li>
</ul>
</section>
<section id="주성분-분석-pca" class="level4">
<h4 class="anchored" data-anchor-id="주성분-분석-pca">주성분 분석 (PCA)</h4>
<p>PCA는 데이터의 분산을 최대화하는 방향(주성분, principal component)을 찾아 데이터를 저차원 공간에 투영하는 방법입니다. SVD와 밀접하게 관련되어 있으며, 데이터의 공분산 행렬(covariance matrix)에 대한 고유값 분해(eigenvalue decomposition)를 통해 주성분을 찾습니다.</p>
<p><strong>PCA 단계:</strong></p>
<ol type="1">
<li><strong>데이터 중심화(Data Centering):</strong> 각 특성(feature)의 평균을 0으로 만듭니다.</li>
<li><strong>공분산 행렬 계산:</strong> 특성 간의 상관 관계를 나타내는 공분산 행렬을 계산합니다.</li>
<li><strong>고유값 분해:</strong> 공분산 행렬의 고유값(eigenvalue)과 고유벡터(eigenvector)를 계산합니다.
<ul>
<li>고유벡터: 주성분의 방향</li>
<li>고유값: 해당 주성분 방향으로의 분산 크기</li>
</ul></li>
<li><strong>주성분 선택:</strong> 가장 큰 고유값에 해당하는 고유벡터부터 <span class="math inline">\(k\)</span>개를 선택합니다. (데이터를 <span class="math inline">\(k\)</span>차원으로 축소)</li>
<li><strong>데이터 투영:</strong> 선택된 <span class="math inline">\(k\)</span>개의 주성분에 데이터를 투영하여 차원을 축소합니다.</li>
</ol>
<p><strong>딥러닝에서의 활용:</strong></p>
<ul>
<li><strong>데이터 전처리:</strong> 이미지, 텍스트 등 고차원 데이터를 저차원 공간에 투영하여 딥러닝 모델의 입력으로 사용하면, 계산 비용을 줄이고 과적합을 방지할 수 있습니다. 특히, 이미지 분류 문제에서 고해상도 이미지를 PCA를 통해 저차원으로 표현하면 모델 학습 속도를 높일 수 있습니다.</li>
<li><strong>특징 추출</strong>: PCA를 통해 추출된 주성분들은 서로 상관관계가 없고, 데이터의 분산을 최대로 보존하는 새로운 특징(feature)으로 해석될 수 있습니다.</li>
</ul>
<p><strong>SVD vs.&nbsp;PCA</strong></p>
<ul>
<li>SVD는 <em>행렬</em> 분해 기법, PCA는 <em>데이터</em>의 차원 축소 기법입니다.</li>
<li>PCA는 SVD를 사용하여 구현할 수 있습니다. (데이터 행렬의 SVD는 공분산 행렬의 고유값 분해와 관련됨)</li>
<li>PCA는 데이터의 평균을 0으로 맞추는 전처리 과정이 필요하지만, SVD는 이러한 과정 없이 바로 적용 가능합니다.</li>
</ul>
<p>SVD와 PCA는 딥러닝에서 데이터를 효율적으로 표현하고, 모델의 성능을 향상시키는 데 중요한 역할을 하는 수학적 도구입니다.</p>
<div id="cell-16" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_02.pca <span class="im">import</span> visualize_pca</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>visualize_pca()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="02_딥러닝의 수학_files/figure-html/cell-6-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Explained variance ratio: 0.5705</code></pre>
</div>
</div>
<p>이 예제는 복잡한 2차원 구조를 1차원으로 투영하는 PCA의 능력을 보여줍니다. 나선형 데이터의 경우, 단일 주성분으로는 모든 변동성을 캡처할 수 없지만, 데이터의 주요 트렌드를 포착할 수 있습니다. 설명된 분산 비율을 통해 이 1차원 표현이 원본 데이터의 구조를 얼마나 잘 보존하는지 평가할 수 있습니다</p>
<p>이 기법들은 복잡한 데이터에서 중요한 패턴을 추출하는 강력한 도구입니다.</p>
<ol type="1">
<li>데이터 전처리: 입력 데이터의 차원 축소</li>
<li>모델 압축: 가중치 행렬의 효율적 근사</li>
<li>특성 추출: 중요 특성 식별 및 선택</li>
</ol>
<p>SVD와 PCA는 고차원 데이터에서 중요한 패턴을 추출하고, 복잡한 데이터 구조를 단순화하는 강력한 도구입니다.</p>
</section>
</section>
</section>
<section id="미적분학과-최적화" class="level2">
<h2 class="anchored" data-anchor-id="미적분학과-최적화">2.2 미적분학과 최적화</h2>
<section id="체인룰" class="level3">
<h3 class="anchored" data-anchor-id="체인룰">2.2.1 체인룰</h3>
<blockquote class="blockquote">
<p><strong>도전과제:</strong> 어떻게 복잡하게 중첩된 함수의 미분을 효율적으로 계산할 수 있을까?</p>
<p><strong>연구자의 고뇌:</strong> 초기 딥러닝 연구자들은 신경망의 가중치를 업데이트하기 위해 역전파 알고리즘을 사용해야 했습니다. 하지만 신경망은 여러 층의 함수가 복잡하게 연결된 구조이기 때문에, 각 가중치에 대한 손실 함수의 미분을 계산하는 것은 매우 어려운 문제였습니다. 특히, 층이 깊어질수록 계산량이 기하급수적으로 증가하여 학습이 비효율적이었습니다.</p>
</blockquote>
<p>딥러닝에서 사용하는 가장 중요한 미적분 규칙은 체인룰(chain rule)입니다. 체인룰은 복합 함수의 미분을 구성 함수들의 미분의 곱으로 표현할 수 있게 해주는 강력하고 우아한 규칙입니다. 체인룰을 시각화하면 그 개념을 더 쉽게 이해할 수 있습니다. 예를 들어, <span class="math inline">\(z\)</span>가 <span class="math inline">\(x\)</span>와 <span class="math inline">\(y\)</span>의 함수이고, <span class="math inline">\(x\)</span>와 <span class="math inline">\(y\)</span>가 각각 <span class="math inline">\(s\)</span>와 <span class="math inline">\(t\)</span>의 함수라고 가정해 봅시다. 이 관계를 트리 다이어그램으로 나타낼 수 있습니다.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../../assets/images/02_01.chain_rule.png" class="img-fluid figure-img"></p>
<figcaption>체인룰</figcaption>
</figure>
</div>
<p>이 다이어그램에서, <span class="math inline">\(z\)</span>의 <span class="math inline">\(s\)</span>에 대한 편미분 <span class="math inline">\(\frac{\partial z}{\partial s}\)</span>는 <span class="math inline">\(z\)</span>에서 <span class="math inline">\(s\)</span>로 가는 모든 경로를 따라 편미분들의 곱을 더한 것과 같습니다.</p>
<p><span class="math inline">\(\frac{\partial z}{\partial s} = \frac{\partial z}{\partial x} \frac{\partial x}{\partial s} + \frac{\partial z}{\partial y} \frac{\partial y}{\partial s}\)</span></p>
<p>이 공식에서</p>
<ul>
<li><span class="math inline">\(\frac{\partial z}{\partial x}\)</span>와 <span class="math inline">\(\frac{\partial z}{\partial y}\)</span>는 <span class="math inline">\(z\)</span>가 <span class="math inline">\(x\)</span>와 <span class="math inline">\(y\)</span>에 대해 어떻게 변하는지를 나타냅니다.</li>
<li><span class="math inline">\(\frac{\partial x}{\partial s}\)</span>와 <span class="math inline">\(\frac{\partial y}{\partial s}\)</span>는 <span class="math inline">\(x\)</span>와 <span class="math inline">\(y\)</span>가 <span class="math inline">\(s\)</span>에 대해 어떻게 변하는지를 나타냅니다.</li>
</ul>
<p>다른 경우로 체인룰을 이용한 전미분 표현을 살펴봅시다. <span class="math inline">\(z\)</span>가 상호 독립적인 변수들의 함수일 때를 고려해 봅시다. 이 경우, 체인룰은 전미분의 형태로 간소화됩니다. 예를 들어, <span class="math inline">\(z = f(x, y)\)</span>이고 <span class="math inline">\(x = g(s)\)</span>, <span class="math inline">\(y = h(t)\)</span>일 때, <span class="math inline">\(s\)</span>와 <span class="math inline">\(t\)</span>가 서로 독립적이라면, <span class="math inline">\(z\)</span>의 전미분은 다음과 같이 표현됩니다.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../../assets/images/02_02_chain_rule.png" class="img-fluid figure-img"></p>
<figcaption>체인룰</figcaption>
</figure>
</div>
<p><span class="math inline">\(dz = \frac{\partial z}{\partial x}dx + \frac{\partial z}{\partial y}dy\)</span></p>
<p>여기서 <span class="math inline">\(dx = \frac{\partial x}{\partial s}ds\)</span>와 <span class="math inline">\(dy = \frac{\partial y}{\partial t}dt\)</span>이므로, 최종적으로 다음과 같은 형태가 됩니다.</p>
<p><span class="math inline">\(dz = \frac{\partial z}{\partial x}\frac{\partial x}{\partial s}ds + \frac{\partial z}{\partial y}\frac{\partial y}{\partial t}dt\)</span></p>
<p>이 식은 체인룰의 형태와 유사해 보이지만, 실제로는 전미분을 나타냅니다. 여기서 중요한 점은 <span class="math inline">\(s\)</span>와 <span class="math inline">\(t\)</span>가 독립적이기 때문에, <span class="math inline">\(\frac{\partial x}{\partial t}\)</span>와 <span class="math inline">\(\frac{\partial y}{\partial s}\)</span>가 0이 된다는 것입니다. 이러한 형태는 전미분입니다. 전미분은 모든 독립 변수의 변화가 함수값에 미치는 총 영향을 나타내며, 각 변수에 대한 편미분의 합으로 표현됩니다.</p>
<p>체인룰의 이런 구조는 복잡한 함수의 미분을 더 간단한 부분들로 분해할 수 있게 해줍니다. 이는 특히 딥러닝에서 중요한데, 신경망은 여러 층의 함수들이 중첩된 구조이기 때문입니다. 트리 다이어그램을 사용하면 더 복잡한 상황에서도 체인룰을 쉽게 적용할 수 있습니다. 종속 변수에서 시작하여 중간 변수들을 거쳐 독립 변수들로 이어지는 모든 경로를 찾고, 각 경로를 따라 편미분들을 곱한 다음, 이 결과들을 모두 더하면 됩니다.</p>
<p>체인룰은 딥러닝에서 역전파 알고리즘의 수학적 기반입니다. 복잡한 신경망 모델의 가중치를 효율적으로 업데이트할 수 있게 해주는 근간이 됩니다.</p>
</section>
<section id="그래디언트와-야코비안" class="level3">
<h3 class="anchored">2.2.2 그래디언트와 야코비안</h3>
<blockquote class="blockquote">
<p><strong>도전 과제</strong>: 다양한 형태의 입출력을 갖는 함수에 대한 미분을 어떻게 일반화 할 수 있을까?</p>
<p><strong>연구자의 고뇌</strong>: 초창기 딥러닝은 주로 스칼라 함수를 다루었지만, 점차 벡터, 행렬 등 다양한 형태의 입출력을 갖는 함수를 다루게 되었습니다. 이러한 함수들의 미분을 통일된 방식으로 표현하고 계산하는 것은 딥러닝 프레임워크 개발에 필수적인 과제였습니다.</p>
</blockquote>
<p>딥러닝에서는 다양한 형태의 입력(스칼라, 벡터, 행렬, 텐서)과 출력(스칼라, 벡터, 행렬, 텐서)을 갖는 함수를 다룹니다. 이에 따라 함수의 미분(도함수) 표현도 달라집니다. 핵심은 이러한 다양한 경우의 미분을 일관성 있게 표현하고, 연쇄 법칙(chain rule)을 적용하여 효율적으로 계산하는 것입니다.</p>
<section id="핵심-개념" class="level4">
<h4 class="anchored" data-anchor-id="핵심-개념">핵심 개념</h4>
<ul>
<li><p><strong>그래디언트(Gradient):</strong> 스칼라 함수를 벡터로 미분할 때 사용하는 표현입니다. 입력 벡터의 각 요소에 대한 함수의 편미분을 요소로 갖는 열벡터입니다. 함수의 가장 가파른 상승 방향을 나타냅니다.</p></li>
<li><p><strong>야코비안 행렬(Jacobian Matrix):</strong> 벡터 함수를 벡터로 미분할 때 사용하는 표현입니다. 출력 벡터의 각 요소를 입력 벡터의 각 요소로 편미분한 값들을 요소로 갖는 행렬입니다.</p></li>
</ul>
</section>
<section id="다양한-입출력-형태에-따른-도함수-표현" class="level4">
<h4 class="anchored" data-anchor-id="다양한-입출력-형태에-따른-도함수-표현">다양한 입출력 형태에 따른 도함수 표현</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 14%">
<col style="width: 12%">
<col style="width: 57%">
<col style="width: 15%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">입력 형태</th>
<th style="text-align: left;">출력 형태</th>
<th style="text-align: left;">도함수 표현</th>
<th style="text-align: left;">차원</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">벡터 (<span class="math inline">\(\mathbf{x}\)</span>)</td>
<td style="text-align: left;">벡터 (<span class="math inline">\(\mathbf{f}\)</span>)</td>
<td style="text-align: left;">야코비안 행렬 (<span class="math inline">\(\mathbf{J} = \frac{\partial \mathbf{f}}{\partial \mathbf{x}}\)</span>)</td>
<td style="text-align: left;"><span class="math inline">\(n \times m\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">행렬 (<span class="math inline">\(\mathbf{X}\)</span>)</td>
<td style="text-align: left;">벡터 (<span class="math inline">\(\mathbf{f}\)</span>)</td>
<td style="text-align: left;">3차원 텐서 (일반적으로 잘 다루지 않음)</td>
<td style="text-align: left;">-</td>
</tr>
<tr class="odd">
<td style="text-align: left;">벡터 (<span class="math inline">\(\mathbf{x}\)</span>)</td>
<td style="text-align: left;">행렬 (<span class="math inline">\(\mathbf{F}\)</span>)</td>
<td style="text-align: left;">3차원 텐서 (일반적으로 잘 다루지 않음)</td>
<td style="text-align: left;">-</td>
</tr>
<tr class="even">
<td style="text-align: left;">스칼라 (<span class="math inline">\(x\)</span>)</td>
<td style="text-align: left;">벡터 (<span class="math inline">\(\mathbf{f}\)</span>)</td>
<td style="text-align: left;">열벡터 (<span class="math inline">\(\frac{\partial \mathbf{f}}{\partial x}\)</span>)</td>
<td style="text-align: left;"><span class="math inline">\(n \times 1\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;">벡터 (<span class="math inline">\(\mathbf{x}\)</span>)</td>
<td style="text-align: left;">스칼라 (<span class="math inline">\(f\)</span>)</td>
<td style="text-align: left;">그래디언트 (<span class="math inline">\(\nabla f = \frac{\partial f}{\partial \mathbf{x}}\)</span>)</td>
<td style="text-align: left;"><span class="math inline">\(m \times 1\)</span> (열벡터)</td>
</tr>
<tr class="even">
<td style="text-align: left;">행렬 (<span class="math inline">\(\mathbf{X}\)</span>)</td>
<td style="text-align: left;">스칼라 (<span class="math inline">\(f\)</span>)</td>
<td style="text-align: left;">행렬 (<span class="math inline">\(\frac{\partial f}{\partial \mathbf{X}}\)</span>)</td>
<td style="text-align: left;"><span class="math inline">\(m \times n\)</span></td>
</tr>
</tbody>
</table>
<p><strong>참고:</strong></p>
<ul>
<li><p><span class="math inline">\(m\)</span>: 입력 벡터/행렬의 차원, <span class="math inline">\(n\)</span>: 출력 벡터/행렬의 차원, <span class="math inline">\(p, q\)</span>: 행렬의 행/열 개수</p></li>
<li><p>행렬 입력, 벡터/행렬 출력의 경우, 도함수는 3차원 텐서가 됩니다. 딥러닝 프레임워크는 내부적으로 이러한 고차원 텐서 연산을 효율적으로 처리하지만, 일반적으로는 벡터/행렬 입출력에 대한 야코비안/그래디언트 계산이 주를 이룹니다.</p></li>
</ul>
</section>
<section id="딥러닝에서의-활용" class="level4">
<h4 class="anchored">딥러닝에서의 활용</h4>
<ul>
<li><strong>역전파 알고리즘:</strong> 야코비안 행렬과 그래디언트는 딥러닝에서 역전파 알고리즘을 구현할 때 핵심적인 역할을 합니다. 신경망의 각 층을 통과하며 연쇄 법칙을 적용하여 가중치에 대한 손실 함수의 그래디언트를 계산하고, 이를 통해 가중치를 업데이트합니다.</li>
<li><strong>자동 미분:</strong> 현대 딥러닝 프레임워크(TensorFlow, PyTorch 등)는 자동 미분(Automatic Differentiation) 기능을 제공하여 이러한 복잡한 미분 계산을 자동으로 처리합니다. 사용자는 복잡한 미분 공식을 직접 구현할 필요 없이, 모델의 구조와 손실 함수만 정의하면 됩니다.</li>
</ul>
<p>이처럼, 그래디언트와 야코비안 행렬의 개념은 딥러닝에서 다양한 형태의 함수에 대한 미분을 일반화하고, 역전파를 통해 효율적으로 모델을 학습시키는 데 필수적인 도구입니다.</p>
<div class="callout callout-style-default callout-note callout-titled" title="클릭하여 내용 보기 (딥다이브: 헤시안 행렬)">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-5-contents" aria-controls="callout-5" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
클릭하여 내용 보기 (딥다이브: 헤시안 행렬)
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-5" class="callout-5-contents callout-collapse collapse">
<section id="헤시안-행렬-hessian-matrix" class="level2 callout-body-container callout-body">
<h2 class="anchored" data-anchor-id="헤시안-행렬-hessian-matrix">헤시안 행렬 (Hessian Matrix)</h2>
<section id="헤시안-행렬의-정의와-의미" class="level3">
<h3 class="anchored" data-anchor-id="헤시안-행렬의-정의와-의미">1. 헤시안 행렬의 정의와 의미</h3>
<ul>
<li><p><strong>정의:</strong> 헤시안 행렬은 스칼라 함수(scalar-valued function)의 이계도 함수(second-order partial derivatives)를 행렬 형태로 표현한 것입니다. 즉, 함수 <span class="math inline">\(f(x_1, x_2, ..., x_n)\)</span>가 주어졌을 때, 헤시안 행렬 <span class="math inline">\(H\)</span>는 다음과 같이 정의됩니다.</p>
<p><span class="math display">\[
H = \begin{bmatrix}
\frac{\partial^2 f}{\partial x_1^2} &amp; \frac{\partial^2 f}{\partial x_1 \partial x_2} &amp; \cdots &amp; \frac{\partial^2 f}{\partial x_1 \partial x_n} \\
\frac{\partial^2 f}{\partial x_2 \partial x_1} &amp; \frac{\partial^2 f}{\partial x_2^2} &amp; \cdots &amp; \frac{\partial^2 f}{\partial x_2 \partial x_n} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\frac{\partial^2 f}{\partial x_n \partial x_1} &amp; \frac{\partial^2 f}{\partial x_n \partial x_2} &amp; \cdots &amp; \frac{\partial^2 f}{\partial x_n^2}
\end{bmatrix}
\]</span></p>
<ul>
<li>각 원소는 함수의 각 변수에 대한 두 번 미분한 값을 나타냅니다.</li>
<li><strong>대칭 행렬(Symmetric Matrix):</strong> 연속인 이계도 함수를 가지는 경우, 편미분의 순서를 바꿔도 결과가 같으므로(Schwarz’s theorem), 헤시안 행렬은 대칭 행렬이 됩니다. (<span class="math inline">\(\frac{\partial^2 f}{\partial x_i \partial x_j} = \frac{\partial^2 f}{\partial x_j \partial x_i}\)</span>)</li>
</ul></li>
<li><p><strong>의미:</strong></p>
<ul>
<li><strong>곡률(Curvature):</strong> 헤시안 행렬은 함수의 국소적인 곡률 정보를 담고 있습니다. 함수의 그래프가 특정 지점에서 얼마나 굽어있는지를 나타냅니다.</li>
<li><strong>변화율의 변화율:</strong> 일계도 함수(그레디언트, Gradient)가 함수의 변화율을 나타낸다면, 헤시안 행렬은 그 변화율이 얼마나 빠르게 변하는지를 나타냅니다.</li>
</ul></li>
</ul>
</section>
<section id="헤시안-행렬과-함수의-극값-critical-point-판별" class="level3">
<h3 class="anchored" data-anchor-id="헤시안-행렬과-함수의-극값-critical-point-판별">2. 헤시안 행렬과 함수의 극값 (Critical Point) 판별</h3>
<ul>
<li><strong>임계점(Critical Point):</strong> 함수의 기울기(gradient)가 0이 되는 지점입니다. 즉, 모든 변수에 대한 일계 편미분 값이 0인 지점입니다. (<span class="math inline">\(\nabla f = 0\)</span>)</li>
<li><strong>극값 판별:</strong>
<ul>
<li>헤시안 행렬은 임계점에서 함수가 극대값(local maximum), 극소값(local minimum), 또는 안장점(saddle point)을 갖는지 판별하는 데 사용됩니다.</li>
<li><strong>극소값(Local Minimum):</strong> 헤시안 행렬이 <strong>양의 정부호(positive definite)</strong> 행렬이면 해당 임계점은 극소값입니다. (모든 고유값(eigenvalue)이 양수)</li>
<li><strong>극대값(Local Maximum):</strong> 헤시안 행렬이 <strong>음의 정부호(negative definite)</strong> 행렬이면 해당 임계점은 극대값입니다. (모든 고유값이 음수)</li>
<li><strong>안장점(Saddle Point):</strong> 헤시안 행렬이 <strong>부정부호(indefinite)</strong> 행렬이면 해당 임계점은 안장점입니다. (양수와 음수 고유값을 모두 가짐)</li>
<li><strong>Semi-definite:</strong> 헤시안이 positive/negative semi-definite이면, 추가적인 정보 없이는 극점의 유형을 판별할 수 없습니다. (고유값이 0을 포함)</li>
</ul></li>
</ul>
</section>
<section id="딥러닝에서-헤시안-행렬의-활용" class="level3">
<h3 class="anchored" data-anchor-id="딥러닝에서-헤시안-행렬의-활용">3. 딥러닝에서 헤시안 행렬의 활용</h3>
<ul>
<li><strong>뉴턴 방법 (Newton’s Method):</strong>
<ul>
<li>함수의 극값을 찾는 최적화 알고리즘 중 하나입니다.</li>
<li>경사 하강법(Gradient Descent)이 일계 미분(그레디언트)을 사용하는 반면, 뉴턴 방법은 이계 미분(헤시안)을 사용하여 더 빠르게 수렴할 수 있습니다.</li>
<li>업데이트 규칙: <span class="math inline">\(x_{k+1} = x_k - H^{-1}(x_k) \nabla f(x_k)\)</span> (H는 헤시안 행렬)</li>
</ul></li>
<li><strong>Curvature Matrix:</strong>
<ul>
<li>헤시안 행렬은 손실 함수(loss function)의 곡률을 나타내는 curvature matrix로 사용될 수 있습니다.</li>
<li>곡률 정보를 활용하여 학습률(learning rate)을 조절하거나, 최적화 알고리즘의 성능을 개선하는 데 활용됩니다. (e.g., Natural Gradient Descent)</li>
</ul></li>
</ul>
</section>
</section>
</div>
</div>
</section>
</section>
<section id="체인룰과-신경망의-역전파" class="level3">
<h3 class="anchored" data-anchor-id="체인룰과-신경망의-역전파">2.2.3 체인룰과 신경망의 역전파</h3>
<p>신경망 학습의 핵심은 역전파(Backpropagation) 알고리즘입니다. 역전파는 출력층에서 발생한 오차를 입력층 방향으로 전파하면서 각 층의 가중치와 편향을 업데이트하는 효율적인 방법입니다. 이 과정에서 체인룰(Chain Rule)은 복잡한 합성 함수의 미분을 간단한 미분들의 곱으로 표현하여 계산을 가능하게 합니다.</p>
<section id="신경망에서의-체인룰-적용" class="level4">
<h4 class="anchored" data-anchor-id="신경망에서의-체인룰-적용">신경망에서의 체인룰 적용</h4>
<p>신경망은 여러 층의 함수들이 합성된 형태입니다. 예를 들어, 2층 신경망은 다음과 같이 표현될 수 있습니다.</p>
<p><span class="math inline">\(\mathbf{z} = f_1(\mathbf{x}; \mathbf{W_1}, \mathbf{b_1})\)</span> <span class="math inline">\(\mathbf{y} = f_2(\mathbf{z}; \mathbf{W_2}, \mathbf{b_2})\)</span></p>
<p>여기서 <span class="math inline">\(\mathbf{x}\)</span>는 입력, <span class="math inline">\(\mathbf{z}\)</span>는 첫 번째 층의 출력(두 번째 층의 입력), <span class="math inline">\(\mathbf{y}\)</span>는 최종 출력, <span class="math inline">\(\mathbf{W_1}\)</span>, <span class="math inline">\(\mathbf{b_1}\)</span>은 첫 번째 층의 가중치와 편향, <span class="math inline">\(\mathbf{W_2}\)</span>, <span class="math inline">\(\mathbf{b_2}\)</span>는 두 번째 층의 가중치와 편향입니다.</p>
<p>역전파 과정에서 우리는 손실 함수 <span class="math inline">\(E\)</span>의 각 파라미터에 대한 그래디언트 (<span class="math inline">\(\frac{\partial E}{\partial \mathbf{W_1}}\)</span>, <span class="math inline">\(\frac{\partial E}{\partial \mathbf{b_1}}\)</span>, <span class="math inline">\(\frac{\partial E}{\partial \mathbf{W_2}}\)</span>, <span class="math inline">\(\frac{\partial E}{\partial \mathbf{b_2}}\)</span>)를 계산해야 합니다. 이때 체인룰을 적용하면 다음과 같이 계산할 수 있습니다.</p>
<p><span class="math inline">\(\frac{\partial E}{\partial \mathbf{W_2}} = \frac{\partial E}{\partial \mathbf{y}} \frac{\partial \mathbf{y}}{\partial \mathbf{W_2}}\)</span> <span class="math inline">\(\frac{\partial E}{\partial \mathbf{b_2}} = \frac{\partial E}{\partial \mathbf{y}} \frac{\partial \mathbf{y}}{\partial \mathbf{b_2}}\)</span> <span class="math inline">\(\frac{\partial E}{\partial \mathbf{W_1}} = \frac{\partial E}{\partial \mathbf{y}} \frac{\partial \mathbf{y}}{\partial \mathbf{z}} \frac{\partial \mathbf{z}}{\partial \mathbf{W_1}}\)</span> <span class="math inline">\(\frac{\partial E}{\partial \mathbf{b_1}} = \frac{\partial E}{\partial \mathbf{y}} \frac{\partial \mathbf{y}}{\partial \mathbf{z}} \frac{\partial \mathbf{z}}{\partial \mathbf{b_1}}\)</span></p>
<p>이처럼 체인룰을 이용하면, 복잡한 신경망의 각 파라미터에 대한 그래디언트를 연쇄적인 미분의 곱으로 분해하여 효율적으로 계산할 수 있습니다. 2.2.4의 이론 딥다이브는 이 과정을 자세히 설명합니다.</p>
</section>
<section id="그래디언트와-방향-도함수" class="level4">
<h4 class="anchored" data-anchor-id="그래디언트와-방향-도함수">그래디언트와 방향 도함수</h4>
<ul>
<li><strong>그래디언트(Gradient):</strong> 다변수 함수의 각 변수에 대한 편미분(partial derivative)을 모아 놓은 벡터입니다. 함수의 가장 가파른 상승 방향을 나타냅니다.</li>
<li><strong>방향 도함수(Directional Derivative):</strong> 특정 방향으로의 함수의 변화율을 나타냅니다. 그래디언트와 방향 벡터의 내적(dot product)으로 계산할 수 있습니다.</li>
</ul>
</section>
<section id="그래디언트-표현의-주의점" class="level4">
<h4 class="anchored" data-anchor-id="그래디언트-표현의-주의점">그래디언트 표현의 주의점</h4>
<ul>
<li><strong>열벡터 vs.&nbsp;행벡터:</strong> 일반적으로 벡터는 열벡터로 표현하는 것이 관례이지만, 딥러닝에서는 문맥에 따라 행벡터로 표현하는 경우도 있습니다. 일관성을 유지하는 것이 중요합니다. (이 책에서는 분자 표기법을 사용합니다.)</li>
<li><strong>야코비안 행렬(Jacobian Matrix):</strong> 여러 개의 입력 변수와 여러 개의 출력 변수를 갖는 함수(벡터 함수)의 경우, 모든 편미분 값을 포함하는 행렬입니다. 딥러닝에서 역전파 계산에 사용됩니다.</li>
</ul>
<p>이러한 개념들을 바탕으로, 다음 절에서는 구체적인 예시와 함께 역전파 과정에서의 그래디언트 계산 방법을 자세히 살펴보겠습니다.</p>
</section>
</section>
<section id="역전파를-위한-그래디언트-계산" class="level3">
<h3 class="anchored" data-anchor-id="역전파를-위한-그래디언트-계산">2.2.4 역전파를 위한 그래디언트 계산</h3>
<p>역전파의 핵심은 손실 함수(Loss Function)의 그래디언트를 계산하여 가중치를 업데이트하는 것입니다. 간단한 선형 변환(<span class="math inline">\(\mathbf{y} = \mathbf{xW} + \mathbf{b}\)</span>)을 예로 들어 역전파 과정을 살펴보겠습니다.</p>
<section id="역전파의-핵심-아이디어" class="level4">
<h4 class="anchored" data-anchor-id="역전파의-핵심-아이디어">1. 역전파의 핵심 아이디어</h4>
<p>역전파는 출력층에서 계산된 오차를 입력층 방향으로 전파하면서, 각 가중치가 오차에 기여한 만큼 가중치를 업데이트하는 알고리즘입니다. 이 과정에서 각 가중치에 대한 손실 함수의 그래디언트를 계산하는 것이 핵심입니다.</p>
</section>
<section id="손실-함수의-그래디언트" class="level4">
<h4 class="anchored" data-anchor-id="손실-함수의-그래디언트">2. 손실 함수의 그래디언트</h4>
<p>평균 제곱 오차(Mean Squared Error, MSE)를 손실 함수로 사용하면, 출력 <span class="math inline">\(\mathbf{y}\)</span>에 대한 손실 함수 <span class="math inline">\(E\)</span>의 그래디언트는 다음과 같습니다.</p>
<p><span class="math inline">\(E = \frac{1}{M} \sum_{i=1}^{M} (y_i - \hat{y}_i)^2\)</span></p>
<p><span class="math inline">\(\frac{\partial E}{\partial \mathbf{y}} = \frac{2}{M}(\mathbf{y} - \hat{\mathbf{y}})\)</span></p>
<p>여기서 <span class="math inline">\(y_i\)</span>는 실제 값, <span class="math inline">\(\hat{y}_i\)</span>는 모델의 예측값, <span class="math inline">\(M\)</span>은 데이터의 개수입니다.</p>
</section>
<section id="가중치에-대한-그래디언트" class="level4">
<h4 class="anchored" data-anchor-id="가중치에-대한-그래디언트">3. 가중치에 대한 그래디언트</h4>
<p>체인룰을 적용하여 가중치 <span class="math inline">\(\mathbf{W}\)</span>에 대한 손실 함수 <span class="math inline">\(E\)</span>의 그래디언트를 계산할 수 있습니다.</p>
<p><span class="math inline">\(\frac{\partial E}{\partial \mathbf{W}} = \frac{\partial E}{\partial \mathbf{y}} \frac{\partial \mathbf{y}}{\partial \mathbf{W}}\)</span></p>
<p><span class="math inline">\(\mathbf{y} = \mathbf{xW} + \mathbf{b}\)</span> 이므로, <span class="math inline">\(\frac{\partial \mathbf{y}}{\partial \mathbf{W}} = \mathbf{x}^T\)</span> 입니다.</p>
<p>최종적으로 가중치에 대한 그래디언트는 다음과 같이 표현됩니다.</p>
<p><span class="math inline">\(\frac{\partial E}{\partial \mathbf{W}} = \mathbf{x}^T \frac{\partial E}{\partial \mathbf{y}}\)</span></p>
</section>
<section id="입력에-대한-그래디언트" class="level4">
<h4 class="anchored" data-anchor-id="입력에-대한-그래디언트">4. 입력에 대한 그래디언트</h4>
<p>입력 <span class="math inline">\(\mathbf{x}\)</span>에 대한 손실 함수 <span class="math inline">\(E\)</span>의 그래디언트는 이전 층으로 오차를 전파하는 데 사용됩니다.</p>
<p><span class="math inline">\(\frac{\partial E}{\partial \mathbf{x}} = \frac{\partial E}{\partial \mathbf{y}} \frac{\partial \mathbf{y}}{\partial \mathbf{x}}\)</span></p>
<p><span class="math inline">\(\mathbf{y} = \mathbf{xW} + \mathbf{b}\)</span> 이므로, <span class="math inline">\(\frac{\partial \mathbf{y}}{\partial \mathbf{x}} = \mathbf{W}^T\)</span> 입니다.</p>
<p>따라서, 입력에 대한 그래디언트는 다음과 같습니다.</p>
<p><span class="math inline">\(\frac{\partial E}{\partial \mathbf{x}} = \frac{\partial E}{\partial \mathbf{y}} \mathbf{W}^T\)</span></p>
</section>
<section id="정리" class="level4">
<h4 class="anchored" data-anchor-id="정리">5. 정리</h4>
<p>역전파는 다음과 같은 핵심 단계를 통해 이루어집니다.</p>
<ol type="1">
<li><strong>순전파(Forward Propagation):</strong> 입력 데이터 <span class="math inline">\(\mathbf{x}\)</span>를 신경망에 통과시켜 예측값 <span class="math inline">\(\hat{\mathbf{y}}\)</span>를 계산합니다.</li>
<li><strong>손실 함수 계산:</strong> 예측값 <span class="math inline">\(\hat{\mathbf{y}}\)</span>와 실제 값 <span class="math inline">\(\mathbf{y}\)</span>를 비교하여 손실 <span class="math inline">\(E\)</span>를 계산합니다.</li>
<li><strong>역전파(Backward Propagation):</strong>
<ul>
<li>출력층에서 손실 함수의 그래디언트 <span class="math inline">\(\frac{\partial E}{\partial \mathbf{y}}\)</span>를 계산합니다.</li>
<li>체인룰을 이용하여 가중치에 대한 그래디언트 <span class="math inline">\(\frac{\partial E}{\partial \mathbf{W}} = \mathbf{x}^T \frac{\partial E}{\partial \mathbf{y}}\)</span>를 계산합니다.</li>
<li>입력에 대한 그래디언트 <span class="math inline">\(\frac{\partial E}{\partial \mathbf{x}} = \frac{\partial E}{\partial \mathbf{y}} \mathbf{W}^T\)</span>를 계산하여 이전 층으로 오차를 전파합니다.</li>
</ul></li>
<li><strong>가중치 업데이트:</strong> 계산된 그래디언트를 사용하여 경사 하강법 등의 최적화 알고리즘으로 가중치를 업데이트합니다.</li>
</ol>
<p>역전파 알고리즘은 딥러닝 모델 학습의 핵심이며, 이를 통해 복잡한 비선형 함수를 효과적으로 근사할 수 있습니다.</p>
<div class="callout callout-style-default callout-note callout-titled" title="클릭하여 내용 보기(딥다이브 : 역전파를 위한 그래디언트 계산)">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-6-contents" aria-controls="callout-6" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
클릭하여 내용 보기(딥다이브 : 역전파를 위한 그래디언트 계산)
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-6" class="callout-6-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>역전파의 핵심은 손실 함수(Loss Function)의 그래디언트를 계산하여 가중치를 업데이트하는 것입니다. 간단한 선형 변환(<span class="math inline">\(\mathbf{y} = \mathbf{xW} + \mathbf{b}\)</span>)을 예로 들어 역전파 과정을 살펴보겠습니다. 여기서는 계산 과정을 최대한 상세하게 풀어서 설명합니다.</p>
<section id="손실-함수의-그래디언트-1" class="level4">
<h4 class="anchored" data-anchor-id="손실-함수의-그래디언트-1">손실 함수의 그래디언트</h4>
<p>신경망 학습의 목표는 손실 함수 <span class="math inline">\(E\)</span>를 최소화하는 것입니다. 평균 제곱 오차(MSE)를 손실 함수로 사용하는 경우는 다음과 같습니다.</p>
<p><span class="math inline">\(E = f(\mathbf{y}) = \frac{1}{M} \sum_{i=1}^{M} (y_i - \hat{y}_i)^2\)</span></p>
<p>여기서 <span class="math inline">\(y_i\)</span>는 실제 값, <span class="math inline">\(\hat{y}_i\)</span>는 예측값, <span class="math inline">\(M\)</span>은 데이터의 개수(또는 출력 벡터의 차원)입니다.</p>
<p><span class="math inline">\(E\)</span>의 <span class="math inline">\(\mathbf{y}\)</span>에 대한 도함수(derivative)는 다음과 같습니다.</p>
<p><span class="math inline">\(\frac{\partial E}{\partial \mathbf{y}} = \frac{2}{M} (\mathbf{y} - \hat{\mathbf{y}})\)</span></p>
<p>여기서 <span class="math inline">\(\mathbf{y}\)</span>는 신경망의 출력 벡터, <span class="math inline">\(\hat{\mathbf{y}}\)</span>는 실제 값(타겟) 벡터입니다. <span class="math inline">\(y_i\)</span>는 상수(타겟의 각 요소)이므로, <span class="math inline">\(\mathbf{y}\)</span>에 대한 편미분만 남게 됩니다.</p>
<p><strong>주의:</strong> 1장의 예제 코드에서는 <span class="math inline">\(-\frac{2}{M}\)</span> 항을 사용했는데, 이는 손실 함수 정의에 음수 부호(-)가 포함되어 있었기 때문입니다. 여기서는 일반적인 MSE 정의를 사용하므로 양수 <span class="math inline">\(\frac{2}{M}\)</span>을 사용합니다. 실제 학습에서는 학습률(learning rate)을 곱하므로 이 상수의 절대적인 크기는 중요하지 않습니다.</p>
</section>
<section id="가중치에-대한-손실-함수의-그래디언트" class="level4">
<h4 class="anchored" data-anchor-id="가중치에-대한-손실-함수의-그래디언트">가중치에 대한 손실 함수의 그래디언트</h4>
<p>이제, 가중치 <span class="math inline">\(\mathbf{W}\)</span>에 대한 손실 함수 <span class="math inline">\(E\)</span>의 그래디언트를 계산해 보겠습니다. <span class="math inline">\(E = f(\mathbf{y})\)</span>이고 <span class="math inline">\(\mathbf{y} = \mathbf{xW} + \mathbf{b}\)</span> 입니다. <span class="math inline">\(\mathbf{x}\)</span>는 입력 벡터, <span class="math inline">\(\mathbf{W}\)</span>는 가중치 행렬, <span class="math inline">\(\mathbf{b}\)</span>는 편향 벡터입니다.</p>
<p><strong>계산 그래프:</strong></p>
<p>역전파 과정을 시각적으로 표현하기 위해 계산 그래프를 사용할 수 있습니다. (계산 그래프 그림 삽입)</p>
<p><span class="math inline">\(E\)</span>는 스칼라 값이고, 각 <span class="math inline">\(w_{ij}\)</span> (가중치 행렬 <span class="math inline">\(\mathbf{W}\)</span>의 각 요소)에 대해 <span class="math inline">\(E\)</span>의 편미분을 구해야 합니다. <span class="math inline">\(\mathbf{W}\)</span>는 (입력 차원) x (출력 차원) 크기의 행렬입니다. 예를 들어, 입력이 3차원(<span class="math inline">\(x_1, x_2, x_3\)</span>), 출력이 2차원(<span class="math inline">\(y_1, y_2\)</span>)이라면, <span class="math inline">\(\mathbf{W}\)</span>는 3x2 행렬이 됩니다.</p>
<p><span class="math inline">\(\frac{\partial E}{\partial \mathbf{W}} = \begin{bmatrix}
\frac{\partial E}{\partial w_{11}} &amp; \frac{\partial E}{\partial w_{12}} \\
\frac{\partial E}{\partial w_{21}} &amp; \frac{\partial E}{\partial w_{22}} \\
\frac{\partial E}{\partial w_{31}} &amp; \frac{\partial E}{\partial w_{32}}
\end{bmatrix}\)</span></p>
<p><span class="math inline">\(E\)</span>의 <span class="math inline">\(\mathbf{y}\)</span>에 대한 도함수는 <span class="math inline">\(\frac{\partial E}{\partial \mathbf{y}} = \begin{bmatrix} \frac{\partial E}{\partial y_1} &amp; \frac{\partial E}{\partial y_2} \end{bmatrix}\)</span> 와 같이 행 벡터로 표현될 수 있습니다. (분자 표기법 사용). 엄밀하게는 그래디언트는 열 벡터로 표현해야 하지만, 여기서는 계산의 편의를 위해 행 벡터를 사용합니다.</p>
<p>체인룰에 의해,</p>
<p><span class="math inline">\(\frac{\partial E}{\partial \mathbf{W}} = \frac{\partial E}{\partial \mathbf{y}} \frac{\partial \mathbf{y}}{\partial \mathbf{W}}\)</span></p>
<p><span class="math inline">\(\frac{\partial E}{\partial w_{ij}} = \sum_k \frac{\partial E}{\partial y_k} \frac{\partial y_k}{\partial w_{ij}}\)</span> (여기서 <span class="math inline">\(k\)</span>는 출력 벡터 <span class="math inline">\(\mathbf{y}\)</span>의 인덱스)</p>
<p>위 식을 풀어서 쓰면,</p>
<p><span class="math inline">\(\frac{\partial E}{\partial \mathbf{W}} = \frac{\partial E}{\partial y_1} \frac{\partial y_1}{\partial \mathbf{W}} + \frac{\partial E}{\partial y_2} \frac{\partial y_2}{\partial \mathbf{W}}\)</span></p>
<p>이제 <span class="math inline">\(\frac{\partial y_k}{\partial w_{ij}}\)</span> 를 계산해야 합니다. <span class="math inline">\(\mathbf{y} = \mathbf{xW} + \mathbf{b}\)</span> 이므로,</p>
<p><span class="math inline">\(y_1 = x_1w_{11} + x_2w_{21} + x_3w_{31} + b_1\)</span> <span class="math inline">\(y_2 = x_1w_{12} + x_2w_{22} + x_3w_{32} + b_2\)</span></p>
<p><span class="math inline">\(\frac{\partial y_1}{\partial w_{ij}} = \begin{bmatrix}
\frac{\partial y_1}{\partial w_{11}} &amp; \frac{\partial y_1}{\partial w_{12}} \\
\frac{\partial y_1}{\partial w_{21}} &amp; \frac{\partial y_1}{\partial w_{22}} \\
\frac{\partial y_1}{\partial w_{31}} &amp; \frac{\partial y_1}{\partial w_{32}}
\end{bmatrix} =
\begin{bmatrix}
x_1 &amp; 0 \\
x_2 &amp; 0 \\
x_3 &amp; 0
\end{bmatrix}\)</span></p>
<p><span class="math inline">\(\frac{\partial y_2}{\partial w_{ij}} = \begin{bmatrix}
0 &amp; x_1 \\
0 &amp; x_2 \\
0 &amp; x_3
\end{bmatrix}\)</span></p>
<p>따라서,</p>
<p><span class="math inline">\(\frac{\partial E}{\partial \mathbf{W}} = \frac{\partial E}{\partial y_1} \begin{bmatrix}
x_1 &amp; 0 \\
x_2 &amp; 0 \\
x_3 &amp; 0
\end{bmatrix} + \frac{\partial E}{\partial y_2} \begin{bmatrix}
0 &amp; x_1 \\
0 &amp; x_2 \\
0 &amp; x_3
\end{bmatrix} = \begin{bmatrix}
\frac{\partial E}{\partial y_1}x_1 &amp; \frac{\partial E}{\partial y_2}x_1 \\
\frac{\partial E}{\partial y_1}x_2 &amp; \frac{\partial E}{\partial y_2}x_2 \\
\frac{\partial E}{\partial y_1}x_3 &amp; \frac{\partial E}{\partial y_2}x_3
\end{bmatrix} = \begin{bmatrix} x_1 \\ x_2 \\ x_3 \end{bmatrix} \begin{bmatrix} \frac{\partial E}{\partial y_1} &amp; \frac{\partial E}{\partial y_2} \end{bmatrix} = \mathbf{x}^T \frac{\partial E}{\partial \mathbf{y}}\)</span></p>
<p><strong>일반화:</strong></p>
<p>입력이 <span class="math inline">\(1 \times m\)</span> 행 벡터 <span class="math inline">\(\mathbf{x}\)</span>, 출력이 <span class="math inline">\(1 \times n\)</span> 행 벡터 <span class="math inline">\(\mathbf{y}\)</span>인 경우, 가중치 <span class="math inline">\(\mathbf{W}\)</span>는 <span class="math inline">\(m \times n\)</span> 행렬이 됩니다. 이때,</p>
<p><span class="math inline">\(\frac{\partial E}{\partial \mathbf{W}} = \mathbf{x}^T \frac{\partial E}{\partial \mathbf{y}}\)</span></p>
</section>
<section id="입력에-대한-손실-함수의-그래디언트" class="level4">
<h4 class="anchored" data-anchor-id="입력에-대한-손실-함수의-그래디언트">입력에 대한 손실 함수의 그래디언트</h4>
<p>입력 <span class="math inline">\(\mathbf{x}\)</span>에 대한 손실 함수 <span class="math inline">\(E\)</span>의 그래디언트도 마찬가지로 체인룰을 사용하여 계산할 수 있습니다.</p>
<p><span class="math inline">\(\frac{\partial E}{\partial \mathbf{x}} = \frac{\partial E}{\partial \mathbf{y}} \frac{\partial \mathbf{y}}{\partial \mathbf{x}}\)</span></p>
<p><span class="math inline">\(\mathbf{y} = \mathbf{xW} + \mathbf{b}\)</span> 이므로, <span class="math inline">\(\frac{\partial \mathbf{y}}{\partial \mathbf{x}} = \mathbf{W}^T\)</span> 입니다.</p>
<p>따라서,</p>
<p><span class="math inline">\(\frac{\partial E}{\partial \mathbf{x}} = \frac{\partial E}{\partial \mathbf{y}} \mathbf{W}^T\)</span></p>
</section>
<section id="편향에-대한-그래디언트" class="level4">
<h4 class="anchored" data-anchor-id="편향에-대한-그래디언트">편향에 대한 그래디언트</h4>
<p>편향 <span class="math inline">\(\mathbf{b}\)</span>에 대한 손실 함수의 그래디언트는 다음과 같습니다.</p>
<p><span class="math inline">\(\frac{\partial E}{\partial \mathbf{b}} = \frac{\partial E}{\partial \mathbf{y}} \frac{\partial \mathbf{y}}{\partial \mathbf{b}}\)</span></p>
<p><span class="math inline">\(\mathbf{y} = \mathbf{xW} + \mathbf{b}\)</span> 이므로, <span class="math inline">\(\frac{\partial \mathbf{y}}{\partial \mathbf{b}} = \begin{bmatrix} 1 &amp; 1 &amp; \dots &amp; 1\end{bmatrix}\)</span> (1로만 이루어진 <span class="math inline">\(1 \times n\)</span> 행벡터)</p>
<p><span class="math inline">\(\frac{\partial E}{\partial \mathbf{b}} = \frac{\partial E}{\partial \mathbf{y}}\)</span></p>
</section>
<section id="정리-및-추가-설명" class="level4">
<h4 class="anchored" data-anchor-id="정리-및-추가-설명">정리 및 추가 설명</h4>
<ol type="1">
<li><strong>가중치에 대한 그래디언트:</strong> <span class="math inline">\(\frac{\partial E}{\partial \mathbf{W}} = \mathbf{x}^T \frac{\partial E}{\partial \mathbf{y}}\)</span>
<ul>
<li>입력 벡터 <span class="math inline">\(\mathbf{x}\)</span>의 전치(transpose)와 출력에 대한 손실 함수의 그래디언트(<span class="math inline">\(\frac{\partial E}{\partial \mathbf{y}}\)</span>, 여기서는 행 벡터로 표현)의 행렬 곱으로 계산됩니다.</li>
</ul></li>
<li><strong>입력에 대한 그래디언트:</strong> <span class="math inline">\(\frac{\partial E}{\partial \mathbf{x}} = \frac{\partial E}{\partial \mathbf{y}} \mathbf{W}^T\)</span>
<ul>
<li>출력에 대한 손실 함수의 그래디언트(<span class="math inline">\(\frac{\partial E}{\partial \mathbf{y}}\)</span>)와 가중치 행렬 <span class="math inline">\(\mathbf{W}\)</span>의 전치(transpose)의 행렬 곱으로 계산됩니다. 이 결과는 이전 층(layer)으로 역전파되어 해당 층의 가중치 업데이트에 사용됩니다.</li>
</ul></li>
<li><strong>편향에 대한 그래디언트</strong>: <span class="math inline">\(\frac{\partial E}{\partial \mathbf{b}} = \frac{\partial E}{\partial \mathbf{y}}\)</span></li>
</ol>
<ul>
<li>출력에 대한 손실 함수의 그래디언트와 같다.</li>
</ul>
<ol start="4" type="1">
<li><p><strong>그래디언트의 활용:</strong> 이렇게 계산된 그래디언트들은 경사 하강법(Gradient Descent)과 같은 최적화 알고리즘에서 가중치와 편향을 업데이트하는 데 사용됩니다. 각 파라미터는 그래디언트의 반대 방향으로 업데이트되어 손실 함수를 최소화합니다.</p></li>
<li><p><strong>표기법:</strong> 위 설명에서는 분자 표기법(numerator layout)을 사용하여 그래디언트를 계산했습니다. 분모 표기법(denominator layout)을 사용할 수도 있지만, 결과적으로 동일한 업데이트 규칙을 얻게 됩니다. 중요한 것은 일관된 표기법을 사용하는 것입니다. 이 책에서는 분자 표기법을 사용합니다.</p></li>
</ol>
<p>이러한 수학적 과정을 통해 딥러닝 모델은 입력 데이터로부터 출력 데이터로의 복잡한 비선형 변환을 학습할 수 있습니다.</p>
</section>
</div>
</div>
</div>
</section>
</section>
</section>
<section id="확률과-통계" class="level2">
<h2 class="anchored" data-anchor-id="확률과-통계">2.3 확률과 통계</h2>
<p>딥러닝은 데이터의 불확실성을 다루는 확률과 통계 이론에 깊이 뿌리를 두고 있습니다. 이 장에서는 확률 분포, 기댓값, 베이즈 정리, 최대 우도 추정 등 핵심 개념을 살펴보겠습니다. 이러한 개념들은 모델의 학습과 추론 과정을 이해하는 데 필수적입니다.</p>
<section id="확률-분포와-기댓값" class="level3">
<h3 class="anchored" data-anchor-id="확률-분포와-기댓값">2.3.1 확률 분포와 기댓값</h3>
<blockquote class="blockquote">
<p><strong>도전 과제</strong>: 실제 데이터의 불확실성을 어떻게 수학적으로 모델링 할 수 있을까?</p>
<p><strong>연구자의 고뇌</strong>: 초창기 머신러닝 연구자들은 현실 세계의 데이터가 결정론적인(deterministic) 규칙으로 설명될 수 없다는 것을 인지했습니다. 데이터에는 측정 오차, 잡음, 예측 불가능한 변동성이 존재하기 때문입니다. 이러한 불확실성을 정량화하고 모델에 반영하기 위한 수학적 도구가 필요했습니다.</p>
</blockquote>
<p>확률 분포는 가능한 모든 결과와 그 발생 확률을 나타냅니다. 이산 확률 분포와 연속 확률 분포로 나눌 수 있습니다.</p>
<section id="이산-확률-분포" class="level4">
<h4 class="anchored" data-anchor-id="이산-확률-분포">이산 확률 분포</h4>
<p>이산 확률 분포는 확률 변수가 취할 수 있는 값이 유한하거나 셀 수 있는 경우를 다룹니다. 각 가능한 결과에 대해 명확한 확률을 할당할 수 있다는 점이 특징입니다.</p>
<p>수학적으로, 이산 확률 분포는 확률 질량 함수(PMF)로 표현됩니다.</p>
<p><span class="math display">\[P(X = x) = p(x)\]</span></p>
<p>여기서 p(x)는 X가 값 x를 가질 확률입니다. 주요 성질은 다음과 같습니다.</p>
<ol type="1">
<li>모든 x에 대해 <span class="math inline">\(0 ≤ p(x) ≤ 1\)</span></li>
<li><span class="math inline">\(\sum_{x} p(x) = 1\)</span></li>
</ol>
<p>대표적인 예로는 베르누이 분포, 이항 분포, 포아송 분포가 있습니다.</p>
<p>주사위 던지기의 확률 질량 함수는 다음과 같습니다.</p>
<p><span class="math display">\[P(X = x) = \begin{cases}
\frac{1}{6} &amp; \text{if } x \in \{1, 2, 3, 4, 5, 6\} \
0 &amp; \text{otherwise}
\end{cases}\]</span></p>
<p>이산 확률 분포는 머신러닝과 딥러닝에서 분류 문제, 강화학습, 자연어 처리 등 다양한 분야에서 활용됩니다. 다음은 주사위 던지기를 시뮤레이션 한 결과입니다.</p>
<div id="cell-25" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_02.statistics <span class="im">import</span> simulate_dice_roll</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>simulate_dice_roll()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="02_딥러닝의 수학_files/figure-html/cell-7-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="연속-확률-분포" class="level4">
<h4 class="anchored" data-anchor-id="연속-확률-분포">연속 확률 분포</h4>
<p>연속 확률 분포는 확률 변수가 연속적인 값을 취할 수 있는 경우를 다룹니다. 이산 확률 분포와 달리, 특정 점에서의 확률은 0이며, 구간에 대한 확률을 다룹니다. 수학적으로, 연속 확률 분포는 확률 밀도 함수(Probability Density Function, PDF)로 표현됩니다.</p>
<p><span class="math display">\[f(x) = \lim_{\Delta x \to 0} \frac{P(x &lt; X \leq x + \Delta x)}{\Delta x}\]</span></p>
<p>여기서 f(x)는 x 근처에서의 확률 밀도를 나타냅니다. 주요 성질은 다음과 같습니다.</p>
<ol type="1">
<li>모든 x에 대해 f(x) ≥ 0</li>
<li><span class="math inline">\(\int_{-\infty}^{\infty} f(x) dx = 1\)</span></li>
</ol>
<p>대표적인 예로는 정규 분포, 지수 분포, 감마 분포가 있습니다.</p>
<p>정규 분포의 확률 밀도 함수는 다음과 같습니다.</p>
<p><span class="math display">\[f(x) = \frac{1}{\sigma\sqrt{2\pi}} e^{-\frac{(x-\mu)^2}{2\sigma^2}}\]</span></p>
<p>여기서 μ는 평균, σ는 표준편차입니다.</p>
<p>연속 확률 분포는 회귀 문제, 신호 처리, 시계열 분석 등 다양한 머신러닝과 딥러닝 응용 분야에서 중요하게 사용됩니다.</p>
<div id="cell-27" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_02.statistics <span class="im">import</span> plot_normal_distribution</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>plot_normal_distribution()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="02_딥러닝의 수학_files/figure-html/cell-8-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="기댓값" class="level4">
<h4 class="anchored" data-anchor-id="기댓값">기댓값</h4>
<p>기댓값은 확률 분포의 중심 경향을 나타내는 중요한 개념입니다. 이는 확률 변수의 가능한 모든 값에 대한 가중 평균으로 해석할 수 있습니다. 이산 확률 분포의 경우, 기댓값은 다음과 같이 계산됩니다.</p>
<p><span class="math display">\[E[X] = \sum_{i} x_i P(X = x_i)\]</span></p>
<p>여기서 <span class="math inline">\(x_i\)</span>는 확률 변수 X의 가능한 값이고, <span class="math inline">\(P(X = x_i)\)</span>는 그 값의 확률입니다. 연속 확률 분포의 경우, 기댓값은 적분을 통해 계산됩니다.</p>
<p><span class="math display">\[E[X] = \int_{-\infty}^{\infty} x f(x) dx\]</span></p>
<p>여기서 <span class="math inline">\(f(x)\)</span>는 확률 밀도 함수입니다. 기댓값은 다음과 같은 중요한 성질을 가집니다.</p>
<ol type="1">
<li>선형성: <span class="math inline">\(E[aX + b] = aE[X] + b\)</span></li>
<li>독립 확률 변수의 곱의 기댓값: <span class="math inline">\(E[XY] = E[X]E[Y]\)</span> (X와 Y가 독립일 때)</li>
</ol>
<p>딥러닝에서 기댓값은 손실 함수의 최소화나 모델 파라미터의 추정에 핵심적으로 사용됩니다. 예를 들어, 평균 제곱 오차(MSE)는 다음과 같이 정의됩니다.</p>
<p><span class="math display">\[MSE = E[(Y - \hat{Y})^2]\]</span></p>
<p>여기서 <span class="math inline">\(Y\)</span>는 실제값, <span class="math inline">\(\hat{Y}\)</span>는 예측값입니다.</p>
<p>기댓값의 개념은 확률적 경사 하강법(Stochastic Gradient Descent)과 같은 최적화 알고리즘의 이론적 기반을 제공하며, 강화학습에서의 가치 함수 추정에도 중요하게 활용됩니다.</p>
<div id="cell-29" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_02.statistics <span class="im">import</span> calculate_dice_expected_value</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>calculate_dice_expected_value()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Expected value of dice roll: 3.5</code></pre>
</div>
</div>
<p>이러한 확률과 통계의 기본 개념들은 딥러닝 모델의 설계, 학습, 평가 과정에서 핵심적인 역할을 합니다. 다음 섹션에서는 이를 바탕으로 베이즈 정리와 최대 우도 추정에 대해 알아보겠습니다.</p>
</section>
</section>
<section id="베이즈-정리와-최대-우도-추정" class="level3">
<h3 class="anchored">2.3.2 베이즈 정리와 최대 우도 추정</h3>
<blockquote class="blockquote">
<p><strong>도전과제:</strong> 어떻게 제한된 데이터를 가지고 모델의 파라미터를 가장 잘 추정할 수 있을까?</p>
<p><strong>연구자의 고뇌:</strong> 초기 통계학자들과 머신러닝 연구자들은 종종 제한된 데이터만을 가지고 모델을 만들어야 하는 상황에 직면했습니다. 데이터가 충분하지 않은 상황에서 모델의 파라미터를 정확하게 추정하는 것은 매우 어려운 문제였습니다. 단순히 데이터에만 의존하는 것이 아니라, 사전 지식이나 믿음을 활용하여 추정의 정확도를 높이는 방법이 필요했습니다.</p>
</blockquote>
<p>베이즈 정리와 최대 우도 추정은 확률론과 통계학의 핵심 개념으로, 딥러닝에서 모델 학습과 추론에 광범위하게 적용됩니다.</p>
<section id="베이즈-정리" class="level4">
<h4 class="anchored" data-anchor-id="베이즈-정리">베이즈 정리</h4>
<p>베이즈 정리는 조건부 확률을 계산하는 방법을 제공합니다. 이는 새로운 증거가 주어졌을 때 가설의 확률을 갱신하는 데 사용됩니다. 베이즈 정리의 수학적 표현은 다음과 같습니다.</p>
<p><span class="math display">\[P(A|B) = \frac{P(B|A)P(A)}{P(B)}\]</span></p>
<p>여기서: - <span class="math inline">\(P(A|B)\)</span>는 B가 주어졌을 때 A의 확률 (사후 확률) - <span class="math inline">\(P(B|A)\)</span>는 A가 주어졌을 때 B의 확률 (우도) - <span class="math inline">\(P(A)\)</span>는 A의 확률 (사전 확률) - <span class="math inline">\(P(B)\)</span>는 B의 확률 (증거)</p>
<p>베이즈 정리는 머신러닝에서 다음과 같이 활용됩니다.</p>
<ol type="1">
<li>분류 문제: 나이브 베이즈 분류기에서 특정 클래스에 속할 확률을 계산합니다.</li>
<li>파라미터 추정: 모델 파라미터의 사후 분포를 계산하는 데 사용됩니다.</li>
<li>의사결정 이론: 불확실성 하에서의 최적 결정을 내리는 데 활용됩니다.</li>
</ol>
</section>
<section id="최대-우도-추정" class="level4">
<h4 class="anchored">최대 우도 추정</h4>
<p>최대 우도 추정(Maximum Likelihood Estimation, MLE)은 주어진 데이터를 가장 잘 설명하는 모델 파라미터를 찾는 방법입니다. 딥러닝의 맥락에서, 이는 신경망이 관측된 데이터를 가장 잘 설명할 수 있는 가중치와 편향을 찾는 과정을 의미합니다. 즉, 최대 우도 추정은 모델이 훈련 데이터를 생성할 확률을 최대화하는 파라미터를 찾는 것으로, 이는 곧 모델의 학습 과정과 직접적으로 연결됩니다. 수학적으로, 데이터 <span class="math inline">\(X = (x_1, ..., x_n)\)</span>가 주어졌을 때, 파라미터 <span class="math inline">\(\theta\)</span>에 대한 우도 함수는 다음과 같이 정의됩니다.</p>
<p><span class="math display">\[L(\theta|X) = P(X|\theta) = \prod_{i=1}^n P(x_i|\theta)\]</span></p>
<p>최대 우도 추정치 <span class="math inline">\(\hat{\theta}_{MLE}\)</span>는 다음과 같이 구합니다.</p>
<p><span class="math display">\[\hat{\theta}_{MLE} = \operatorname{argmax}_{\theta} L(\theta|X)\]</span></p>
<p>실제로는 로그 우도를 최대화하는 것이 계산상 더 편리합니다.</p>
<p><span class="math display">\[\hat{\theta}_{MLE} = \operatorname{argmax}_{\theta} \log L(\theta|X) = \operatorname{argmax}_{\theta} \sum_{i=1}^n \log P(x_i|\theta)\]</span></p>
<p>로그 우도를 사용하는 데에는 여러 가지 중요한 수학적 장점이 있습니다.</p>
<ol type="1">
<li>곱셈을 덧셈으로 변환: 로그 함수의 특성상 <span class="math inline">\(\log(ab) = \log(a) + \log(b)\)</span>이므로, 확률의 곱을 로그 확률의 합으로 바꿀 수 있습니다. 이는 계산을 단순화하고 수치적 안정성을 높입니다.</li>
<li>수치적 안정성 향상: 매우 작은 확률값들을 다룰 때, 곱셈은 언더플로우를 일으킬 수 있습니다. 로그를 사용하면 이러한 문제를 피할 수 있습니다.</li>
<li>미분 간소화: 최적화 과정에서 미분을 계산할 때, 로그 함수를 사용하면 계산이 더 간단해집니다. 특히 지수 분포의 경우 이점이 두드러집니다.</li>
<li>단조 증가 함수: 로그 함수는 단조 증가 함수이므로, 우도를 최대화하는 것과 로그 우도를 최대화하는 것은 동일한 결과를 얻습니다.</li>
</ol>
<p>이러한 이유로, 딥러닝을 포함한 많은 기계학습 알고리즘에서는 로그 우도를 사용하여 최적화를 수행합니다.</p>
<p>최대 우도 추정은 딥러닝에서 다음과 같이 활용됩니다.</p>
<ol type="1">
<li>모델 학습: 신경망의 가중치를 학습할 때 손실 함수를 최소화하는 과정은 사실상 최대 우도 추정과 동일합니다.</li>
<li>확률 모델링: 생성 모델에서 데이터의 분포를 추정하는 데 사용됩니다.</li>
<li>하이퍼파라미터 튜닝: 모델의 하이퍼파라미터를 선택하는 데 활용될 수 있습니다.</li>
</ol>
<p>베이즈 정리와 최대 우도 추정은 서로 밀접한 관련이 있습니다. 베이즈 추정에서 사전 확률이 균일 분포일 경우, 최대 사후 확률(MAP) 추정은 최대 우도 추정과 동일해집니다. 수학적으로 표현하면, <span class="math inline">\(P(\theta|X) \propto P(X|\theta)P(\theta)\)</span>에서 <span class="math inline">\(P(\theta)\)</span>가 상수일 때, <span class="math inline">\(\operatorname{argmax}_{\theta} P(\theta|X) = \operatorname{argmax}_{\theta} P(X|\theta)P(\theta)\)</span>가 됩니다. 이는 사전 확률이 파라미터에 대한 추가적인 정보를 제공하지 않을 때, 데이터만을 기반으로 한 추정(MLE)이 베이즈 추정(MAP)과 일치함을 의미합니다.</p>
<p>이러한 개념들은 딥러닝 모델의 학습과 추론 과정을 이해하고 최적화하는 데 필수적입니다. 다음 섹션에서는 정보 이론의 기초에 대해 알아보겠습니다.</p>
<div class="callout callout-style-default callout-note callout-titled" title="딥다이브: 베이즈 정리의 심층 분석">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-7-contents" aria-controls="callout-7" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
딥다이브: 베이즈 정리의 심층 분석
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-7" class="callout-7-contents callout-collapse collapse">
<section id="베이즈-정리-bayes-theorem---심층-분석" class="level2 callout-body-container callout-body">
<h2 class="anchored" data-anchor-id="베이즈-정리-bayes-theorem---심층-분석">베이즈 정리 (Bayes’ Theorem) - 심층 분석</h2>
<section id="베이즈-정리의-엄밀한-유도-및-확률-공간" class="level3">
<h3 class="anchored" data-anchor-id="베이즈-정리의-엄밀한-유도-및-확률-공간">1. 베이즈 정리의 엄밀한 유도 및 확률 공간</h3>
<ul>
<li><strong>확률 공간 (Probability Space):</strong> 베이즈 정리는 확률 공간 <span class="math inline">\((\Omega, \mathcal{F}, P)\)</span> 위에서 정의됩니다.
<ul>
<li><span class="math inline">\(\Omega\)</span>: 표본 공간 (Sample Space, 모든 가능한 결과들의 집합)</li>
<li><span class="math inline">\(\mathcal{F}\)</span>: 사건 공간 (Event Space, 표본 공간의 부분집합들의 집합, <span class="math inline">\(\sigma\)</span>-algebra)</li>
<li><span class="math inline">\(P\)</span>: 확률 측도 (Probability Measure, 사건 공간의 각 사건에 확률을 할당하는 함수)</li>
</ul></li>
<li><strong>조건부 확률의 엄밀한 정의:</strong>
<ul>
<li>사건 <span class="math inline">\(B \in \mathcal{F}\)</span>에 대해 <span class="math inline">\(P(B) &gt; 0\)</span>일 때, 사건 <span class="math inline">\(A \in \mathcal{F}\)</span>의 조건부 확률 <span class="math inline">\(P(A|B)\)</span>는 다음과 같이 정의됩니다. <span class="math inline">\(P(A|B) = \frac{P(A \cap B)}{P(B)}\)</span></li>
</ul></li>
<li><strong>결합 확률:</strong>
<ul>
<li>두 사건 <span class="math inline">\(A, B \in \mathcal{F}\)</span>의 결합 확률 <span class="math inline">\(P(A \cap B)\)</span>는 두 사건이 동시에 일어날 확률을 나타냅니다.</li>
<li>조건부 확률의 정의를 이용하여 다음과 같이 표현할 수 있습니다.
<ul>
<li><span class="math inline">\(P(A \cap B) = P(A|B)P(B)\)</span></li>
<li><span class="math inline">\(P(A \cap B) = P(B|A)P(A)\)</span></li>
</ul></li>
</ul></li>
<li><strong>베이즈 정리 유도:</strong>
<ol type="1">
<li><span class="math inline">\(P(A|B)P(B) = P(B|A)P(A)\)</span> (결합 확률의 두 표현)</li>
<li><span class="math inline">\(P(A|B) = \frac{P(B|A)P(A)}{P(B)}\)</span> (양변을 <span class="math inline">\(P(B)\)</span>로 나눔, <span class="math inline">\(P(B) &gt; 0\)</span>)</li>
</ol></li>
</ul>
</section>
<section id="베이즈-정리-각-항의-심층적-의미와-통계적-해석" class="level3">
<h3 class="anchored" data-anchor-id="베이즈-정리-각-항의-심층적-의미와-통계적-해석">2. 베이즈 정리 각 항의 심층적 의미와 통계적 해석</h3>
<ul>
<li><strong><span class="math inline">\(P(A|B)\)</span>: 사후 확률 (Posterior Probability)</strong>
<ul>
<li><strong>해석:</strong> 관측 데이터 <span class="math inline">\(B\)</span>를 얻은 후, 가설 <span class="math inline">\(A\)</span>에 대한 갱신된 확률 분포. 데이터 기반의 추론(inference) 결과를 나타냅니다.</li>
<li><strong>베이지안 관점:</strong> 사후 확률은 사전 확률과 가능도의 결합을 통해 불확실성을 정량화하고, 의사 결정(decision making)의 기반을 제공합니다.</li>
</ul></li>
<li><strong><span class="math inline">\(P(B|A)\)</span>: 가능도, 우도 (Likelihood)</strong>
<ul>
<li><strong>해석:</strong> 가설 <span class="math inline">\(A\)</span>가 참이라고 가정했을 때, 관측 데이터 <span class="math inline">\(B\)</span>가 나타날 확률. 가설 <span class="math inline">\(A\)</span>가 데이터 <span class="math inline">\(B\)</span>를 얼마나 잘 설명하는지를 나타냅니다.</li>
<li><strong>빈도주의 관점 vs.&nbsp;베이지안 관점:</strong>
<ul>
<li><strong>빈도주의:</strong> 가능도는 고정된 모수(parameter)에 대한 함수로, 데이터의 분포를 설명합니다.</li>
<li><strong>베이지안:</strong> 가능도는 데이터가 주어졌을 때, 모수에 대한 정보를 제공하는 함수입니다.</li>
</ul></li>
</ul></li>
<li><strong><span class="math inline">\(P(A)\)</span>: 사전 확률 (Prior Probability)</strong>
<ul>
<li><strong>해석:</strong> 관측 데이터 <span class="math inline">\(B\)</span>를 얻기 전, 가설 <span class="math inline">\(A\)</span>에 대한 사전 믿음(prior belief)을 나타내는 확률 분포.</li>
<li><strong>주관적 vs.&nbsp;객관적 사전 확률:</strong>
<ul>
<li><strong>주관적 (Subjective):</strong> 전문가의 지식, 이전 경험 등을 바탕으로 설정.</li>
<li><strong>객관적 (Objective):</strong> 균등 분포(uniform distribution) 또는 무정보 사전 분포(non-informative prior) 등, 최소한의 정보를 담은 사전 분포를 사용.</li>
</ul></li>
</ul></li>
<li><strong><span class="math inline">\(P(B)\)</span>: 증거 (Evidence) 또는 주변 가능도 (Marginal Likelihood)</strong>
<ul>
<li><strong>해석:</strong> 모든 가능한 가설 하에서 관측 데이터 <span class="math inline">\(B\)</span>가 나타날 확률. <span class="math inline">\(P(A|B)\)</span>를 확률 분포로 만들기 위한 정규화 상수(normalizing constant) 역할.</li>
<li><strong>계산:</strong> <span class="math inline">\(P(B) = \sum_{A'} P(B|A')P(A')\)</span> (이산 확률 변수) <span class="math inline">\(P(B) = \int P(B|A)p(A) dA\)</span> (연속 확률 변수, <span class="math inline">\(p(A)\)</span>는 확률 밀도 함수)</li>
<li><strong>모델 비교:</strong> 베이즈 요인(Bayes factor) 계산 등, 서로 다른 모델의 증거를 비교하는 데 사용.</li>
</ul></li>
</ul>
</section>
<section id="베이즈-정리와-베이지안-추론-bayesian-inference" class="level3">
<h3 class="anchored" data-anchor-id="베이즈-정리와-베이지안-추론-bayesian-inference">3. 베이즈 정리와 베이지안 추론 (Bayesian Inference)</h3>
<ul>
<li><strong>핵심:</strong> 베이즈 정리는 데이터가 주어졌을 때, 모수(parameter) 또는 가설에 대한 확률 분포를 추론하는 베이지안 추론의 핵심 원리입니다.</li>
<li><strong>과정:</strong>
<ol type="1">
<li><strong>Prior:</strong> 모수 <span class="math inline">\(\theta\)</span>에 대한 사전 분포 <span class="math inline">\(p(\theta)\)</span>를 설정합니다.</li>
<li><strong>Likelihood:</strong> 주어진 모수 <span class="math inline">\(\theta\)</span> 하에서 데이터 <span class="math inline">\(x\)</span>가 관측될 확률 <span class="math inline">\(p(x|\theta)\)</span> (가능도 함수)를 정의합니다.</li>
<li><strong>Posterior:</strong> 베이즈 정리를 이용하여 사후 분포 <span class="math inline">\(p(\theta|x)\)</span>를 계산합니다. <span class="math inline">\(p(\theta|x) = \frac{p(x|\theta)p(\theta)}{p(x)} = \frac{p(x|\theta)p(\theta)}{\int p(x|\theta')p(\theta') d\theta'}\)</span></li>
<li><strong>Inference:</strong> 사후 분포를 기반으로 모수에 대한 추정, 구간 추정, 가설 검정 등을 수행합니다.</li>
</ol></li>
<li><strong>반복적 갱신:</strong> 새로운 데이터가 들어올 때마다 이전의 사후 분포를 새로운 사전 분포로 사용하여 계속해서 믿음을 업데이트하는 것이 가능합니다. (Sequential Bayesian updating)</li>
</ul>
</section>
<section id="베이즈-정리의-확장-및-응용" class="level3">
<h3 class="anchored" data-anchor-id="베이즈-정리의-확장-및-응용">4. 베이즈 정리의 확장 및 응용</h3>
<ul>
<li><strong>연속 확률 변수:</strong> 확률 밀도 함수를 이용한 베이즈 정리</li>
<li><strong>켤레 사전 분포 (Conjugate Prior):</strong>
<ul>
<li>사후 분포가 사전 분포와 같은 분포족(family)에 속하도록 하는 사전 분포. 계산의 편리성 때문에 많이 사용됩니다. (e.g., 베타 분포 - 베르누이 분포, 감마 분포 - 포아송 분포)</li>
</ul></li>
<li><strong>변분 베이즈 (Variational Bayes):</strong>
<ul>
<li>복잡한 사후 분포를 근사하는 방법.</li>
<li>사후 분포와 유사한, 다루기 쉬운 분포를 찾고, 두 분포 사이의 Kullback-Leibler divergence를 최소화하는 방식으로 근사.</li>
</ul></li>
<li><strong>마르코프 연쇄 몬테 카를로 (Markov Chain Monte Carlo, MCMC):</strong>
<ul>
<li>사후 분포에서 표본(sample)을 추출하여 사후 분포의 특성을 추정하는 방법.</li>
<li>Metropolis-Hastings 알고리즘, Gibbs sampling 등.</li>
</ul></li>
<li><strong>딥러닝에서의 응용:</strong>
<ul>
<li><strong>Bayesian Neural Networks:</strong> 신경망의 가중치(weight)를 확률 변수로 취급하여, 예측의 불확실성을 정량화.</li>
<li><strong>Gaussian Processes:</strong> 커널(kernel)을 이용하여 함수 공간에 사전 분포를 정의하고, 베이즈 정리를 통해 예측 분포를 계산.</li>
</ul></li>
</ul>
</section>
</section>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled" title="딥다이브: 최대가능도추정(MLE)의 심층 분석 및 MAP 비교 (석사 이상)">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-8-contents" aria-controls="callout-8" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
딥다이브: 최대가능도추정(MLE)의 심층 분석 및 MAP 비교 (석사 이상)
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-8" class="callout-8-contents callout-collapse collapse">
<section id="최대가능도추정-maximum-likelihood-estimation-mle---심층-분석-및-map-비교" class="level2 callout-body-container callout-body">
<h2 class="anchored" data-anchor-id="최대가능도추정-maximum-likelihood-estimation-mle---심층-분석-및-map-비교">최대가능도추정 (Maximum Likelihood Estimation, MLE) - 심층 분석 및 MAP 비교</h2>
<section id="mle-계산의-구체적인-예시" class="level3">
<h3 class="anchored" data-anchor-id="mle-계산의-구체적인-예시">1. MLE 계산의 구체적인 예시</h3>
<p>MLE는 주어진 데이터를 가장 잘 설명하는 모수(parameter)를 찾는 방법입니다. 관측된 데이터의 가능도(likelihood)를 최대화하는 모수 값을 찾는 것입니다.</p>
<ul>
<li><p><strong>가능도 함수 (Likelihood Function):</strong></p>
<ul>
<li>데이터 <span class="math inline">\(x_1, x_2, ..., x_n\)</span>이 독립적으로 동일한 확률 분포(i.i.d)에서 추출되었다고 가정할 때, 가능도 함수는 다음과 같이 정의됩니다. <span class="math display">\[L(\theta; x_1, ..., x_n) = \prod_{i=1}^{n} p(x_i | \theta)\]</span>
<ul>
<li><span class="math inline">\(\theta\)</span>: 모수 (parameter)</li>
<li><span class="math inline">\(p(x_i | \theta)\)</span>: 모수 <span class="math inline">\(\theta\)</span>가 주어졌을 때, 데이터 <span class="math inline">\(x_i\)</span>가 나타날 확률 (또는 확률 밀도)</li>
</ul></li>
</ul></li>
<li><p><strong>로그 가능도 함수 (Log-Likelihood Function):</strong></p>
<ul>
<li>계산의 편의를 위해 가능도 함수에 로그를 취한 로그 가능도 함수를 사용합니다. <span class="math display">\[l(\theta; x_1, ..., x_n) = \log L(\theta; x_1, ..., x_n) = \sum_{i=1}^{n} \log p(x_i | \theta)\]</span></li>
<li>로그를 취해도 최댓값의 위치는 변하지 않으므로, 로그 가능도를 최대화하는 모수를 찾아도 됩니다.</li>
</ul></li>
<li><p><strong>MLE 계산 절차:</strong></p>
<ol type="1">
<li>주어진 데이터와 확률 분포 모델에 대한 가능도 함수를 정의합니다.</li>
<li>가능도 함수에 로그를 취하여 로그 가능도 함수를 구합니다.</li>
<li>로그 가능도 함수를 모수 <span class="math inline">\(\theta\)</span>에 대해 미분합니다.</li>
<li>미분 값이 0이 되는 <span class="math inline">\(\theta\)</span> 값을 찾습니다. (필요시 이계도 함수를 이용하여 극대/극소 판별)</li>
<li>찾은 <span class="math inline">\(\theta\)</span> 값이 MLE 추정값이 됩니다.</li>
</ol></li>
<li><p><strong>구체적인 예시:</strong></p>
<ul>
<li><strong>정규 분포 (Normal Distribution):</strong>
<ul>
<li>데이터 <span class="math inline">\(x_1, ..., x_n\)</span>이 평균 <span class="math inline">\(\mu\)</span>와 분산 <span class="math inline">\(\sigma^2\)</span>을 갖는 정규 분포를 따른다고 가정합니다.</li>
<li>로그 가능도 함수: <span class="math display">\[l(\mu, \sigma^2; x_1, ..., x_n) = -\frac{n}{2}\log(2\pi\sigma^2) - \frac{1}{2\sigma^2}\sum_{i=1}^{n}(x_i - \mu)^2\]</span></li>
<li><span class="math inline">\(\mu\)</span>와 <span class="math inline">\(\sigma^2\)</span>에 대해 각각 편미분하여 0이 되는 지점을 찾으면, MLE 추정값은 다음과 같습니다.
<ul>
<li><span class="math inline">\(\hat{\mu}_{MLE} = \frac{1}{n}\sum_{i=1}^{n} x_i\)</span> (표본 평균)</li>
<li><span class="math inline">\(\hat{\sigma}^2_{MLE} = \frac{1}{n}\sum_{i=1}^{n} (x_i - \hat{\mu}_{MLE})^2\)</span> (표본 분산)</li>
</ul></li>
</ul></li>
<li><strong>베르누이 분포 (Bernoulli Distribution):</strong>
<ul>
<li>데이터 <span class="math inline">\(x_1, ..., x_n\)</span>이 성공 확률 <span class="math inline">\(p\)</span>를 갖는 베르누이 분포를 따른다고 가정합니다. (<span class="math inline">\(x_i = 1\)</span> (성공), <span class="math inline">\(x_i = 0\)</span> (실패))</li>
<li>로그 가능도 함수: <span class="math display">\[ l(p; x_1, ..., x_n) = \sum_{i=1}^n [x_i \log p + (1-x_i) \log (1-p)] \]</span></li>
<li><span class="math inline">\(p\)</span>에 대해 미분하여 0이 되는 지점을 찾으면, MLE 추정량은 다음과 같습니다.
<ul>
<li><span class="math inline">\(\hat{p}_{MLE} = \frac{1}{n}\sum_{i=1}^{n} x_i\)</span> (성공 횟수 / 전체 시행 횟수)</li>
</ul></li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="mle의-장점과-단점" class="level3">
<h3 class="anchored" data-anchor-id="mle의-장점과-단점">2. MLE의 장점과 단점</h3>
<ul>
<li><strong>장점:</strong>
<ul>
<li><strong>계산의 용이성:</strong> 비교적 간단한 계산으로 모수 추정이 가능합니다. (특히, 지수족(exponential family) 분포의 경우)</li>
<li><strong>점근적 성질 (Asymptotic Properties):</strong> (아래에서 더 자세히 설명)
<ul>
<li><strong>일치성 (Consistency):</strong> 표본 크기가 커질수록 MLE 추정량은 실제 모수에 수렴합니다.</li>
<li><strong>점근적 정규성 (Asymptotic Normality):</strong> 표본 크기가 커질수록 MLE 추정량은 정규 분포에 가까워집니다.</li>
<li><strong>효율성 (Efficiency):</strong> 점근적으로 가장 작은 분산을 갖는 불편 추정량입니다(Cramér–Rao lower bound).</li>
</ul></li>
</ul></li>
<li><strong>단점:</strong>
<ul>
<li><strong>과적합 (Overfitting) 가능성:</strong> 특히 표본 크기가 작을 때, 데이터에 과도하게 맞춰져 일반화 성능이 떨어질 수 있습니다.</li>
<li><strong>이상치(Outlier)에 민감:</strong> 이상치가 존재할 경우, MLE 추정값이 크게 왜곡될 수 있습니다.</li>
<li><strong>모든 분포에 적용 가능하지 않음:</strong> MLE는 확률 모델(probabilistic model)이 주어져야 적용 가능합니다(non-parametric 방법에는 적용 불가).</li>
<li><strong>편향 (Bias) 가능성:</strong> 일부 경우, MLE 추정량은 편향될 수 있습니다(e.g., 정규 분포의 분산 추정).</li>
</ul></li>
</ul>
</section>
<section id="최대-사후-확률-추정-maximum-a-posteriori-map과의-비교" class="level3">
<h3 class="anchored" data-anchor-id="최대-사후-확률-추정-maximum-a-posteriori-map과의-비교">3. 최대 사후 확률 추정 (Maximum A Posteriori, MAP)과의 비교</h3>
<ul>
<li><p><strong>MAP:</strong> 베이즈 정리를 기반으로, 사전 확률(prior probability)과 가능도(likelihood)를 결합하여 사후 확률(posterior probability)을 최대화하는 모수를 찾는 방법입니다.</p></li>
<li><p><strong>MAP 추정:</strong> <span class="math display">\[
  \hat{\theta}_{MAP} = \arg\max_{\theta} p(\theta|x) = \arg\max_{\theta} \frac{p(x|\theta)p(\theta)}{p(x)} = \arg\max_{\theta} p(x|\theta)p(\theta)
  \]</span></p>
<ul>
<li><span class="math inline">\(p(\theta|x)\)</span>: 사후 확률 (Posterior Probability)</li>
<li><span class="math inline">\(p(x|\theta)\)</span>: 가능도 (Likelihood)</li>
<li><span class="math inline">\(p(\theta)\)</span>: 사전 확률 (Prior Probability)</li>
<li><span class="math inline">\(p(x)\)</span>: 증거 (Evidence, 상수이므로 무시 가능)</li>
</ul></li>
<li><p><strong>MLE vs.&nbsp;MAP:</strong></p>
<table class="caption-top table">
<colgroup>
<col style="width: 9%">
<col style="width: 44%">
<col style="width: 46%">
</colgroup>
<thead>
<tr class="header">
<th>특징</th>
<th>MLE</th>
<th>MAP</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>기반</strong></td>
<td>빈도주의 (Frequentist)</td>
<td>베이지안 (Bayesian)</td>
</tr>
<tr class="even">
<td><strong>목표</strong></td>
<td>가능도 최대화</td>
<td>사후 확률 최대화</td>
</tr>
<tr class="odd">
<td><strong>사전 확률</strong></td>
<td>고려하지 않음</td>
<td>고려함</td>
</tr>
<tr class="even">
<td><strong>결과</strong></td>
<td>점 추정 (Point Estimate)</td>
<td>점 추정 (일반적으로) 또는 분포 추정 (베이지안 추론의 경우)</td>
</tr>
<tr class="odd">
<td><strong>과적합</strong></td>
<td>과적합 가능성 높음</td>
<td>사전 확률을 통해 과적합 방지 가능 (e.g., Regularization 효과)</td>
</tr>
<tr class="even">
<td><strong>계산 복잡도</strong></td>
<td>일반적으로 낮음</td>
<td>사전 확률에 따라 복잡도가 증가할 수 있음 (특히, 켤레 사전 분포가 아닌 경우)</td>
</tr>
</tbody>
</table>
<ul>
<li><strong>사전 확률의 영향:</strong>
<ul>
<li><strong>무정보 사전 분포 (Non-informative Prior):</strong> <span class="math inline">\(p(\theta) \propto 1\)</span> (상수)와 같이 사전 확률이 균등 분포를 따르는 경우, MAP 추정은 MLE 추정과 동일해집니다.</li>
<li><strong>정보성 사전 분포 (Informative Prior):</strong> 사전 확률이 특정 분포를 따르는 경우 (e.g., 정규 분포, 베타 분포), MAP 추정은 사전 확률의 영향을 받아 MLE 추정과 달라집니다. 사전 분포가 강한 prior belief를 나타낼수록, posterior는 prior에 더 가까워집니다.</li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="mle의-점근적-성질-asymptotic-property" class="level3">
<h3 class="anchored" data-anchor-id="mle의-점근적-성질-asymptotic-property">4. MLE의 점근적 성질 (Asymptotic Property)</h3>
<ul>
<li><strong>일치성 (Consistency):</strong>
<ul>
<li>표본 크기 <span class="math inline">\(n\)</span>이 무한대로 커질 때, MLE 추정량 <span class="math inline">\(\hat{\theta}_{MLE}\)</span>는 실제 모수 <span class="math inline">\(\theta_0\)</span>로 확률 수렴(converge in probability)합니다. <span class="math display">\[\hat{\theta}_{MLE} \xrightarrow{p} \theta_0 \text{ as } n \rightarrow \infty\]</span></li>
</ul></li>
<li><strong>점근적 정규성 (Asymptotic Normality):</strong>
<ul>
<li>표본 크기 <span class="math inline">\(n\)</span>이 충분히 클 때, MLE 추정량 <span class="math inline">\(\hat{\theta}_{MLE}\)</span>의 분포는 다음 정규 분포에 근사합니다. <span class="math display">\[\sqrt{n}(\hat{\theta}_{MLE} - \theta_0) \xrightarrow{d} N(0, I(\theta_0)^{-1})\]</span>
<ul>
<li><span class="math inline">\(I(\theta_0)\)</span>: Fisher Information Matrix (FIM)
<ul>
<li><span class="math inline">\(I(\theta) = -E[\frac{\partial^2}{\partial \theta^2} l(\theta; x_1, ...,x_n)]\)</span> (단일 모수인 경우)</li>
<li>FIM은 로그 가능도 함수의 곡률(curvature)을 나타내며, 모수에 대한 정보량을 의미.</li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>효율성 (Efficiency):</strong>
<ul>
<li>MLE는 점근적으로 Cramér–Rao 하한(Cramér–Rao lower bound, CRLB)을 달성하는 효율적인 추정량입니다. 다른 불편 추정량(unbiased estimator)에 비해 점근적으로 가장 작은 분산을 갖습니다.</li>
</ul></li>
</ul>
</section>
</section>
</div>
</div>
</section>
</section>
<section id="정보-이론-기초" class="level3">
<h3 class="anchored">2.3.3 정보 이론 기초</h3>
<blockquote class="blockquote">
<p><strong>도전과제:</strong> 어떻게 정보의 양을 측정하고, 불확실성을 정량화할 수 있을까?</p>
<p><strong>연구자의 고뇌:</strong> 클로드 섀넌은 통신 시스템에서 정보의 효율적인 전송과 압축에 대한 근본적인 질문에 직면했습니다. 정보를 정량화하고, 정보의 손실 없이 데이터를 얼마나 압축할 수 있는지, 그리고 노이즈가 있는 채널을 통해 얼마나 많은 정보를 안정적으로 전송할 수 있는지에 대한 이론적 근거가 필요했습니다.</p>
</blockquote>
<p>정보 이론은 데이터의 압축, 전송, 저장에 관한 수학적 이론으로, 딥러닝에서 모델의 성능 평가와 최적화에 중요한 역할을 합니다. 이 섹션에서는 정보 이론의 핵심 개념인 엔트로피, 상호 정보량, KL 발산에 대해 알아보겠습니다.</p>
<section id="엔트로피" class="level4">
<h4 class="anchored" data-anchor-id="엔트로피">엔트로피</h4>
<p>엔트로피는 정보의 불확실성을 측정하는 척도입니다. 확률 분포 P에 대한 엔트로피 H(P)는 다음과 같이 정의됩니다.</p>
<p><span class="math display">\[H(P) = -\sum_{x} P(x) \log P(x)\]</span></p>
<p>여기서 x는 가능한 모든 사건을 나타냅니다. 엔트로피의 주요 특성은 다음과 같습니다.</p>
<ol type="1">
<li>비음수성: <span class="math inline">\(H(P) ≥ 0\)</span></li>
<li>균등 분포일 때 최대: 모든 사건의 확률이 동일할 때 엔트로피가 최대가 됩니다.</li>
<li>확실한 사건의 엔트로피는 0: <span class="math inline">\(P(x) = 1\)</span>일 때 <span class="math inline">\(H(P) = 0\)</span></li>
</ol>
<p>딥러닝에서 엔트로피는 주로 분류 문제의 손실 함수로 사용되는 교차 엔트로피의 기반이 됩니다. 다음 예제는 다양한 확률 분포의 엔트로피를 계산하고, 이진 분포의 엔트로피를 시각화합니다.</p>
<div id="cell-35" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_02.information_theory <span class="im">import</span> calculate_entropy</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>calculate_entropy()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Entropy of fair coin: 0.69
Entropy of biased coin: 0.33
Entropy of fair die: 1.39</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="02_딥러닝의 수학_files/figure-html/cell-10-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="상호-정보량" class="level4">
<h4 class="anchored" data-anchor-id="상호-정보량">상호 정보량</h4>
<p>상호 정보량(Mutual Information)은 두 확률 변수 X와 Y 사이의 상호 의존성을 측정합니다. 수학적으로 다음과 같이 정의됩니다.</p>
<p><span class="math display">\[I(X;Y) = \sum_{x}\sum_{y} P(x,y) \log \frac{P(x,y)}{P(x)P(y)}\]</span></p>
<p>상호 정보량의 주요 특성은 다음과 같습니다.</p>
<ol type="1">
<li>비음수성: <span class="math inline">\(I(X;Y) \ge 0\)</span></li>
<li>대칭성: <span class="math inline">\(I(X;Y) = I(Y;X)\)</span></li>
<li>X와 Y가 독립일 때 0: X와 Y가 독립이면 <span class="math inline">\(I(X;Y) = 0\)</span></li>
</ol>
<p>상호 정보량은 특징 선택, 차원 축소 등 다양한 머신러닝 작업에서 활용됩니다. 다음 예제는 간단한 결합 확률 분포에 대한 상호 정보량을 계산하고 시각화합니다.</p>
<div id="cell-37" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_02.information_theory <span class="im">import</span> mutual_information_example</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>mutual_information_example()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Mutual Information: 0.0058</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="02_딥러닝의 수학_files/figure-html/cell-11-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="kl-발산" class="level4">
<h4 class="anchored">KL 발산</h4>
<p>KL(Kullback-Leibler) 발산은 두 확률 분포 P와 Q의 차이를 측정하는 방법입니다. P에 대한 Q의 KL 발산은 다음과 같이 정의됩니다.</p>
<p><span class="math display">\[D_{KL}(P||Q) = \sum_{x} P(x) \log \frac{P(x)}{Q(x)}\]</span></p>
<p>KL 발산의 주요 특성은 다음과 같습니다.</p>
<ol type="1">
<li>비음수성: <span class="math inline">\(D_{KL}(P||Q) \ge 0\)</span></li>
<li>P = Q일 때만 0: <span class="math inline">\(D_{KL}(P||Q) = 0\)</span> if and only if <span class="math inline">\(P = Q\)</span></li>
<li>비대칭성: 일반적으로 <span class="math inline">\(D_{KL}(P||Q) \ne D_{KL}(Q||P)\)</span></li>
</ol>
<p>KL 발산은 딥러닝에서 다음과 같이 활용됩니다.</p>
<ol type="1">
<li>변분 추론: 근사 분포와 실제 분포의 차이를 최소화하는 데 사용됩니다.</li>
<li>모델 압축: 교사-학생 네트워크에서 지식 증류에 활용됩니다.</li>
<li>이상 탐지: 정상 데이터 분포와의 차이를 측정하는 데 사용됩니다.</li>
</ol>
<p>정보 이론의 개념들은 서로 밀접하게 연관되어 있습니다. 예를 들어, 상호 정보량은 엔트로피와 조건부 엔트로피의 차이로 표현할 수 있습니다.</p>
<p><span class="math inline">\(I(X;Y) = H(X) - H(X|Y)\)</span></p>
<p>또한, KL 발산은 교차 엔트로피와 엔트로피의 차이로 나타낼 수 있습니다.</p>
<p><span class="math inline">\(D_{KL}(P||Q) = H(P,Q) - H(P)\)</span></p>
<p>여기서 <span class="math inline">\(H(P,Q)\)</span>는 <span class="math inline">\(P\)</span>와 <span class="math inline">\(Q\)</span>의 교차 엔트로피입니다. 다음은 두 확률 분포 간의 KL 발산을 계산하고 분포를 시각화합니다.</p>
<div id="cell-39" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_02.information_theory <span class="im">import</span> kl_divergence_example</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>kl_divergence_example()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>KL(P||Q): 0.0823
KL(Q||P): 0.0872</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="02_딥러닝의 수학_files/figure-html/cell-12-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>이러한 정보 이론의 개념들은 딥러닝 모델의 설계와 최적화에 광범위하게 적용됩니다. 예를 들어, 오토인코더의 손실 함수로 재구성 오차와 KL 발산의 조합을 사용하거나, 강화학습에서 정책 최적화를 위해 KL 발산을 제약 조건으로 사용하는 등 다양한 방식으로 활용됩니다.</p>
<p>다음 장에서는 이러한 확률, 통계, 정보 이론의 개념들이 실제 딥러닝 모델에서 어떻게 적용되는지 살펴보겠습니다.</p>
<div class="callout callout-style-default callout-note callout-titled" title="딥다이브: 정보 이론 핵심 개념">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-9-contents" aria-controls="callout-9" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
딥다이브: 정보 이론 핵심 개념
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-9" class="callout-9-contents callout-collapse collapse">
<section id="정보-이론-핵심-개념---information-content-cross-entropy-kl-divergence-mutual-information" class="level2 callout-body-container callout-body">
<h2 class="anchored" data-anchor-id="정보-이론-핵심-개념---information-content-cross-entropy-kl-divergence-mutual-information">정보 이론 핵심 개념 - Information Content, Cross Entropy, KL-Divergence, Mutual Information</h2>
<section id="information-content-self-information" class="level3">
<h3 class="anchored" data-anchor-id="information-content-self-information">1. Information Content (Self-information)</h3>
<ul>
<li><p><strong>정의:</strong> 정보량 (Information Content, Self-information)은 특정 사건이 발생했을 때 얻을 수 있는 정보의 양을 나타냅니다. 드물게 발생하는 사건일수록 더 높은 정보량을 갖습니다.</p></li>
<li><p><strong>수식:</strong> <span class="math display">\[I(x) = -\log(P(x))\]</span></p>
<ul>
<li><span class="math inline">\(x\)</span>: 사건</li>
<li><span class="math inline">\(P(x)\)</span>: 사건 <span class="math inline">\(x\)</span>가 발생할 확률</li>
<li><span class="math inline">\(\log\)</span>: 로그의 밑(base)은 2 (단위: bits), <span class="math inline">\(e\)</span> (단위: nats), 또는 10 등이 사용될 수 있습니다. 일반적으로 딥러닝에서는 자연로그(<span class="math inline">\(e\)</span>)를 사용합니다.</li>
</ul></li>
<li><p><strong>직관적 설명:</strong></p>
<ul>
<li><strong>희귀성:</strong> 확률이 낮은 사건(희귀한 사건)일수록 정보량이 큽니다. 예를 들어, “해가 동쪽에서 뜬다”는 당연한 사실이므로 정보량이 거의 없지만, “오늘 로또 1등에 당첨되었다”는 매우 드문 사건이므로 정보량이 큽니다.</li>
<li><strong>불확실성 감소:</strong> 정보량은 사건 발생 전의 불확실성이 사건 발생 후 얼마나 감소했는지를 나타내는 척도로 해석할 수 있습니다.</li>
</ul></li>
<li><p><strong>성질:</strong></p>
<ul>
<li><span class="math inline">\(0 \le P(x) \le 1\)</span> 이므로, <span class="math inline">\(I(x) \ge 0\)</span> 입니다.</li>
<li><span class="math inline">\(P(x) = 1\)</span> (확실한 사건) 이면, <span class="math inline">\(I(x) = 0\)</span> 입니다.</li>
<li><span class="math inline">\(P(x)\)</span>가 작을수록 <span class="math inline">\(I(x)\)</span>는 커집니다.</li>
<li>두 독립 사건 <span class="math inline">\(x\)</span>와 <span class="math inline">\(y\)</span>에 대해, <span class="math inline">\(I(x, y) = I(x) + I(y)\)</span> 입니다. (정보량의 가법성)</li>
</ul></li>
</ul>
</section>
<section id="cross-entropy" class="level3">
<h3 class="anchored" data-anchor-id="cross-entropy">2. Cross Entropy</h3>
<ul>
<li><p><strong>정의:</strong> 크로스 엔트로피(Cross Entropy)는 두 확률 분포 <span class="math inline">\(P\)</span>와 <span class="math inline">\(Q\)</span>가 얼마나 다른지를 측정하는 척도입니다. <span class="math inline">\(P\)</span>를 참 분포, <span class="math inline">\(Q\)</span>를 추정 분포라고 할 때, <span class="math inline">\(Q\)</span>를 사용하여 <span class="math inline">\(P\)</span>를 나타낼 때 필요한 평균 비트 수를 나타냅니다.</p></li>
<li><p><strong>유도:</strong></p>
<ol type="1">
<li><strong>정보량:</strong> 참 분포 <span class="math inline">\(P\)</span>를 따르는 사건 <span class="math inline">\(x\)</span>의 정보량: <span class="math inline">\(I(x) = -\log P(x)\)</span></li>
<li><strong>평균 정보량 (엔트로피):</strong> 참 분포 <span class="math inline">\(P\)</span>에 대한 평균 정보량 (엔트로피): <span class="math inline">\(H(P) = -\sum_{x} P(x) \log P(x)\)</span></li>
<li><strong>추정 분포 사용:</strong> 추정 분포 <span class="math inline">\(Q\)</span>를 사용하여 참 분포 <span class="math inline">\(P\)</span>를 나타낼 때, 각 사건 <span class="math inline">\(x\)</span>에 대한 정보량: <span class="math inline">\(-\log Q(x)\)</span></li>
<li><strong>Cross Entropy:</strong> 추정 분포 <span class="math inline">\(Q\)</span>를 사용하여 참 분포 <span class="math inline">\(P\)</span>를 나타낼 때의 평균 정보량: <span class="math display">\[H(P, Q) = -\sum_{x} P(x) \log Q(x)\]</span></li>
</ol></li>
<li><p><strong>직관적 설명:</strong></p>
<ul>
<li><span class="math inline">\(P\)</span>와 <span class="math inline">\(Q\)</span>가 비슷할수록 크로스 엔트로피는 작아집니다.</li>
<li><span class="math inline">\(P = Q\)</span> 일 때, 크로스 엔트로피는 최소값(엔트로피 <span class="math inline">\(H(P)\)</span>)을 갖습니다.</li>
<li><span class="math inline">\(P\)</span>와 <span class="math inline">\(Q\)</span>가 다를수록 크로스 엔트로피는 커집니다. 즉, 추정 분포가 실제 분포를 잘 반영하지 못할수록 정보 손실이 발생합니다.</li>
</ul></li>
<li><p><strong>Binary Cross Entropy (BCE):</strong></p>
<ul>
<li>두 개의 클래스 (0 또는 1)를 갖는 이진 분류 문제에서 사용됩니다.</li>
<li><span class="math inline">\(P = [p, 1-p]\)</span> (실제 클래스 확률 분포, <span class="math inline">\(p\)</span>는 클래스 1의 확률)</li>
<li><span class="math inline">\(Q = [q, 1-q]\)</span> (예측 클래스 확률 분포, <span class="math inline">\(q\)</span>는 클래스 1로 예측할 확률)</li>
<li><span class="math display">\[H(P, Q) = -[p \log q + (1-p) \log (1-q)]\]</span></li>
</ul></li>
<li><p><strong>Categorical Cross Entropy (CCE):</strong></p>
<ul>
<li>여러 개의 클래스를 갖는 다중 클래스 분류 문제에서 사용됩니다.</li>
<li><span class="math inline">\(P = [p_1, p_2, ..., p_k]\)</span> (실제 클래스 확률 분포, <span class="math inline">\(p_i\)</span>는 <span class="math inline">\(i\)</span>번째 클래스의 확률, one-hot encoding)</li>
<li><span class="math inline">\(Q = [q_1, q_2, ..., q_k]\)</span> (예측 클래스 확률 분포, <span class="math inline">\(q_i\)</span>는 <span class="math inline">\(i\)</span>번째 클래스로 예측할 확률, softmax)</li>
<li><span class="math display">\[H(P, Q) = -\sum_{i=1}^{k} p_i \log q_i\]</span></li>
</ul></li>
</ul>
</section>
<section id="cross-entropy와-likelihood" class="level3">
<h3 class="anchored" data-anchor-id="cross-entropy와-likelihood">3. Cross Entropy와 Likelihood</h3>
<ul>
<li><strong>Likelihood (가능도):</strong> 주어진 데이터가 특정 모델(모수)에서 발생했을 확률.</li>
<li><strong>Negative Log-Likelihood (NLL):</strong> 가능도에 로그를 취하고 음수를 붙인 값.</li>
<li><strong>Cross Entropy와 NLL의 관계:</strong>
<ul>
<li>분류 문제에서, 모델의 출력(예측 확률 분포)을 <span class="math inline">\(Q\)</span>, 실제 레이블(one-hot encoding)을 <span class="math inline">\(P\)</span>라고 할 때, Cross Entropy는 Negative Log-Likelihood와 같습니다.</li>
<li>Cross Entropy를 최소화하는 것은 Likelihood를 최대화하는 것과 같습니다(Maximum Likelihood Estimation, MLE).</li>
</ul></li>
<li><strong>딥러닝에서의 활용:</strong>
<ul>
<li>딥러닝에서 분류 문제의 손실 함수(loss function)로 Cross Entropy를 사용하는 것은 모델의 출력이 실제 레이블의 분포를 따르도록 학습하는 것과 같습니다(MLE 관점).</li>
</ul></li>
</ul>
</section>
<section id="kl-divergence와-cross-entropy-관계" class="level3">
<h3 class="anchored" data-anchor-id="kl-divergence와-cross-entropy-관계">4. KL-Divergence와 Cross Entropy 관계</h3>
<ul>
<li><p><strong>KL-Divergence (Kullback-Leibler Divergence):</strong></p>
<ul>
<li>두 확률 분포 <span class="math inline">\(P\)</span>와 <span class="math inline">\(Q\)</span>의 차이를 측정하는 또 다른 방법입니다(거리 개념은 아님, 비대칭적).</li>
<li><span class="math inline">\(P\)</span>에서 <span class="math inline">\(Q\)</span>로의 KL-Divergence는 <span class="math inline">\(Q\)</span>를 사용하여 <span class="math inline">\(P\)</span>를 표현할 때 추가적으로 필요한 정보량을 나타냅니다.</li>
<li><span class="math display">\[D_{KL}(P||Q) = \sum_{x} P(x) \log \frac{P(x)}{Q(x)} = \sum_{x} P(x)[\log P(x) - \log Q(x)]\]</span></li>
</ul></li>
<li><p><strong>KL-Divergence와 Cross Entropy의 관계:</strong></p>
<p><span class="math display">\[D_{KL}(P||Q) = \sum_{x} P(x) \log P(x) - \sum_{x} P(x) \log Q(x) =  -\sum_{x} P(x) \log Q(x)  - (-\sum_{x} P(x) \log P(x))\]</span> <span class="math display">\[D_{KL}(P||Q) = H(P, Q) - H(P)\]</span></p>
<ul>
<li><p><span class="math inline">\(H(P,Q)\)</span>: Cross Entropy</p></li>
<li><p><span class="math inline">\(H(P)\)</span>: Entropy</p></li>
<li><p>KL-Divergence는 Cross Entropy에서 <span class="math inline">\(P\)</span>의 Entropy를 뺀 값입니다.</p></li>
<li><p><span class="math inline">\(P\)</span>가 고정되어 있을 때, Cross Entropy를 최소화하는 것은 KL-Divergence를 최소화하는 것과 같습니다.</p></li>
</ul></li>
</ul>
</section>
<section id="mutual-information과-conditional-entropy-관계" class="level3">
<h3 class="anchored" data-anchor-id="mutual-information과-conditional-entropy-관계">5. Mutual Information과 Conditional Entropy 관계</h3>
<ul>
<li><p><strong>Mutual Information (상호 정보량):</strong></p>
<ul>
<li>두 확률 변수 <span class="math inline">\(X\)</span>와 <span class="math inline">\(Y\)</span>가 서로 얼마나 많은 정보를 공유하는지를 나타내는 척도입니다.</li>
<li><span class="math inline">\(X\)</span>를 알 때 <span class="math inline">\(Y\)</span>에 대한 불확실성이 얼마나 감소하는지 (또는 그 반대)를 나타냅니다.</li>
<li><span class="math display">\[I(X;Y) = \sum_{x, y} P(x, y) \log \frac{P(x, y)}{P(x)P(y)}\]</span>
<ul>
<li><span class="math inline">\(P(x,y)\)</span>: 결합 확률 분포 (Joint Probability Distribution)</li>
<li><span class="math inline">\(P(x)\)</span>, <span class="math inline">\(P(y)\)</span>: 주변 확률 분포 (Marginal Probability Distribution)</li>
</ul></li>
</ul></li>
<li><p><strong>Conditional Entropy (조건부 엔트로피):</strong></p>
<ul>
<li>확률 변수 <span class="math inline">\(Y\)</span>가 주어졌을 때, 확률 변수 <span class="math inline">\(X\)</span>의 불확실성을 나타냅니다. <span class="math display">\[H(X|Y) = -\sum_{y} P(y) \sum_{x} P(x|y) \log P(x|y) =  -\sum_{x,y} P(x,y) \log P(x|y)\]</span></li>
</ul></li>
<li><p><strong>Mutual Information과 Conditional Entropy 관계</strong>: <span class="math display">\[I(X;Y) = H(X) - H(X|Y) = H(Y) - H(Y|X)\]</span></p>
<ul>
<li><span class="math inline">\(X\)</span>와 <span class="math inline">\(Y\)</span>의 상호 정보량은 <span class="math inline">\(X\)</span>의 엔트로피에서 <span class="math inline">\(Y\)</span>를 알 때의 <span class="math inline">\(X\)</span>의 조건부 엔트로피를 뺀 값과 같습니다.</li>
<li><span class="math inline">\(Y\)</span>를 알게 됨으로써 <span class="math inline">\(X\)</span>의 불확실성이 감소하는 정도를 나타냅니다.</li>
</ul></li>
</ul>
</section>
<section id="jensenshannon-divergence" class="level3">
<h3 class="anchored" data-anchor-id="jensenshannon-divergence">6. Jensen–Shannon Divergence</h3>
<ul>
<li><strong>Jensen–Shannon Divergence (JSD):</strong>
<ul>
<li>두 확률 분포 <span class="math inline">\(P\)</span>와 <span class="math inline">\(Q\)</span> 사이의 거리를 측정하는 또 다른 방법입니다. KL-Divergence와 달리 대칭적(symmetric)이며, bounded (0과 1 사이)입니다.</li>
<li><span class="math display">\[JSD(P||Q) = \frac{1}{2}D_{KL}(P||M) + \frac{1}{2}D_{KL}(Q||M)\]</span>
<ul>
<li><span class="math inline">\(M = \frac{1}{2}(P + Q)\)</span>: <span class="math inline">\(P\)</span>와 <span class="math inline">\(Q\)</span>의 평균 분포</li>
</ul></li>
</ul></li>
<li><strong>특징:</strong>
<ul>
<li>대칭성: <span class="math inline">\(JSD(P||Q) = JSD(Q||P)\)</span></li>
<li>유계성: <span class="math inline">\(0 \le JSD(P||Q) \le 1\)</span> (log base 2를 사용할 때)</li>
<li>JSD의 제곱근은 거리 함수(metric)의 조건을 만족합니다.</li>
</ul></li>
</ul>
</section>
</section>
</div>
</div>
</section>
</section>
<section id="손실-함수" class="level3">
<h3 class="anchored" data-anchor-id="손실-함수">2.3.4 손실 함수</h3>
<p>손실 함수(Loss Function)는 머신러닝 모델의 예측이 실제 값과 얼마나 차이가 나는지를 측정하는 함수입니다. 모델 학습의 목표는 이 손실 함수의 값을 최소화하는 파라미터(가중치와 편향)를 찾는 것입니다. 적절한 손실 함수를 선택하는 것은 모델의 성능에 큰 영향을 미치므로, 문제의 유형과 데이터의 특성에 맞게 신중하게 선택해야 합니다.</p>
<section id="손실-함수의-정의" class="level4">
<h4 class="anchored" data-anchor-id="손실-함수의-정의">손실 함수의 정의</h4>
<p>일반적으로 손실 함수 <span class="math inline">\(L\)</span>은 모델의 파라미터를 <span class="math inline">\(\theta\)</span>, 데이터 포인트를 <span class="math inline">\((x_i, y_i)\)</span>라고 할 때, 다음과 같이 표현할 수 있습니다. (여기서 <span class="math inline">\(y_i\)</span>는 실제 값, <span class="math inline">\(f(x_i; \theta)\)</span>는 모델의 예측값)</p>
<p><span class="math inline">\(L(\theta) = \frac{1}{N} \sum_{i=1}^{N} l(y_i, f(x_i; \theta))\)</span></p>
<p><span class="math inline">\(N\)</span>은 데이터 포인트의 개수, <span class="math inline">\(l\)</span>은 개별 데이터 포인트에 대한 손실을 나타내는 함수(loss term)입니다.</p>
</section>
<section id="주요-손실-함수" class="level4">
<h4 class="anchored" data-anchor-id="주요-손실-함수">주요 손실 함수</h4>
<p>다음은 머신러닝과 딥러닝에서 자주 사용되는 손실 함수들입니다.</p>
<section id="평균-제곱-오차-mean-squared-error-mse" class="level5">
<h5 class="anchored" data-anchor-id="평균-제곱-오차-mean-squared-error-mse">1. 평균 제곱 오차 (Mean Squared Error, MSE)</h5>
<ul>
<li><strong>수식:</strong> <span class="math inline">\(MSE = \frac{1}{N} \sum_{i=1}^N (y_i - \hat{y}_i)^2\)</span> (<span class="math inline">\(y_i\)</span>: 실제값, <span class="math inline">\(\hat{y}_i\)</span>: 예측값)</li>
<li><strong>특징:</strong>
<ul>
<li>오차를 제곱하기 때문에 이상치(outlier)에 민감합니다.</li>
<li>미분 가능하며, 볼록 함수(convex function)이므로 경사 하강법을 사용하여 최적해를 찾기 쉽습니다.</li>
</ul></li>
<li><strong>용도:</strong> 주로 회귀(Regression) 문제에 사용됩니다.</li>
</ul>
</section>
<section id="평균-절대-오차-mean-absolute-error-mae" class="level5">
<h5 class="anchored" data-anchor-id="평균-절대-오차-mean-absolute-error-mae">2. 평균 절대 오차 (Mean Absolute Error, MAE)</h5>
<ul>
<li><strong>수식:</strong> <span class="math inline">\(MAE = \frac{1}{N} \sum_{i=1}^N |y_i - \hat{y}_i|\)</span></li>
<li><strong>특징:</strong>
<ul>
<li>MSE보다 이상치에 덜 민감합니다.</li>
<li>x=0에서 미분 불가능하지만, 딥러닝 프레임워크에서 자동 미분으로 처리 가능.</li>
</ul></li>
<li><strong>용도</strong>: 회귀 문제에 사용</li>
</ul>
</section>
<section id="교차-엔트로피-손실-cross-entropy-loss" class="level5">
<h5 class="anchored" data-anchor-id="교차-엔트로피-손실-cross-entropy-loss">3. 교차 엔트로피 손실 (Cross-Entropy Loss)</h5>
<ul>
<li><strong>수식:</strong>
<ul>
<li><strong>이진 분류 (Binary Classification):</strong> <span class="math inline">\(L = -\frac{1}{N} \sum_{i=1}^N [y_i \log(\hat{y}_i) + (1-y_i) \log(1-\hat{y}_i)]\)</span></li>
<li><strong>다중 클래스 분류 (Multi-class Classification):</strong> <span class="math inline">\(L = -\frac{1}{N} \sum_{i=1}^N \sum_{j=1}^C y_{ij} \log(\hat{y}_{ij})\)</span> (<span class="math inline">\(C\)</span>: 클래스 수)</li>
</ul></li>
<li><strong>특징:</strong>
<ul>
<li>모델이 예측한 확률 분포와 실제 분포 사이의 차이를 측정합니다.</li>
<li>분류 문제에서 MSE보다 빠르게 수렴하는 경향이 있습니다.</li>
<li>출력층에 소프트맥스(softmax) 활성화 함수와 함께 사용.</li>
</ul></li>
<li><strong>용도:</strong> 분류 문제 (이진 분류, 다중 클래스 분류)</li>
</ul>
</section>
<section id="힌지-손실-hinge-loss" class="level5">
<h5 class="anchored" data-anchor-id="힌지-손실-hinge-loss">4. 힌지 손실 (Hinge Loss)</h5>
<ul>
<li><strong>수식:</strong> <span class="math inline">\(L = \max(0, 1 - y \cdot f(x))\)</span> (<span class="math inline">\(y\)</span>: {-1, 1} 실제 클래스, <span class="math inline">\(f(x)\)</span>: 모델 예측값)</li>
<li><strong>특징:</strong>
<ul>
<li>“정답”과 “오답” 사이의 마진(margin)을 최대화.</li>
<li>x=1에서 미분 불가능</li>
</ul></li>
<li><strong>용도:</strong> 주로 서포트 벡터 머신(SVM)과 같은 이진 분류 문제.</li>
</ul>
</section>
</section>
<section id="손실-함수-선택-기준" class="level4">
<h4 class="anchored" data-anchor-id="손실-함수-선택-기준">손실 함수 선택 기준</h4>
<ul>
<li><strong>문제 유형:</strong> 회귀 문제인지, 분류 문제인지에 따라 적합한 손실 함수가 달라집니다.</li>
<li><strong>데이터 특성:</strong> 이상치 유무, 클래스 불균형 등에 따라 적절한 손실 함수를 선택해야 합니다.</li>
<li><strong>모델</strong>: 사용하는 모델에 따라 적절한 손실함수가 달라진다.</li>
</ul>
</section>
<section id="추가적인-손실-함수" class="level4">
<h4 class="anchored">추가적인 손실 함수</h4>
<ul>
<li><strong>Kullback-Leibler Divergence (KLD):</strong> 두 확률 분포 P와 Q 사이의 차이를 측정합니다. 주로 변분 오토인코더(VAE)와 같은 생성 모델에서 사용.</li>
<li><strong>Focal Loss:</strong> 불균형 데이터에서 잘 작동하도록 Cross-Entropy를 조정한 손실함수. 객체 인식 문제에서 주로 활용.</li>
<li><strong>Huber Loss:</strong> MSE와 MAE를 결합한 형태로, 이상치에 강건하면서도 미분 가능합니다.</li>
<li><strong>Log-Cosh Loss:</strong> Huber Loss와 유사하지만, 모든 지점에서 두 번 미분 가능하다는 장점이 있습니다.</li>
<li><strong>Contrastive Loss:</strong> Siamese Network 등에서 사용되며, 유사한 샘플 쌍은 가깝게, 유사하지 않은 샘플 쌍은 멀리 떨어지도록 임베딩을 학습하는 데 사용됩니다.</li>
<li><strong>Triplet Loss:</strong> Anchor, Positive, Negative 세 개의 샘플을 사용하여, Anchor와 Positive 샘플 사이의 거리는 가깝게, Anchor와 Negative 샘플 사이의 거리는 멀어지도록 임베딩을 학습합니다.</li>
<li><strong>CTC Loss:</strong> 음성 인식, 필기 인식 등에서 입력 시퀀스와 출력 시퀀스의 길이가 다를 때 사용되는 손실 함수입니다.</li>
</ul>
<div class="callout callout-style-default callout-note callout-titled" title="클릭하여 내용 보기(딥다이브 : 손실 함수 심층 분석)">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-10-contents" aria-controls="callout-10" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
클릭하여 내용 보기(딥다이브 : 손실 함수 심층 분석)
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-10" class="callout-10-contents callout-collapse collapse">
<section id="손실-함수-심층-분석" class="level3 callout-body-container callout-body">
<h3 class="anchored" data-anchor-id="손실-함수-심층-분석">손실 함수 심층 분석</h3>
<section id="손실-함수와-최대-우도-추정-maximum-likelihood-estimation-mle" class="level4">
<h4 class="anchored" data-anchor-id="손실-함수와-최대-우도-추정-maximum-likelihood-estimation-mle">손실 함수와 최대 우도 추정 (Maximum Likelihood Estimation, MLE)</h4>
<p>많은 머신러닝 모델의 학습은 최대 우도 추정(MLE) 관점에서 설명할 수 있습니다. MLE는 주어진 데이터를 가장 잘 설명하는 모델 파라미터를 찾는 방법입니다. 데이터가 독립적이고 동일한 분포(i.i.d)를 따른다고 가정할 때, 우도 함수(Likelihood Function)는 다음과 같이 정의됩니다.</p>
<p><span class="math inline">\(L(\theta) = P(D|\theta) = \prod_{i=1}^{N} P(y_i | x_i; \theta)\)</span></p>
<p>여기서 <span class="math inline">\(D = \{(x_1, y_1), (x_2, y_2), ..., (x_N, y_N)\}\)</span>는 훈련 데이터, <span class="math inline">\(\theta\)</span>는 모델 파라미터입니다. <span class="math inline">\(P(y_i | x_i; \theta)\)</span>는 모델이 <span class="math inline">\(x_i\)</span>를 입력으로 받았을 때 <span class="math inline">\(y_i\)</span>를 출력할 확률(또는 확률 밀도)입니다.</p>
<p>MLE의 목표는 우도 함수 <span class="math inline">\(L(\theta)\)</span>를 최대화하는 파라미터 <span class="math inline">\(\theta\)</span>를 찾는 것입니다. 실제로는 로그 우도 함수(log-likelihood function)를 최대화하는 것이 계산상 더 편리합니다.</p>
<p><span class="math inline">\(\log L(\theta) = \sum_{i=1}^{N} \log P(y_i | x_i; \theta)\)</span></p>
<ul>
<li><p><strong>MSE와 MLE:</strong> 선형 회귀 모델에서 오차가 평균이 0이고 분산이 <span class="math inline">\(\sigma^2\)</span>인 정규 분포를 따른다고 가정하면, MLE는 MSE를 최소화하는 것과 동일합니다.</p>
<p><span class="math inline">\(P(y_i | x_i; \theta) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(y_i - f(x_i; \theta))^2}{2\sigma^2}\right)\)</span></p>
<p>로그 우도 함수는 다음과 같습니다. <span class="math inline">\(\log L(\theta) = -\frac{N}{2}\log(2\pi\sigma^2) - \frac{1}{2\sigma^2}\sum_{i=1}^{N}(y_i - f(x_i;\theta))^2\)</span></p>
<p>상수를 제외하고, <span class="math inline">\(\sigma^2\)</span>이 상수라고 가정하면 로그 우도 함수를 최대화하는 것은 MSE를 <em>최소화</em>하는 것과 같습니다.</p></li>
<li><p><strong>Cross-Entropy와 MLE:</strong> 분류 문제에서, 출력 <span class="math inline">\(\hat{y}_i\)</span>를 베르누이 분포(이진 분류) 또는 다항 분포(다중 클래스 분류)의 파라미터로 해석할 수 있습니다. 이 경우, MLE는 Cross-Entropy Loss를 최소화하는 것과 동일합니다.</p>
<ul>
<li><p><strong>이진 분류 (베르누이 분포):</strong> <span class="math inline">\(\hat{y_i}\)</span>를 모델이 예측한, <span class="math inline">\(y_i=1\)</span>일 확률이라고 하면, <span class="math inline">\(P(y_i|x_i;\theta) = \hat{y_i}^{y_i} (1 - \hat{y_i})^{(1-y_i)}\)</span> 로그 우도: <span class="math inline">\(\log L(\theta) = \sum_{i=1}^{N} [y_i \log(\hat{y}_i) + (1 - y_i)\log(1 - \hat{y}_i)]\)</span></p></li>
<li><p><strong>다중 클래스 분류 (Categorical/Multinoulli Distribution):</strong> <span class="math inline">\(P(y_i | x_i; \theta) = \prod_{j=1}^{C} \hat{y}_{ij}^{y_{ij}}\)</span> (one-hot encoding) 로그 우도: <span class="math inline">\(\log L(\theta) = \sum_{i=1}^N \sum_{j=1}^C y_{ij} \log(\hat{y}_{ij})\)</span></p></li>
</ul>
<p>따라서 Cross-Entropy Loss를 최소화하는 것은 데이터의 분포를 가장 잘 모델링하는 파라미터를 찾는 MLE와 동일한 과정입니다.</p></li>
</ul>
</section>
<section id="추가적인-손실-함수-kld-focal-loss" class="level4">
<h4 class="anchored" data-anchor-id="추가적인-손실-함수-kld-focal-loss">추가적인 손실 함수 (KLD, Focal Loss)</h4>
<ul>
<li><p><strong>Kullback-Leibler Divergence (KLD):</strong></p>
<ul>
<li><strong>설명:</strong> 두 확률 분포 P와 Q 사이의 차이를 측정합니다. P는 실제 데이터의 분포, Q는 모델이 추정한 분포를 나타냅니다.</li>
<li><strong>수식:</strong> <span class="math inline">\(D_{KL}(P||Q) = \sum_{x} P(x) \log \frac{P(x)}{Q(x)}\)</span></li>
<li><strong>특징:</strong>
<ul>
<li>비대칭적(asymmetric): <span class="math inline">\(D_{KL}(P||Q) \neq D_{KL}(Q||P)\)</span></li>
<li>항상 0 이상: <span class="math inline">\(D_{KL}(P||Q) \ge 0\)</span>, <span class="math inline">\(P=Q\)</span>일 때만 <span class="math inline">\(D_{KL}(P||Q) = 0\)</span></li>
<li>P(x) = 0 인 곳에서 정의되지 않는 문제</li>
</ul></li>
<li><strong>VAE와의 관계:</strong>
<ul>
<li>변분 오토인코더(Variational Autoencoder, VAE)에서는 잠재 변수(latent variable)의 사후 분포(posterior distribution)를 정규 분포와 같은 사전 분포(prior distribution)에 가깝게 만들기 위해 KL Divergence를 사용합니다.</li>
<li>VAE의 손실 함수는 재구성 손실(reconstruction loss)과 KL Divergence 항으로 구성됩니다.</li>
</ul></li>
</ul></li>
<li><p><strong>Focal Loss:</strong></p>
<ul>
<li><strong>설명:</strong> Cross-Entropy Loss를 변형하여 클래스 불균형 문제, 특히 “쉬운” 샘플(easy examples)과 “어려운” 샘플(hard examples) 간의 불균형을 해결하기 위해 제안되었습니다.</li>
<li><strong>수식:</strong> <span class="math inline">\(FL(p_t) = -\alpha_t (1 - p_t)^\gamma \log(p_t)\)</span>
<ul>
<li><span class="math inline">\(p_t\)</span>: 모델이 예측한 정답 클래스에 대한 확률</li>
<li><span class="math inline">\(\gamma\)</span>: focusing parameter (<span class="math inline">\(\gamma \ge 0\)</span>, 일반적으로 2)</li>
<li><span class="math inline">\(\alpha_t\)</span>: 클래스별 가중치 (선택 사항)</li>
</ul></li>
<li><strong>특징:</strong>
<ul>
<li><span class="math inline">\(\gamma = 0\)</span> 이면, 일반적인 Cross-Entropy Loss와 동일.</li>
<li><span class="math inline">\(\gamma &gt; 0\)</span> 이면, 잘 분류되는 샘플(<span class="math inline">\(p_t\)</span>가 큼)의 손실은 줄이고, 잘 분류되지 않는 샘플(<span class="math inline">\(p_t\)</span>가 작음)의 손실은 상대적으로 크게 유지. 즉, 어려운 샘플에 더 집중하여 학습.</li>
<li><span class="math inline">\(\alpha_t\)</span>를 사용하여 클래스별 가중치를 조절할 수 있음 (예: 수가 적은 클래스에 더 큰 가중치 부여).</li>
</ul></li>
<li><strong>객체 인식(Object Detection)에서의 활용:</strong>
<ul>
<li>객체 검출 문제는 배경 영역(negative)이 객체 영역(positive)보다 훨씬 많아 심각한 클래스 불균형 발생.</li>
<li>Focal Loss는 이러한 불균형을 완화하여, 객체 검출 모델이 배경보다는 실제 객체에 더 집중하여 학습하도록 유도.</li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="다양한-손실-함수-고급" class="level4">
<h4 class="anchored" data-anchor-id="다양한-손실-함수-고급">다양한 손실 함수 (고급)</h4>
<ul>
<li><p><strong>Huber Loss:</strong> MSE와 MAE의 장점을 결합한 손실 함수입니다. 오차가 특정 값(<span class="math inline">\(\delta\)</span>)보다 작을 때는 MSE처럼 제곱 오차를 사용하고, 오차가 클 때는 MAE처럼 절댓값 오차를 사용합니다. 이상치에 강건하면서도 미분 가능합니다.</p>
<p><span class="math inline">\(L_\delta(y, \hat{y}) = \begin{cases}
\frac{1}{2}(y - \hat{y})^2 &amp; \text{if } |y - \hat{y}| \le \delta \\
\delta(|y - \hat{y}| - \frac{1}{2}\delta) &amp; \text{otherwise}
\end{cases}\)</span></p></li>
<li><p><strong>Log-Cosh Loss:</strong> <span class="math inline">\(\log(\cosh(y - \hat{y}))\)</span>로 정의됩니다. Huber Loss와 유사하게 이상치에 강건하며, 모든 지점에서 두 번 미분 가능하다는 장점이 있습니다.</p></li>
<li><p><strong>Quantile Loss:</strong> 특정 분위수(quantile)에서의 예측 오차를 최소화하는 데 사용됩니다.</p></li>
<li><p><strong>Contrastive Loss, Triplet Loss:</strong> Siamese Network, Triplet Network 등에서 사용되며, 유사한 샘플 쌍/세 쌍 간의 거리를 조절하는 데 사용됩니다. (자세한 내용은 관련 논문 참고)</p></li>
<li><p><strong>Connectionist Temporal Classification (CTC) Loss</strong>: 음성 인식, 필기 인식 등 입력 시퀀스와 출력 시퀀스 간의 정렬(alignment)이 명확하지 않은 경우에 사용됩니다.</p></li>
</ul>
</section>
<section id="손실-함수-선택-가이드라인-심화" class="level4">
<h4 class="anchored" data-anchor-id="손실-함수-선택-가이드라인-심화">손실 함수 선택 가이드라인 (심화)</h4>
<ul>
<li><strong>이상치 처리:</strong> 이상치가 많고, 이상치에 강건(robust)해야 하는 경우, MAE, Huber Loss, Quantile Loss 등을 고려할 수 있습니다.</li>
<li><strong>미분 가능성:</strong> 경사 하강법 기반 최적화를 위해서는 미분 가능한 손실 함수가 필요합니다. 하지만 Hinge Loss, MAE와 같이 미분 불가능한 점이 있는 경우에도, subgradient(subdifferential)를 사용하거나, 딥러닝 프레임워크의 자동 미분을 통해 해결할 수 있습니다.</li>
<li><strong>확률적 모델링:</strong> 모델의 출력을 확률 분포로 해석하고 싶은 경우, Cross-Entropy Loss가 적합합니다.</li>
<li><strong>클래스 불균형:</strong> 클래스 불균형이 심한 경우, Focal Loss, Weighted Cross-Entropy 등을 고려할 수 있습니다.</li>
<li><strong>다중 출력</strong>: 여러 출력이 있고 출력간의 상관관계가 존재한다면, 각 출력에 대한 손실함수를 결합해서 사용.</li>
</ul>
<p>손실 함수는 딥러닝 모델의 성능을 결정하는 중요한 요소 중 하나입니다. 문제의 특성과 데이터의 분포, 그리고 모델의 구조를 고려하여 적절한 손실 함수를 선택하고, 필요하다면 새로운 손실 함수를 설계하는 능력이 딥러닝 엔지니어에게 요구됩니다.</p>
</section>
</section>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled" title="클릭하여 내용 보기(딥다이브 : 새로운 손실 함수 설계)">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-11-contents" aria-controls="callout-11" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
클릭하여 내용 보기(딥다이브 : 새로운 손실 함수 설계)
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-11" class="callout-11-contents callout-collapse collapse">
<section id="새로운-손실-함수-설계" class="level3 callout-body-container callout-body">
<h3 class="anchored" data-anchor-id="새로운-손실-함수-설계">새로운 손실 함수 설계</h3>
<p>기존의 손실 함수(MSE, Cross-Entropy 등)가 항상 최적의 선택은 아닙니다. 문제의 특수한 요구 사항, 데이터의 분포, 모델의 구조 등에 따라 새로운 손실 함수를 설계해야 할 필요가 있습니다. 새로운 손실 함수를 설계하는 것은 딥러닝 연구의 중요한 부분이며, 모델의 성능을 크게 향상시킬 수 있는 잠재력을 가지고 있습니다.</p>
<section id="새로운-손실-함수가-필요한-경우" class="level4">
<h4 class="anchored" data-anchor-id="새로운-손실-함수가-필요한-경우">새로운 손실 함수가 필요한 경우</h4>
<ul>
<li><strong>데이터의 특수한 구조:</strong> 데이터가 일반적인 분포(가우시안, 베르누이 등)를 따르지 않거나, 특별한 구조(예: 순위, 희소성, 계층 구조, 그래프 구조)를 갖는 경우.</li>
<li><strong>문제의 특수한 제약 조건:</strong> 모델의 예측에 특정한 제약 조건(예: monotonicity, sparsity, fairness, robustness)을 부여하고 싶은 경우.</li>
<li><strong>기존 손실 함수의 한계:</strong> 기존 손실 함수가 특정 문제에서 잘 작동하지 않거나(예: 이상치에 민감, 클래스 불균형), 원하는 목표를 충분히 반영하지 못하는 경우. 특정 metric을 직접 최적화하고 싶은 경우.</li>
<li><strong>다중 목표 최적화 (Multi-objective optimization):</strong> 여러 개의 손실 함수를 결합하여 동시에 최적화해야 하는 경우. (예: 예측 정확도와 모델 복잡도 사이의 균형)</li>
<li><strong>생성 모델</strong>: 생성 모델(Generative Models)은 데이터의 분포를 학습하는 것이 목표이므로, 일반적인 분류/회귀 문제와는 다른 손실 함수가 필요.</li>
</ul>
</section>
<section id="새로운-손실-함수-설계-원칙" class="level4">
<h4 class="anchored" data-anchor-id="새로운-손실-함수-설계-원칙">새로운 손실 함수 설계 원칙</h4>
<p>새로운 손실 함수를 설계할 때는 다음 원칙들을 고려해야 합니다.</p>
<ol type="1">
<li><p><strong>문제 정의와 목표:</strong> 해결하고자 하는 문제와 모델의 궁극적인 목표를 명확히 정의해야 합니다. 손실 함수는 모델이 무엇을 학습해야 하는지를 정의하는 핵심 요소입니다. (예: 단순히 분류 정확도를 높이는 것인지, 특정 클래스를 더 잘 맞추는 것인지, False Positive/False Negative 비율을 조절하는 것인지 등)</p></li>
<li><p><strong>수학적 타당성:</strong></p>
<ul>
<li><strong>미분 가능성 (Differentiability):</strong> 경사 하강법 기반 최적화를 위해서는 손실 함수가 (거의) 모든 지점에서 미분 가능해야 합니다. 미분 불가능한 점이 있더라도 subgradient(subdifferential)를 사용할 수 있어야 합니다.</li>
<li><strong>볼록성 (Convexity):</strong> 손실 함수가 볼록 함수이면, 전역 최적해(global minimum)를 찾을 수 있음을 보장합니다. 볼록하지 않은(non-convex) 함수인 경우에도, 좋은 지역 최적해(local minimum)를 찾을 수 있도록 설계해야 합니다.</li>
<li><strong>Gradient Vanishing/Exploding 방지:</strong> 너무 크거나 작은 gradient는 학습을 불안정하게 만듭니다. ReLU의 “dying ReLU” 문제나 sigmoid/tanh의 vanishing gradient 문제처럼, 특정 상황에서 gradient가 0이나 매우 작은 값이 되지 않도록 주의해야 합니다.</li>
<li><strong>Scale Invariance</strong>: 입력 데이터나 파라미터의 스케일에 따라 손실 함수의 값이 크게 변하지 않도록 설계.</li>
</ul></li>
<li><p><strong>해석 가능성 (Interpretability):</strong> 손실 함수의 의미를 직관적으로 이해할 수 있으면, 모델의 학습 과정을 분석하고 디버깅하는 데 도움이 됩니다. 각 항(term)이 어떤 역할을 하는지, 어떤 의미를 가지는지 명확해야 합니다. 하이퍼파라미터의 의미와 영향도 명확해야 합니다.</p></li>
<li><p><strong>계산 효율성 (Computational Efficiency):</strong> 손실 함수는 매 반복(iteration)마다, 그리고 모든(또는 미니배치) 데이터 포인트에 대해 계산되므로, 계산 비용이 너무 크면 학습 속도가 느려질 수 있습니다.</p></li>
</ol>
</section>
<section id="새로운-손실-함수-설계-방법론" class="level4">
<h4 class="anchored" data-anchor-id="새로운-손실-함수-설계-방법론">새로운 손실 함수 설계 방법론</h4>
<ol type="1">
<li><p><strong>기존 손실 함수 변형/결합:</strong></p>
<ul>
<li><strong>가중치 추가:</strong> 특정 데이터 포인트, 클래스, 또는 출력에 더 큰 가중치를 부여합니다 (예: Weighted Cross-Entropy, Focal Loss).</li>
<li><strong>정규화 항 추가:</strong> 모델의 복잡도를 제한하거나, 특정 속성을 장려하기 위해 정규화 항(regularization term)을 추가합니다 (예: L1 regularization, L2 regularization, Elastic Net). 출력의 smoothness를 위한 정규화 항을 추가할 수도 있습니다.</li>
<li><strong>여러 손실 함수 결합:</strong> 여러 개의 기존 손실 함수를 선형 결합(weighted sum)하거나, 다른 방식으로 결합합니다. (예: Multi-task learning)</li>
<li><strong>Soft/Hard Label Smoothing</strong>: Label Smoothing Regularization은 모델이 정답에 대해 너무 확신하는 것을 방지합니다.</li>
</ul></li>
<li><p><strong>확률적 모델링 기반 설계:</strong></p>
<ul>
<li><strong>최대 우도 추정 (MLE):</strong> 데이터의 분포를 가정하고, 그 분포의 파라미터를 추정하는 관점에서 손실 함수를 설계합니다. (예: MSE는 가우시안 분포 가정 하의 MLE, Cross-Entropy는 베르누이/다항 분포 가정 하의 MLE)</li>
<li><strong>Variational Inference:</strong> 근사 추론(variational inference) 방법을 사용하여, intractable한 사후 분포(posterior distribution)를 근사하는 손실 함수(ELBO, Evidence Lower Bound)를 설계합니다. (예: Variational Autoencoder)</li>
<li><strong>Implicit Likelihood</strong>: 생성 모델에서 likelihood를 명시적으로 계산하기 어려울 때, likelihood-free 방법(예: GAN)을 사용</li>
</ul></li>
<li><p><strong>문제 특화 손실 함수 설계:</strong></p>
<ul>
<li><strong>Ranking Loss:</strong> 순위(ranking)를 매기는 문제에 적합한 손실 함수를 설계합니다 (예: pairwise ranking loss, listwise ranking loss, margin ranking loss).</li>
<li><strong>Object Detection Loss:</strong> 객체 검출(object detection) 문제에서, bounding box regression과 class classification을 동시에 고려하는 손실 함수를 설계합니다 (예: YOLO, SSD, Faster R-CNN의 손실 함수).</li>
<li><strong>Segmentation Loss:</strong> 이미지 분할(image segmentation) 문제에서, 각 픽셀의 클래스를 예측하고, ground truth segmentation map과의 차이를 최소화하는 손실 함수를 설계합니다 (예: Dice Loss, IoU Loss, Tversky Loss).</li>
<li><strong>Generative Model Loss:</strong> GAN, VAE와 같은 생성 모델에서 사용되는 생성자와 판별자를 위한 손실 함수 (예: Wasserstein distance, Adversarial Loss).</li>
<li><strong>Metric Learning Loss</strong>: Contrastive Loss, Triplet Loss, N-pair Loss 등.</li>
<li><strong>Sequence Loss:</strong> CTC Loss, Sequence-to-sequence 모델의 Cross-Entropy 등.</li>
<li><strong>Graph Data Loss</strong>: Graph Neural Networks에서 사용되는 손실 함수 (node classification, link prediction, graph classification 등)</li>
</ul></li>
</ol>
</section>
<section id="새로운-손실-함수-설계-시-주의사항" class="level4">
<h4 class="anchored" data-anchor-id="새로운-손실-함수-설계-시-주의사항">새로운 손실 함수 설계 시 주의사항</h4>
<ul>
<li><strong>과도한 복잡성:</strong> 너무 복잡한 손실 함수는 학습을 어렵게 만들고, 과적합(overfitting)을 유발할 수 있습니다. 간단한 손실 함수에서 시작하여 점진적으로 복잡도를 늘려가는 것이 좋습니다.</li>
<li><strong>하이퍼파라미터 튜닝:</strong> 새로운 손실 함수는 추가적인 하이퍼파라미터(예: Focal Loss의 <span class="math inline">\(\gamma\)</span>, 가중치 조합 시 가중치)를 포함하는 경우가 많습니다. 이러한 하이퍼파라미터를 적절하게 튜닝하는 것이 중요하며, 교차 검증(cross-validation) 등을 통해 최적의 값을 찾아야 합니다.</li>
<li><strong>이론적/경험적 근거:</strong> 새로운 손실 함수를 제안할 때는 왜 이 손실 함수가 잘 작동하는지에 대한 이론적 근거(예: 특정 문제의 수학적 특성, MLE와의 관계) 또는 경험적 근거(예: 실험 결과)를 제시해야 합니다.</li>
</ul>
<p>새로운 손실 함수를 설계하는 것은 창의적인 과정이지만, 동시에 신중한 접근이 필요합니다. 문제의 본질을 깊이 이해하고, 수학적/통계적 원리에 기반하여 설계하며, 철저한 실험을 통해 성능을 검증하는 것이 중요합니다.</p>
</section>
</section>
</div>
</div>
<p>이 장에서는 딥러닝의 수학적 기초를 살펴보았습니다. 선형 대수, 미적분, 확률 및 통계, 정보 이론 등 다양한 분야의 개념들이 딥러닝 모델의 설계, 학습, 분석에 어떻게 활용되는지 알아보았습니다. 이러한 수학적 도구들은 복잡한 신경망 구조를 이해하고, 효율적인 학습 알고리즘을 개발하며, 모델의 성능을 평가하고 개선하는 데 필수적입니다. 또한 딥러닝 연구의 최전선에서 새로운 돌파구를 찾는 데에도 중요한 역할을 합니다.</p>
</section>
</section>
</section>
<section id="연습문제" class="level2">
<h2 class="anchored" data-anchor-id="연습문제">연습문제</h2>
<section id="선형대수학" class="level3">
<h3 class="anchored">1. 선형대수학</h3>
<section id="기본" class="level4">
<h4 class="anchored" data-anchor-id="기본">기본</h4>
<ol type="1">
<li><p>두 벡터 <span class="math inline">\(\mathbf{a} = \begin{bmatrix} 1 \\ 2 \end{bmatrix}\)</span> 와 <span class="math inline">\(\mathbf{b} = \begin{bmatrix} 3 \\ 4 \end{bmatrix}\)</span> 의 내적(dot product)을 계산하시오.</p></li>
<li><p>행렬 <span class="math inline">\(\mathbf{A} = \begin{bmatrix} 1 &amp; 2 \\ 3 &amp; 4 \end{bmatrix}\)</span> 와 벡터 <span class="math inline">\(\mathbf{b} = \begin{bmatrix} 5 \\ 6 \end{bmatrix}\)</span> 의 곱 <span class="math inline">\(\mathbf{Ab}\)</span> 를 계산하시오.</p></li>
<li><p>2x2 단위 행렬(identity matrix)을 생성하시오.</p></li>
<li><p>벡터의 L1 norm과 L2 norm의 정의를 쓰고, 벡터 <span class="math inline">\(\mathbf{v} = \begin{bmatrix} 3 \\ -4 \end{bmatrix}\)</span> 의 L1 norm과 L2 norm을 계산하시오.</p></li>
</ol>
</section>
<section id="응용" class="level4">
<h4 class="anchored" data-anchor-id="응용">응용</h4>
<ol type="1">
<li><p>행렬 <span class="math inline">\(\mathbf{A} = \begin{bmatrix} 2 &amp; 1 \\ 1 &amp; 2 \end{bmatrix}\)</span> 의 고유값(eigenvalue)과 고유벡터(eigenvector)를 구하시오.</p></li>
<li><p>주어진 행렬의 역행렬이 존재하는지 판별하고, 존재한다면 역행렬을 계산하시오. <span class="math inline">\(\mathbf{A} = \begin{bmatrix} 1 &amp; 2 \\ 3 &amp; 4 \end{bmatrix}\)</span></p></li>
<li><p>선형 변환 <span class="math inline">\(T(\mathbf{x}) = \mathbf{Ax}\)</span> 가 주어졌을 때, 기저(basis) 벡터 <span class="math inline">\(\mathbf{e_1} = \begin{bmatrix} 1 \\ 0 \end{bmatrix}\)</span> 와 <span class="math inline">\(\mathbf{e_2} = \begin{bmatrix} 0 \\ 1 \end{bmatrix}\)</span> 가 어떻게 변환되는지 설명하고, 그 결과를 시각화하시오. (단, <span class="math inline">\(\mathbf{A} = \begin{bmatrix} 2 &amp; -1 \\ 1 &amp; 1 \end{bmatrix}\)</span>)</p></li>
<li><p>다음 행렬의 랭크(rank)를 계산하시오. <span class="math inline">\(\mathbf{A} = \begin{bmatrix} 1 &amp; 2 &amp; 3 \\ 4 &amp; 5 &amp; 6 \\ 7 &amp; 8 &amp; 9 \end{bmatrix}\)</span></p></li>
</ol>
</section>
<section id="심화" class="level4">
<h4 class="anchored">심화</h4>
<ol type="1">
<li><p>특이값 분해(Singular Value Decomposition, SVD)의 정의를 쓰고, 주어진 행렬 <span class="math inline">\(\mathbf{A}\)</span> 를 SVD로 분해하시오. <span class="math inline">\(\mathbf{A} = \begin{bmatrix} 1 &amp; 2 \\ 3 &amp; 4 \\ 5 &amp; 6 \end{bmatrix}\)</span></p></li>
<li><p>주성분 분석(Principal Component Analysis, PCA)의 목적과 과정을 설명하고, 주어진 데이터셋에 대해 PCA를 수행하여 1차원으로 차원을 축소하시오.</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> np.array([[<span class="dv">1</span>, <span class="dv">2</span>], [<span class="dv">2</span>, <span class="dv">3</span>], [<span class="dv">3</span>, <span class="dv">4</span>], [<span class="dv">4</span>, <span class="dv">5</span>], [<span class="dv">5</span>, <span class="dv">6</span>]])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li><p>다음 행렬의 영공간(null space)과 열공간(column space)의 기저(basis)를 구하시오. <span class="math inline">\(\mathbf{A} = \begin{bmatrix} 1 &amp; 2 &amp; 3 \\ 4 &amp; 5 &amp; 6 \\ 7 &amp; 8 &amp; 9 \end{bmatrix}\)</span></p></li>
<li><p>QR 분해의 정의를 쓰고, 주어진 행렬 <span class="math inline">\(\mathbf{A}\)</span> 를 QR 분해하시오. (QR 분해는 수치적으로 안정적인 방법으로, 선형 방정식의 해를 구하거나, 고유값 문제를 해결하는 데 사용됩니다.) <span class="math inline">\(\mathbf{A} = \begin{bmatrix} 1 &amp; 2 \\ 3 &amp; 4 \end{bmatrix}\)</span></p></li>
</ol>
<div class="callout callout-style-default callout-note callout-titled" title="클릭하여 내용 보기(해답)">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-12-contents" aria-controls="callout-12" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
클릭하여 내용 보기(해답)
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-12" class="callout-12-contents callout-collapse collapse">
<section id="연습문제-해답" class="level2 callout-body-container callout-body">
<h2 class="anchored" data-anchor-id="연습문제-해답">연습문제 해답</h2>
<section id="선형대수학-1" class="level3">
<h3 class="anchored" data-anchor-id="선형대수학-1">1. 선형대수학</h3>
<section id="기본-1" class="level4">
<h4 class="anchored" data-anchor-id="기본-1">기본</h4>
<ol type="1">
<li><p><strong>내적 계산:</strong> <span class="math inline">\(\mathbf{a} \cdot \mathbf{b} = (1)(3) + (2)(4) = 3 + 8 = 11\)</span></p></li>
<li><p><strong>행렬-벡터 곱:</strong> <span class="math inline">\(\mathbf{Ab} = \begin{bmatrix} 1 &amp; 2 \\ 3 &amp; 4 \end{bmatrix} \begin{bmatrix} 5 \\ 6 \end{bmatrix} = \begin{bmatrix} (1)(5) + (2)(6) \\ (3)(5) + (4)(6) \end{bmatrix} = \begin{bmatrix} 17 \\ 39 \end{bmatrix}\)</span></p></li>
<li><p><strong>2x2 단위 행렬:</strong> <span class="math inline">\(\mathbf{I} = \begin{bmatrix} 1 &amp; 0 \\ 0 &amp; 1 \end{bmatrix}\)</span></p></li>
<li><p><strong>L1, L2 Norm:</strong></p>
<ul>
<li>L1 Norm (Manhattan Distance): <span class="math inline">\(||\mathbf{v}||_1 = \sum_{i} |v_i|\)</span></li>
<li>L2 Norm (Euclidean Distance): <span class="math inline">\(||\mathbf{v}||_2 = \sqrt{\sum_{i} v_i^2}\)</span></li>
</ul>
<p><span class="math inline">\(\mathbf{v} = \begin{bmatrix} 3 \\ -4 \end{bmatrix}\)</span> <span class="math inline">\(||\mathbf{v}||_1 = |3| + |-4| = 3 + 4 = 7\)</span> <span class="math inline">\(||\mathbf{v}||_2 = \sqrt{(3)^2 + (-4)^2} = \sqrt{9 + 16} = \sqrt{25} = 5\)</span></p></li>
</ol>
</section>
<section id="응용-1" class="level4">
<h4 class="anchored" data-anchor-id="응용-1">응용</h4>
<ol type="1">
<li><p><strong>고유값, 고유벡터:</strong> <span class="math inline">\(\mathbf{A} = \begin{bmatrix} 2 &amp; 1 \\ 1 &amp; 2 \end{bmatrix}\)</span></p>
<ul>
<li><p><strong>특성 방정식:</strong> <span class="math inline">\(\det(\mathbf{A} - \lambda\mathbf{I}) = 0\)</span> <span class="math inline">\((2-\lambda)^2 - (1)(1) = 0\)</span> <span class="math inline">\(\lambda^2 - 4\lambda + 3 = 0\)</span> <span class="math inline">\((\lambda - 3)(\lambda - 1) = 0\)</span> <span class="math inline">\(\lambda_1 = 3\)</span>, <span class="math inline">\(\lambda_2 = 1\)</span></p></li>
<li><p><strong>고유벡터 (λ = 3):</strong> <span class="math inline">\((\mathbf{A} - 3\mathbf{I})\mathbf{v} = 0\)</span> <span class="math inline">\(\begin{bmatrix} -1 &amp; 1 \\ 1 &amp; -1 \end{bmatrix} \begin{bmatrix} x \\ y \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \end{bmatrix}\)</span> <span class="math inline">\(x = y\)</span>, <span class="math inline">\(\mathbf{v_1} = \begin{bmatrix} 1 \\ 1 \end{bmatrix}\)</span> (또는 임의의 상수배)</p></li>
<li><p><strong>고유벡터 (λ = 1):</strong> <span class="math inline">\((\mathbf{A} - \mathbf{I})\mathbf{v} = 0\)</span> <span class="math inline">\(\begin{bmatrix} 1 &amp; 1 \\ 1 &amp; 1 \end{bmatrix} \begin{bmatrix} x \\ y \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \end{bmatrix}\)</span> <span class="math inline">\(x = -y\)</span>, <span class="math inline">\(\mathbf{v_2} = \begin{bmatrix} -1 \\ 1 \end{bmatrix}\)</span> (또는 임의의 상수배)</p></li>
</ul></li>
<li><p><strong>역행렬:</strong> <span class="math inline">\(\mathbf{A} = \begin{bmatrix} 1 &amp; 2 \\ 3 &amp; 4 \end{bmatrix}\)</span></p>
<ul>
<li><strong>존재 판별:</strong> <span class="math inline">\(\det(\mathbf{A}) = (1)(4) - (2)(3) = 4 - 6 = -2 \neq 0\)</span>. 역행렬 존재.</li>
<li><strong>역행렬 계산:</strong> <span class="math inline">\(\mathbf{A}^{-1} = \frac{1}{\det(\mathbf{A})} \begin{bmatrix} 4 &amp; -2 \\ -3 &amp; 1 \end{bmatrix} = \frac{1}{-2} \begin{bmatrix} 4 &amp; -2 \\ -3 &amp; 1 \end{bmatrix} = \begin{bmatrix} -2 &amp; 1 \\ 1.5 &amp; -0.5 \end{bmatrix}\)</span></li>
</ul></li>
<li><p><strong>선형 변환 시각화:</strong></p>
<ul>
<li><span class="math inline">\(T(\mathbf{e_1}) = \mathbf{A}\mathbf{e_1} = \begin{bmatrix} 2 &amp; -1 \\ 1 &amp; 1 \end{bmatrix} \begin{bmatrix} 1 \\ 0 \end{bmatrix} = \begin{bmatrix} 2 \\ 1 \end{bmatrix}\)</span></li>
<li><span class="math inline">\(T(\mathbf{e_2}) = \mathbf{A}\mathbf{e_2} = \begin{bmatrix} 2 &amp; -1 \\ 1 &amp; 1 \end{bmatrix} \begin{bmatrix} 0 \\ 1 \end{bmatrix} = \begin{bmatrix} -1 \\ 1 \end{bmatrix}\)</span></li>
<li>시각화: 원래의 기저 벡터 <span class="math inline">\(\mathbf{e_1}\)</span>, <span class="math inline">\(\mathbf{e_2}\)</span> 가 각각 <span class="math inline">\(\begin{bmatrix} 2 \\ 1 \end{bmatrix}\)</span>, <span class="math inline">\(\begin{bmatrix} -1 \\ 1 \end{bmatrix}\)</span> 로 변환되는 것을 좌표 평면에 그립니다.</li>
</ul></li>
<li><p><strong>랭크 계산:</strong> <span class="math inline">\(\mathbf{A} = \begin{bmatrix} 1 &amp; 2 &amp; 3 \\ 4 &amp; 5 &amp; 6 \\ 7 &amp; 8 &amp; 9 \end{bmatrix}\)</span> 행 사다리꼴 형태로 변환하면, 두 개의 행이 0이 아닌 값을 가지므로 랭크는 2입니다. (세 번째 행은 첫 번째 행과 두 번째 행의 선형 조합으로 표현 가능)</p></li>
</ol>
</section>
<section id="심화-1" class="level4">
<h4 class="anchored" data-anchor-id="심화-1">심화</h4>
<ol type="1">
<li><p><strong>SVD:</strong> <span class="math inline">\(\mathbf{A} = \mathbf{U\Sigma V^T}\)</span></p>
<ul>
<li><span class="math inline">\(\mathbf{U}\)</span>: <span class="math inline">\(\mathbf{A}\mathbf{A}^T\)</span> 의 고유벡터를 열로 갖는 직교 행렬</li>
<li><span class="math inline">\(\mathbf{\Sigma}\)</span>: 특이값(<span class="math inline">\(\mathbf{A}\mathbf{A}^T\)</span> 의 고유값의 제곱근)을 대각 원소로 갖는 대각 행렬</li>
<li><span class="math inline">\(\mathbf{V}\)</span>: <span class="math inline">\(\mathbf{A}^T\mathbf{A}\)</span> 의 고유벡터를 열로 갖는 직교 행렬</li>
</ul>
<p>(계산 과정은 생략. NumPy 등의 라이브러리를 사용하여 계산 가능: <code>U, S, V = np.linalg.svd(A)</code>)</p></li>
<li><p><strong>PCA:</strong></p>
<div class="sourceCode" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> np.array([[<span class="dv">1</span>, <span class="dv">2</span>], [<span class="dv">2</span>, <span class="dv">3</span>], [<span class="dv">3</span>, <span class="dv">4</span>], [<span class="dv">4</span>, <span class="dv">5</span>], [<span class="dv">5</span>, <span class="dv">6</span>]])</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. 데이터 중심화 (평균 빼기)</span></span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>mean <span class="op">=</span> np.mean(data, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>centered_data <span class="op">=</span> data <span class="op">-</span> mean</span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. 공분산 행렬 계산</span></span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>covariance_matrix <span class="op">=</span> np.cov(centered_data.T)</span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. 고유값, 고유벡터 계산</span></span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a>eigenvalues, eigenvectors <span class="op">=</span> np.linalg.eig(covariance_matrix)</span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a><span class="co"># 4. 주성분 선택 (가장 큰 고유값에 해당하는 고유벡터)</span></span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a><span class="co">#    고유값을 내림차순으로 정렬하고, 가장 큰 고유값에 해당하는 고유벡터 선택</span></span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a>sorted_indices <span class="op">=</span> np.argsort(eigenvalues)[::<span class="op">-</span><span class="dv">1</span>]  <span class="co"># 내림차순 정렬 인덱스</span></span>
<span id="cb19-18"><a href="#cb19-18" aria-hidden="true" tabindex="-1"></a>largest_eigenvector <span class="op">=</span> eigenvectors[:, sorted_indices[<span class="dv">0</span>]]</span>
<span id="cb19-19"><a href="#cb19-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-20"><a href="#cb19-20" aria-hidden="true" tabindex="-1"></a><span class="co"># 5. 1차원으로 투영</span></span>
<span id="cb19-21"><a href="#cb19-21" aria-hidden="true" tabindex="-1"></a>projected_data <span class="op">=</span> centered_data.dot(largest_eigenvector)</span>
<span id="cb19-22"><a href="#cb19-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-23"><a href="#cb19-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(projected_data)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li><p><strong>영공간, 열공간 기저:</strong> <span class="math inline">\(\mathbf{A} = \begin{bmatrix} 1 &amp; 2 &amp; 3 \\ 4 &amp; 5 &amp; 6 \\ 7 &amp; 8 &amp; 9 \end{bmatrix}\)</span></p>
<ul>
<li><p><strong>영공간 (Null Space):</strong> <span class="math inline">\(\mathbf{Ax} = 0\)</span> 을 만족하는 <span class="math inline">\(\mathbf{x}\)</span> 를 찾는 것. 행 사다리꼴 형태로 변환하여 해를 구하면, <span class="math inline">\(\mathbf{x} = t\begin{bmatrix} 1 \\ -2 \\ 1 \end{bmatrix}\)</span> (t는 임의의 상수) 형태. 따라서 영공간의 기저는 <span class="math inline">\(\begin{bmatrix} 1 \\ -2 \\ 1 \end{bmatrix}\)</span></p></li>
<li><p><strong>열공간 (Column Space):</strong> 행렬 <span class="math inline">\(\mathbf{A}\)</span> 의 열벡터들의 선형 조합으로 생성되는 공간. 행 사다리꼴 형태에서 pivot column에 해당하는 원래 행렬의 열벡터들이 기저가 됨. <span class="math inline">\(\begin{bmatrix} 1 \\ 4 \\ 7 \end{bmatrix}\)</span>, <span class="math inline">\(\begin{bmatrix} 2 \\ 5 \\ 8 \end{bmatrix}\)</span></p></li>
</ul></li>
<li><p><strong>QR 분해:</strong> <span class="math inline">\(\mathbf{A} = \mathbf{QR}\)</span></p>
<ul>
<li><span class="math inline">\(\mathbf{Q}\)</span>: 열벡터들이 정규직교(orthonormal)인 행렬</li>
<li><span class="math inline">\(\mathbf{R}\)</span>: 상삼각행렬(upper triangular matrix)</li>
</ul>
<p>(계산 과정은 Gram-Schmidt 직교화 과정을 사용하거나, NumPy 등의 라이브러리를 사용하여 계산: <code>Q, R = np.linalg.qr(A)</code>)</p></li>
</ol>
</section>
</section>
</section>
</div>
</div>
</section>
</section>
</section>
<section id="연습문제-1" class="level2">
<h2 class="anchored" data-anchor-id="연습문제-1">연습문제</h2>
<section id="미적분학과-최적화-1" class="level3">
<h3 class="anchored">2 미적분학과 최적화</h3>
<section id="기본-2" class="level4">
<h4 class="anchored" data-anchor-id="기본-2">기본</h4>
<ol type="1">
<li><p>함수 <span class="math inline">\(f(x) = x^3 - 6x^2 + 9x + 1\)</span> 의 도함수 <span class="math inline">\(f'(x)\)</span> 를 구하시오.</p></li>
<li><p>함수 <span class="math inline">\(f(x, y) = x^2y + 2xy^2\)</span> 의 편미분 <span class="math inline">\(\frac{\partial f}{\partial x}\)</span> 와 <span class="math inline">\(\frac{\partial f}{\partial y}\)</span> 를 구하시오.</p></li>
<li><p>함수 <span class="math inline">\(f(x) = \sin(x^2)\)</span> 의 도함수 <span class="math inline">\(f'(x)\)</span> 를 체인 룰을 사용하여 구하시오.</p></li>
</ol>
</section>
<section id="응용-2" class="level4">
<h4 class="anchored" data-anchor-id="응용-2">응용</h4>
<ol type="1">
<li><p>함수 <span class="math inline">\(f(x, y) = e^{x^2 + y^2}\)</span> 의 그래디언트 <span class="math inline">\(\nabla f\)</span> 를 구하고, 점 (1, 1) 에서의 그래디언트 값을 계산하시오.</p></li>
<li><p>함수 <span class="math inline">\(f(x) = x^4 - 4x^3 + 4x^2\)</span> 의 임계점(critical point)을 모두 찾고, 각 임계점이 극댓값, 극솟값, 또는 안장점(saddle point)인지 판별하시오.</p></li>
<li><p>다음 함수의 야코비안 행렬을 구하시오. <span class="math inline">\(f(x, y) = \begin{bmatrix} x^2 + y^2 \\ 2xy \end{bmatrix}\)</span></p></li>
</ol>
</section>
<section id="심화-2" class="level4">
<h4 class="anchored">심화</h4>
<ol type="1">
<li><p>라그랑주 승수법(Lagrange multiplier method)을 사용하여, 제약 조건 <span class="math inline">\(g(x, y) = x^2 + y^2 - 1 = 0\)</span> 하에서 함수 <span class="math inline">\(f(x, y) = xy\)</span> 의 최댓값과 최솟값을 구하시오.</p></li>
<li><p>경사 하강법(Gradient Descent)을 사용하여 함수 <span class="math inline">\(f(x) = x^4 - 4x^3 + 4x^2\)</span> 의 최솟값을 찾으시오. (초기값 <span class="math inline">\(x_0 = 3\)</span>, 학습률 <span class="math inline">\(\alpha = 0.01\)</span>, 반복 횟수 100회)</p></li>
<li><p>함수 <span class="math inline">\(f(\mathbf{x}) = \mathbf{x}^T \mathbf{A} \mathbf{x}\)</span> 의 그래디언트 <span class="math inline">\(\nabla f\)</span> 를 <span class="math inline">\(\mathbf{A}\)</span> 와 <span class="math inline">\(\mathbf{x}\)</span> 를 사용하여 나타내시오. (단, <span class="math inline">\(\mathbf{A}\)</span> 는 대칭 행렬)</p></li>
<li><p>뉴턴 방법(Newton’s method)을 사용하여 방정식 <span class="math inline">\(x^3 - 2x - 5 = 0\)</span> 의 근을 찾으시오.</p></li>
</ol>
<div class="callout callout-style-default callout-note callout-titled" title="클릭하여 내용 보기(해답)">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-13-contents" aria-controls="callout-13" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
클릭하여 내용 보기(해답)
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-13" class="callout-13-contents callout-collapse collapse">
<section id="연습문제-해답-1" class="level2 callout-body-container callout-body">
<h2 class="anchored" data-anchor-id="연습문제-해답-1">연습문제 해답</h2>
<section id="미적분학과-최적화-2" class="level3">
<h3 class="anchored" data-anchor-id="미적분학과-최적화-2">2 미적분학과 최적화</h3>
<section id="기본-3" class="level4">
<h4 class="anchored" data-anchor-id="기본-3">기본</h4>
<ol type="1">
<li><p><strong>도함수:</strong> <span class="math inline">\(f(x) = x^3 - 6x^2 + 9x + 1\)</span> <span class="math inline">\(f'(x) = 3x^2 - 12x + 9\)</span></p></li>
<li><p><strong>편미분:</strong> <span class="math inline">\(f(x, y) = x^2y + 2xy^2\)</span> <span class="math inline">\(\frac{\partial f}{\partial x} = 2xy + 2y^2\)</span> <span class="math inline">\(\frac{\partial f}{\partial y} = x^2 + 4xy\)</span></p></li>
<li><p><strong>체인 룰:</strong> <span class="math inline">\(f(x) = \sin(x^2)\)</span> <span class="math inline">\(f'(x) = \cos(x^2) \cdot (2x) = 2x\cos(x^2)\)</span></p></li>
</ol>
</section>
<section id="응용-3" class="level4">
<h4 class="anchored" data-anchor-id="응용-3">응용</h4>
<ol type="1">
<li><p><strong>그래디언트:</strong> <span class="math inline">\(f(x, y) = e^{x^2 + y^2}\)</span> <span class="math inline">\(\nabla f = \begin{bmatrix} \frac{\partial f}{\partial x} \\ \frac{\partial f}{\partial y} \end{bmatrix} = \begin{bmatrix} 2xe^{x^2 + y^2} \\ 2ye^{x^2 + y^2} \end{bmatrix}\)</span> <span class="math inline">\(\nabla f(1, 1) = \begin{bmatrix} 2e^2 \\ 2e^2 \end{bmatrix}\)</span></p></li>
<li><p><strong>임계점, 극값 판별:</strong> <span class="math inline">\(f(x) = x^4 - 4x^3 + 4x^2\)</span> <span class="math inline">\(f'(x) = 4x^3 - 12x^2 + 8x = 4x(x-1)(x-2)\)</span> 임계점: <span class="math inline">\(x = 0, 1, 2\)</span></p>
<p><span class="math inline">\(f''(x) = 12x^2 - 24x + 8\)</span></p>
<ul>
<li><span class="math inline">\(f''(0) = 8 &gt; 0\)</span>: 극솟값</li>
<li><span class="math inline">\(f''(1) = -4 &lt; 0\)</span>: 극댓값</li>
<li><span class="math inline">\(f''(2) = 8 &gt; 0\)</span>: 극솟값</li>
</ul></li>
<li><p><strong>야코비안 행렬:</strong> <span class="math inline">\(f(x, y) = \begin{bmatrix} x^2 + y^2 \\ 2xy \end{bmatrix}\)</span> <span class="math inline">\(\mathbf{J} = \begin{bmatrix} \frac{\partial f_1}{\partial x} &amp; \frac{\partial f_1}{\partial y} \\ \frac{\partial f_2}{\partial x} &amp; \frac{\partial f_2}{\partial y} \end{bmatrix} = \begin{bmatrix} 2x &amp; 2y \\ 2y &amp; 2x \end{bmatrix}\)</span></p></li>
</ol>
</section>
<section id="심화-3" class="level4">
<h4 class="anchored" data-anchor-id="심화-3">심화</h4>
<ol type="1">
<li><p><strong>라그랑주 승수법:</strong> <span class="math inline">\(L(x, y, \lambda) = xy - \lambda(x^2 + y^2 - 1)\)</span> <span class="math inline">\(\frac{\partial L}{\partial x} = y - 2\lambda x = 0\)</span> <span class="math inline">\(\frac{\partial L}{\partial y} = x - 2\lambda y = 0\)</span> <span class="math inline">\(\frac{\partial L}{\partial \lambda} = x^2 + y^2 - 1 = 0\)</span></p>
<ul>
<li><span class="math inline">\(x = \pm \frac{1}{\sqrt{2}}\)</span>, <span class="math inline">\(y = \pm \frac{1}{\sqrt{2}}\)</span>, <span class="math inline">\(\lambda = \pm \frac{1}{2}\)</span></li>
<li>최댓값: <span class="math inline">\(f(\frac{1}{\sqrt{2}}, \frac{1}{\sqrt{2}}) = f(-\frac{1}{\sqrt{2}}, -\frac{1}{\sqrt{2}}) = \frac{1}{2}\)</span></li>
<li>최솟값: <span class="math inline">\(f(\frac{1}{\sqrt{2}}, -\frac{1}{\sqrt{2}}) = f(-\frac{1}{\sqrt{2}}, \frac{1}{\sqrt{2}}) = -\frac{1}{2}\)</span></li>
</ul></li>
<li><p><strong>경사 하강법:</strong></p>
<div class="sourceCode" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> gradient_descent(f, df, x0, alpha, iterations):</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> x0</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(iterations):</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x <span class="op">-</span> alpha <span class="op">*</span> df(x)</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>f <span class="op">=</span> <span class="kw">lambda</span> x: x<span class="op">**</span><span class="dv">4</span> <span class="op">-</span> <span class="dv">4</span><span class="op">*</span>x<span class="op">**</span><span class="dv">3</span> <span class="op">+</span> <span class="dv">4</span><span class="op">*</span>x<span class="op">**</span><span class="dv">2</span></span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> <span class="kw">lambda</span> x: <span class="dv">4</span><span class="op">*</span>x<span class="op">**</span><span class="dv">3</span> <span class="op">-</span> <span class="dv">12</span><span class="op">*</span>x<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> <span class="dv">8</span><span class="op">*</span>x</span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>x_min <span class="op">=</span> gradient_descent(f, df, <span class="dv">3</span>, <span class="fl">0.01</span>, <span class="dv">100</span>)</span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(x_min) <span class="co"># 대략 2에 수렴</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li><p><strong>그래디언트 (행렬 형태):</strong> <span class="math inline">\(f(\mathbf{x}) = \mathbf{x}^T \mathbf{A} \mathbf{x}\)</span> <span class="math inline">\(\nabla f = (\mathbf{A} + \mathbf{A}^T)\mathbf{x}\)</span>. <span class="math inline">\(\mathbf{A}\)</span> 가 대칭 행렬이므로, <span class="math inline">\(\nabla f = 2\mathbf{A}\mathbf{x}\)</span></p></li>
<li><p><strong>뉴턴 방법:</strong> <span class="math inline">\(f(x) = x^3 - 2x - 5\)</span> <span class="math inline">\(f'(x) = 3x^2 - 2\)</span> <span class="math inline">\(x_{n+1} = x_n - \frac{f(x_n)}{f'(x_n)}\)</span></p>
<div class="sourceCode" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> newton_method(f, df, x0, iterations):</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>  x <span class="op">=</span> x0</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(iterations):</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>      x <span class="op">=</span> x <span class="op">-</span> f(x) <span class="op">/</span> df(x)</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> x</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>f <span class="op">=</span> <span class="kw">lambda</span> x: x<span class="op">**</span><span class="dv">3</span> <span class="op">-</span> <span class="dv">2</span><span class="op">*</span>x <span class="op">-</span> <span class="dv">5</span></span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> <span class="kw">lambda</span> x: <span class="dv">3</span><span class="op">*</span>x<span class="op">**</span><span class="dv">2</span> <span class="op">-</span> <span class="dv">2</span></span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>root <span class="op">=</span> newton_method(f, df, <span class="dv">2</span>, <span class="dv">5</span>) <span class="co"># 초기값 x0 = 2, 5회 반복</span></span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(root)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
</ol>
</section>
</section>
</section>
</div>
</div>
</section>
</section>
</section>
<section id="연습문제-2" class="level2">
<h2 class="anchored" data-anchor-id="연습문제-2">연습문제</h2>
<section id="확률과-통계-1" class="level3">
<h3 class="anchored">3 확률과 통계</h3>
<section id="기본-4" class="level4">
<h4 class="anchored" data-anchor-id="기본-4">기본</h4>
<ol type="1">
<li><p>동전을 세 번 던졌을 때, 앞면이 두 번 나올 확률을 계산하시오.</p></li>
<li><p>주사위를 던졌을 때, 짝수가 나올 확률을 계산하시오.</p></li>
<li><p>정규 분포의 확률 밀도 함수(PDF)를 쓰고, 평균과 분산의 의미를 설명하시오.</p></li>
</ol>
</section>
<section id="응용-4" class="level4">
<h4 class="anchored" data-anchor-id="응용-4">응용</h4>
<ol type="1">
<li><p>베이즈 정리(Bayes’ theorem)를 설명하고, 다음 문제에 적용하시오.</p>
<ul>
<li>어떤 질병의 발병률이 1%이고, 이 질병을 진단하는 검사의 정확도(민감도와 특이도)가 99%라고 할 때, 검사 결과가 양성으로 나왔을 때 실제로 질병에 걸렸을 확률은 얼마인가?</li>
</ul></li>
<li><p>최대 우도 추정(Maximum Likelihood Estimation, MLE)의 개념을 설명하고, 동전을 5번 던져서 앞면이 3번 나왔을 때, 동전의 앞면이 나올 확률에 대한 MLE를 구하시오.</p></li>
<li><p>기댓값(expectation)의 정의를 쓰고, 이산 확률 변수와 연속 확률 변수에 대한 기댓값 계산 공식을 각각 쓰시오.</p></li>
</ol>
</section>
<section id="심화-4" class="level4">
<h4 class="anchored">심화</h4>
<ol type="1">
<li><p>엔트로피(entropy)의 정의를 쓰고, 다음 확률 분포의 엔트로피를 계산하시오.</p>
<ul>
<li>P(X=1) = 0.5, P(X=2) = 0.25, P(X=3) = 0.25</li>
</ul></li>
<li><p>두 확률 변수 X와 Y의 결합 확률 분포(joint probability distribution)가 다음과 같을 때, 상호 정보량(mutual information) I(X;Y)를 계산하시오.</p>
<pre><code>P(X=0, Y=0) = 0.1, P(X=0, Y=1) = 0.2
P(X=1, Y=0) = 0.3, P(X=1, Y=1) = 0.4</code></pre></li>
<li><p>두 확률 분포 P와 Q가 다음과 같을 때, KL 발산(Kullback-Leibler divergence) <span class="math inline">\(D_{KL}(P||Q)\)</span>를 계산하시오.</p>
<ul>
<li>P(X=1) = 0.6, P(X=2) = 0.4</li>
<li>Q(X=1) = 0.8, Q(X=2) = 0.2</li>
</ul></li>
<li><p>푸아송 분포(Poisson distribution)의 확률 질량 함수(PMF)를 쓰고, 어떤 경우에 사용되는지 예를 들어 설명하시오.</p></li>
</ol>
<div class="callout callout-style-default callout-note callout-titled" title="클릭하여 내용 보기(해답)">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-14-contents" aria-controls="callout-14" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
클릭하여 내용 보기(해답)
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-14" class="callout-14-contents callout-collapse collapse">
<section id="연습문제-해답-2" class="level2 callout-body-container callout-body">
<h2 class="anchored" data-anchor-id="연습문제-해답-2">연습문제 해답</h2>
<section id="확률과-통계-2" class="level3">
<h3 class="anchored" data-anchor-id="확률과-통계-2">3 확률과 통계</h3>
<section id="기본-5" class="level4">
<h4 class="anchored" data-anchor-id="기본-5">기본</h4>
<ol type="1">
<li><p><strong>동전 던지기:</strong> 확률 = (3번 중 2번 앞면이 나오는 경우의 수) * (앞면 확률)^2 * (뒷면 확률)^1 = 3C2 * (1/2)^2 * (1/2)^1 = 3 * (1/4) * (1/2) = 3/8</p></li>
<li><p><strong>주사위 던지기:</strong> 확률 = (짝수가 나오는 경우의 수) / (전체 경우의 수) = 3 / 6 = 1/2</p></li>
<li><p><strong>정규 분포:</strong> <span class="math inline">\(f(x) = \frac{1}{\sigma\sqrt{2\pi}} e^{-\frac{(x-\mu)^2}{2\sigma^2}}\)</span></p>
<ul>
<li><span class="math inline">\(\mu\)</span>: 평균 (분포의 중심)</li>
<li><span class="math inline">\(\sigma\)</span>: 표준편차 (분포의 퍼짐 정도)</li>
</ul></li>
</ol>
</section>
<section id="응용-5" class="level4">
<h4 class="anchored" data-anchor-id="응용-5">응용</h4>
<ol type="1">
<li><p><strong>베이즈 정리:</strong> <span class="math inline">\(P(A|B) = \frac{P(B|A)P(A)}{P(B)}\)</span></p>
<ul>
<li><span class="math inline">\(P(A)\)</span>: 질병에 걸렸을 확률 (사전 확률) = 0.01</li>
<li><span class="math inline">\(P(B|A)\)</span>: 질병에 걸렸을 때 검사 결과가 양성일 확률 (민감도) = 0.99</li>
<li><span class="math inline">\(P(B|\neg A)\)</span>: 질병에 걸리지 않았을 때 검사 결과가 양성일 확률 (1 - 특이도) = 0.01 (특이도가 0.99라고 가정)</li>
<li><span class="math inline">\(P(B)\)</span>: 검사 결과가 양성일 확률 = <span class="math inline">\(P(B|A)P(A) + P(B|\neg A)P(\neg A) = (0.99)(0.01) + (0.01)(0.99) = 0.0198\)</span></li>
</ul>
<p><span class="math inline">\(P(A|B) = \frac{(0.99)(0.01)}{0.0198} = 0.5\)</span> (50%)</p></li>
<li><p><strong>최대 우도 추정 (MLE):</strong></p></li>
</ol>
<ul>
<li>우도 함수: <span class="math inline">\(L(p) = p^3 (1-p)^2\)</span> (p는 동전 앞면이 나올 확률)</li>
<li>로그 우도 함수: <span class="math inline">\(\log L(p) = 3\log p + 2\log(1-p)\)</span></li>
<li><span class="math inline">\(\frac{d}{dp} \log L(p) = \frac{3}{p} - \frac{2}{1-p} = 0\)</span></li>
<li><span class="math inline">\(3(1-p) - 2p = 0\)</span></li>
<li><span class="math inline">\(3 - 5p = 0\)</span></li>
<li><span class="math inline">\(p = \frac{3}{5} = 0.6\)</span></li>
</ul>
<ol start="3" type="1">
<li><strong>기댓값:</strong>
<ul>
<li><strong>정의:</strong> 확률 변수의 가능한 값들에 대한 가중 평균 (확률로 가중)</li>
<li><strong>이산 확률 변수:</strong> <span class="math inline">\(E[X] = \sum_{i} x_i P(X = x_i)\)</span></li>
<li><strong>연속 확률 변수:</strong> <span class="math inline">\(E[X] = \int_{-\infty}^{\infty} x f(x) dx\)</span> (f(x)는 확률 밀도 함수)</li>
</ul></li>
</ol>
</section>
<section id="심화-5" class="level4">
<h4 class="anchored" data-anchor-id="심화-5">심화</h4>
<ol type="1">
<li><p><strong>엔트로피:</strong></p>
<ul>
<li><strong>정의:</strong> 확률 분포의 불확실성, 무질서도, 또는 정보량. <span class="math inline">\(H(P) = -\sum_{x} P(x) \log P(x)\)</span> (로그의 밑은 보통 2 또는 자연로그 e)</li>
</ul>
<p><span class="math inline">\(H(P) = -(0.5 \log 0.5 + 0.25 \log 0.25 + 0.25 \log 0.25)\)</span> (밑이 2인 로그를 사용하면) <span class="math inline">\(H(P) \approx 1.5\)</span> bits</p></li>
<li><p><strong>상호 정보량:</strong> <span class="math inline">\(I(X;Y) = \sum_{x}\sum_{y} P(x,y) \log \frac{P(x,y)}{P(x)P(y)}\)</span></p>
<ul>
<li><span class="math inline">\(P(X=0) = 0.1 + 0.2 = 0.3\)</span></li>
<li><span class="math inline">\(P(X=1) = 0.3 + 0.4 = 0.7\)</span></li>
<li><span class="math inline">\(P(Y=0) = 0.1 + 0.3 = 0.4\)</span></li>
<li><span class="math inline">\(P(Y=1) = 0.2 + 0.4 = 0.6\)</span></li>
</ul>
<p><span class="math inline">\(I(X;Y) = (0.1)\log\frac{0.1}{(0.3)(0.4)} + (0.2)\log\frac{0.2}{(0.3)(0.6)} + (0.3)\log\frac{0.3}{(0.7)(0.4)} + (0.4)\log\frac{0.4}{(0.7)(0.6)}\)</span> <span class="math inline">\(I(X;Y) \approx 0.0867\)</span> (밑이 2인 로그 사용)</p></li>
<li><p><strong>KL 발산:</strong> <span class="math inline">\(D_{KL}(P||Q) = \sum_{x} P(x) \log \frac{P(x)}{Q(x)}\)</span></p>
<p><span class="math inline">\(D_{KL}(P||Q) = 0.6 \log \frac{0.6}{0.8} + 0.4 \log \frac{0.4}{0.2} \approx 0.083\)</span></p></li>
<li><p><strong>푸아송 분포:</strong></p>
<ul>
<li><strong>확률 질량 함수 (PMF):</strong> <span class="math inline">\(P(X=k) = \frac{\lambda^k e^{-\lambda}}{k!}\)</span> (<span class="math inline">\(k\)</span>는 발생 횟수, <span class="math inline">\(\lambda\)</span>는 단위 시간/공간 동안의 평균 발생 횟수)</li>
<li><strong>용도 예시</strong>:
<ul>
<li>특정 시간 동안 콜센터에 걸려오는 전화 횟수</li>
<li>특정 지역에서 발생하는 교통 사고 건수</li>
<li>어떤 책에서 발견되는 오타의 수</li>
<li>어떤 웹사이트에 접속하는 방문자 수</li>
<li>방사성 붕괴, 유전자 돌연변이</li>
</ul></li>
</ul></li>
</ol>
</section>
</section>
</section>
</div>
</div>
</section>
</section>
</section>
<section id="참고-문헌" class="level2">
<h2 class="anchored" data-anchor-id="참고-문헌">참고 문헌</h2>
<section id="필수-참고-자료" class="level4">
<h4 class="anchored" data-anchor-id="필수-참고-자료">필수 참고 자료</h4>
<ol type="1">
<li><strong>Linear Algebra and Its Applications (Gilbert Strang, 4th Edition)</strong>:
<ul>
<li>선형대수학의 기본 개념과 응용을 다루는 교과서입니다. 딥러닝에 필요한 핵심 내용을 명확하게 설명합니다.</li>
<li><a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/">Gilbert Strang’s Linear Algebra Course (MIT OCW)</a></li>
</ul></li>
<li><strong>Calculus (James Stewart, 8th Edition)</strong>:
<ul>
<li>미적분학의 기본 원리를 상세하게 설명하는 교과서입니다. 딥러닝의 최적화 알고리즘을 이해하는 데 필요한 배경 지식을 제공합니다.</li>
</ul></li>
<li><strong>Probability and Statistics for Engineering and the Sciences (Jay L. Devore, 9th Edition)</strong>:
<ul>
<li>확률과 통계의 기본 개념을 공학적 응용과 함께 설명하는 교과서입니다. 딥러닝의 확률적 모델링과 불확실성 추론을 이해하는 데 도움이 됩니다.</li>
</ul></li>
<li><strong>Pattern Recognition and Machine Learning (Christopher Bishop)</strong>:
<ul>
<li>패턴 인식과 머신러닝의 고전적인 교과서입니다. 확률적 모델링, 베이즈 추론, 정보 이론 등 딥러닝의 이론적 배경을 심도 있게 다룹니다.</li>
</ul></li>
<li><strong>The Elements of Statistical Learning (Trevor Hastie, Robert Tibshirani, Jerome Friedman)</strong>:
<ul>
<li>통계적 학습 이론의 핵심 개념을 명확하게 설명하는 교과서입니다. 딥러닝 모델의 일반화 성능과 과적합 문제를 이해하는 데 유용합니다.</li>
<li><a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://hastie.su.domains/ElemStatLearn/">The Elements of Statistical Learning (Free PDF)</a></li>
</ul></li>
<li><strong>Deep Learning (Ian Goodfellow, Yoshua Bengio, Aaron Courville)</strong>:
<ul>
<li>딥러닝의 기본 개념과 최신 기술을 포괄적으로 다루는 교과서입니다. 딥러닝에 필요한 수학적 기초를 간략하게 소개합니다.</li>
<li>Deep Learning Book (Free PDF)](https://www.deeplearningbook.org/)</li>
</ul></li>
<li><strong>Understanding Machine Learning: From Theory to Algorithms (Shai Shalev-Shwartz, Shai Ben-David)</strong>:
<ul>
<li>머신러닝의 이론적 기초를 탄탄하게 다지는 교과서입니다. PAC 학습 이론, VC 차원, 편향-분산 트레이드오프 등 딥러닝 모델의 일반화 성능을 이해하는 데 중요한 개념들을 설명합니다.</li>
</ul></li>
<li><strong>Information Theory, Inference, and Learning Algorithms (David J.C. MacKay)</strong>:
<ul>
<li>정보 이론과 베이즈 추론을 중심으로 머신러닝의 원리를 설명하는 교과서입니다. 딥러닝의 확률적 해석과 생성 모델을 이해하는 데 도움이 됩니다.</li>
<li><a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=http://www.inference.org.uk/itprnn/book.pdf">Information Theory, Inference, and Learning Algorithms (Free PDF)</a></li>
</ul></li>
<li><strong>Mathematics for Machine Learning (Marc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong)</strong>
<ul>
<li>머신 러닝에 필요한 수학적 배경지식을 폭넓게 다룹니다.</li>
</ul></li>
<li><strong>Matrix Computations (Gene H. Golub, Charles F. Van Loan, 4th Edition)</strong>:
<ul>
<li>행렬 연산과 관련된 수치적 방법들을 심도 있게 다루는 교과서입니다. 딥러닝의 최적화 알고리즘 구현과 성능 개선에 필요한 지식을 제공합니다.</li>
</ul></li>
</ol>


</section>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>