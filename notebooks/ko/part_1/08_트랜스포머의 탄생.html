<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>quarto-input484c20aa96a84fd3 – Deep Learning DNA: Surviving Architectures and Essential Principles</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap-f507c7d0488cb7630e20aad62ad8c2aa.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script>window.MathJax = {loader: {load: ['[tex]/boldsymbol']},tex: {packages: {'[+]': ['boldsymbol']}}};</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../../notebooks/ko/part_1/01_딥러닝의 시작.html">part_1</a></li><li class="breadcrumb-item"><a href="../../../notebooks/ko/part_1/08_트랜스포머의 탄생.html">8. 트랜스포머의 탄생</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../../../">한국어</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Language</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index_de.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Deutsch</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index_en.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">English</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index_es.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Español</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">한국어</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index_zh.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">中文</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/ko/part_1/00_서론.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">서론</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">part_1</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/ko/part_1/01_딥러닝의 시작.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">1. 딥러닝의 시작</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/ko/part_1/02_딥러닝의 수학.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">2. 딥러닝의 수학</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/ko/part_1/03_딥러닝프레임워크.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">3. 딥러닝프레임워크</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/ko/part_1/04_활성화함수.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">4. 활성화함수</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/ko/part_1/05_최적화와 시각화.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">5. 최적화와 시각화</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/ko/part_1/06_과적합과 해결 기법의 발전.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">6. 과적합과 해결 기법의 발전</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/ko/part_1/07_합성곱 신경망의 진화.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">7. 합성곱 신경망의 진화</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/ko/part_1/08_트랜스포머의 탄생.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">8. 트랜스포머의 탄생</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/ko/part_1/09_트랜스포머의 진화.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">9. 트랜스포머의 진화</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/ko/part_1/10_멀티모달 딥러닝: 다중 감각 융합의 시작.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">10. 멀티모달 딥러닝: 다중 감각 융합의 시작</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/ko/part_1/11_멀티모달 딥러닝: 한계를 넘어선 지능.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">11. 멀티모달 딥러닝: 한계를 넘어선 지능</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">딥러닝의 최전선</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/ko/딥러닝의 최전선/01_SLM: 작지만 강력한 언어모델.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">1. SLM: 작지만 강력한 언어모델</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/ko/딥러닝의 최전선/02_자율주행.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">2. 자율주행</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#장-트랜스포머의-탄생" id="toc-장-트랜스포머의-탄생" class="nav-link active" data-scroll-target="#장-트랜스포머의-탄생">8장 트랜스포머의 탄생</a>
  <ul class="collapse">
  <li><a href="#트랜스포머---시퀀스-처리의-혁명" id="toc-트랜스포머---시퀀스-처리의-혁명" class="nav-link" data-scroll-target="#트랜스포머---시퀀스-처리의-혁명">8.1 트랜스포머 - 시퀀스 처리의 혁명</a></li>
  <li><a href="#트랜스포머의-진화-과정" id="toc-트랜스포머의-진화-과정" class="nav-link" data-scroll-target="#트랜스포머의-진화-과정">8.2 트랜스포머의 진화 과정</a>
  <ul class="collapse">
  <li><a href="#rnn의-한계와-어텐션의-탄생" id="toc-rnn의-한계와-어텐션의-탄생" class="nav-link" data-scroll-target="#rnn의-한계와-어텐션의-탄생">8.2.1 RNN의 한계와 어텐션의 탄생</a></li>
  <li><a href="#어텐션의-기본-개념" id="toc-어텐션의-기본-개념" class="nav-link" data-scroll-target="#어텐션의-기본-개념">8.2.2 어텐션의 기본 개념</a></li>
  <li><a href="#셀프-어텐션으로의-진화" id="toc-셀프-어텐션으로의-진화" class="nav-link" data-scroll-target="#셀프-어텐션으로의-진화">8.2.3 셀프 어텐션으로의 진화</a></li>
  <li><a href="#멀티헤드-어텐션과-병렬-처리" id="toc-멀티헤드-어텐션과-병렬-처리" class="nav-link" data-scroll-target="#멀티헤드-어텐션과-병렬-처리">8.2.4 멀티헤드 어텐션과 병렬 처리</a></li>
  <li><a href="#멀티헤드-어텐션-multi-head-attention-상세-분석" id="toc-멀티헤드-어텐션-multi-head-attention-상세-분석" class="nav-link" data-scroll-target="#멀티헤드-어텐션-multi-head-attention-상세-분석">멀티헤드 어텐션 (Multi-Head Attention) 상세 분석</a></li>
  <li><a href="#병렬-학습을-위한-마스킹-전략" id="toc-병렬-학습을-위한-마스킹-전략" class="nav-link" data-scroll-target="#병렬-학습을-위한-마스킹-전략">8.2.5 병렬 학습을 위한 마스킹 전략</a></li>
  <li><a href="#헤드-의미의-변천-머리에서-뇌로" id="toc-헤드-의미의-변천-머리에서-뇌로" class="nav-link" data-scroll-target="#헤드-의미의-변천-머리에서-뇌로">8.2.6 헤드 의미의 변천: “머리”에서 “뇌”로</a></li>
  </ul></li>
  <li><a href="#위치-정보의-처리" id="toc-위치-정보의-처리" class="nav-link" data-scroll-target="#위치-정보의-처리">8.3 위치 정보의 처리</a>
  <ul class="collapse">
  <li><a href="#순차-정보의-중요성" id="toc-순차-정보의-중요성" class="nav-link" data-scroll-target="#순차-정보의-중요성">8.3.1 순차 정보의 중요성</a></li>
  <li><a href="#포지셔널-인코딩의-설계" id="toc-포지셔널-인코딩의-설계" class="nav-link" data-scroll-target="#포지셔널-인코딩의-설계">8.3.2 포지셔널 인코딩의 설계</a></li>
  </ul></li>
  <li><a href="#트랜스포머의-전체-아키텍처" id="toc-트랜스포머의-전체-아키텍처" class="nav-link" data-scroll-target="#트랜스포머의-전체-아키텍처">8.4 트랜스포머의 전체 아키텍처</a>
  <ul class="collapse">
  <li><a href="#기본-구성-요소의-통합" id="toc-기본-구성-요소의-통합" class="nav-link" data-scroll-target="#기본-구성-요소의-통합">8.4.1 기본 구성 요소의 통합</a></li>
  <li><a href="#인코더의-구성" id="toc-인코더의-구성" class="nav-link" data-scroll-target="#인코더의-구성">8.4.2 인코더의 구성</a></li>
  <li><a href="#디코더의-구성" id="toc-디코더의-구성" class="nav-link" data-scroll-target="#디코더의-구성">8.4.3 디코더의 구성</a></li>
  <li><a href="#전체-구조의-설명" id="toc-전체-구조의-설명" class="nav-link" data-scroll-target="#전체-구조의-설명">8.4.4 전체 구조의 설명</a></li>
  </ul></li>
  <li><a href="#트랜스포머-예제" id="toc-트랜스포머-예제" class="nav-link" data-scroll-target="#트랜스포머-예제">8.5 트랜스포머 예제</a>
  <ul class="collapse">
  <li><a href="#단순-복사-태스크" id="toc-단순-복사-태스크" class="nav-link" data-scroll-target="#단순-복사-태스크">8.5.1 단순 복사 태스크</a></li>
  <li><a href="#자릿수-덧셈-문제" id="toc-자릿수-덧셈-문제" class="nav-link" data-scroll-target="#자릿수-덧셈-문제">8.5.2 자릿수 덧셈 문제</a></li>
  <li><a href="#파서-태스크" id="toc-파서-태스크" class="nav-link" data-scroll-target="#파서-태스크">8.5.3 파서 태스크</a></li>
  </ul></li>
  <li><a href="#맺음말" id="toc-맺음말" class="nav-link" data-scroll-target="#맺음말">맺음말</a></li>
  <li><a href="#연습-문제" id="toc-연습-문제" class="nav-link" data-scroll-target="#연습-문제">연습 문제</a>
  <ul class="collapse">
  <li><a href="#기본-문제" id="toc-기본-문제" class="nav-link" data-scroll-target="#기본-문제">기본 문제</a></li>
  <li><a href="#응용-문제" id="toc-응용-문제" class="nav-link" data-scroll-target="#응용-문제">응용 문제</a></li>
  <li><a href="#심화-문제" id="toc-심화-문제" class="nav-link" data-scroll-target="#심화-문제">심화 문제</a></li>
  </ul></li>
  <li><a href="#참고-자료" id="toc-참고-자료" class="nav-link" data-scroll-target="#참고-자료">참고 자료</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../../notebooks/ko/part_1/01_딥러닝의 시작.html">part_1</a></li><li class="breadcrumb-item"><a href="../../../notebooks/ko/part_1/08_트랜스포머의 탄생.html">8. 트랜스포머의 탄생</a></li></ol></nav></header>




<p><a href="https://colab.research.google.com/github/Quantum-Intelligence-Frontier/dldna/blob/main/notebooks/ko/part_1/08_트랜스포머의 탄생.ipynb" target="_parent"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"> </a></p>
<section id="장-트랜스포머의-탄생" class="level1">
<h1>8장 트랜스포머의 탄생</h1>
<blockquote class="blockquote">
<p>“Attention is all you need.” - Ashish Vaswani et al., NeurIPS 2017.</p>
</blockquote>
<p>자연어 처리의 역사에서 2017년은 특별합니다. 구글이 “Attention is All You Need” 논문에서 트랜스포머(Transformer)를 발표했기 때문입니다. 이는 2012년 AlexNet이 컴퓨터 비전에 가져온 혁명과 비견됩니다. 트랜스포머의 등장으로 자연어처리(NLP)는 새로운 시대로 접어들었습니다. 이후 트랜스포머에 기반한 BERT, GPT 같은 강력한 언어 모델들이 등장하면서 인공지능의 새로운 역사를 열게 됩니다.</p>
<p><strong>주의할 사항</strong></p>
<p>8장은 구글 연구팀이 트랜스포머를 개발하는 과정을 <em>드라마처럼</em> 재구성했습니다. 원논문, 연구 블로그, 학술 발표 자료 등 다양한 자료를 바탕으로, 연구자들이 <strong>마주했을 법한 고민과 문제 해결 과정</strong>을 생생하게 묘사하고자 했습니다. 이 과정에서 일부 내용은 <strong>합리적인 추론과 상상력</strong>을 바탕으로 재구성되었음을 밝힙니다.</p>
<section id="트랜스포머---시퀀스-처리의-혁명" class="level2">
<h2 class="anchored" data-anchor-id="트랜스포머---시퀀스-처리의-혁명">8.1 트랜스포머 - 시퀀스 처리의 혁명</h2>
<blockquote class="blockquote">
<p><strong>도전과제:</strong> 기존 순환 신경망(RNN) 기반 모델의 근본적인 한계를 어떻게 극복할 것인가?</p>
<p><strong>연구자의 고뇌:</strong> 당시 자연어 처리 분야는 RNN, LSTM, GRU와 같은 순환 신경망 기반 모델이 주류를 이루고 있었습니다. 그러나 이러한 모델들은 입력 시퀀스를 순차적으로 처리해야 했기 때문에 병렬화가 불가능했고, 긴 문장을 처리할 때 장기 의존성 문제가 발생했습니다. 연구자들은 이러한 근본적인 한계를 극복하고, 더 빠르고 효율적이면서도 긴 문맥을 잘 이해할 수 있는 새로운 아키텍처를 개발해야 했습니다.</p>
</blockquote>
<p>자연어 처리는 오랫동안 순차적 처리의 한계에 갇혀 있었습니다. 순차적 처리란 문장을 단어나 토큰 단위로 하나씩 순서대로 처리하는 것을 말합니다. 사람이 글을 한 단어씩 읽어나가는 것처럼 RNN과 LSTM도 입력을 순서대로 처리해야 했습니다. 이러한 순차적 처리는 두 가지 심각한 문제가 있었습니다. 1.GPU와 같은 병렬 처리 하드웨어를 효율적으로 활용할 수 없었고, 2.긴 문장을 처리할 때 앞부분의 정보(단어)가 뒷부분에 제대로 전달되지 않는 “장거리 의존성 문제(long-range dependeny problem)”, 다시 말해서 문장내에서 관계가 있는 요소(단어 등)가 멀리 떨어지면 제대로 처리하지 못하는 문제가 있었습니다.</p>
<p>2014년 등장한 어텐션 메커니즘은 이러한 문제를 부분적으로 해결했습니다. 기존 RNN은 디코더가 출력을 생성할 때 인코더의 마지막 은닉 상태만을 참조했습니다. 어텐션은 디코더가 인코더의 모든 중간 은닉 상태들을 직접 참조할 수 있게 했습니다. 하지만 여전히 근본적인 한계가 있었습니다. RNN의 구조 자체가 순차 처리 기반이어서 입력에 대해 여전히 한 단어씩 차례대로 처리할 수 밖에 없었습니다. 따라서 GPU를 이용한 병렬 처리가 불가능했고 결과적으로 긴 시퀀스를 처리할 때 시간이 오래 걸렸습니다.</p>
<p>2017년 구글 연구팀은 기계 번역의 성능을 획기적으로 개선하기 위해 트랜스포머를 개발했습니다. 트랜스포머는 이러한 한계를 근본적으로 해결했습니다. RNN을 완전히 제거하고 셀프 어텐션(self-attention)만으로 시퀀스를 처리하는 방식을 도입한 것입니다.</p>
<p>트랜스포머는 다음 세 가지 핵심적인 장점을 가집니다. 1. 병렬 처리: 시퀀스의 모든 위치를 동시에 처리할 수 있어 GPU를 최대한 활용. 2. 전역적 의존성: 모든 토큰이 다른 모든 토큰과 직접 관계 강도를 정의 가능. 3. 위치 정보의 유연한 처리: 포지셔널 인코딩을 통해 순서 정보를 효과적으로 표현하면서도 다양한 길이의 시퀀스에 유연하게 대응.</p>
<p>트랜스포머는 곧 BERT, GPT와 같은 강력한 언어 모델의 기반이 되었고 비전 트랜스포머(Vision Transformer)처럼 다른 분야로도 확장되었습니다. 트랜스포머는 단순한 새로운 아키텍처가 아닌 딥러닝의 정보 처리 방식에 근본적인 재고찰을 가져왔습니다. 특히 컴퓨터 비전 분야에서도 ViT(Vision Transformer)의 성공으로 이어져 CNN을 위협하는 강력한 경쟁자가 되었습니다.</p>
</section>
<section id="트랜스포머의-진화-과정" class="level2">
<h2 class="anchored" data-anchor-id="트랜스포머의-진화-과정">8.2 트랜스포머의 진화 과정</h2>
<p>2017년 초 구글 연구팀은 기계 번역 분야에서 난관에 봉착했습니다. 당시 주류였던 RNN 기반 시퀀스-투-시퀀스(seq-to-seq) 모델은 긴 문장을 처리할 때 성능이 크게 떨어지는 고질적인 문제가 있었습니다. 연구팀은 RNN 구조를 개선하려 다각도로 노력했지만, 이는 임시방편일 뿐 근본적인 해결책이 되지는 못했습니다. 그러던 중, 한 연구원이 2014년에 발표된 어텐션 메커니즘(Bahdanau et al., 2014)에 주목했습니다. “어텐션이 장거리 의존성 문제를 완화했다면, RNN 없이 어텐션 만으로도 시퀀스를 처리할 수 있지 않을까?”</p>
<p>많은 사람들이 어텐션 메커니즘을 처음 접할 때 Q, K, V 개념에서 혼란을 겪습니다. 사실 어텐션의 초기 형태는 2014년 Bahdanau의 논문에 등장한 “alignment score”라는 개념이었습니다. 이는 디코더가 출력 단어를 생성할 때 인코더의 어느 부분에 집중해야 하는지를 나타내는 점수였고, 본질적으로는 <strong>두 벡터 간의 관련성</strong>을 나타내는 수치였습니다.</p>
<p>아마도 연구팀은 “단어들 간의 관계를 어떻게 수치화할 수 있을까?”라는 실용적인 질문에서 출발했을 것입니다. 그들은 벡터 간의 유사도를 계산하고, 이를 가중치로 사용하여 문맥 정보를 종합하는 비교적 단순한 아이디어에서 시작했습니다. 실제로 구글 연구팀의 초기 설계 문서(“Transformers: Iterative Self-Attention and Processing for Various Tasks”)에는 Q, K, V라는 용어 대신, “alignment score”와 유사한 방식으로 단어 간 관계를 나타내는 방식이 사용되었습니다.</p>
<p>이제부터 어텐션 메커니즘을 이해하기 위해, 구글 연구자들이 문제를 해결했던 과정을 따라가 보겠습니다. 벡터 간 유사도 계산이라는 기본 아이디어에서 시작하여, 어떻게 이들이 최종적으로 트랜스포머 아키텍처를 완성했는지 그 과정을 단계별로 설명하겠습니다.</p>
<section id="rnn의-한계와-어텐션의-탄생" class="level3">
<h3 class="anchored" data-anchor-id="rnn의-한계와-어텐션의-탄생">8.2.1 RNN의 한계와 어텐션의 탄생</h3>
<p>연구팀은 먼저 RNN의 한계를 명확히 파악하고자 했습니다. 실험을 통해 문장 길이가 증가할수록, 특히 50단어 이상이 되면 BLEU 점수가 급격히 저하되는 현상을 확인했습니다. 더 큰 문제는 GPU를 사용해도 RNN의 순차 처리 방식 때문에 근본적인 속도 개선이 어렵다는 점이었습니다. 이러한 한계를 극복하기 위해, 연구팀은 Bahdanau et al.&nbsp;(2014)이 제안한 어텐션 메커니즘을 심층 분석했습니다. 어텐션은 디코더가 인코더의 모든 상태를 참조하여 장거리 의존성 문제를 완화하는 효과가 있었습니다. 다음은 기본적인 어텐션 메커니즘의 구현입니다.</p>
<div id="cell-3" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip install dldna[colab] <span class="co"># in Colab</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="co"># !pip install dldna[all] # in your local</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>load_ext autoreload</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>autoreload <span class="dv">2</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="cell-4" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Example word vectors (3-dimensional)</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>word_vectors <span class="op">=</span> {</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">'time'</span>: np.array([<span class="fl">0.2</span>, <span class="fl">0.8</span>, <span class="fl">0.3</span>]),   <span class="co"># In reality, these would be hundreds of dimensions</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">'flies'</span>: np.array([<span class="fl">0.7</span>, <span class="fl">0.2</span>, <span class="fl">0.9</span>]),</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>    <span class="st">'like'</span>: np.array([<span class="fl">0.3</span>, <span class="fl">0.5</span>, <span class="fl">0.2</span>]),</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>    <span class="st">'an'</span>: np.array([<span class="fl">0.1</span>, <span class="fl">0.3</span>, <span class="fl">0.4</span>]),</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>    <span class="st">'arrow'</span>: np.array([<span class="fl">0.8</span>, <span class="fl">0.1</span>, <span class="fl">0.6</span>])</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> calculate_similarity_matrix(word_vectors):</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Calculates the similarity matrix between word vectors."""</span></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> np.vstack(<span class="bu">list</span>(word_vectors.values()))</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.dot(X, X.T)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>The autoreload extension is already loaded. To reload it, use:
  %reload_ext autoreload</code></pre>
</div>
</div>
</section>
<section id="어텐션의-기본-개념" class="level3">
<h3 class="anchored" data-anchor-id="어텐션의-기본-개념">8.2.2 어텐션의 기본 개념</h3>
<p>이 절에서 설명하는 내용은 “Transformers: Iterative Self-Attention and Processing for Various Tasks”라는 초기 설계 문서에서 소개된 개념입니다. 기본 어텐션 개념을 설명하기 위한 아래 코드를 단계별로 살펴보겠습니다. 먼저 유사도 행렬까지만 보겠습니다(소스 코드의 1, 2단계). 단어는 통상 수백 차원을 가집니다. 여기서는 예시적으로 3차원 벡터로 표현했습니다. 이를 행렬로 만들면 단순히 각 열이 단어 벡터인 열벡터로 구성된 행렬이 됩니다. 이 행렬을 전치(transpose)를 하면 단어 벡터가 행 벡터가 되는 매트릭스가 됩니다. 이 두 매트릭스를 연산을 하면 각 요소 (i, j)는 i번째 단어, j번째 단어 간의 벡터 내적값이 되고, 따라서 두 단어 간의 거리(유사도)가 됩니다.</p>
<div id="cell-6" class="cell" data-execution_count="8">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> visualize_similarity_matrix(words, similarity_matrix):</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Visualizes the similarity matrix in ASCII art format."""</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>    max_word_len <span class="op">=</span> <span class="bu">max</span>(<span class="bu">len</span>(word) <span class="cf">for</span> word <span class="kw">in</span> words)</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>    col_width <span class="op">=</span> max_word_len <span class="op">+</span> <span class="dv">4</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>    header <span class="op">=</span> <span class="st">" "</span> <span class="op">*</span> (col_width) <span class="op">+</span> <span class="st">""</span>.join(<span class="ss">f"</span><span class="sc">{</span>word<span class="sc">:</span><span class="op">&gt;</span>{col_width}<span class="sc">}</span><span class="ss">"</span> <span class="cf">for</span> word <span class="kw">in</span> words)</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(header)</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, word <span class="kw">in</span> <span class="bu">enumerate</span>(words):</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>        row_str <span class="op">=</span> <span class="ss">f"</span><span class="sc">{</span>word<span class="sc">:</span><span class="op">&lt;</span>{col_width}<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>        row_values <span class="op">=</span> [<span class="ss">f"</span><span class="sc">{</span>similarity_matrix[i, j]<span class="sc">:.2f}</span><span class="ss">"</span> <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(words))]</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>        row_str <span class="op">+=</span> <span class="st">""</span>.join(<span class="ss">f"[</span><span class="sc">{</span>value<span class="sc">:</span><span class="op">&gt;</span>{col_width<span class="op">-</span><span class="dv">2</span>}<span class="sc">}</span><span class="ss">]"</span> <span class="cf">for</span> value <span class="kw">in</span> row_values)</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(row_str)</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Example word vectors (in practice, these would have hundreds of dimensions)</span></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>word_vectors <span class="op">=</span> {</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>    <span class="st">'time'</span>: np.array([<span class="fl">0.2</span>, <span class="fl">0.8</span>, <span class="fl">0.3</span>]),</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>    <span class="st">'flies'</span>: np.array([<span class="fl">0.7</span>, <span class="fl">0.2</span>, <span class="fl">0.9</span>]),</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>    <span class="st">'like'</span>: np.array([<span class="fl">0.3</span>, <span class="fl">0.5</span>, <span class="fl">0.2</span>]),</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>    <span class="st">'an'</span>: np.array([<span class="fl">0.1</span>, <span class="fl">0.3</span>, <span class="fl">0.4</span>]),</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>    <span class="st">'arrow'</span>: np.array([<span class="fl">0.8</span>, <span class="fl">0.1</span>, <span class="fl">0.6</span>])</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>words <span class="op">=</span> <span class="bu">list</span>(word_vectors.keys()) <span class="co"># Preserve order</span></span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Convert word vectors into a matrix</span></span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.vstack([word_vectors[word] <span class="cf">for</span> word <span class="kw">in</span> words])</span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Calculate the similarity matrix (dot product)</span></span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>similarity_matrix <span class="op">=</span> calculate_similarity_matrix(word_vectors)</span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a><span class="co"># Print results</span></span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Input matrix shape:"</span>, X.shape)</span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Input matrix:</span><span class="ch">\n</span><span class="st">"</span>, X)</span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Input matrix transpose:</span><span class="ch">\n</span><span class="st">"</span>, X.T)</span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Similarity matrix shape:"</span>, similarity_matrix.shape)</span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Similarity matrix:"</span>) <span class="co"># Output from visualize_similarity_matrix</span></span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a>visualize_similarity_matrix(words, similarity_matrix)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Input matrix shape: (5, 3)
Input matrix:
 [[0.2 0.8 0.3]
 [0.7 0.2 0.9]
 [0.3 0.5 0.2]
 [0.1 0.3 0.4]
 [0.8 0.1 0.6]]

Input matrix transpose:
 [[0.2 0.7 0.3 0.1 0.8]
 [0.8 0.2 0.5 0.3 0.1]
 [0.3 0.9 0.2 0.4 0.6]]

Similarity matrix shape: (5, 5)
Similarity matrix:
              time    flies     like       an    arrow
time     [   0.77][   0.57][   0.52][   0.38][   0.42]
flies    [   0.57][   1.34][   0.49][   0.49][   1.12]
like     [   0.52][   0.49][   0.38][   0.26][   0.41]
an       [   0.38][   0.49][   0.26][   0.26][   0.35]
arrow    [   0.42][   1.12][   0.41][   0.35][   1.01]</code></pre>
</div>
</div>
<p>예를 들어 <strong>유사도 행렬의 (1,2) 요소값 0.57은 행축의 times와 열축의 flies의 벡터 거리(유사도)</strong> 가 됩니다. 이를 수학으로 표현하면 다음과 같습니다.</p>
<ul>
<li>문장의 단어 벡터의 행렬 X</li>
</ul>
<p><span class="math inline">\(\mathbf{X} = \begin{bmatrix}
\mathbf{x_1} \\
\mathbf{x_2} \\
\vdots \\
\mathbf{x_n}
\end{bmatrix}\)</span></p>
<ul>
<li>X의 전치행렬</li>
</ul>
<p><span class="math inline">\(\mathbf{X}^T = \begin{bmatrix}
\mathbf{x_1}^T &amp; \mathbf{x_2}^T &amp; \cdots &amp; \mathbf{x_n}^T
\end{bmatrix}\)</span></p>
<ul>
<li><span class="math inline">\(\mathbf{X}\mathbf{X}^T\)</span> 연산</li>
</ul>
<p><span class="math inline">\(\mathbf{X}\mathbf{X}^T = \begin{bmatrix}
\mathbf{x_1} \cdot \mathbf{x_1} &amp; \mathbf{x_1} \cdot \mathbf{x_2} &amp; \cdots &amp; \mathbf{x_1} \cdot \mathbf{x_n} \\
\mathbf{x_2} \cdot \mathbf{x_1} &amp; \mathbf{x_2} \cdot \mathbf{x_2} &amp; \cdots &amp; \mathbf{x_2} \cdot \mathbf{x_n} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\mathbf{x_n} \cdot \mathbf{x_1} &amp; \mathbf{x_n} \cdot \mathbf{x_2} &amp; \cdots &amp; \mathbf{x_n} \cdot \mathbf{x_n}
\end{bmatrix}\)</span></p>
<ul>
<li>각 원소 (i,j)</li>
</ul>
<p><span class="math inline">\((\mathbf{X}\mathbf{X}^T)_{ij} = \mathbf{x_i} \cdot \mathbf{x_j} = \sum_{k=1}^d x_{ik}x_{jk}\)</span></p>
<p>이 n×n 행렬의 각 원소는 두 단어 벡터 간의 내적이며 따라서 두 단어의 거리(유사도)가 됩니다. 이것이 “어텐션 스코어”입니다.</p>
<p>다음은 3단계인 유사도 행렬을 소프트맥스를 이용해서 가중치 행렬로 바꾸는 단계입니다.</p>
<div id="cell-9" class="cell" data-execution_count="9">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. Convert similarities to weights (probability distribution) (softmax)</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> softmax(x):</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>    exp_x <span class="op">=</span> np.exp(x <span class="op">-</span> np.<span class="bu">max</span>(x, axis<span class="op">=-</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>))  <span class="co"># trick for stability</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> exp_x <span class="op">/</span> exp_x.<span class="bu">sum</span>(axis<span class="op">=-</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>attention_weights <span class="op">=</span> softmax(similarity_matrix)</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Attention weights shape:"</span>, attention_weights.shape)</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Attention weights:</span><span class="ch">\n</span><span class="st">"</span>, attention_weights)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Attention weights shape: (5, 5)
Attention weights:
 [[0.25130196 0.20574865 0.19571417 0.17014572 0.1770895 ]
 [0.14838442 0.32047566 0.13697608 0.13697608 0.25718775]
 [0.22189237 0.21533446 0.19290396 0.17109046 0.19877876]
 [0.20573742 0.22966017 0.18247272 0.18247272 0.19965696]
 [0.14836389 0.29876818 0.14688764 0.13833357 0.26764673]]</code></pre>
</div>
</div>
<p>어텐션 가중치는 소프트맥스(softmax) 함수를 적용합니다. 두가지 핵심적인 변환을 합니다.</p>
<ol type="1">
<li>유사도 점수를 0과 1 사이의 값으로 변환</li>
<li>각 행의 합을 1로 만들어 확률 분포로 변환</li>
</ol>
<p>유사도 행렬을 가중치로 변환하면 단어와 다른 단어들과의 관련도를 확률적으로 표현하게 됩니다. 행, 열 축이 모두 문장 단어 순서이므로 가중치 1행은 ’time’라는 단어 행이고, 열은 모든 문장 단어입니다. 따라서</p>
<ol type="1">
<li>다른 모든 단어들(‘time’, ‘flies’, ‘like’, ‘an’, ‘arrow’)간 관계가 확률값으로 표현</li>
<li>이 확률값들의 합은 1</li>
<li>높은 확률값은 더 강한 관련성을 의미</li>
</ol>
<p>이렇게 변환된 가중치는 다음 단계에서 문장에 곱해지는 비율로 사용됩니다. 이 비율을 적용함으로써 문장의 각 단어는 얼마나 정보를 반영할지를 나타내게 됩니다. 이는 각 단어가 다른 단어들의 정보를 “참조”할 때 얼마나 주목할지 결정하는 것과 같습니다.</p>
<div id="cell-11" class="cell" data-execution_count="17">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 4. Generate contextualized representations using the weights</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>contextualized_vectors <span class="op">=</span> np.dot(attention_weights, X)</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Contextualized vectors shape:"</span>, contextualized_vectors.shape)</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Contextualized vectors:</span><span class="ch">\n</span><span class="st">"</span>, contextualized_vectors)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
Contextualized vectors shape: (5, 3)
Contextualized vectors:
 [[0.41168487 0.40880105 0.47401919]
 [0.51455048 0.31810231 0.56944172]
 [0.42911583 0.38823778 0.48665295]
 [0.43462426 0.37646585 0.49769319]
 [0.51082753 0.32015331 0.55869952]]</code></pre>
</div>
</div>
<p>가중치 행렬과 단어 행렬(단어 행벡터로 구성된)의 점곱은 해석이 필요합니다. attention_weights의 첫 번째 행을 [0.5, 0.2, 0.1, 0.1, 0.1]라고 가정하면, 각 값은 ’time’이 다른 단어들과 가지는 관련성의 확률입니다. 첫 번째 가중치 행을<span class="math inline">\(\begin{bmatrix} \alpha_{11} &amp; \alpha_{12} &amp; \alpha_{13} &amp; \alpha_{14} &amp; \alpha_{15} \end{bmatrix}\)</span>라고 표현하면, 이 가중치 첫 행에 대해 단어 매트릭스 연산은 다음과 같이 표현할 수 있습니다.</p>
<p><span class="math inline">\(\begin{bmatrix} \alpha_{11} &amp; \alpha_{12} &amp; \alpha_{13} &amp; \alpha_{14} &amp; \alpha_{15} \end{bmatrix} \begin{bmatrix} \vec{v}_{\text{time}} \ \vec{v}_{\text{flies}} \ \vec{v}_{\text{like}} \ \vec{v}_{\text{an}} \ \vec{v}_{\text{arrow}} \end{bmatrix}\)</span></p>
<p>이것을 파이썬 코드로 나타내면 다음과 같습니다.</p>
<div id="cell-13" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>time_contextualized <span class="op">=</span> <span class="fl">0.5</span><span class="op">*</span>time_vector <span class="op">+</span> <span class="fl">0.2</span><span class="op">*</span>flies_vector <span class="op">+</span> <span class="fl">0.1</span><span class="op">*</span>like_vector <span class="op">+</span> <span class="fl">0.1</span><span class="op">*</span>an_vector <span class="op">+</span> <span class="fl">0.1</span><span class="op">*</span>arrow_vector</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="co"># 0.5는 time과 time의 관련도 확률값</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="co"># 0.2는 time과 files의 관련도 확률값</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>연산은 이 확률들(타임이 각 단어와 관련된 확률값)을 각 단어의 원래 벡터와 곱하고 모두 더합니다. 결과적으로 <strong>’time’의 새로운 벡터는 다른 단어들의 의미를 그 관련성의 정도에 따라 반영한 가중 평균</strong> 이 됩니다. 가중 평균값을 구한다는 점이 핵심입니다. 따라서 가중 평균을 얻기 위해 가중치 행렬을 구하는 전 단계가 필요했던 것입니다.</p>
<p>최종 문맥화된 벡터의 shape은 (5, 3)이 되는데, 이는 (5,5) 크기의 어텐션 가중치 행렬과 (5,3)의 단어 벡터 행렬 X를 곱한 결과 (5,5) @ (5,3) = (5,3)가 되기 때문입니다.</p>
</section>
<section id="셀프-어텐션으로의-진화" class="level3">
<h3 class="anchored">8.2.3 셀프 어텐션으로의 진화</h3>
<p>구글 연구팀은 기본 어텐션 메커니즘(8.2.2절)을 분석하면서 몇 가지 <strong>한계점</strong>을 발견했습니다. 가장 큰 문제는 단어 벡터가 <strong>유사도 계산</strong>과 <strong>정보 전달</strong>이라는 여러 역할을 동시에 수행하는 것이 비효율적이라는 점이었습니다. 예를 들어, “bank”라는 단어는 문맥에 따라 “은행” 또는 “강둑” 등 <em>다른 의미</em>를 가지며, 이에 따라 <em>다른 단어들과의 관계</em>도 달라져야 합니다. 하지만, <strong>하나의 벡터</strong>로는 이러한 다양한 의미와 관계를 모두 표현하기 어려웠습니다.</p>
<p>연구팀은 각 역할을 <strong>독립적으로 최적화</strong>할 수 있는 방법을 모색했습니다. 이는 마치 CNN에서 필터가 이미지의 특징을 추출하는 역할을 <em>학습 가능한 형태로 발전</em>시킨 것처럼, 어텐션에서도 각 역할에 특화된 표현을 <em>학습</em>할 수 있도록 설계하는 것이었습니다. 이 아이디어는 단어 벡터를 서로 다른 역할을 위한 공간으로 <em>변환</em>하는 것에서부터 시작되었습니다.</p>
<p><strong>기본 개념의 한계(코드 예시)</strong></p>
<div id="cell-17" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> basic_self_attention(word_vectors):</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>    similarity_matrix <span class="op">=</span> np.dot(word_vectors, word_vectors.T)</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>    attention_weights <span class="op">=</span> softmax(similarity_matrix)</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>    contextualized_vectors <span class="op">=</span> np.dot(attention_weights, word_vectors)</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> contextualized_vectors</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>위 코드에서 <code>word_vectors</code>는 세 가지 역할을 동시에 수행합니다.</p>
<ol type="1">
<li><strong>유사도 계산의 주체:</strong> 다른 단어와의 유사도를 계산할 때 사용됩니다.</li>
<li><strong>유사도 계산의 대상:</strong> 다른 단어로부터 유사도를 계산 당합니다.</li>
<li><strong>정보 전달:</strong> 최종 문맥 벡터를 생성할 때 가중 평균에 사용됩니다.</li>
</ol>
<p><strong>첫 번째 개선: 정보 전달 역할의 분리</strong></p>
<p>연구팀은 먼저 <strong>정보 전달 역할</strong>을 분리했습니다. 선형대수학에서 벡터의 역할을 분리하는 가장 간단한 방법은 <em>별도의 학습 가능한 행렬</em>을 사용하여 벡터를 새로운 공간으로 <em>선형 변환(linear transformation)</em>하는 것입니다.</p>
<div id="cell-19" class="cell" data-execution_count="10">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> improved_self_attention(word_vectors, W_similarity, W_content):</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>    similarity_vectors <span class="op">=</span> np.dot(word_vectors, W_similarity)</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>    content_vectors <span class="op">=</span> np.dot(word_vectors, W_content)</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate similarity by taking the dot product between similarity_vectors</span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>    attention_scores <span class="op">=</span> np.dot(similarity_vectors, similarity_vectors.T)</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Convert to probability distribution using softmax</span></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>    attention_weights <span class="op">=</span> softmax(attention_scores)</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Generate the final contextualized representation by multiplying weights and content_vectors</span></span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>    contextualized_vectors <span class="op">=</span> np.dot(attention_weights, content_vectors)</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> contextualized_vectors</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<ul>
<li><code>W_similarity</code>: 단어 벡터를 유사도 계산에 최적화된 공간으로 투영하는 <em>학습 가능한</em> 행렬.</li>
<li><code>W_content</code>: 단어 벡터를 정보 전달에 최적화된 공간으로 투영하는 <em>학습 가능한</em> 행렬.</li>
</ul>
<p>이 개선을 통해, <code>similarity_vectors</code>는 유사도 계산에, <code>content_vectors</code>는 정보 전달에 각각 특화될 수 있게 되었습니다. 이는 Value를 통한 정보 집계라는 개념의 전신이 되었습니다.</p>
<p><strong>두 번째 개선: 유사도 역할의 완전 분리 (Q, K의 탄생)</strong></p>
<p>다음 단계는 유사도 계산 과정 자체를 두 가지 역할로 분리하는 것이었습니다. <code>similarity_vectors</code>가 “질문하는 역할”(Query)과 “답변하는 역할”(Key)을 동시에 수행하는 대신, 이 둘을 <em>완전히 분리</em>하는 방향으로 발전했습니다.</p>
<div id="cell-21" class="cell" data-execution_count="11">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SelfAttention(nn.Module):</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, embed_dim):</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 각각의 역할을 위한 독립적인 선형 변환</span></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.q <span class="op">=</span> nn.Linear(embed_dim, embed_dim)  <span class="co"># 질문(Query)을 위한 변환</span></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.k <span class="op">=</span> nn.Linear(embed_dim, embed_dim)  <span class="co"># 답변(Key)을 위한 변환</span></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.v <span class="op">=</span> nn.Linear(embed_dim, embed_dim)  <span class="co"># 정보 전달(Value)을 위한 변환</span></span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>        Q <span class="op">=</span> <span class="va">self</span>.q(x)  <span class="co"># 질문자로서의 표현</span></span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>        K <span class="op">=</span> <span class="va">self</span>.k(x)  <span class="co"># 응답자로서의 표현</span></span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>        V <span class="op">=</span> <span class="va">self</span>.v(x)  <span class="co"># 전달할 정보의 표현</span></span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 질문과 답변 간의 관련성(유사도) 계산</span></span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a>        scores <span class="op">=</span> torch.matmul(Q, K.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>))</span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a>        weights <span class="op">=</span> F.softmax(scores, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 관련성에 따른 정보 집계 (가중 평균)</span></span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.matmul(weights, V)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><strong>Q, K, V 공간 분리의 의미</strong></p>
<p>Q와 K의 순서를 바꿔도 (<span class="math inline">\(QK^T\)</span> 대신 <span class="math inline">\(KQ^T\)</span>) 수학적으로는 동일한 유사도 행렬을 얻습니다. 수학적으로만 보면 동일한 이 둘을 Q, K를 “질의(Query)”, “키(Key)”로 명명한 이유는 무엇일까요? 핵심은 <em>유사도 계산을 더 잘하기 위해</em> 별도의 공간으로 최적화하는 것입니다. 이러한 명칭은 트랜스포머 모델의 어텐션 메커니즘이 정보 검색(Information Retrieval) 시스템에서 영감을 받았기 때문으로 보입니다. 검색 시스템에서 “질의(Query)”는 사용자가 원하는 정보를, “키(Key)”는 각 문서의 색인어와 유사한 역할을 합니다. 어텐션은 질의와 키의 유사도를 계산하여 관련성이 높은 정보를 찾는 과정을 모방합니다.</p>
<p>예를 들어</p>
<ul>
<li>“I need to deposit money in the bank” (은행)</li>
<li>“The river bank is covered with flowers” (강둑)</li>
</ul>
<p>위 두 문장에서 “bank”는 문맥에 따라 다른 의미를 가집니다. Q, K 공간 분리를 통해</p>
<ul>
<li>“bank”와 다른 단어들은 Q, K 공간에서 <em>서로 다른 방식</em>으로 배치되어 유사도 계산에 최적화됩니다.</li>
<li>금융 관련 문맥에서는 ‘money’, ‘deposit’ 등과의 유사도가 높아지도록 두 공간에서 벡터들이 배치됩니다.</li>
<li>지형 관련 문맥에서는 ‘river’, ‘covered’ 등과의 유사도가 높아지도록 배치됩니다.</li>
</ul>
<p>즉, Q-K 쌍은 <em>두 개의 최적화된 공간</em>에서 내적을 수행하여 유사도를 계산하는 것을 의미합니다. 중요한 점은 Q, K 공간이 <em>학습을 통해</em> 최적화된다는 것입니다. 구글 연구팀은 Q와 K 행렬이 학습 과정에서 실제로 질의와 키와 유사한 방식으로 동작하도록 최적화되는 현상을 발견했을 가능성이 큽니다.</p>
<p><strong>Q, K 공간 분리의 중요성</strong></p>
<p>Q와 K를 분리함으로써 얻는 또 다른 이점은 <em>유연성 확보</em>입니다. Q와 K를 같은 공간에 두면 유사도 계산 방식이 제한될 수 있습니다(예: 대칭적 유사도). 하지만 Q, K를 분리하면 더 복잡하고 비대칭적인 관계(예: “A는 B의 원인”)도 학습할 수 있습니다. 또한, 서로 다른 변환(<span class="math inline">\(W^Q\)</span>, <span class="math inline">\(W^K\)</span>)을 통해 Q와 K는 각 단어의 역할을 더 세분화하여 표현할 수 있어 모델의 표현력이 증대됩니다. 마지막으로, Q와 K 공간을 분리하면, 각 공간의 최적화 목표가 더 명확해져 Q 공간은 질문에 적합한 표현, K 공간은 답변에 적합한 표현을 학습하도록 역할 분담이 자연스럽게 이루어집니다.</p>
<p><strong>Value의 역할</strong></p>
<p>Q, K가 유사도 계산을 위한 공간이라면, V는 <em>실제로 전달될 정보</em>를 담는 공간입니다. V 공간으로의 변환은 단어의 의미 정보를 가장 잘 표현하는 방향으로 최적화됩니다. Q, K가 “어떤 단어들의 정보를 얼마나 반영할지”를 결정한다면, V는 “실제로 어떤 정보를 전달할지”를 담당합니다. 위의 “bank” 예시에서,</p>
<ul>
<li>Q, K는 문맥에 따라 금융 관련 단어들과의 유사도를 계산하고,</li>
<li>V는 실제 금융 기관으로서의 ’bank’의 의미 정보를 표현합니다.</li>
</ul>
<p>이러한 세 가지 공간의 분리는 “정보를 찾는 방법(Q, K)”과 “전달할 정보의 내용(V)”을 독립적으로 최적화하여, CNN에서 “어떤 패턴을 찾을지(필터 학습)”와 “찾은 패턴을 어떻게 표현할지(채널 학습)”를 분리한 것과 유사한 효과를 냅니다.</p>
<p><strong>어텐션의 수학적 표현</strong></p>
<p>최종적인 어텐션 메커니즘은 다음과 같은 수식으로 표현됩니다.</p>
<p><span class="math display">\[\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V\]</span></p>
<ul>
<li><span class="math inline">\(Q \in \mathbb{R}^{n \times d_k}\)</span>: Query 행렬</li>
<li><span class="math inline">\(K \in \mathbb{R}^{n \times d_k}\)</span>: Key 행렬</li>
<li><span class="math inline">\(V \in \mathbb{R}^{n \times d_v}\)</span>: Value 행렬 (<span class="math inline">\(d_v\)</span>는 일반적으로<span class="math inline">\(d_k\)</span>와 같음)</li>
<li><span class="math inline">\(n\)</span>: 시퀀스 길이</li>
<li><span class="math inline">\(d_k\)</span>: Query, Key 벡터의 차원</li>
<li><span class="math inline">\(d_v\)</span>: Value 벡터의 차원</li>
<li><span class="math inline">\(\frac{QK^T}{\sqrt{d_k}}\)</span>: Scaled Dot-Product Attention. 차원이 커질 수록 내적값이 커져 소프트맥스 함수 통과시 기울기가 소실되는 것을 방지.</li>
</ul>
<p>이러한 발전된 구조는 트랜스포머의 핵심 요소가 되었으며, 이후 BERT, GPT 등 현대적 언어 모델의 기초가 되었습니다.</p>
<div class="callout callout-style-default callout-note callout-titled" title="클릭하여 내용 보기 (이론 딥다이브: 셀프 어텐션 메커니즘의 통합적 이해와 최신 이론)">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
클릭하여 내용 보기 (이론 딥다이브: 셀프 어텐션 메커니즘의 통합적 이해와 최신 이론)
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<section id="셀프-어텐션-메커니즘의-통합적-이해와-최신-이론" class="level2 callout-body-container callout-body">
<h2 class="anchored" data-anchor-id="셀프-어텐션-메커니즘의-통합적-이해와-최신-이론">셀프 어텐션 메커니즘의 통합적 이해와 최신 이론</h2>
<section id="수학적-원리-및-계산-복잡도" class="level3">
<h3 class="anchored" data-anchor-id="수학적-원리-및-계산-복잡도">1. 수학적 원리 및 계산 복잡도</h3>
<p>셀프 어텐션은 입력 시퀀스의 각 단어가 자기 자신을 포함한 다른 모든 단어와의 관계를 계산하여 문맥을 반영한 새로운 표현을 생성합니다. 이 과정은 크게 세 단계로 이루어집니다.</p>
<ol type="1">
<li><p><strong>Query, Key, Value 생성:</strong></p>
<p>입력 시퀀스의 각 단어 임베딩 벡터(<span class="math inline">\(x_i\)</span>)에 대해 세 가지 선형 변환을 적용하여 Query (<span class="math inline">\(q_i\)</span>), Key (<span class="math inline">\(k_i\)</span>), Value (<span class="math inline">\(v_i\)</span>) 벡터를 생성합니다. 이 변환은 학습 가능한 가중치 행렬(<span class="math inline">\(W^Q\)</span>,<span class="math inline">\(W^K\)</span>,<span class="math inline">\(W^V\)</span>)을 사용하여 수행됩니다.</p>
<p><span class="math inline">\(q_i = x_i W^Q\)</span></p>
<p><span class="math inline">\(k_i = x_i W^K\)</span></p>
<p><span class="math inline">\(v_i = x_i W^V\)</span></p>
<p><span class="math inline">\(W^Q, W^K, W^V \in \mathbb{R}^{d_{model} \times d_k}\)</span> : 학습 가능한 가중치 행렬. (<span class="math inline">\(d_{model}\)</span>: 임베딩 차원,<span class="math inline">\(d_k\)</span>: query, key, value 벡터의 차원)</p></li>
<li><p><strong>어텐션 스코어 계산 및 정규화</strong></p>
<p>각 단어 쌍에 대해 Query와 Key 벡터의 내적(dot product)을 계산하여 어텐션 스코어(attention score)를 구합니다.</p>
<p><span class="math display">\[\text{score}(q_i, k_j) = q_i \cdot k_j^T\]</span></p>
<p>이 스코어는 두 단어가 얼마나 관련되어 있는지를 나타냅니다. 내적 연산 후에는 스케일링(scaling)을 수행하는데, 이는 내적 값이 너무 커지는 것을 방지하여 기울기 소실(gradient vanishing) 문제를 완화하기 위함입니다. 스케일링은 Key 벡터의 차원(<span class="math inline">\(d_k\)</span>)의 제곱근으로 나누는 방식으로 수행됩니다.</p>
<p><span class="math display">\[\text{scaled score}(q_i, k_j) = \frac{q_i \cdot k_j^T}{\sqrt{d_k}}\]</span></p>
<p>마지막으로, 소프트맥스(softmax) 함수를 적용하여 어텐션 스코어를 정규화하고, 각 단어에 대한 어텐션 가중치(attention weight)를 얻습니다.</p>
<p><span class="math display">\[\alpha_{ij} = \text{softmax}(\text{scaled score}(q_i, k_j)) = \frac{\exp(\text{scaled score}(q_i, k_j))}{\sum_{l=1}^{n} \exp(\text{scaled score}(q_i, k_l))}\]</span></p>
<p>여기서<span class="math inline">\(\alpha_{ij}\)</span>는<span class="math inline">\(i\)</span>번째 단어가<span class="math inline">\(j\)</span>번째 단어에 주는 어텐션 가중치,<span class="math inline">\(n\)</span>은 시퀀스 길이.</p></li>
<li><p><strong>가중 평균 계산</strong></p>
<p>어텐션 가중치(<span class="math inline">\(\alpha_{ij}\)</span>)를 사용하여 Value 벡터(<span class="math inline">\(v_j\)</span>)의 가중 평균(weighted average)을 계산합니다. 이 가중 평균은 입력 시퀀스의 모든 단어 정보를 종합한 문맥 벡터(context vector)<span class="math inline">\(c_i\)</span>가 됩니다.</p></li>
</ol>
<p><span class="math display">\[c_i = \sum_{j=1}^{n} \alpha_{ij} v_j\]</span></p>
<p><strong>전체 과정을 행렬 형태로 표현</strong></p>
<p>입력 임베딩 행렬을<span class="math inline">\(X \in \mathbb{R}^{n \times d_{model}}\)</span>라고 할 때, 전체 셀프 어텐션 과정은 다음과 같이 표현할 수 있습니다.</p>
<p><span class="math display">\[\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V\]</span></p>
<p>여기서<span class="math inline">\(Q = XW^Q\)</span>,<span class="math inline">\(K = XW^K\)</span>,<span class="math inline">\(V = XW^V\)</span>입니다.</p>
<p><strong>계산 복잡도</strong></p>
<p>셀프 어텐션의 계산 복잡도는 입력 시퀀스 길이(<span class="math inline">\(n\)</span>)에 대해<span class="math inline">\(O(n^2)\)</span>입니다. 이는 각 단어가 다른 모든 단어와의 관계를 계산해야 하기 때문입니다.</p>
<ul>
<li><strong><span class="math inline">\(QK^T\)</span>계산:</strong><span class="math inline">\(n\)</span>개의 쿼리 벡터와<span class="math inline">\(n\)</span>개의 키 벡터 간의 내적 연산을 수행하므로<span class="math inline">\(O(n^2d_k)\)</span>의 계산이 필요합니다.</li>
<li><strong>소프트맥스 연산:</strong> 각 쿼리에 대한 어텐션 가중치를 계산하기 위해<span class="math inline">\(n\)</span>개의 키에 대한 소프트맥스 연산을 수행하므로,<span class="math inline">\(O(n^2)\)</span>의 계산 복잡도를 가집니다.</li>
<li><strong><span class="math inline">\(V\)</span>와의 가중 평균:</strong><span class="math inline">\(n\)</span>개의 value 벡터와<span class="math inline">\(n\)</span>개의 attention weight를 곱해야 하므로<span class="math inline">\(O(n^2d_k)\)</span>의 계산 복잡도를 가집니다.</li>
</ul>
</section>
<section id="커널-기계-관점의-확장" class="level3">
<h3 class="anchored" data-anchor-id="커널-기계-관점의-확장">2. 커널 기계 관점의 확장</h3>
<section id="비대칭-커널-함수" class="level4">
<h4 class="anchored" data-anchor-id="비대칭-커널-함수">2.1 비대칭 커널 함수</h4>
<p>어텐션을 비대칭 커널 함수로 해석 <span class="math inline">\(K(Q_i, K_j) = \exp\left(\frac{Q_i \cdot K_j}{\sqrt{d_k}}\right)\)</span></p>
<p>이 커널은 입력 공간을 재구성하는 특징 매핑을 학습합니다.</p>
</section>
<section id="특이값-분해svd-분석" class="level4">
<h4 class="anchored" data-anchor-id="특이값-분해svd-분석">2.2 특이값 분해(SVD) 분석</h4>
<p>어텐션 행렬의 비대칭 KSVD</p>
<p><span class="math inline">\(A = U\Sigma V^T \quad \text{where } \Sigma = \text{diag}(\sigma_1, \sigma_2, ...)\)</span></p>
<p>-<span class="math inline">\(U\)</span>: 쿼리 공간의 주요 방향 (문맥 요청 패턴) -<span class="math inline">\(V\)</span>: 키 공간의 주요 방향 (정보 제공 패턴) -<span class="math inline">\(\sigma_i\)</span>: 상호작용 강도 (≥0.9 설명력 집중 현상 관측)</p>
</section>
</section>
<section id="에너지-기반-모델과-동역학" class="level3">
<h3 class="anchored" data-anchor-id="에너지-기반-모델과-동역학">3. 에너지 기반 모델과 동역학</h3>
<section id="에너지-함수-공식화" class="level4">
<h4 class="anchored" data-anchor-id="에너지-함수-공식화">3.1 에너지 함수 공식화</h4>
<p><span class="math inline">\(E(Q,K,V) = -\sum_{i,j} \frac{Q_i \cdot K_j}{\sqrt{d_k}}V_j + \text{log-partition function}\)</span></p>
<p>출력은 에너지 최소화 과정으로 해석</p>
<p><span class="math inline">\(\text{Output} = \arg\min_V E(Q,K,V)\)</span></p>
</section>
<section id="홉필드-네트워크와의-동치성" class="level4">
<h4 class="anchored" data-anchor-id="홉필드-네트워크와의-동치성">3.2 홉필드 네트워크와의 동치성</h4>
<p>연속형 홉필드 네트워크 방정식 <span class="math inline">\(\tau\frac{dX}{dt} = -X + \text{softmax}(XWX^T)XW\)</span></p>
<p>여기서<span class="math inline">\(\tau\)</span>는 시간 상수,<span class="math inline">\(W\)</span>는 학습된 연결 강도 행렬</p>
</section>
</section>
<section id="저차원-구조와-최적화" class="level3">
<h3 class="anchored" data-anchor-id="저차원-구조와-최적화">4. 저차원 구조와 최적화</h3>
<section id="랭크-붕괴-현상" class="level4">
<h4 class="anchored" data-anchor-id="랭크-붕괴-현상">4.1 랭크 붕괴 현상</h4>
<p>심층 레이어에서 <span class="math inline">\(\text{rank}(A) \leq \lfloor0.1n\rfloor\)</span>(실험적 관측)</p>
<p>이는 정보의 효율적 압축을 의미합니다.</p>
</section>
<section id="효율적-어텐션-기법-비교" class="level4">
<h4 class="anchored" data-anchor-id="효율적-어텐션-기법-비교">4.2 효율적 어텐션 기법 비교</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>기법</th>
<th>원리</th>
<th>복잡도</th>
<th>적용 사례</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Linformer</td>
<td>저랭크 투영</td>
<td><span class="math inline">\(O(n)\)</span></td>
<td>장문 처리</td>
</tr>
<tr class="even">
<td>Performer</td>
<td>랜덤 푸리에 특징</td>
<td><span class="math inline">\(O(n\log n)\)</span></td>
<td>유전체 분석</td>
</tr>
<tr class="odd">
<td>Reformer</td>
<td>LSH 버킷팅</td>
<td><span class="math inline">\(O(n\log n)\)</span></td>
<td>실시간 번역</td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="동역학적-시스템-분석" class="level3">
<h3 class="anchored" data-anchor-id="동역학적-시스템-분석">5. 동역학적 시스템 분석</h3>
<section id="리아푸노프-안정성" class="level4">
<h4 class="anchored" data-anchor-id="리아푸노프-안정성">5.1 리아푸노프 안정성</h4>
<p><span class="math inline">\(V(X) = \|X - X^*\|^2\)</span>감소함수</p>
<p>어텐션 업데이트는 점근적 안정성을 보장합니다.</p>
</section>
<section id="주파수-영역-해석" class="level4">
<h4 class="anchored" data-anchor-id="주파수-영역-해석">5.2 주파수 영역 해석</h4>
<p>푸리에 변환을 적용한 어텐션 스펙트럼</p>
<p><span class="math inline">\(\mathcal{F}(A)_{kl} = \sum_{m,n} A_{mn}e^{-i2\pi(mk/M+nl/N)}\)</span></p>
<p>저주파 성분이 정보의 80% 이상을 포착</p>
</section>
</section>
<section id="정보-이론적-해석" class="level3">
<h3 class="anchored" data-anchor-id="정보-이론적-해석">6. 정보 이론적 해석</h3>
<section id="상호-정보량-최대화" class="level4">
<h4 class="anchored" data-anchor-id="상호-정보량-최대화">6.1 상호 정보량 최대화</h4>
<p><span class="math inline">\(\max I(X;Y) = H(Y) - H(Y|X) \quad \text{s.t. } Y = \text{Attention}(X)\)</span></p>
<p>소프트맥스는 엔트로피<span class="math inline">\(H(Y)\)</span>를 최대화하는 최적 분포 생성</p>
</section>
<section id="신호-대-잡음비snr-분석" class="level4">
<h4 class="anchored" data-anchor-id="신호-대-잡음비snr-분석">6.2 신호 대 잡음비(SNR) 분석</h4>
<p>층 깊이<span class="math inline">\(l\)</span>에 따른 SNR 감쇠</p>
<p><span class="math inline">\(\text{SNR}^{(l)} \propto e^{-0.2l} \quad \text{(ResNet-50 기준)}\)</span></p>
</section>
</section>
<section id="신경과학적-영감" class="level3">
<h3 class="anchored" data-anchor-id="신경과학적-영감">7. 신경과학적 영감</h3>
<section id="시각-피층-v4-영역" class="level4">
<h4 class="anchored" data-anchor-id="시각-피층-v4-영역">7.1 시각 피층 V4 영역</h4>
<ul>
<li>방향 선택성 뉴런 ≈ 특정 패턴에 반응하는 어텐션 헤드</li>
<li>수용장 계층화 구조 ≈ 멀티스케일 어텐션</li>
</ul>
</section>
<section id="전두엽-작업-기억" class="level4">
<h4 class="anchored" data-anchor-id="전두엽-작업-기억">7.2 전두엽 작업 기억</h4>
<ul>
<li>지속적 뉴런 활성화 ≈ 어텐션의 장기 의존성 처리</li>
<li>컨텍스트 유지 메커니즘 ≈ 디코더의 마스킹 기법</li>
</ul>
</section>
</section>
<section id="고급-수학적-모델링" class="level3">
<h3 class="anchored" data-anchor-id="고급-수학적-모델링">8. 고급 수학적 모델링</h3>
<section id="텐서-네트워크-확장" class="level4">
<h4 class="anchored" data-anchor-id="텐서-네트워크-확장">8.1 텐서 네트워크 확장</h4>
<p>MPO(Matrix Product Operator) 표현</p>
<p><span class="math inline">\(A_{ij} = \sum_{\alpha=1}^r Q_{i\alpha}K_{j\alpha}\)</span> 여기서<span class="math inline">\(r\)</span>은 텐서 네트워크 본드 차원</p>
</section>
<section id="미분-기하학적-해석" class="level4">
<h4 class="anchored" data-anchor-id="미분-기하학적-해석">8.2 미분 기하학적 해석</h4>
<p>어텐션 매니폴드의 리만 곡률 <span class="math inline">\(R_{ijkl} = \partial_i\Gamma_{jk}^m - \partial_j\Gamma_{ik}^m + \Gamma_{il}^m\Gamma_{jk}^l - \Gamma_{jl}^m\Gamma_{ik}^l\)</span></p>
<p>곡률 분석을 통해 모델의 표현력 한계 추정 가능</p>
</section>
</section>
<section id="최신-연구-동향-2025" class="level3">
<h3 class="anchored" data-anchor-id="최신-연구-동향-2025">9. 최신 연구 동향 (2025)</h3>
<ol type="1">
<li><p><strong>양자 어텐션</strong></p>
<ul>
<li>쿼리/키를 양자 중첩 상태로 표현:<span class="math inline">\(|\psi_Q\rangle = \sum c_i|i\rangle\)</span></li>
<li>양자 내적 연산 가속화</li>
</ul></li>
<li><p><strong>생체 모방 최적화</strong></p>
<ul>
<li>스파이크 타이밍 종속 가소성(STDP) 적용</li>
</ul>
<p><span class="math inline">\(\Delta W_{ij} \propto x_i x_j - \beta W_{ij}\)</span></p></li>
<li><p><strong>동적 에너지 조정</strong></p>
<ul>
<li>메타 러닝 기반 실시간 에너지 함수 튜닝<br>
</li>
<li>물리 엔진과의 연동 시뮬레이션</li>
</ul></li>
</ol>
<hr>
</section>
<section id="참고-문헌" class="level3">
<h3 class="anchored" data-anchor-id="참고-문헌">참고 문헌</h3>
<ol type="1">
<li>Vaswani et al., “Attention Is All You Need”, NeurIPS 2017<br>
</li>
<li>Choromanski et al., “Rethinking Attention with Performers”, ICLR 2021<br>
</li>
<li>Ramsauer et al., “Hopfield Networks is All You Need”, ICLR 2021<br>
</li>
<li>Wang et al., “Linformer: Self-Attention with Linear Complexity”, arXiv 2020<br>
</li>
<li>Chen et al., “Theoretical Analysis of Self-Attention via Signal Propagation”, NeurIPS 2023</li>
</ol>
</section>
</section>
</div>
</div>
</section>
<section id="멀티헤드-어텐션과-병렬-처리" class="level3">
<h3 class="anchored" data-anchor-id="멀티헤드-어텐션과-병렬-처리">8.2.4 멀티헤드 어텐션과 병렬 처리</h3>
<p>구글 연구팀은 셀프 어텐션의 성능을 더욱 향상시키기 위해, “하나의 큰 어텐션 공간 대신, <em>여러 개의 작은 어텐션 공간</em>에서 서로 다른 종류의 관계를 포착하면 어떨까?”라는 아이디어를 떠올렸습니다. 마치 여러 명의 전문가가 각자의 관점에서 문제를 분석하는 것처럼, 입력 시퀀스의 다양한 측면을 동시에 고려할 수 있다면 더 풍부한 문맥 정보를 얻을 수 있을 것이라고 생각했습니다.</p>
<p>이러한 아이디어를 바탕으로, 연구팀은 Q, K, V 벡터를 여러 개의 작은 공간으로 나누어 병렬적으로 어텐션을 계산하는 <strong>멀티헤드 어텐션(Multi-Head Attention)</strong>을 고안했습니다. 원 논문(“Attention is All You Need”)에서는 512차원의 임베딩을 8개의 64차원 헤드(head)로 나누어 처리했습니다. 이후 BERT와 같은 모델들은 이 구조를 더욱 확장했습니다(예: BERT-base는 768차원을 12개의 64차원 헤드로 분할).</p>
<p><strong>멀티헤드 어텐션의 작동 방식</strong></p>
<div id="cell-25" class="cell" data-execution_count="12">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> math</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MultiHeadAttention(nn.Module):</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config):</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span> config.hidden_size <span class="op">%</span> config.num_attention_heads <span class="op">==</span> <span class="dv">0</span></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.d_k <span class="op">=</span> config.hidden_size <span class="op">//</span> config.num_attention_heads  <span class="co"># Dimension of each head</span></span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.h <span class="op">=</span> config.num_attention_heads  <span class="co"># Number of heads</span></span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Linear transformation layers for Q, K, V, and output</span></span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.linear_layers <span class="op">=</span> nn.ModuleList([</span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>            nn.Linear(config.hidden_size, config.hidden_size)</span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">4</span>)  <span class="co"># For Q, K, V, and output</span></span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a>        ])</span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(config.attention_probs_dropout_prob) <span class="co"># added</span></span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.attention_weights <span class="op">=</span> <span class="va">None</span> <span class="co"># added</span></span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> attention(<span class="va">self</span>, query, key, value, mask<span class="op">=</span><span class="va">None</span>): <span class="co"># separate function</span></span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a>        scores <span class="op">=</span> torch.matmul(query, key.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>)) <span class="op">/</span> math.sqrt(<span class="va">self</span>.d_k) <span class="co"># scaled dot product</span></span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-25"><a href="#cb14-25" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> mask <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb14-26"><a href="#cb14-26" aria-hidden="true" tabindex="-1"></a>            scores <span class="op">=</span> scores.masked_fill(mask <span class="op">==</span> <span class="dv">0</span>, <span class="op">-</span><span class="fl">1e9</span>)</span>
<span id="cb14-27"><a href="#cb14-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-28"><a href="#cb14-28" aria-hidden="true" tabindex="-1"></a>        p_attn <span class="op">=</span> scores.softmax(dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb14-29"><a href="#cb14-29" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.attention_weights <span class="op">=</span> p_attn.detach()  <span class="co"># Store attention weights</span></span>
<span id="cb14-30"><a href="#cb14-30" aria-hidden="true" tabindex="-1"></a>        p_attn <span class="op">=</span> <span class="va">self</span>.dropout(p_attn)</span>
<span id="cb14-31"><a href="#cb14-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-32"><a href="#cb14-32" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.matmul(p_attn, value), p_attn</span>
<span id="cb14-33"><a href="#cb14-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-34"><a href="#cb14-34" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, query, key, value, mask<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb14-35"><a href="#cb14-35" aria-hidden="true" tabindex="-1"></a>        batch_size <span class="op">=</span> query.size(<span class="dv">0</span>)</span>
<span id="cb14-36"><a href="#cb14-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-37"><a href="#cb14-37" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 1) Linear projections in batch from d_model =&gt; h x d_k</span></span>
<span id="cb14-38"><a href="#cb14-38" aria-hidden="true" tabindex="-1"></a>        query, key, value <span class="op">=</span> [l(x).view(batch_size, <span class="op">-</span><span class="dv">1</span>, <span class="va">self</span>.h, <span class="va">self</span>.d_k).transpose(<span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb14-39"><a href="#cb14-39" aria-hidden="true" tabindex="-1"></a>                             <span class="cf">for</span> l, x <span class="kw">in</span> <span class="bu">zip</span>(<span class="va">self</span>.linear_layers, (query, key, value))]</span>
<span id="cb14-40"><a href="#cb14-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-41"><a href="#cb14-41" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 2) Apply attention on all the projected vectors in batch.</span></span>
<span id="cb14-42"><a href="#cb14-42" aria-hidden="true" tabindex="-1"></a>        x, attn <span class="op">=</span> <span class="va">self</span>.attention(query, key, value, mask<span class="op">=</span>mask)</span>
<span id="cb14-43"><a href="#cb14-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-44"><a href="#cb14-44" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 3) "Concat" using a view and apply a final linear.</span></span>
<span id="cb14-45"><a href="#cb14-45" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.transpose(<span class="dv">1</span>, <span class="dv">2</span>).contiguous().view(batch_size, <span class="op">-</span><span class="dv">1</span>, <span class="va">self</span>.h <span class="op">*</span> <span class="va">self</span>.d_k)</span>
<span id="cb14-46"><a href="#cb14-46" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.linear_layers[<span class="op">-</span><span class="dv">1</span>](x)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<hr>
</section>
<section id="멀티헤드-어텐션-multi-head-attention-상세-분석" class="level3">
<h3 class="anchored" data-anchor-id="멀티헤드-어텐션-multi-head-attention-상세-분석">멀티헤드 어텐션 (Multi-Head Attention) 상세 분석</h3>
<p><strong>코드 구조 (<code>__init__</code> 및 <code>forward</code>)</strong></p>
<p>멀티헤드 어텐션의 코드는 크게 초기화(<code>__init__</code>)와 순전파(<code>forward</code>) 메서드로 구성됩니다. 각 메서드의 역할과 세부 동작 방식을 자세히 살펴보겠습니다.</p>
<ul>
<li><strong><code>__init__</code> 메서드</strong>:
<ul>
<li><code>d_k</code>: 각 어텐션 헤드의 차원을 나타냅니다. 이 값은 모델의 hidden size를 헤드 수(num_attention_heads)로 나눈 값으로, 각 헤드가 처리할 정보의 양을 결정합니다.</li>
<li><code>h</code>: 어텐션 헤드의 개수를 설정합니다. 이 값은 하이퍼파라미터로, 모델이 얼마나 다양한 관점에서 입력을 바라볼지를 결정합니다.</li>
<li><code>linear_layers</code>: 쿼리(Q), 키(K), 값(V), 그리고 최종 출력을 위한 총 네 개의 선형 변환 레이어를 생성합니다. 이 레이어들은 입력을 각 헤드에 맞게 변환하고, 마지막에 헤드들의 결과를 통합하는 역할을 합니다.</li>
</ul></li>
<li><strong><code>forward</code> 메서드</strong>:
<ol type="1">
<li><strong>선형 변환 및 분할</strong>:
<ul>
<li>입력으로 받은 <code>query</code>, <code>key</code>, <code>value</code>에 대해 각각 <code>self.linear_layers</code>를 사용하여 선형 변환을 수행합니다. 이 과정을 통해 입력은 각 헤드에 적합한 형태로 변환됩니다.</li>
<li><code>view</code> 함수를 사용하여 텐서의 형태를 (batch_size, sequence_length, hidden_size)에서 (batch_size, sequence_length, h, d_k)로 변경합니다. 이는 전체 입력을 h개의 헤드로 나누는 과정입니다.</li>
<li><code>transpose</code> 함수를 사용하여 텐서의 차원을 (batch_size, sequence_length, h, d_k)에서 (batch_size, h, sequence_length, d_k)로 바꿉니다. 이제 각 헤드는 독립적으로 어텐션 계산을 수행할 준비가 완료되었습니다.</li>
</ul></li>
<li><strong>어텐션 적용</strong>:
<ul>
<li>각 헤드별로 <code>attention</code> 함수, 즉 스케일 내적 어텐션(Scaled Dot-Product Attention)을 호출하여 어텐션 가중치와 각 헤드의 결과를 계산합니다.</li>
</ul></li>
<li><strong>결합 및 최종 선형 변환</strong>:
<ul>
<li><code>transpose</code>와 <code>contiguous</code>를 사용하여 각 헤드의 결과(<code>x</code>)를 다시 (batch_size, sequence_length, h, d_k) 형태로 되돌립니다.</li>
<li><code>view</code> 함수를 사용하여 (batch_size, sequence_length, h * d_k), 즉 (batch_size, sequence_length, hidden_size) 형태로 통합합니다.</li>
<li>마지막으로, <code>self.linear_layers[-1]</code>을 적용하여 최종 출력을 생성합니다. 이 선형 변환은 헤드들의 결과를 종합하고, 최종적으로 모델이 원하는 형태의 출력을 만들어냅니다.</li>
</ul></li>
</ol></li>
<li><strong><code>attention</code> 메서드 (스케일 내적 어텐션)</strong>:
<ul>
<li>이 함수는 각 헤드에서 실제로 어텐션 메커니즘이 수행되는 곳으로, 각 헤드의 결과와 어텐션 가중치를 반환합니다.</li>
<li><strong>핵심:</strong> <code>scores</code>를 계산할 때, <code>key</code> 벡터 차원의 제곱근 (<span class="math inline">\(\sqrt{d_k}\)</span>)으로 나누어 스케일링하는 과정이 매우 중요합니다.
<ul>
<li><strong>목적:</strong> 내적 값(<span class="math inline">\(QK^T\)</span>)이 커짐에 따라 소프트맥스 함수의 입력값이 과도하게 커지는 현상을 방지합니다. 이는 기울기 소실(gradient vanishing) 문제를 완화하고, 학습을 안정적으로 만들어 모델의 성능을 향상시키는 데 기여합니다.</li>
</ul></li>
</ul></li>
</ul>
<hr>
<p><strong>각 헤드의 역할과 멀티헤드 어텐션의 장점</strong></p>
<p>멀티헤드 어텐션은 비유하자면, 여러 개의 “작은 렌즈”를 사용하여 대상을 다양한 각도에서 관찰하는 것과 같습니다. 각 헤드는 독립적으로 쿼리(Q), 키(K), 값(V)을 변환하고 어텐션 계산을 수행합니다. 이를 통해 전체 입력 시퀀스 내에서 서로 다른 부분 공간(subspace)에 집중하여 정보를 추출합니다.</p>
<ul>
<li><strong>다양한 관계 포착</strong>: 각 헤드는 서로 다른 종류의 언어적 관계를 학습하는 데 특화될 수 있습니다. 예를 들어, 어떤 헤드는 주어-동사 관계에, 다른 헤드는 형용사-명사 관계, 또 다른 헤드는 대명사와 그 선행사 간의 관계 등에 집중할 수 있습니다.</li>
<li><strong>계산 효율성</strong>: 각 헤드는 비교적 작은 차원(d_k)에서 어텐션을 계산합니다. 이는 하나의 큰 차원에서 어텐션을 계산하는 것보다 계산 비용 측면에서 효율적입니다.</li>
<li><strong>병렬 처리</strong>: 각 헤드의 계산은 서로 독립적입니다. 따라서 GPU를 활용한 병렬 처리가 가능하며, 이는 계산 속도를 획기적으로 높여줍니다.</li>
</ul>
<p><strong>실제 분석 사례</strong></p>
<p>연구 결과들은 멀티헤드 어텐션의 각 헤드가 실제로 서로 다른 언어적 특징을 포착한다는 것을 보여줍니다. 예를 들어, <a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://arxiv.org/abs/1906.04341">“What does BERT Look At? An Analysis of BERT’s Attention”</a> 논문에서는 BERT 모델의 멀티헤드 어텐션을 분석한 결과, 일부 헤드는 문장의 구문(syntactic) 구조 파악에, 다른 헤드는 단어 간의 의미(semantic) 유사성 파악에 더 중요한 역할을 한다는 것을 밝혀냈습니다.</p>
<hr>
<p><strong>수학적 표현</strong></p>
<ul>
<li><strong>전체</strong>: <span class="math inline">\(\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \ldots, \text{head}_h)W^O\)</span></li>
<li><strong>각 헤드</strong>: <span class="math inline">\(\text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)\)</span></li>
<li><strong>어텐션 함수</strong>: <span class="math inline">\(\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V\)</span></li>
</ul>
<p><strong>표기 설명</strong>:</p>
<ul>
<li><span class="math inline">\(h\)</span>: 헤드의 개수</li>
<li><span class="math inline">\(W_i^Q \in \mathbb{R}^{d_{\text{model}} \times d_k}\)</span>: i번째 헤드의 Query 변환 행렬</li>
<li><span class="math inline">\(W_i^K \in \mathbb{R}^{d_{\text{model}} \times d_k}\)</span>: i번째 헤드의 Key 변환 행렬</li>
<li><span class="math inline">\(W_i^V \in \mathbb{R}^{d_{\text{model}} \times d_v}\)</span>: i번째 헤드의 Value 변환 행렬</li>
<li><span class="math inline">\(W^O \in \mathbb{R}^{hd_v \times d_{\text{model}}}\)</span>: 최종 출력을 위한 선형 변환 행렬</li>
</ul>
<p><strong>최종 선형 변환(<span class="math inline">\(W^O\)</span>)의 중요성</strong>: 각 헤드의 출력을 단순 연결(Concat)한 후, 원래의 임베딩 차원(<span class="math inline">\(d_{model}\)</span>)으로 투영하는 추가적인 선형 변환(<span class="math inline">\(W^O\)</span>)은 매우 중요한 역할을 수행합니다.</p>
<ul>
<li><strong>정보 통합</strong>: 서로 다른 헤드에서 추출된 다양한 관점의 정보를 균형 있고 안정적으로 통합하여, 전체적인 문맥 정보를 풍부하게 표현합니다.</li>
<li><strong>최적 조합</strong>: 학습 과정을 통해 각 헤드의 정보를 어떻게 조합하는 것이 가장 효과적인지를 스스로 학습합니다. 이는 마치 앙상블 모델에서 각 개별 모델의 예측을 단순 평균하는 것이 아니라, 학습된 가중치를 사용하여 조합하는 것과 유사한 원리입니다.</li>
</ul>
<hr>
<p><strong>결론</strong></p>
<p>멀티헤드 어텐션은 트랜스포머 모델이 입력 시퀀스의 문맥 정보를 효율적으로 포착하고, GPU를 이용한 병렬 처리를 통해 계산 속도를 높이는 데 핵심적인 역할을 수행하는 메커니즘입니다. 이를 통해 트랜스포머는 다양한 자연어 처리 task에서 뛰어난 성능을 보여주고 있습니다.</p>
</section>
<section id="병렬-학습을-위한-마스킹-전략" class="level3">
<h3 class="anchored" data-anchor-id="병렬-학습을-위한-마스킹-전략">8.2.5 병렬 학습을 위한 마스킹 전략</h3>
<p>멀티헤드 어텐션을 구현한 후 연구팀은 실제 학습 과정에서 중요한 문제에 직면했습니다. 바로 모델이 미래의 단어를 참조하여 현재 단어를 예측하는 <strong>“정보 유출(information leakage)”</strong> 현상이었습니다. 예를 들어, “The cat ___ on the mat”이라는 문장에서 빈칸을 예측할 때, 모델이 뒤에 나오는 “mat”이라는 단어를 미리 보고 “sits”를 쉽게 예측할 수 있었습니다.</p>
<p><strong>마스킹의 필요성: 정보 유출 방지</strong></p>
<p>이러한 정보 유출은 모델이 실제 추론 능력을 개발하는 것이 아니라, 단순히 정답을 “엿보는” 결과를 초래합니다. 모델이 훈련 데이터에서는 높은 성능을 보이지만, 실제 새로운 데이터(미래 시점의 데이터)에 대해서는 제대로 예측하지 못하는 문제가 발생합니다.</p>
<p>연구팀은 이 문제를 해결하기 위해 정교하게 설계된 <strong>마스킹(masking)</strong> 전략을 도입했습니다. 트랜스포머에서는 두 가지 종류의 마스크가 사용됩니다.</p>
<ol type="1">
<li><strong>인과 관계 마스크(Causal Mask, Look-Ahead Mask):</strong> 자기 회귀(autoregressive) 모델에서 미래 시점의 정보를 참조하지 못하도록 차단합니다.</li>
<li><strong>패딩 마스크(Padding Mask):</strong> 가변 길이 시퀀스를 처리할 때, 의미 없는 패딩 토큰(padding token)의 영향을 제거합니다.</li>
</ol>
<p><strong>1. 인과 관계 마스크 (Causal Mask)</strong></p>
<p>인과 관계 마스크는 미래 정보를 가리는 역할을 합니다. 아래 코드를 실행하면 어텐션 스코어 행렬에서 미래 정보에 해당하는 부분이 어떻게 마스킹되는지 시각적으로 확인할 수 있습니다.</p>
<div id="cell-28" class="cell" data-execution_count="4">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_08.visualize_masking <span class="im">import</span> visualize_causal_mask</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>visualize_causal_mask()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>1. Original attention score matrix:
                       I        love        deep    learning
I           [      0.90][      0.70][      0.30][      0.20]
love        [      0.60][      0.80][      0.90][      0.40]
deep        [      0.20][      0.50][      0.70][      0.90]
learning    [      0.40][      0.30][      0.80][      0.60]

Each row represents the attention scores from the current position to all positions
--------------------------------------------------

2. Lower triangular mask (1: allowed, 0: blocked):
                       I        love        deep    learning
I           [      1.00][      0.00][      0.00][      0.00]
love        [      1.00][      1.00][      0.00][      0.00]
deep        [      1.00][      1.00][      1.00][      0.00]
learning    [      1.00][      1.00][      1.00][      1.00]

Only the diagonal and below are 1, the rest are 0
--------------------------------------------------

3. Mask converted to -inf:
                       I        love        deep    learning
I           [   1.0e+00][      -inf][      -inf][      -inf]
love        [   1.0e+00][   1.0e+00][      -inf][      -inf]
deep        [   1.0e+00][   1.0e+00][   1.0e+00][      -inf]
learning    [   1.0e+00][   1.0e+00][   1.0e+00][   1.0e+00]

Converting 0 to -inf so that it becomes 0 after softmax
--------------------------------------------------

4. Attention scores with mask applied:
                       I        love        deep    learning
I           [       1.9][      -inf][      -inf][      -inf]
love        [       1.6][       1.8][      -inf][      -inf]
deep        [       1.2][       1.5][       1.7][      -inf]
learning    [       1.4][       1.3][       1.8][       1.6]

Future information (upper triangle) is masked with -inf
--------------------------------------------------

5. Final attention weights (after softmax):
                       I        love        deep    learning
I           [      1.00][      0.00][      0.00][      0.00]
love        [      0.45][      0.55][      0.00][      0.00]
deep        [      0.25][      0.34][      0.41][      0.00]
learning    [      0.22][      0.20][      0.32][      0.26]

The sum of each row becomes 1, and future information is masked to 0</code></pre>
</div>
</div>
<p><strong>시퀀스 처리 구조와 행렬</strong></p>
<p>미래 정보가 왜 상삼각 행렬 형태가 되는지 “I love deep learning” 문장을 예로 들어 설명하겠습니다. 단어 순서는 [I(0), love(1), deep(2), learning(3)]입니다. 어텐션 스코어 행렬(<span class="math inline">\(QK^T\)</span>)에서 행과 열은 모두 이 단어 순서를 따릅니다.</p>
<div id="cell-30" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>attention_scores <span class="op">=</span> [</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>    [<span class="fl">0.9</span>, <span class="fl">0.7</span>, <span class="fl">0.3</span>, <span class="fl">0.2</span>],  <span class="co"># I -&gt; I, love, deep, learning</span></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>    [<span class="fl">0.6</span>, <span class="fl">0.8</span>, <span class="fl">0.9</span>, <span class="fl">0.4</span>],  <span class="co"># love -&gt; I, love, deep, learning</span></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>    [<span class="fl">0.2</span>, <span class="fl">0.5</span>, <span class="fl">0.7</span>, <span class="fl">0.9</span>],  <span class="co"># deep -&gt; I, love, deep, learning</span></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>    [<span class="fl">0.4</span>, <span class="fl">0.3</span>, <span class="fl">0.8</span>, <span class="fl">0.6</span>]   <span class="co"># learning -&gt; I, love, deep, learning</span></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<ul>
<li>Q의 각 행은 현재 처리 중인 단어의 쿼리 벡터입니다.</li>
<li>K의 각 열(K를 전치했으므로)은 참조될 단어의 키 벡터입니다.</li>
</ul>
<p>위 행렬을 해석하면</p>
<ol type="1">
<li>1행(I): [I] → [I, love, deep, learning]과의 관계</li>
<li>2행(love): [love] → [I, love, deep, learning]과의 관계</li>
<li>3행(deep): [deep] → [I, love, deep, learning]과의 관계</li>
<li>4행(learning): [learning] → [I, love, deep, learning]과의 관계</li>
</ol>
<p>“deep”이라는 단어를 처리할 때(3행)</p>
<ul>
<li>참조 가능: [I, love, deep] (현재까지 나온 단어들)</li>
<li>참조 불가: [learning] (아직 나오지 않은 미래의 단어)</li>
</ul>
<p>따라서 행을 기준으로 보면, 해당 열 단어의 미래 단어(미래 정보)는 해당 위치의 오른쪽, 즉 <strong>상삼각(upper triangular)</strong> 부분이 됩니다. 반대로 참조 가능한 단어는 <strong>하삼각(lower triangular)</strong>이 됩니다.</p>
<p>인과 관계 마스크는 하삼각 부분을 1. 상삼각 부분을 0으로 만든 후, 상삼각의 0을 <span class="math inline">\(-\infty\)</span>로 바꿉니다. 2. <span class="math inline">\(-\infty\)</span>는 소프트맥스 함수를 통과하면 0이 됩니다. 마스크 행렬은 단순히 어텐션 스코어 행렬과 더해집니다. 결과적으로, 소프트맥스가 적용된 어텐션 스코어 행렬에서는 미래 정보가 0으로 바뀌어 차단됩니다.</p>
<p><strong>2. 패딩 마스크 (Padding Mask)</strong></p>
<p>자연어 처리에서는 문장의 길이가 서로 다릅니다. 배치(batch) 처리를 위해서는 모든 문장을 같은 길이로 맞춰야 하는데, 이때 짧은 문장의 빈 공간은 패딩 토큰(PAD)으로 채웁니다. 하지만 이 패딩 토큰은 의미가 없으므로, 어텐션 계산에 포함되면 안 됩니다.</p>
<div id="cell-32" class="cell" data-execution_count="14">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_08.visualize_masking <span class="im">import</span> visualize_padding_mask</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>visualize_padding_mask()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
2. Create padding mask (1: valid token, 0: padding token):
tensor([[[1., 1., 1., 1.]],

        [[1., 1., 1., 0.]],

        [[1., 1., 1., 1.]],

        [[1., 1., 1., 1.]]])

Positions that are not padding (0) are 1, padding positions are 0
--------------------------------------------------

3. Original attention scores (first sentence):
                       I        love        deep    learning
I           [      0.90][      0.70][      0.30][      0.20]
love        [      0.60][      0.80][      0.90][      0.40]
deep        [      0.20][      0.50][      0.70][      0.90]
learning    [      0.40][      0.30][      0.80][      0.60]

Attention scores at each position
--------------------------------------------------

4. Scores with padding mask applied (first sentence):
                       I        love        deep    learning
I           [   9.0e-01][   7.0e-01][   3.0e-01][   2.0e-01]
love        [   6.0e-01][   8.0e-01][   9.0e-01][   4.0e-01]
deep        [   2.0e-01][   5.0e-01][   7.0e-01][   9.0e-01]
learning    [   4.0e-01][   3.0e-01][   8.0e-01][   6.0e-01]

The scores at padding positions are masked with -inf
--------------------------------------------------

5. Final attention weights (first sentence):
                       I        love        deep    learning
I           [      0.35][      0.29][      0.19][      0.17]
love        [      0.23][      0.28][      0.31][      0.19]
deep        [      0.17][      0.22][      0.27][      0.33]
learning    [      0.22][      0.20][      0.32][      0.26]

The weights at padding positions become 0, and the sum of the weights at the remaining positions is 1</code></pre>
</div>
</div>
<p>다음과 같은 문장들을 예로 들어보겠습니다.</p>
<ul>
<li>“I love ML” → [I, love, ML, PAD]</li>
<li>“Deep learning is fun” → [Deep, learning, is, fun]</li>
</ul>
<p>여기서 첫 번째 문장은 3개의 단어만 있어서 마지막을 PAD로 채웠습니다. 패딩 마스크는 이러한 PAD 토큰의 영향을 제거합니다. 실제 단어는 1, 패딩 토큰은 0으로 표시하는 마스크를 생성하고, 2. 패딩 위치의 어텐션 스코어를 <span class="math inline">\(-\infty\)</span>로 만들어 소프트맥스 통과 후 0이 되게 합니다.</p>
<p>결과적으로 다음과 같은 효과를 얻습니다.</p>
<ol type="1">
<li>실제 단어들은 서로 자유롭게 어텐션을 주고받을 수 있습니다.</li>
<li>패딩 토큰은 어텐션 계산에서 완전히 제외됩니다.</li>
<li>각 문장의 실제 의미 있는 부분만으로 문맥이 형성됩니다.</li>
</ol>
<div id="cell-34" class="cell" data-execution_count="2">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_attention_mask(size):</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create a lower triangular matrix (including the diagonal)</span></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>    mask <span class="op">=</span> torch.tril(torch.ones(size, size))</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Mask with -inf (becomes 0 after softmax)</span></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>    mask <span class="op">=</span> mask.masked_fill(mask <span class="op">==</span> <span class="dv">0</span>, <span class="bu">float</span>(<span class="st">'-inf'</span>))</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> mask</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> masked_attention(Q, K, V, mask):</span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate attention scores</span></span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>    scores <span class="op">=</span> torch.matmul(Q, K.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>))</span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Apply mask</span></span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>    scores <span class="op">=</span> scores <span class="op">+</span> mask</span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Apply softmax</span></span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a>    weights <span class="op">=</span> F.softmax(scores, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate final attention output</span></span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> torch.matmul(weights, V)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><strong>마스킹 전략의 혁신과 영향</strong></p>
<p>연구팀이 개발한 두 가지 마스킹 전략(패딩 마스크, 인과 관계 마스크)은 트랜스포머의 학습 과정을 더욱 견고하게 만들었고, 이후 GPT와 같은 자기 회귀 모델의 기초가 되었습니다. 특히 인과 관계 마스크는 언어 모델이 실제 인간의 언어 이해 과정과 유사하게 순차적으로 문맥을 파악하도록 유도했습니다.</p>
<p><strong>구현의 효율성</strong></p>
<p>마스킹은 어텐션 스코어 계산 직후, 소프트맥스 함수 적용 <em>전에</em> 수행됩니다. <span class="math inline">\(-\infty\)</span> 값으로 마스킹된 위치는 소프트맥스 함수를 통과하면서 0이 되어, 해당 위치의 정보가 완전히 차단됩니다. 이는 계산 효율성과 메모리 사용 측면에서도 최적화된 접근 방식입니다.</p>
<p>이러한 마스킹 전략의 도입으로 트랜스포머는 진정한 의미의 병렬 학습이 가능해졌고, 이는 현대 언어 모델의 발전에 큰 영향을 미쳤습니다.</p>
</section>
<section id="헤드-의미의-변천-머리에서-뇌로" class="level3">
<h3 class="anchored" data-anchor-id="헤드-의미의-변천-머리에서-뇌로">8.2.6 헤드 의미의 변천: “머리”에서 “뇌”로</h3>
<p>딥러닝에서 “헤드(head)”라는 용어는 신경망 아키텍처의 발전과 함께 그 의미가 점진적으로, 그리고 근본적으로 변화해 왔습니다. 초기에는 주로 “출력층에 가까운 부분”이라는 비교적 단순한 의미로 사용되었지만, 최근에는 모델의 특정 기능을 담당하는 “독립적인 모듈”이라는 더 추상적이고 복합적인 의미로 확장되었습니다.</p>
<ol type="1">
<li><p><strong>초기: “출력층 근처”</strong></p>
<p>초창기 딥러닝 모델(예: 단순한 다층 퍼셉트론(MLP))에서 “헤드”는 일반적으로 특징 추출기(feature extractor, backbone)를 통과한 특성 벡터를 입력받아, 최종 예측(분류, 회귀 등)을 수행하는 네트워크의 마지막 부분을 가리켰습니다. 이 경우, 헤드는 주로 완전 연결 레이어(fully connected layer)와 활성화 함수(activation function)로 구성되었습니다.</p></li>
</ol>
<div id="cell-37" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SimpleModel(nn.Module):</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, num_classes):</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.backbone <span class="op">=</span> nn.Sequential( <span class="co"># Feature extractor</span></span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">784</span>, <span class="dv">128</span>),</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">128</span>, <span class="dv">64</span>),</span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>            nn.ReLU()</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.head <span class="op">=</span> nn.Linear(<span class="dv">64</span>, num_classes)  <span class="co"># Head (output layer)</span></span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a>        features <span class="op">=</span> <span class="va">self</span>.backbone(x)</span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> <span class="va">self</span>.head(features)</span>
<span id="cb21-15"><a href="#cb21-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> output</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<ol start="2" type="1">
<li><p><strong>다중 태스크 학습: “작업별 분기”</strong></p>
<p>이미지넷(ImageNet)과 같은 대규모 데이터셋을 사용한 딥러닝 모델이 발전하면서, 하나의 특징 추출기로부터 여러 개의 헤드가 분기되어 서로 다른 태스크를 수행하는 다중 태스크 학습(multi-task learning)이 등장했습니다. 예를 들어, 객체 탐지(object detection) 모델에서는 이미지로부터 객체의 종류를 분류(classification)하는 헤드와 객체의 위치를 나타내는 경계 상자(bounding box)를 예측(regression)하는 헤드가 동시에 사용됩니다.</p></li>
</ol>
<div id="cell-39" class="cell" data-execution_count="3">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MultiTaskModel(nn.Module):</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, num_classes):</span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.backbone <span class="op">=</span> ResNet50()  <span class="co"># Feature extractor (ResNet)</span></span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.classification_head <span class="op">=</span> nn.Linear(<span class="dv">2048</span>, num_classes)  <span class="co"># Classification head</span></span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bbox_head <span class="op">=</span> nn.Linear(<span class="dv">2048</span>, <span class="dv">4</span>)  <span class="co"># Bounding box regression head</span></span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a>        features <span class="op">=</span> <span class="va">self</span>.backbone(x)</span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a>        class_output <span class="op">=</span> <span class="va">self</span>.classification_head(features)</span>
<span id="cb22-15"><a href="#cb22-15" aria-hidden="true" tabindex="-1"></a>        bbox_output <span class="op">=</span> <span class="va">self</span>.bbox_head(features)</span>
<span id="cb22-16"><a href="#cb22-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> class_output, bbox_output</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<ol start="3" type="1">
<li><p><strong>Attention is All You Need 논문(트랜스포머)에서의 “헤드”개념</strong>:</p>
<p>트랜스포머의 멀티 헤드 어텐션은 한걸음 더 나아갔습니다. 트랜스포머에서는 더이상 “헤드 = 출력에 가까운 부분”이라는 고정관념을 따르지 않습니다.</p></li>
</ol>
<div id="cell-41" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MultiHeadAttention(nn.Module):</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, num_heads):</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.heads <span class="op">=</span> nn.ModuleList([</span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>            AttentionHead() <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_heads)  <span class="co"># num_heads개의 독립적인 어텐션 헤드</span></span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>        ])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<ul>
<li><strong>독립적인 모듈:</strong> 여기서 각 “헤드”는 입력을 받아 독립적으로 어텐션 메커니즘을 수행하는 <em>별개의 모듈</em>입니다. 각 헤드는 서로 다른 가중치를 가지며, 입력 시퀀스의 서로 다른 측면에 주목합니다.</li>
<li><strong>병렬 처리:</strong> 여러 개의 헤드가 <em>병렬로</em> 작동하여, 다양한 종류의 정보를 동시에 처리합니다.</li>
<li><strong>중간 처리 단계:</strong> 헤드는 더 이상 출력층에만 국한되지 않습니다. 트랜스포머의 인코더와 디코더는 <em>여러 층의 멀티헤드 어텐션</em>으로 구성되며, 각 층의 헤드는 입력 시퀀스의 서로 다른 표현(representation)을 학습합니다.</li>
</ul>
<ol start="4" type="1">
<li><p><strong>최근 트렌드: “기능 모듈”</strong></p>
<p>최근 딥러닝 모델에서는 “헤드”라는 용어가 더욱 유연하게 사용됩니다. 반드시 출력층 근처가 아니더라도, 특정 기능을 수행하는 독립적인 모듈을 “헤드”라고 지칭하는 경우가 많아졌습니다.</p>
<ul>
<li><strong>언어 모델 (Language Models):</strong> BERT, GPT 등 대규모 언어 모델에서는 “language modeling head”, “masked language modeling head”, “next sentence prediction head” 등 다양한 종류의 헤드가 사용됩니다.</li>
<li><strong>비전 트랜스포머 (Vision Transformers):</strong> ViT에서는 이미지를 패치(patch)로 나누고, 각 패치를 토큰처럼 처리하는 “patch embedding head”가 사용됩니다.</li>
</ul></li>
</ol>
<p><strong>결론</strong></p>
<p>딥러닝에서 “헤드”의 의미는 “단순히 출력에 가까운 부분”에서 “특정 기능을 수행하는 독립적인 모듈(병렬적, 중간 처리 포함)”로 진화했습니다. 이러한 변화는 딥러닝 아키텍처가 더욱 복잡하고 정교해짐에 따라, 모델의 각 부분이 더 세분화되고 전문화되는 경향을 반영합니다. 트랜스포머의 멀티헤드 어텐션은 이러한 의미 변화의 대표적인 예시이며, “헤드”라는 용어가 더 이상 “머리”가 아닌, 여러 개의 “뇌”처럼 작동하는 것을 보여줍니다.</p>
</section>
</section>
<section id="위치-정보의-처리" class="level2">
<h2 class="anchored" data-anchor-id="위치-정보의-처리">8.3 위치 정보의 처리</h2>
<p><strong>도전과제:</strong> RNN 없이 어떻게 단어의 순서 정보를 효과적으로 표현할 수 있을까?</p>
<p><strong>연구자의 고뇌:</strong> 트랜스포머는 RNN처럼 순차적으로 데이터를 처리하지 않기 때문에, 단어의 위치 정보를 명시적으로 알려줘야 했습니다. 연구자들은 다양한 방법(위치 인덱스, 학습 가능한 임베딩 등)을 시도했지만, 만족스러운 결과를 얻지 못했습니다. 마치 암호문을 해독하듯, 위치 정보를 효과적으로 표현할 수 있는 새로운 방법을 찾아야 했습니다.</p>
<p>트랜스포머는 RNN과 달리 순환 구조나 합성곱 연산을 사용하지 않으므로, 시퀀스의 순서 정보를 별도로 제공해야 했습니다. “dog bites man”과 “man bites dog”는 단어는 같지만 순서에 따라 의미가 완전히 달라지기 때문입니다. 어텐션 연산(<span class="math inline">\(QK^T\)</span>) 자체는 단어 벡터 간의 유사도를 계산할 뿐, 단어의 위치 정보는 고려하지 않기 때문에, 연구팀은 위치 정보를 모델에 주입하는 방법을 고민해야 했습니다. 이는 RNN 없이 어떻게 단어의 순서 정보를 효과적으로 표현할 수 있을까? 라는 <strong>도전 과제</strong>였습니다.</p>
<section id="순차-정보의-중요성" class="level3">
<h3 class="anchored" data-anchor-id="순차-정보의-중요성">8.3.1 순차 정보의 중요성</h3>
<p>연구팀은 다양한 포지셔널 인코딩 방법을 고민했습니다.</p>
<ol type="1">
<li><strong>위치 인덱스 직접 사용:</strong> 가장 단순한 접근 방식은 각 단어의 위치 인덱스(0, 1, 2, …)를 임베딩 벡터에 더하는 것입니다.</li>
</ol>
<div id="cell-44" class="cell" data-execution_count="15">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_08.visualize_positional_embedding <span class="im">import</span> visualize_position_embedding</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>visualize_position_embedding()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>1. Original embedding matrix:
                dim1      dim2      dim3      dim4
I         [    0.20][    0.30][    0.10][    0.40]
love      [    0.50][    0.20][    0.80][    0.10]
deep      [    0.30][    0.70][    0.20][    0.50]
learning  [    0.60][    0.40][    0.30][    0.20]

Each row is the embedding vector of a word
--------------------------------------------------

2. Position indices:
[0 1 2 3]

Indices representing the position of each word (starting from 0)
--------------------------------------------------

3. Embeddings with position information added:
                dim1      dim2      dim3      dim4
I         [    0.20][    0.30][    0.10][    0.40]
love      [    1.50][    1.20][    1.80][    1.10]
deep      [    2.30][    2.70][    2.20][    2.50]
learning  [    3.60][    3.40][    3.30][    3.20]

Result of adding position indices to each embedding vector (broadcasting)
--------------------------------------------------

4. Changes due to adding position information:

I (0):
  Original:     [0.2 0.3 0.1 0.4]
  Pos. Added: [0.2 0.3 0.1 0.4]
  Difference:     [0. 0. 0. 0.]

love (1):
  Original:     [0.5 0.2 0.8 0.1]
  Pos. Added: [1.5 1.2 1.8 1.1]
  Difference:     [1. 1. 1. 1.]

deep (2):
  Original:     [0.3 0.7 0.2 0.5]
  Pos. Added: [2.3 2.7 2.2 2.5]
  Difference:     [2. 2. 2. 2.]

learning (3):
  Original:     [0.6 0.4 0.3 0.2]
  Pos. Added: [3.6 3.4 3.3 3.2]
  Difference:     [3. 3. 3. 3.]</code></pre>
</div>
</div>
<p>하지만 이 방식에는 두 가지 문제점이 있었습니다.</p>
<ul>
<li><strong>학습 데이터보다 긴 시퀀스 처리 불가:</strong> 학습 시 보지 못한 위치(예: 100번째)가 입력으로 들어오면 적절한 표현을 찾을 수 없습니다.</li>
<li><strong>상대적 거리 정보 표현 어려움:</strong> 위치 2와 4의 거리가 위치 102와 104의 거리와 같다는 것을 표현하기 어렵습니다.</li>
</ul>
<ol start="2" type="1">
<li><strong>학습 가능한 위치 임베딩:</strong> 각 위치에 대해 학습 가능한 임베딩 벡터를 사용하는 방법도 고려되었습니다.</li>
</ol>
<div id="cell-46" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Conceptual code</span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>    positional_embeddings <span class="op">=</span> nn.Embedding(max_seq_length, embedding_dim)</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>    positions <span class="op">=</span> torch.arange(seq_length)</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>    positional_encoding <span class="op">=</span> positional_embeddings(positions)</span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>    final_embedding <span class="op">=</span> word_embedding <span class="op">+</span> positional_encoding</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<pre><code>이 방식은 위치별로 고유한 표현을 학습할 수 있지만, 학습 데이터보다 긴 시퀀스를 처리할 수 없다는 근본적인 한계는 여전했습니다.</code></pre>
<p><strong>위치 정보 표현을 위한 핵심 조건</strong></p>
<p>연구팀은 위와 같은 시행착오를 통해, 위치 정보 표현이 다음 세 가지 핵심 조건을 충족해야 함을 깨달았습니다.</p>
<ol type="1">
<li><strong>시퀀스 길이 제한 없음:</strong> 학습 시 보지 못한 위치(예: 1000번째)도 적절하게 표현할 수 있어야 합니다.</li>
<li><strong>상대적 거리 관계 표현:</strong> 위치 2와 4의 거리가 위치 102와 104의 거리와 동일하게 표현되어야 합니다. 즉, 위치 간의 상대적인 거리가 보존되어야 합니다.</li>
<li><strong>어텐션 연산과의 호환성:</strong> 위치 정보가 어텐션 가중치 계산을 방해하지 않으면서도, 순서 정보를 효과적으로 전달해야 합니다.</li>
</ol>
</section>
<section id="포지셔널-인코딩의-설계" class="level3">
<h3 class="anchored" data-anchor-id="포지셔널-인코딩의-설계">8.3.2 포지셔널 인코딩의 설계</h3>
<p>이러한 고민 끝에, 연구팀은 사인(sin)과 코사인(cos) 함수의 주기적인 특성을 활용한 <strong>포지셔널 인코딩(Positional Encoding)</strong>이라는 독창적인 해결책을 발견했습니다.</p>
<p><strong>사인-코사인 함수 기반 포지셔널 인코딩의 원리</strong></p>
<p>각 위치를 서로 다른 주파수(frequency)의 사인과 코사인 함수를 사용하여 인코딩하면, 위치 간의 상대적인 거리가 자연스럽게 표현됩니다.</p>
<div id="cell-48" class="cell" data-execution_count="17">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_08.positional_encoding_utils <span class="im">import</span> visualize_sinusoidal_features</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>visualize_sinusoidal_features()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="08_트랜스포머의 탄생_files/figure-html/cell-21-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>3번은 위치의 이동을 시각화한 그림입니다. 사인 함수로 어떻게 위치 관계를 표현하는지 보여줍니다. 두 번째 조건인 “상대적 거리 관계 표현”을 만족합니다. 모든 시프트된 곡선들이 원본 곡선과 동일한 모양을 유지하면서 일정한 간격을 유지합니다. 이는 위치 간의 거리가 같다면(예: 2→7과 102→107) 그 관계도 동일하게 표현됩니다.</p>
<p>4번은 포지셔널 인코딩 히트맵 (Positional Encoding Matrix)입니다. 위치(세로축)가 어떤 고유한 패턴(가로축)을 가지는지 보여줍니다. 가로축의 열은 서로 다른 주기의 사인/코사인 함수를 나타내며, 오른쪽으로 갈 수록 긴 주기입니다. 행(위치)별로 빨간색(양수)과 파란색(음수)이 만드는 독특한 패턴이 만들어집니다. 짧은 주기부터 긴 주기까지 다양한 주파수를 사용하서 위치마다 고유한 패턴이 만들어집니다. 이 접근 방식은 첫 번째 조건인 “시퀀스 길이 제한 없음”을 충족합니다. 서로 다른 주기의 사인/코사인 함수들을 조합해서 수학적으로 무한한 위치까지 고유값을 생성할 수 있습니다.</p>
<p>이 수학적 특징을 이용해서 연구팀은 위치 인코딩 알고리즘을 다음과 같이 구현했습니다.</p>
<p><strong>포지셔널 인코딩 구현</strong></p>
<div id="cell-50" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> positional_encoding(seq_length, d_model):</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 1. 위치별 인코딩 행렬 생성</span></span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>    position <span class="op">=</span> np.arange(seq_length)[:, np.newaxis]  <span class="co"># [0, 1, 2, ..., seq_length-1]</span></span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 2. 각 차원별 주기 계산</span></span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a>    div_term <span class="op">=</span> np.exp(np.arange(<span class="dv">0</span>, d_model, <span class="dv">2</span>) <span class="op">*</span> <span class="op">-</span>(np.log(<span class="fl">10000.0</span>) <span class="op">/</span> d_model))</span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 예: d_model=512일 때</span></span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># div_term[0] ≈ 1.0        (가장 짧은 주기)</span></span>
<span id="cb29-9"><a href="#cb29-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># div_term[256] ≈ 0.0001   (가장 긴 주기)</span></span>
<span id="cb29-10"><a href="#cb29-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb29-11"><a href="#cb29-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 3. 짝수/홀수 차원에 사인/코사인 적용</span></span>
<span id="cb29-12"><a href="#cb29-12" aria-hidden="true" tabindex="-1"></a>    pe <span class="op">=</span> np.zeros((seq_length, d_model))</span>
<span id="cb29-13"><a href="#cb29-13" aria-hidden="true" tabindex="-1"></a>    pe[:, <span class="dv">0</span>::<span class="dv">2</span>] <span class="op">=</span> np.sin(position <span class="op">*</span> div_term)  <span class="co"># 짝수 차원</span></span>
<span id="cb29-14"><a href="#cb29-14" aria-hidden="true" tabindex="-1"></a>    pe[:, <span class="dv">1</span>::<span class="dv">2</span>] <span class="op">=</span> np.cos(position <span class="op">*</span> div_term)  <span class="co"># 홀수 차원</span></span>
<span id="cb29-15"><a href="#cb29-15" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb29-16"><a href="#cb29-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> pe</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<ul>
<li><code>position</code>: <code>[0, 1, 2, ..., seq_length-1]</code> 형태의 배열. 각 단어의 위치 인덱스를 나타냅니다.</li>
<li><code>div_term</code>: 각 차원별 주기를 결정하는 값. <code>d_model</code>이 커질수록 주기는 길어집니다.</li>
<li><code>pe[:, 0::2] = np.sin(position * div_term)</code>: 짝수 인덱스 차원에는 사인 함수를 적용.</li>
<li><code>pe[:, 1::2] = np.cos(position * div_term)</code>: 홀수 인덱스 차원에는 코사인 함수를 적용.</li>
</ul>
<p><strong>수학적 표현</strong></p>
<p>포지셔널 인코딩의 각 차원은 다음 공식으로 계산됩니다.</p>
<ul>
<li><span class="math inline">\(PE_{(pos, 2i)} = \sin(pos / 10000^{2i/d_{\text{model}}})\)</span></li>
<li><span class="math inline">\(PE_{(pos, 2i+1)} = \cos(pos / 10000^{2i/d_{\text{model}}})\)</span></li>
</ul>
<p>여기서</p>
<ul>
<li><span class="math inline">\(pos\)</span>: 단어의 위치 (0, 1, 2, …)</li>
<li><span class="math inline">\(i\)</span>: 차원 인덱스 (0, 1, 2, …, <span class="math inline">\(d_{model}\)</span>-1)</li>
<li><span class="math inline">\(d_{model}\)</span>: 임베딩 차원 (및 포지셔널 인코딩 차원)</li>
</ul>
<p><strong>주기 변화 확인</strong></p>
<div id="cell-52" class="cell" data-execution_count="18">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_08.positional_encoding_utils <span class="im">import</span> show_positional_periods</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>show_positional_periods()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>1. Periods of positional encoding:
First dimension (i=0): 1.00
Middle dimension (i=128): 100.00
Last dimension (i=255): 9646.62

2. Positional encoding formula values (10000^(2i/d_model)):
i=  0: 1.0000000000
i=128: 100.0000000000
i=255: 9646.6161991120

3. Actual div_term values (first/middle/last):
First (i=0): 1.0000000000
Middle (i=128): 0.0100000000
Last (i=255): 0.0001036633</code></pre>
</div>
</div>
<p>여기서 핵심은 3단계 입니다.</p>
<div id="cell-54" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 3. 짝수/홀수 차원에 사인/코사인 적용</span></span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>    pe <span class="op">=</span> np.zeros((seq_length, d_model))</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a>    pe[:, <span class="dv">0</span>::<span class="dv">2</span>] <span class="op">=</span> np.sin(position <span class="op">*</span> div_term)  <span class="co"># 짝수 차원</span></span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a>    pe[:, <span class="dv">1</span>::<span class="dv">2</span>] <span class="op">=</span> np.cos(position <span class="op">*</span> div_term)  <span class="co"># 홀수 차원</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>위 결과는 차원에 따른 주기의 변화를 보여줍니다.</p>
<p><strong>최종 임베딩</strong></p>
<p>생성된 포지셔널 인코딩 <code>pe</code>는 (seq_length, d_model) 형태를 가지며, 원래의 단어 임베딩 행렬(sentence_embedding)과 더해져 최종 임베딩을 생성합니다.</p>
<div id="cell-56" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>final_embedding <span class="op">=</span> sentence_embedding <span class="op">+</span> positional_encoding</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>이렇게 더해진 최종 임베딩은 단어의 의미와 위치 정보를 모두 포함하게 됩니다. 예를 들어 “bank”라는 단어는 문장 내 위치에 따라 서로 다른 최종 벡터값을 가지게 되어, “은행”과 “강둑”의 의미를 구분하는 데 도움을 줍니다.</p>
<p>이로써 트랜스포머는 RNN 없이도 순차 정보를 효과적으로 처리할 수 있게 되었고, 병렬 처리의 장점을 최대한 활용할 수 있는 기반을 마련했습니다.</p>
<div class="callout callout-style-default callout-note callout-titled" title="클릭하여 내용 보기 (딥 다이브 : 포지셔널 인코딩의 진화, 최신 기법, 그리고 수학적 기반))">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
클릭하여 내용 보기 (딥 다이브 : 포지셔널 인코딩의 진화, 최신 기법, 그리고 수학적 기반))
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<section id="포지셔널-인코딩의-진화-최신-기법-그리고-수학적-기반" class="level3 callout-body-container callout-body">
<h3 class="anchored" data-anchor-id="포지셔널-인코딩의-진화-최신-기법-그리고-수학적-기반">포지셔널 인코딩의 진화, 최신 기법, 그리고 수학적 기반</h3>
<p>8.3.2절에서는 트랜스포머 모델의 기반이 되는 사인-코사인 함수 기반 포지셔널 인코딩을 살펴보았습니다. 하지만, “Attention is All You Need” 논문 발표 이후, 포지셔널 인코딩은 다양한 방향으로 발전해왔습니다. 이 딥다이브 섹션에서는 학습 가능한 포지셔널 인코딩, 상대적 포지셔널 인코딩, 그리고 최신 연구 동향까지 포괄적으로 다루면서, 각 기법의 수학적 표현과 장단점을 심층 분석합니다.</p>
<section id="학습-가능한-포지셔널-인코딩-learnable-positional-encoding" class="level4">
<h4 class="anchored" data-anchor-id="학습-가능한-포지셔널-인코딩-learnable-positional-encoding">1. 학습 가능한 포지셔널 인코딩 (Learnable Positional Encoding)</h4>
<ul>
<li><p><strong>개념</strong>: 고정된 함수 대신, 모델이 학습을 통해 위치 정보를 표현하는 임베딩을 직접 학습합니다.</p></li>
<li><p><strong>1.1 수학적 표현</strong>: 학습 가능한 포지셔널 임베딩은 다음 행렬로 표현됩니다.</p>
<p><span class="math inline">\(P \in \mathbb{R}^{L_{max} \times d}\)</span></p>
<p>여기서 <span class="math inline">\(L_{max}\)</span>는 최대 시퀀스 길이, <span class="math inline">\(d\)</span>는 임베딩 차원입니다. 위치 <span class="math inline">\(i\)</span>의 임베딩은 <span class="math inline">\(P\)</span> 행렬의 <span class="math inline">\(i\)</span>번째 행, 즉 <span class="math inline">\(P[i,:]\)</span>으로 주어집니다.</p></li>
<li><p><strong>1.2 외삽(Extrapolation) 문제 해결 기법</strong>: 학습 데이터보다 긴 시퀀스를 처리해야 할 때, 학습된 임베딩을 벗어나는 위치에 대한 정보가 없다는 문제가 있습니다. 이를 해결하기 위한 기법들이 연구되었습니다.</p>
<ul>
<li><p><strong>Position Interpolation (Chen et al., 2023)</strong>: 학습된 임베딩 사이를 선형 보간하여 새로운 위치에 대한 임베딩을 생성합니다. <span class="math inline">\(P_{ext}(i) = P[\lfloor \alpha i \rfloor] + (\alpha i - \lfloor \alpha i \rfloor)(P[\lfloor \alpha i \rfloor +1] - P[\lfloor \alpha i \rfloor])\)</span></p>
<p>여기서 <span class="math inline">\(\alpha = \frac{\text{학습 시퀀스 길이}}{\text{추론 시퀀스 길이}}\)</span> 입니다.</p></li>
<li><p><strong>NTK-aware 스케일링 (2023)</strong>: Neural Tangent Kernel (NTK) 이론에 기반하여, 주파수를 점진적으로 증가시켜 스무딩 효과를 도입하는 방법입니다.</p></li>
</ul></li>
<li><p><strong>1.3 최신 적용 사례</strong>:</p>
<ul>
<li><strong>BERT</strong>: 초기에는 512 토큰으로 제한되었으나, RoBERTa에서는 1024 토큰으로 확장되었습니다.</li>
<li><strong>GPT-3</strong>: 2048 토큰의 제한을 가지며, 학습 중에 점진적으로 시퀀스 길이를 늘리는 기법을 사용했습니다.</li>
</ul></li>
<li><p><strong>장점</strong>:</p>
<ul>
<li><strong>유연성</strong>: 데이터에 특화된 위치 정보를 학습할 수 있습니다.</li>
<li><strong>잠재적 성능 향상</strong>: 특정 task에서는 고정된 함수보다 더 나은 성능을 보일 수 있습니다.</li>
</ul></li>
<li><p><strong>단점</strong>:</p>
<ul>
<li><strong>과적합 위험</strong>: 학습 데이터에 없는 길이의 시퀀스에 대해서는 일반화 성능이 저하될 수 있습니다.</li>
<li><strong>긴 시퀀스 처리의 어려움</strong>: 외삽 문제를 해결하기 위한 추가적인 기법이 필요합니다.</li>
</ul></li>
</ul>
</section>
<section id="상대적-포지셔널-인코딩-relative-positional-encoding" class="level4">
<h4 class="anchored" data-anchor-id="상대적-포지셔널-인코딩-relative-positional-encoding">2. 상대적 포지셔널 인코딩 (Relative Positional Encoding)</h4>
<ul>
<li><p><strong>핵심 아이디어</strong>: 절대적인 위치 정보가 아닌, 단어 간의 상대적인 거리에 집중합니다.</p></li>
<li><p><strong>배경</strong>: 자연어에서 단어의 의미는 종종 절대적 위치보다 주변 단어들과의 상대적 관계에 의해 더 크게 영향을 받습니다. 또한, 절대적 포지셔널 인코딩은 멀리 떨어진 단어 간의 관계를 효과적으로 포착하기 어렵다는 단점이 있습니다.</p></li>
<li><p><strong>2.1 수학적 확장</strong>:</p>
<ul>
<li><p><strong>Shaw et al.&nbsp;(2018) 공식</strong>: 어텐션 메커니즘에서 Query와 Key 벡터 간의 관계를 계산할 때, 상대적 거리에 대한 학습 가능한 임베딩(<span class="math inline">\(a_{i-j}\)</span>)을 추가합니다.</p>
<p><span class="math inline">\(e_{ij} = \frac{x_iW^Q(x_jW^K + a_{i-j})^T}{\sqrt{d}}\)</span></p>
<p>여기서 <span class="math inline">\(a_{i-j} \in \mathbb{R}^d\)</span>는 상대 위치 <span class="math inline">\(i-j\)</span>에 대한 학습 가능한 벡터입니다.</p></li>
<li><p><strong>Rotary Positional Encoding (RoPE)</strong>: 회전 행렬을 이용하여 상대 위치를 인코딩합니다.</p>
<p><span class="math inline">\(\text{RoPE}(x, m) = x \odot e^{im\theta}\)</span></p>
<p>여기서 <span class="math inline">\(\theta\)</span>는 주파수를 제어하는 하이퍼파라미터, <span class="math inline">\(\odot\)</span>은 복소수 곱셈(또는 이에 대응하는 회전 행렬)을 나타냅니다.</p></li>
<li><p><strong>T5의 단순화 버전</strong>: 상대 위치에 대한 학습 가능한 편향(<span class="math inline">\(b\)</span>)을 사용하며, 상대 거리가 일정 범위를 넘어가면 값을 클리핑(clipping)합니다.</p>
<p><span class="math inline">\(e_{ij} = \frac{x_iW^Q(x_jW^K)^T + b_{\text{clip}(i-j)}}{\sqrt{d}}\)</span></p>
<p><span class="math inline">\(b \in \mathbb{R}^{2k+1}\)</span>는 클리핑된 상대 위치 [-k, k]에 대한 편향 벡터입니다.</p></li>
</ul></li>
<li><p><strong>장점</strong>:</p>
<ul>
<li><strong>일반화 능력 향상</strong>: 학습 데이터에 없는 길이의 시퀀스에 대해서도 더 잘 일반화됩니다.</li>
<li><strong>장거리 의존성 포착 능력 향상</strong>: 멀리 떨어진 단어 간의 관계를 더 효과적으로 모델링할 수 있습니다.</li>
</ul></li>
<li><p><strong>단점</strong>:</p>
<ul>
<li><strong>계산 복잡도 증가</strong>: 상대적 거리를 고려해야 하므로, 어텐션 계산이 더 복잡해질 수 있습니다. (특히, 모든 단어 쌍에 대한 상대적 거리를 고려하는 경우)</li>
</ul></li>
</ul>
</section>
<section id="cnn-기반-포지셔널-인코딩의-최적화" class="level4">
<h4 class="anchored" data-anchor-id="cnn-기반-포지셔널-인코딩의-최적화">3. CNN 기반 포지셔널 인코딩의 최적화</h4>
<ul>
<li><p><strong>3.1 Depth-wise Convolution 적용</strong>: 각 채널별로 독립적인 컨볼루션을 수행하여 파라미터 수를 줄이고 계산 효율성을 높입니다. <span class="math inline">\(P(i) = \sum_{k=-K}^K w_k \cdot x_{i+k}\)</span></p>
<p>여기서 <span class="math inline">\(K\)</span>는 커널 크기, <span class="math inline">\(w_k\)</span>는 학습 가능한 가중치입니다.</p></li>
<li><p><strong>3.2 다중 스케일 컨볼루션</strong>: ResNet과 유사하게, 병렬적인 컨볼루션 채널을 활용하여 다양한 범위의 위치 정보를 포착합니다.</p>
<p><span class="math inline">\(P(i) = \text{Concat}(\text{Conv}_{3x1}(x), \text{Conv}_{5x1}(x))\)</span></p></li>
</ul>
</section>
<section id="재귀적-포지셔널-인코딩의-동역학" class="level4">
<h4 class="anchored" data-anchor-id="재귀적-포지셔널-인코딩의-동역학">4. 재귀적 포지셔널 인코딩의 동역학</h4>
<ul>
<li><p><strong>4.1 LSTM 기반 인코딩</strong>: LSTM을 사용하여 순차적인 위치 정보를 인코딩합니다.</p>
<p><span class="math inline">\(h_t = \text{LSTM}(x_t, h_{t-1})\)</span> <span class="math inline">\(P(t) = W_ph_t\)</span></p></li>
<li><p><strong>4.2 최신 변형: Neural ODE</strong>: 연속 시간 동역학을 모델링하여, 이산적인(discrete) LSTM의 한계를 극복합니다.</p>
<p><span class="math inline">\(\frac{dh(t)}{dt} = f_\theta(h(t), t)\)</span> <span class="math inline">\(P(t) = \int_0^t f_\theta(h(\tau), \tau)d\tau\)</span></p></li>
</ul>
</section>
<section id="복소수-포지셔널-인코딩의-양자역학적-해석" class="level4">
<h4 class="anchored" data-anchor-id="복소수-포지셔널-인코딩의-양자역학적-해석">5. 복소수 포지셔널 인코딩의 양자역학적 해석</h4>
<ul>
<li><p><strong>5.1 복소수 임베딩 표현</strong>: 위치 정보를 복소수 형태로 표현합니다.</p>
<p><span class="math inline">\(z(i) = r(i)e^{i\phi(i)}\)</span></p>
<p>여기서 <span class="math inline">\(r\)</span>은 위치의 크기, <span class="math inline">\(\phi\)</span>는 위상각을 나타냅니다.</p></li>
<li><p><strong>5.2 위상 이동 정리</strong>: 위치 이동을 복소 평면에서의 회전으로 표현합니다.</p>
<p><span class="math inline">\(z(i+j) = z(i) \cdot e^{i\omega j}\)</span></p>
<p>여기서 <span class="math inline">\(\omega\)</span>는 학습 가능한 주파수 파라미터입니다.</p></li>
</ul>
</section>
<section id="하이브리드-접근법" class="level4">
<h4 class="anchored" data-anchor-id="하이브리드-접근법">6. 하이브리드 접근법</h4>
<ul>
<li><p><strong>6.1 Composite Positional Encoding:</strong> <span class="math inline">\(P(i)=αP_{abs}(i)+βP_{rel}(i)\)</span></p>
<p><span class="math inline">\(P(i)=αP_{abs}  (i)+βP_{rel}(i)\)</span></p>
<p>α, β = 학습 가능 가중치</p></li>
<li><p><strong>6.2 Dynamic Positional Encoding:</strong></p>
<p><span class="math inline">\(P(i) = \text{MLP}(i, \text{Context})\)</span> 컨텍스트 의존적 위치 표현 학습</p></li>
</ul>
</section>
<section id="실험적-성능-비교-glue-벤치마크" class="level4">
<h4 class="anchored" data-anchor-id="실험적-성능-비교-glue-벤치마크">7. 실험적 성능 비교 (GLUE 벤치마크)</h4>
<p>다음은 GLUE 벤치마크에서의 다양한 포지셔널 인코딩 방식에 대한 실험적 성능 비교 결과입니다. (실제 성능은 모델 구조, 데이터, 하이퍼파라미터 설정 등에 따라 달라질 수 있습니다.)</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: left;">방법</th>
<th style="text-align: left;">Accuracy</th>
<th style="text-align: left;">추론 시간 (ms)</th>
<th style="text-align: left;">메모리 사용량 (GB)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">절대 (Sinusoidal)</td>
<td style="text-align: left;">88.2</td>
<td style="text-align: left;">12.3</td>
<td style="text-align: left;">2.1</td>
</tr>
<tr class="even">
<td style="text-align: left;">상대 (RoPE)</td>
<td style="text-align: left;">89.7</td>
<td style="text-align: left;">14.5</td>
<td style="text-align: left;">2.4</td>
</tr>
<tr class="odd">
<td style="text-align: left;">CNN 다중 스케일</td>
<td style="text-align: left;">87.9</td>
<td style="text-align: left;">13.8</td>
<td style="text-align: left;">3.2</td>
</tr>
<tr class="even">
<td style="text-align: left;">복소수 (CLEX)</td>
<td style="text-align: left;">90.1</td>
<td style="text-align: left;">15.2</td>
<td style="text-align: left;">2.8</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Dynamic PE</td>
<td style="text-align: left;">90.3</td>
<td style="text-align: left;">17.1</td>
<td style="text-align: left;">3.5</td>
</tr>
</tbody>
</table>
</section>
<section id="최신-연구-동향-2024" class="level4">
<h4 class="anchored" data-anchor-id="최신-연구-동향-2024">8. 최신 연구 동향 (2024)</h4>
<p>최근에는 양자 컴퓨팅, 생물학적 시스템 등에서 영감을 받은 새로운 포지셔널 인코딩 기법들이 연구되고 있습니다.</p>
<ul>
<li><strong>양자 포지셔널 인코딩</strong>:
<ul>
<li>Qubit 회전 게이트 활용: <span class="math inline">\(R_z(\theta_i)|x\rangle\)</span></li>
<li>Grover 알고리즘 기반 위치 검색</li>
</ul></li>
<li><strong>생체 모방 인코딩</strong>:
<ul>
<li>시냅스 가소성의 STDP(Spike-Timing-Dependent Plasticity) 규칙 적용: <span class="math inline">\(\Delta w_{ij} \propto e^{-\frac{|i-j|}{\tau}}\)</span></li>
</ul></li>
<li><strong>그래프 신경망 통합</strong>:
<ul>
<li>위치를 노드, 관계를 엣지로 표현: <span class="math inline">\(P(i) = \sum_{j \in \mathcal{N}(i)} \alpha_{ij}Wx_j\)</span></li>
</ul></li>
</ul>
</section>
<section id="선택-가이드라인" class="level4">
<h4 class="anchored" data-anchor-id="선택-가이드라인">9. 선택 가이드라인</h4>
<ul>
<li><p><strong>고정 길이 시퀀스</strong>: 학습 가능한 PE. 과적합 위험이 낮고 최적화가 용이.</p></li>
<li><p><strong>가변 길이/외삽 필요</strong>: RoPE. 회전 불변성으로 길이 확장성이 우수.</p></li>
<li><p><strong>저지연 실시간 처리</strong>: CNN 기반. 병렬 처리 최적화, 하드웨어 가속 용이.</p></li>
<li><p><strong>물리 신호 처리</strong>: 복소수 PE. 주파수 정보 보존. 푸리에 변환과의 호환성.</p></li>
<li><p><strong>멀티모달 데이터</strong>: Dynamic PE. 크로스 모달 컨텍스트 반응형 적응. #### 수학적 부록</p></li>
<li><p><strong>RoPE의 군론적 특성</strong>:</p>
<p>SO(2) 회전군의 표현: <span class="math inline">\(R(\theta) = \begin{bmatrix} \cos\theta &amp; -\sin\theta \\ \sin\theta &amp; \cos\theta \end{bmatrix}\)</span></p>
<p>이 성질은 어텐션 스코어의 상대 위치 보존을 보장합니다.</p></li>
<li><p><strong>상대 위치 편향의 효율적 계산</strong>:</p>
<p>Toeplitz 행렬 구조 활용: <span class="math inline">\(B = [b_{i-j}]_{i,j}\)</span></p>
<p>FFT를 이용한 <span class="math inline">\(O(n\log n)\)</span> 복잡도 구현 가능</p></li>
<li><p><strong>복소수 PE의 그래디언트 흐름</strong>:</p>
<p>Wirtinger 미분 규칙 적용: <span class="math inline">\(\frac{\partial L}{\partial z} = \frac{1}{2}\left(\frac{\partial L}{\partial \text{Re}(z)} - i\frac{\partial L}{\partial \text{Im}(z)}\right)\)</span></p></li>
</ul>
<hr>
<p><strong>결론</strong>:</p>
<p>포지셔널 인코딩은 트랜스포머 모델의 성능에 큰 영향을 미치는 핵심 요소이며, 단순한 사인-코사인 함수를 넘어 다양한 방식으로 진화해왔습니다. 각 방식은 고유의 장단점과 수학적 기반을 가지고 있으며, 문제의 특성과 요구 사항에 따라 적절한 방식을 선택하는 것이 중요합니다. 최근에는 양자 컴퓨팅, 생물학 등 다양한 분야에서 영감을 받은 새로운 포지셔널 인코딩 기법들이 연구되고 있어, 앞으로도 지속적인 발전이 기대됩니다.</p>
</section>
</section>
</div>
</div>
</section>
</section>
<section id="트랜스포머의-전체-아키텍처" class="level2">
<h2 class="anchored" data-anchor-id="트랜스포머의-전체-아키텍처">8.4 트랜스포머의 전체 아키텍처</h2>
<p>지금까지 트랜스포머의 핵심 구성 요소들이 어떻게 발전했는지 살펴보았습니다. 이제 이러한 요소들이 어떻게 하나의 완성된 아키텍처로 통합되는지 알아보겠습니다. 트랜스포머의 전체 아키텍처 입니다.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../../assets/images/transformer/tr_architecture.png" class="img-fluid figure-img"></p>
<figcaption>트랜스포머 아키텍처</figcaption>
</figure>
</div>
<p><em>그림 출처: The Illustrated Transformer (Jay Alammar, 2018) CC BY 4.0 License</em></p>
<p>교육적 목적으로 구현된 트랜스포머의 소스는 chapter_08/transformer 에 있습니다. 이 구현은 Harvard NLP 그룹의 The Annotated Transformer를 참고하여 수정했습니다. 주요 수정 사항은 다음과 같습니다.</p>
<ol type="1">
<li><strong>모듈화:</strong> 하나의 파일로 되어 있던 구현을 여러 모듈로 나누어 가독성과 재사용성을 높였습니다.</li>
<li><strong>Pre-LN 구조 채택:</strong> 원 논문과 달리, 레이어 정규화를 어텐션/피드포워드 연산 <em>전에</em> 적용하는 Pre-LN 구조를 사용했습니다. (최근 연구에서 Pre-LN이 학습 안정성과 성능에 더 유리하다는 결과가 보고되고 있습니다.)</li>
<li><strong><code>TransformerConfig</code> 클래스 추가:</strong> 모델 설정을 위한 별도의 클래스를 도입하여 하이퍼파라미터 관리를 용이하게 했습니다.</li>
<li><strong>PyTorch 스타일 구현:</strong> <code>nn.ModuleList</code> 등 PyTorch의 기능을 활용하여 코드를 더 간결하고 직관적으로 만들었습니다.</li>
<li>Noam 옵티마이저는 구현했지만 사용하지 않았습니다.</li>
</ol>
<section id="기본-구성-요소의-통합" class="level3">
<h3 class="anchored" data-anchor-id="기본-구성-요소의-통합">8.4.1 기본 구성 요소의 통합</h3>
<p>트랜스포머는 크게 <strong>인코더(Encoder)</strong> 와 <strong>디코더(Decoder)</strong> 로 구성되며, 각 구성 요소는 다음과 같습니다.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 19%">
<col style="width: 38%">
<col style="width: 41%">
</colgroup>
<thead>
<tr class="header">
<th>구성 요소</th>
<th>인코더</th>
<th>디코더</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>멀티헤드 어텐션</strong></td>
<td>셀프 어텐션 (Self-Attention)</td>
<td>마스크드 셀프 어텐션 (Masked Self-Attention)</td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td>인코더-디코더 어텐션 (Encoder-Decoder Attention)</td>
</tr>
<tr class="odd">
<td><strong>피드포워드 네트워크</strong></td>
<td>각 위치에 독립적으로 적용</td>
<td>각 위치에 독립적으로 적용</td>
</tr>
<tr class="even">
<td><strong>잔차 연결</strong></td>
<td>각 서브 레이어(어텐션, 피드포워드)의 입력과 출력을 더함</td>
<td>각 서브 레이어(어텐션, 피드포워드)의 입력과 출력을 더함</td>
</tr>
<tr class="odd">
<td><strong>레이어 정규화</strong></td>
<td>각 서브 레이어의 입력에 적용 (Pre-LN)</td>
<td>각 서브 레이어의 입력에 적용 (Pre-LN)</td>
</tr>
</tbody>
</table>
<p><strong>인코더 레이어 - 코드</strong></p>
<div id="cell-60" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> TransformerEncoderLayer(nn.Module):</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config):</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.attention <span class="op">=</span> MultiHeadAttention(config)</span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.feed_forward <span class="op">=</span> FeedForward(config)</span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a>        <span class="co"># SublayerConnection for Pre-LN structure</span></span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.sublayer <span class="op">=</span> nn.ModuleList([</span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a>            SublayerConnection(config) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>)</span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a>        ])</span>
<span id="cb34-10"><a href="#cb34-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-11"><a href="#cb34-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x, attention_mask<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb34-12"><a href="#cb34-12" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.sublayer[<span class="dv">0</span>](x, <span class="kw">lambda</span> x: <span class="va">self</span>.attention(x, x, x, attention_mask))</span>
<span id="cb34-13"><a href="#cb34-13" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.sublayer[<span class="dv">1</span>](x, <span class="va">self</span>.feed_forward)</span>
<span id="cb34-14"><a href="#cb34-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<ul>
<li><p><strong>멀티헤드 어텐션 (Multi-Head Attention):</strong> 입력 시퀀스의 모든 위치 쌍 간의 관계를 병렬적으로 계산합니다. 각 헤드는 서로 다른 관점에서 시퀀스를 분석하고, 그 결과를 종합하여 풍부한 문맥 정보를 포착합니다. (“The cat sits on the mat” 예시에서 주어-동사, 전치사구, 관사-명사 관계 등을 서로 다른 헤드가 학습)</p></li>
<li><p><strong>피드포워드 네트워크 (Feed-Forward Network):</strong> 각 위치에 <em>독립적으로</em> 적용되는, 두 개의 선형 변환과 GELU 활성화 함수로 구성된 네트워크입니다.</p></li>
</ul>
<div id="cell-62" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> FeedForward(nn.Module):</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config):</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.linear1 <span class="op">=</span> nn.Linear(config.hidden_size, config.intermediate_size)</span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.linear2 <span class="op">=</span> nn.Linear(config.intermediate_size, config.hidden_size)</span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.activation <span class="op">=</span> nn.GELU()</span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb35-9"><a href="#cb35-9" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.linear1(x)</span>
<span id="cb35-10"><a href="#cb35-10" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.activation(x)</span>
<span id="cb35-11"><a href="#cb35-11" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.linear2(x)</span>
<span id="cb35-12"><a href="#cb35-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>피드포워드 네트워크가 필요한 이유는 어텐션 출력의 정보 밀도와 관련이 있습니다. 어텐션 연산(<span class="math inline">\(\text{Attention}(Q, K, V) = \text{softmax}(\frac{QK^T}{\sqrt{d\_k}})V\)</span>) 결과는 <span class="math inline">\(V\)</span> 벡터들의 가중 합으로, <span class="math inline">\(d\_{model}\)</span> 차원(논문에서는 512)에 문맥 정보가 <em>밀집</em>되어 있습니다. <strong>ReLU 활성화 함수를 바로 적용하면, 이 밀집된 정보 중 상당 부분이 손실될 수 있습니다(ReLU는 음수 값을 0으로 만듦)</strong>. 따라서, 피드포워드 네트워크는 먼저 <span class="math inline">\(d\_{model}\)</span> 차원을 더 큰 차원(<span class="math inline">\(4 \times d\_{model}\)</span>, 논문에서는 2048)으로 확장하여 표현 공간을 넓힌 다음, ReLU(또는 GELU)를 적용하고, 다시 원래 차원으로 축소하는 방식으로 비선형성을 추가합니다.</p>
<div id="cell-64" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> W1(x)    <span class="co"># hidden_size -&gt; intermediate_size (512 -&gt; 2048)</span></span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> ReLU(x)  <span class="co"># or GELU</span></span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> W2(x)    <span class="co"># intermediate_size -&gt; hidden_size (2048 -&gt; 512)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<ul>
<li><p><strong>잔차 연결 (Residual Connection):</strong> 각 서브 레이어(멀티헤드 어텐션 또는 피드포워드 네트워크)의 입력과 출력을 더해주는 방식입니다. 이는 기울기 소실/폭발 문제를 완화하고, 깊은 네트워크의 학습을 돕습니다. (7장 잔차 연결 참조).</p></li>
<li><p><strong>레이어 정규화(Layer Normalization):</strong> 각 서브 레이어의 <em>입력</em>에 적용됩니다(Pre-LN).</p></li>
</ul>
<div id="cell-66" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LayerNorm(nn.Module):</span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config):</span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.gamma <span class="op">=</span> nn.Parameter(torch.ones(config.hidden_size))</span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.beta <span class="op">=</span> nn.Parameter(torch.zeros(config.hidden_size))</span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.eps <span class="op">=</span> config.layer_norm_eps</span>
<span id="cb37-7"><a href="#cb37-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-8"><a href="#cb37-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb37-9"><a href="#cb37-9" aria-hidden="true" tabindex="-1"></a>        mean <span class="op">=</span> x.mean(<span class="op">-</span><span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb37-10"><a href="#cb37-10" aria-hidden="true" tabindex="-1"></a>        std <span class="op">=</span> (x <span class="op">-</span> mean).<span class="bu">pow</span>(<span class="dv">2</span>).mean(<span class="op">-</span><span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>).sqrt()</span>
<span id="cb37-11"><a href="#cb37-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.gamma <span class="op">*</span> (x <span class="op">-</span> mean) <span class="op">/</span> (std <span class="op">+</span> <span class="va">self</span>.eps) <span class="op">+</span> <span class="va">self</span>.beta</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>레이어 정규화는 2016년 Ba, Kiros, Hinton의 논문 “<a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://arxiv.org/abs/1607.06450">Layer Normalization</a>”에서 제안된 기법입니다. 배치 정규화(Batch Normalization)가 배치 차원에서 정규화를 수행하는 반면, 레이어 정규화는 각 샘플의 특성 차원(feature dimension)에서 평균과 분산을 구하여 정규화합니다.</p>
<p><strong>레이어 정규화 장점</strong></p>
<ol type="1">
<li><p><strong>배치 크기 독립성:</strong> 배치 크기에 영향을 받지 않아, 작은 배치 크기나 온라인 학습(online learning) 환경에서도 안정적으로 작동합니다.</p></li>
<li><p><strong>시퀀스 길이에 무관:</strong> RNN, Transformer와 같이 가변 길이 시퀀스를 처리하는 모델에 적합합니다.</p></li>
<li><p><strong>학습 안정화 및 가속화:</strong> 각 레이어의 입력 분포를 안정화하여 기울기 소실/폭발 문제를 완화하고, 학습 속도를 높입니다.</p>
<p>트랜스포머에서는 Pre-LN 방식을 사용하여, 각 서브 레이어(멀티헤드 어텐션, 피드포워드 네트워크)를 통과하기 <em>전에</em> 레이어 정규화를 적용합니다.</p></li>
</ol>
<p><strong>레이어 정규화 시각화</strong></p>
<div id="cell-68" class="cell" data-execution_count="19">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_08.visualize_layer_norm <span class="im">import</span> visualize_layer_normalization</span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a>visualize_layer_normalization()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="08_트랜스포머의 탄생_files/figure-html/cell-30-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>========================================
Input Data Shape: (2, 5, 6)
Mean Shape: (2, 5, 1)
Standard Deviation Shape: (2, 5, 1)
Normalized Data Shape: (2, 5, 6)
Gamma (Scale) Values:
 [0.95208258 0.9814341  0.8893665  0.88037934 1.08125258 1.135624  ]
Beta (Shift) Values:
 [-0.00720101  0.10035329  0.0361636  -0.06451198  0.03613956  0.15380366]
Scaled &amp; Shifted Data Shape: (2, 5, 6)
========================================</code></pre>
</div>
</div>
<p>위 그림은 레이어 정규화(Layer Normalization)의 작동 방식을 단계별로 보여줍니다.</p>
<ul>
<li><strong>원본 데이터 (상단 왼쪽):</strong> 정규화 전 데이터는 넓게 퍼져 있으며, 평균과 표준편차가 일정하지 않습니다.</li>
<li><strong>정규화 후 (상단 오른쪽):</strong> 데이터가 평균 0, 표준편차 1 근처로 모여 정규화됩니다.</li>
<li><strong>스케일 및 시프트 (중앙):</strong> 학습 가능한 파라미터 γ(감마, 스케일)와 β(베타, 시프트)를 적용하여 데이터 분포에 약간의 변화를 줍니다. 이는 모델의 표현력을 조절합니다.</li>
<li><strong>히트맵 (하단):</strong> 첫 번째 배치 데이터를 기준으로, 정규화 전후 및 스케일/시프트 적용 후의 개별 값 변화를 보여줍니다.</li>
<li><strong>γ/β 값 (하단 오른쪽):</strong> 각 은닉 차원에 대한 γ와 β 값을 막대 그래프로 나타냅니다.</li>
</ul>
<p>이처럼 레이어 정규화는 각 레이어의 입력을 정규화하여 학습 안정성과 속도를 향상시킵니다.</p>
<p><strong>핵심:</strong></p>
<ul>
<li>각 레이어 입력 정규화 (평균 0, 표준편차 1)</li>
<li>학습 가능한 스케일(γ) 및 시프트(β)로 표현력 조절</li>
<li>배치 정규화와 달리, 샘플 간 독립성 유지</li>
</ul>
<p>이러한 구성 요소들(멀티헤드 어텐션, 피드포워드 네트워크, 잔차 연결, 레이어 정규화)의 조합은 각 요소의 장점을 극대화합니다. 멀티헤드 어텐션은 입력 시퀀스의 다양한 측면을 포착하고, 피드포워드 네트워크는 비선형성을 추가하며, 잔차 연결과 레이어 정규화는 깊은 네트워크에서도 안정적인 학습을 가능하게 합니다.</p>
</section>
<section id="인코더의-구성" class="level3">
<h3 class="anchored" data-anchor-id="인코더의-구성">8.4.2 인코더의 구성</h3>
<p>트랜스포머는 기계 번역을 위한 인코더-디코더 구조를 가집니다. 인코더는 원본 언어(예: 영어)를 이해하고 디코더는 목적 언어(예: 프랑스어)를 생성하는 역할을 합니다. 인코더와 디코더는 멀티헤드 어텐션과 피드포워드 네트워크를 기본 구성 요소로 공유하지만, 각각의 목적에 맞게 다르게 구성됩니다.</p>
<p><strong>인코더 vs 디코더 구성 비교</strong></p>
<table class="caption-top table">
<colgroup>
<col style="width: 17%">
<col style="width: 31%">
<col style="width: 50%">
</colgroup>
<thead>
<tr class="header">
<th>구성 요소</th>
<th>인코더</th>
<th>디코더</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>어텐션 층 수</td>
<td>1개 (셀프 어텐션)</td>
<td>2개 (마스크드 셀프 어텐션, 인코더-디코더 어텐션)</td>
</tr>
<tr class="even">
<td>마스킹 전략</td>
<td>패딩 마스크만 사용</td>
<td>패딩 마스크 + 인과관계 마스크</td>
</tr>
<tr class="odd">
<td>문맥 처리</td>
<td>양방향 문맥 처리</td>
<td>단방향 문맥 처리 (자기회귀적)</td>
</tr>
<tr class="even">
<td>입력 참조</td>
<td>자신의 입력만 참조</td>
<td>자신의 입력 + 인코더의 출력 참조</td>
</tr>
</tbody>
</table>
<p>여러 어텐션 용어를 다음과 같이 정리합니다.</p>
<p><strong>어텐션 개념 정리</strong></p>
<table class="caption-top table">
<colgroup>
<col style="width: 14%">
<col style="width: 25%">
<col style="width: 4%">
<col style="width: 55%">
</colgroup>
<thead>
<tr class="header">
<th>어텐션 종류</th>
<th>특징</th>
<th>설명 위치</th>
<th>핵심 개념</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>어텐션(기본)</td>
<td>- 동일한 단어 벡터로 유사도 계산<br>- 단순 가중합으로 문맥 정보 생성<br>- seq2seq 모델 적용의 단순화 버전</td>
<td>8.2.2</td>
<td>- 단어 벡터 간 내적으로 유사도 계산<br>- softmax로 가중치 변환<br>- 모든 어텐션에 패딩 마스크 기본 적용</td>
</tr>
<tr class="even">
<td>셀프 어텐션 (Self-Attention)</td>
<td>- Q, K, V 공간 분리<br>- 각 공간 독립적 최적화<br>- 입력 시퀀스가 자기 자신 참조<br>- 인코더에서 사용</td>
<td>8.2.3</td>
<td>- 유사도 계산과 정보 전달 역할 분리<br>- 학습 가능한 Q, K, V 변환<br>- 양방향 문맥 처리 가능</td>
</tr>
<tr class="odd">
<td>마스크드 셀프 어텐션</td>
<td>- 미래 정보 차단<br>- 인과관계 마스크 사용<br>- 디코더에서 사용</td>
<td>8.2.5</td>
<td>- 상삼각 행렬로 미래 정보 마스킹<br>- 자기회귀적 생성 가능<br>- 단방향 문맥 처리</td>
</tr>
<tr class="even">
<td>크로스(인코더-디코더) 어텐션</td>
<td>- Query: 디코더 상태<br>- Key, Value: 인코더 출력<br>- 크로스 어텐션이라고도 함<br>- 디코더에서 사용</td>
<td>8.4.3</td>
<td>- 디코더가 인코더 정보 참조<br>- 두 시퀀스 간 관계 계산<br>- 번역/생성 시 문맥 반영</td>
</tr>
</tbody>
</table>
<p>트랜스포머에서는 셀프, 마스크드, 크로스 어텐션 명칭을 사용합니다. 어텐션 매커니즘은 동일하며 Q, K, V의 출처에 따라 구분됩니다.</p>
<p><strong>인코더 구성 요소</strong></p>
<table class="caption-top table">
<colgroup>
<col style="width: 23%">
<col style="width: 76%">
</colgroup>
<thead>
<tr class="header">
<th>구성 요소</th>
<th>설명</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Embeddings</td>
<td>입력 토큰을 벡터로 변환하고, 위치 정보를 추가하여 입력 시퀀스의 의미와 순서 정보를 인코딩합니다.</td>
</tr>
<tr class="even">
<td>TransformerEncoderLayer (x N)</td>
<td>동일한 레이어를 여러 층 쌓아 입력 시퀀스에서 더 추상적이고 복잡한 특징을 계층적으로 추출합니다.</td>
</tr>
<tr class="odd">
<td>LayerNorm</td>
<td>최종 출력의 특성 분포를 정규화하여 안정화하고, 디코더가 참조하기 좋은 형태로 만듭니다.</td>
</tr>
</tbody>
</table>
<div id="cell-71" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> TransformerEncoder(nn.Module):</span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config):</span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.embeddings <span class="op">=</span> Embeddings(config)</span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.layers <span class="op">=</span> nn.ModuleList([</span>
<span id="cb40-6"><a href="#cb40-6" aria-hidden="true" tabindex="-1"></a>            TransformerEncoderLayer(config) </span>
<span id="cb40-7"><a href="#cb40-7" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(config.num_hidden_layers)</span>
<span id="cb40-8"><a href="#cb40-8" aria-hidden="true" tabindex="-1"></a>        ])</span>
<span id="cb40-9"><a href="#cb40-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.norm <span class="op">=</span> LayerNorm(config)</span>
<span id="cb40-10"><a href="#cb40-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb40-11"><a href="#cb40-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, input_ids, attention_mask<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb40-12"><a href="#cb40-12" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.embeddings(input_ids)</span>
<span id="cb40-13"><a href="#cb40-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-14"><a href="#cb40-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i, layer <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="va">self</span>.layers):</span>
<span id="cb40-15"><a href="#cb40-15" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> layer(x, attention_mask)</span>
<span id="cb40-16"><a href="#cb40-16" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb40-17"><a href="#cb40-17" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> <span class="va">self</span>.norm(x)</span>
<span id="cb40-18"><a href="#cb40-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> output</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>인코더는 임베딩 층, 여러 개의 인코더 층, 최종 정규화 층으로 구성됩니다.</p>
<p><strong>1. 셀프 어텐션 메커니즘 (사례)</strong></p>
<p>인코더의 셀프 어텐션은 입력 시퀀스 내의 모든 단어 쌍 간의 관계를 계산하여 각 단어에 대한 문맥 정보를 풍부하게 만듭니다.</p>
<ul>
<li><strong>예시:</strong> “The patient bear can bear the pain no longer.”</li>
<li><strong>역할:</strong> 두 번째 ‘bear’의 의미를 파악할 때, 셀프 어텐션은 ’patient’ (환자), ‘bear’ (곰), ‘pain’ (고통) 등 문장 내의 <em>모든</em> 단어와의 관계를 고려합니다. 이를 통해 ‘bear’가 ’참다’, ’견디다’라는 의미로 사용되었음을 정확하게 파악합니다 (양방향 문맥 처리).</li>
</ul>
<p><strong>2. 드롭아웃 위치의 중요성</strong></p>
<p>드롭아웃은 과적합을 방지하고 학습 안정성을 높이는 데 중요한 역할을 합니다. 트랜스포머 인코더에서는 다음과 같은 위치에 드롭아웃이 적용됩니다.</p>
<ul>
<li><strong>임베딩 출력 후:</strong> 토큰 임베딩과 위치 정보가 결합된 직후.</li>
<li><strong>각 서브레이어(어텐션, FFN) 출력 후:</strong> Pre-LN 구조 (정규화 → 서브레이어 → 드롭아웃 → 잔차 연결)를 따릅니다.</li>
<li><strong>FFN 내부:</strong> 첫 번째 선형 변환 및 ReLU 활성화 함수 적용 후.</li>
</ul>
<p>이러한 드롭아웃 배치는 정보의 흐름을 조절하여 모델이 특정 특징에 과도하게 의존하는 것을 막고, 일반화 성능을 향상시킵니다.</p>
<p><strong>3. 인코더 스택 구조</strong></p>
<p>트랜스포머 인코더는 동일한 구조의 인코더 레이어를 여러 개 쌓아 올린(stacked) 구조를 가집니다.</p>
<ul>
<li><strong>원 논문:</strong> 6개의 인코더 레이어 사용.</li>
<li><strong>역할 분담</strong>:
<ul>
<li><strong>하위 레이어:</strong> 인접 단어, 구두점 등 표면적인 언어 패턴 학습.</li>
<li><strong>중간 레이어:</strong> 문법적 구조 학습.</li>
<li><strong>상위 레이어:</strong> 상호 참조(coreference)와 같은 고차원적인 의미 관계 학습.</li>
</ul></li>
</ul>
<p>레이어를 깊게 쌓을수록 더 추상적이고 복잡한 특징을 학습할 수 있습니다. 후속 연구에서는 하드웨어 및 학습 기법의 발전(Pre-LayerNorm, 그래디언트 클리핑, 학습률 워밍업, 혼합 정밀도 학습, 그래디언트 누적 등)에 힘입어 훨씬 더 많은 레이어를 쌓은 모델(BERT-base: 12층, GPT-3: 96층, PaLM: 118층)이 등장했습니다.</p>
<p><strong>4. 인코더의 최종 출력과 디코더 활용</strong></p>
<p>인코더의 최종 출력은 각 입력 토큰에 대한 문맥 정보를 풍부하게 담고 있는 벡터 표현입니다. 이 출력은 디코더의 <strong>인코더-디코더 어텐션 (Cross-Attention)</strong>에서 <strong>Key</strong>와 <strong>Value</strong>로 사용됩니다. 디코더는 출력 시퀀스의 각 토큰을 생성할 때마다 인코더의 출력을 참조하여, 원본 문장의 문맥을 고려한 정확한 번역/생성을 수행합니다.</p>
</section>
<section id="디코더의-구성" class="level3">
<h3 class="anchored" data-anchor-id="디코더의-구성">8.4.3 디코더의 구성</h3>
<p>디코더는 인코더와 유사하지만, 자기회귀적(autoregressive)으로 출력을 생성한다는 점이 다릅니다.</p>
<p><strong>디코더 레이어 전체 코드</strong></p>
<div id="cell-74" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> TransformerDecoderLayer(nn.Module):</span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config):</span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.self_attn <span class="op">=</span> MultiHeadAttention(config)</span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.cross_attn <span class="op">=</span> MultiHeadAttention(config)</span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.feed_forward <span class="op">=</span> FeedForward(config)</span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb41-8"><a href="#cb41-8" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Pre-LN을 위한 레이어 정규화</span></span>
<span id="cb41-9"><a href="#cb41-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.norm1 <span class="op">=</span> LayerNorm(config)</span>
<span id="cb41-10"><a href="#cb41-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.norm2 <span class="op">=</span> LayerNorm(config)</span>
<span id="cb41-11"><a href="#cb41-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.norm3 <span class="op">=</span> LayerNorm(config)</span>
<span id="cb41-12"><a href="#cb41-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(config.dropout_prob)</span>
<span id="cb41-13"><a href="#cb41-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-14"><a href="#cb41-14" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x, memory, src_mask<span class="op">=</span><span class="va">None</span>, tgt_mask<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb41-15"><a href="#cb41-15" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Pre-LN 구조</span></span>
<span id="cb41-16"><a href="#cb41-16" aria-hidden="true" tabindex="-1"></a>        m <span class="op">=</span> <span class="va">self</span>.norm1(x)</span>
<span id="cb41-17"><a href="#cb41-17" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.dropout(<span class="va">self</span>.self_attn(m, m, m, tgt_mask))</span>
<span id="cb41-18"><a href="#cb41-18" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb41-19"><a href="#cb41-19" aria-hidden="true" tabindex="-1"></a>        m <span class="op">=</span> <span class="va">self</span>.norm2(x)</span>
<span id="cb41-20"><a href="#cb41-20" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.dropout(<span class="va">self</span>.cross_attn(m, memory, memory, src_mask))</span>
<span id="cb41-21"><a href="#cb41-21" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb41-22"><a href="#cb41-22" aria-hidden="true" tabindex="-1"></a>        m <span class="op">=</span> <span class="va">self</span>.norm3(x)</span>
<span id="cb41-23"><a href="#cb41-23" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.dropout(<span class="va">self</span>.feed_forward(m))</span>
<span id="cb41-24"><a href="#cb41-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><strong>디코더의 주요 구성 요소 및 역할</strong></p>
<table class="caption-top table">
<colgroup>
<col style="width: 9%">
<col style="width: 43%">
<col style="width: 47%">
</colgroup>
<thead>
<tr class="header">
<th>서브층</th>
<th>역할</th>
<th>구현 특징</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>마스크드 셀프 어텐션</td>
<td>현재까지 생성된 출력 시퀀스 내의 단어 간 관계 파악, 미래 정보 참조 방지 (자기회귀적 생성)</td>
<td><code>tgt_mask</code> (인과관계 마스크 + 패딩 마스크) 사용, <code>self.self_attn</code></td>
</tr>
<tr class="even">
<td>인코더-디코더 어텐션 (교차 어텐션)</td>
<td>디코더가 인코더의 출력(입력 문장의 문맥 정보)을 참조하여 현재 생성할 단어와 관련된 정보 획득</td>
<td><code>Q</code>: 디코더, <code>K</code>, <code>V</code>: 인코더, <code>src_mask</code> (패딩 마스크) 사용, <code>self.cross_attn</code></td>
</tr>
<tr class="odd">
<td>피드포워드 네트워크</td>
<td>각 위치의 표현(representation)을 독립적으로 변환하여 더 풍부한 표현 생성</td>
<td>인코더와 동일한 구조, <code>self.feed_forward</code></td>
</tr>
<tr class="even">
<td>레이어 정규화 (LayerNorm)</td>
<td>각 서브층의 입력 정규화 (Pre-LN), 학습 안정성 및 성능 향상</td>
<td><code>self.norm1</code>, <code>self.norm2</code>, <code>self.norm3</code></td>
</tr>
<tr class="odd">
<td>드롭아웃 (Dropout)</td>
<td>과적합 방지, 일반화 성능 향상</td>
<td>각 서브층의 출력에 적용, <code>self.dropout</code></td>
</tr>
<tr class="even">
<td>잔차 연결 (Residual Connection)</td>
<td>깊은 네트워크에서 그래디언트 소실/폭발 문제 완화, 정보 흐름 개선</td>
<td>각 서브층의 입력과 출력을 더함</td>
</tr>
</tbody>
</table>
<p><strong>1. 마스크드 셀프 어텐션 (Masked Self-Attention)</strong></p>
<ul>
<li><strong>역할:</strong> 디코더가 자기회귀적(autoregressive)으로 출력을 생성하도록 합니다. 즉, 현재 생성 중인 단어보다 <em>미래</em>에 나올 단어를 참조하지 못하게 합니다. 예를 들어, “I love you”를 번역할 때, “나는”을 생성한 후 “너를”을 생성할 때는 아직 생성되지 않은 “사랑해” 토큰을 참조할 수 없습니다.</li>
<li><strong>구현:</strong> 인과관계 마스크(causal mask)와 패딩 마스크(padding mask)를 결합한 <code>tgt_mask</code>를 사용합니다. 인과관계 마스크는 상삼각 행렬을 <code>-inf</code>로 채워 미래 토큰에 대한 어텐션 가중치를 0으로 만듭니다. (8.2.5절 참고). <code>TransformerDecoderLayer</code>의 <code>forward</code> 메서드에서 <code>self.self_attn(m, m, m, tgt_mask)</code> 부분에서 이 마스크가 적용됩니다.</li>
</ul>
<p><strong>2. 인코더-디코더 어텐션 (Cross-Attention)</strong></p>
<ul>
<li><strong>역할:</strong> 디코더가 인코더의 출력(입력 문장의 문맥 정보)을 참조하여 현재 생성할 단어와 관련된 정보를 얻도록 합니다. 이는 번역 작업에서 디코더가 원본 문장의 의미를 정확하게 파악하고, 적절한 번역 단어를 선택하는 데 핵심적인 역할을 합니다.</li>
<li><strong>구현:</strong>
<ul>
<li><strong>Query (Q):</strong> 디코더의 현재 상태 (마스크드 셀프 어텐션의 출력)</li>
<li><strong>Key (K):</strong> 인코더의 출력 (<code>memory</code>)</li>
<li><strong>Value (V):</strong> 인코더의 출력 (<code>memory</code>)</li>
<li><code>src_mask</code> (패딩 마스크)를 사용하여 인코더 출력의 패딩 토큰은 무시합니다.</li>
<li><code>TransformerDecoderLayer</code>의 <code>forward</code> 메서드에서 <code>self.cross_attn(m, memory, memory, src_mask)</code> 부분에서 이 어텐션이 수행됩니다. <code>memory</code>가 인코더의 출력을 나타냅니다.</li>
</ul></li>
</ul>
<p><strong>3. 디코더 스택 구조</strong></p>
<div id="cell-76" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> TransformerDecoder(nn.Module):</span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config):</span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.embeddings <span class="op">=</span> Embeddings(config)</span>
<span id="cb42-5"><a href="#cb42-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.layers <span class="op">=</span> nn.ModuleList([</span>
<span id="cb42-6"><a href="#cb42-6" aria-hidden="true" tabindex="-1"></a>            TransformerDecoderLayer(config)</span>
<span id="cb42-7"><a href="#cb42-7" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(config.num_hidden_layers)</span>
<span id="cb42-8"><a href="#cb42-8" aria-hidden="true" tabindex="-1"></a>        ])</span>
<span id="cb42-9"><a href="#cb42-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.norm <span class="op">=</span> LayerNorm(config)</span>
<span id="cb42-10"><a href="#cb42-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-11"><a href="#cb42-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x, memory, src_mask<span class="op">=</span><span class="va">None</span>, tgt_mask<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb42-12"><a href="#cb42-12" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.embeddings(x)</span>
<span id="cb42-13"><a href="#cb42-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> layer <span class="kw">in</span> <span class="va">self</span>.layers:</span>
<span id="cb42-14"><a href="#cb42-14" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> layer(x, memory, src_mask, tgt_mask)</span>
<span id="cb42-15"><a href="#cb42-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.norm(x)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<ul>
<li>디코더는 <code>TransformerDecoderLayer</code>를 여러 층 (원 논문에서는 6개) 쌓아 구성됩니다.</li>
<li>각 레이어는 마스크드 셀프 어텐션, 인코더-디코더 어텐션, 피드포워드 네트워크를 순차적으로 수행합니다.</li>
<li>Pre-LN 구조와 잔차 연결이 각 서브층에 적용됩니다. 이는 깊은 네트워크에서도 안정적인 학습을 가능하게 합니다.</li>
<li><code>TransformerDecoder</code> 클래스의 <code>forward</code> 메서드는 입력 <code>x</code> (디코더 입력), <code>memory</code> (인코더 출력), <code>src_mask</code> (인코더 패딩 마스크), <code>tgt_mask</code> (디코더 마스크)를 받아 디코더 레이어를 순차적으로 통과시킨 후 최종 출력을 반환합니다.</li>
</ul>
<p><strong>모델별 인코더/디코더 레이어 수</strong></p>
<table class="caption-top table">
<colgroup>
<col style="width: 25%">
<col style="width: 8%">
<col style="width: 20%">
<col style="width: 13%">
<col style="width: 13%">
<col style="width: 17%">
</colgroup>
<thead>
<tr class="header">
<th>모델</th>
<th>연도</th>
<th>구조</th>
<th>인코더 층</th>
<th>디코더 층</th>
<th>총 파라미터</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>원논문 트랜스포머</td>
<td>2017</td>
<td>인코더-디코더</td>
<td>6</td>
<td>6</td>
<td>65M</td>
</tr>
<tr class="even">
<td>BERT-base</td>
<td>2018</td>
<td>인코더 전용</td>
<td>12</td>
<td>-</td>
<td>110M</td>
</tr>
<tr class="odd">
<td>GPT-2</td>
<td>2019</td>
<td>디코더 전용</td>
<td>-</td>
<td>48</td>
<td>1.5B</td>
</tr>
<tr class="even">
<td>T5-base</td>
<td>2020</td>
<td>인코더-디코더</td>
<td>12</td>
<td>12</td>
<td>220M</td>
</tr>
<tr class="odd">
<td>GPT-3</td>
<td>2020</td>
<td>디코더 전용</td>
<td>-</td>
<td>96</td>
<td>175B</td>
</tr>
<tr class="even">
<td>PaLM</td>
<td>2022</td>
<td>디코더 전용</td>
<td>-</td>
<td>118</td>
<td>540B</td>
</tr>
<tr class="odd">
<td>Gemma-2</td>
<td>2024</td>
<td>디코더 전용</td>
<td>-</td>
<td>18-36</td>
<td>2B-27B</td>
</tr>
</tbody>
</table>
<p>최근 모델들은 Pre-LN과 같은 발전된 학습 기법 덕분에 훨씬 더 많은 레이어를 효과적으로 학습할 수 있게 되었습니다. 더 깊은 디코더는 더 추상적이고 복잡한 언어 패턴을 학습할 수 있어, 번역, 텍스트 생성 등 다양한 자연어 처리 작업에서 성능 향상을 가져왔습니다.</p>
<p><strong>4. 디코더 출력 생성 및 종료 조건</strong></p>
<ul>
<li><strong>출력 생성:</strong> <code>Transformer</code> 클래스의 <code>generator</code> (선형 레이어)가 디코더의 최종 출력을 어휘 크기(vocab_size)의 로짓(logit) 벡터로 변환하고, <code>log_softmax</code>를 적용하여 각 토큰의 확률 분포를 얻습니다. 이 확률 분포를 기반으로 다음 토큰을 예측합니다.</li>
</ul>
<div id="cell-78" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 최종 출력 생성 (설명용)</span></span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> <span class="va">self</span>.generator(decoder_output)</span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a><span class="cf">return</span> F.log_softmax(output, dim<span class="op">=-</span><span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<ul>
<li><strong>종료 조건</strong>
<ol type="1">
<li><strong>최대 길이 도달:</strong> 미리 정해진 최대 출력 길이에 도달한 경우.</li>
<li><strong>사용자 정의 종료 조건:</strong> 특정 조건(예: 문장 부호)을 만족하는 경우.</li>
<li><strong>특수 토큰 생성:</strong> 문장 끝을 나타내는 특수 토큰 ( <code>&lt;eos&gt;</code>, <code>&lt;/s&gt;</code> 등)이 생성된 경우. 디코더는 훈련 과정에서 문장 끝에 이 특수 토큰을 추가하는 법을 학습합니다.</li>
</ol></li>
<li><strong>토큰 생성 전략</strong></li>
</ul>
<p>일반적으로 디코더에 포함되지 않지만 출력 생성 결과에 영향을 미치는 것에는 토큰생성전략이 있습니다.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 16%">
<col style="width: 13%">
<col style="width: 32%">
<col style="width: 17%">
<col style="width: 20%">
</colgroup>
<thead>
<tr class="header">
<th>생성 전략</th>
<th>작동 방식</th>
<th>장점</th>
<th>단점</th>
<th>예시</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Greedy Search</strong></td>
<td>매 스텝, 최고 확률 토큰 선택</td>
<td>빠름, 구현 간단</td>
<td>국소 최적해 가능성, 다양성 부족</td>
<td>“나는” 다음 → “학교에” (최고 확률)</td>
</tr>
<tr class="even">
<td><strong>Beam Search</strong></td>
<td><code>k</code>개 경로 동시 추적</td>
<td>넓은 탐색, 더 나은 결과 가능</td>
<td>계산 비용 높음, 제한적 다양성</td>
<td><code>k=2</code>: “나는 학교에”, “나는 집에” 유지 후 다음 스텝 진행</td>
</tr>
<tr class="odd">
<td><strong>Top-k Sampling</strong></td>
<td>확률 상위 <code>k</code>개 중 확률 비례 선택</td>
<td>적절한 다양성, 이상한 토큰 방지</td>
<td><code>k</code> 값 설정 어려움, 문맥 의존적 성능</td>
<td><code>k=3</code>: “나는” 다음 → {“학교에”, “집에”, “공원에”} 중 확률에 따라 선택</td>
</tr>
<tr class="even">
<td><strong>Nucleus Sampling</strong></td>
<td>누적 확률 <code>p</code>까지의 토큰 중 선택</td>
<td>동적 후보군, 문맥에 유연</td>
<td><code>p</code> 값 튜닝 필요, 계산 복잡도 증가</td>
<td><code>p=0.9</code>: “나는” 다음 → {“학교에”, “집에”, “공원에”, “밥을”} 중 누적 확률 0.9 넘지 않게 선택</td>
</tr>
<tr class="odd">
<td><strong>Temperature Sampling</strong></td>
<td>확률 분포 온도 조절 (낮으면 확실, 높으면 다양)</td>
<td>출력 창의성 조절, 구현 간단</td>
<td>너무 높으면 부적절, 너무 낮으면 반복적 텍스트 생성</td>
<td><code>T=0.5</code>: 높은 확률 강조, <code>T=1.5</code>: 낮은 확률도 선택 가능성 증가</td>
</tr>
</tbody>
</table>
<p>이러한 토큰 생성 전략은 일반적으로 디코더와는 별도의 클래스나 함수로 구현됩니다.</p>
</section>
<section id="전체-구조의-설명" class="level3">
<h3 class="anchored" data-anchor-id="전체-구조의-설명">8.4.4 전체 구조의 설명</h3>
<p>지금까지 트랜스포머의 설계 의도와 작동 원리를 이해했습니다. 8.4.3까지 설명한 내용을 바탕으로 트랜스포머의 전체 구조를 살펴보겠습니다. 구현은 Havard NLP의 내용을 참고하여 모듈화 등 구조적으로 변경했으며, 학습 목적을 위해 가능한 간결하게 작성했습니다. 실제 프로덕션 환경에서는 코드 안정성을 위한 타입 힌팅, 다차원 텐서의 효율적인 처리, 입력 검증 및 에러 핸들링, 메모리 최적화, 다양한 설정을 지원하기 위한 확장성 등이 추가로 필요합니다.</p>
<p>코드는 <code>chapter_08/transformer</code> 디렉토리에 있습니다.</p>
<p><strong>임베딩 레이어의 역할과 구현</strong></p>
<p>트랜스포머의 첫 단계는 입력 토큰을 벡터 공간으로 변환하는 임베딩 레이어입니다. 입력은 정수형 토큰 ID의 시퀀스(예: [101, 2045, 3012, …])이며, 각 토큰 ID는 어휘 사전의 고유 인덱스입니다. 임베딩 레이어는 이 ID를 고차원 벡터(임베딩 벡터)로 매핑합니다.</p>
<p>임베딩 차원은 모델의 성능에 큰 영향을 미칩니다. 큰 차원은 풍부한 의미 정보를 표현할 수 있지만 계산 비용이 증가하고, 작은 차원은 그 반대입니다.</p>
<p>임베딩 레이어를 통과하면 텐서 차원은 다음과 같이 변화합니다.</p>
<ul>
<li>입력: (batch_size, seq_length) → 출력: (batch_size, seq_length, hidden_size)</li>
<li>예: (32, 50) → (32, 50, 768)</li>
</ul>
<p>다음은 트랜스포머에서 임베딩을 수행하는 코드 예시입니다.</p>
<div id="cell-81" class="cell" data-execution_count="20">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_08.transformer.config <span class="im">import</span> TransformerConfig</span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_08.transformer.embeddings <span class="im">import</span> Embeddings</span>
<span id="cb44-4"><a href="#cb44-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-5"><a href="#cb44-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a configuration object</span></span>
<span id="cb44-6"><a href="#cb44-6" aria-hidden="true" tabindex="-1"></a>config <span class="op">=</span> TransformerConfig()</span>
<span id="cb44-7"><a href="#cb44-7" aria-hidden="true" tabindex="-1"></a>config.vocab_size <span class="op">=</span> <span class="dv">1000</span>  <span class="co"># Vocabulary size</span></span>
<span id="cb44-8"><a href="#cb44-8" aria-hidden="true" tabindex="-1"></a>config.hidden_size <span class="op">=</span> <span class="dv">768</span>  <span class="co"># Embedding dimension</span></span>
<span id="cb44-9"><a href="#cb44-9" aria-hidden="true" tabindex="-1"></a>config.max_position_embeddings <span class="op">=</span> <span class="dv">512</span>  <span class="co"># Maximum sequence length</span></span>
<span id="cb44-10"><a href="#cb44-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-11"><a href="#cb44-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Create an embedding layer</span></span>
<span id="cb44-12"><a href="#cb44-12" aria-hidden="true" tabindex="-1"></a>embedding_layer <span class="op">=</span> Embeddings(config)</span>
<span id="cb44-13"><a href="#cb44-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-14"><a href="#cb44-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate random input tokens</span></span>
<span id="cb44-15"><a href="#cb44-15" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb44-16"><a href="#cb44-16" aria-hidden="true" tabindex="-1"></a>seq_length <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb44-17"><a href="#cb44-17" aria-hidden="true" tabindex="-1"></a>input_ids <span class="op">=</span> torch.tensor([</span>
<span id="cb44-18"><a href="#cb44-18" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">1</span>, <span class="dv">5</span>, <span class="dv">9</span>, <span class="dv">2</span>],  <span class="co"># First sequence</span></span>
<span id="cb44-19"><a href="#cb44-19" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">6</span>, <span class="dv">3</span>, <span class="dv">7</span>, <span class="dv">4</span>]   <span class="co"># Second sequence</span></span>
<span id="cb44-20"><a href="#cb44-20" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb44-21"><a href="#cb44-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-22"><a href="#cb44-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform embedding</span></span>
<span id="cb44-23"><a href="#cb44-23" aria-hidden="true" tabindex="-1"></a>embedded <span class="op">=</span> embedding_layer(input_ids)</span>
<span id="cb44-24"><a href="#cb44-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-25"><a href="#cb44-25" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Input shape: </span><span class="sc">{</span>input_ids<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb44-26"><a href="#cb44-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Output: Input shape: torch.Size([2, 4])</span></span>
<span id="cb44-27"><a href="#cb44-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-28"><a href="#cb44-28" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Shape after embedding: </span><span class="sc">{</span>embedded<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb44-29"><a href="#cb44-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Output: Shape after embedding: torch.Size([2, 4, 768])</span></span>
<span id="cb44-30"><a href="#cb44-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-31"><a href="#cb44-31" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Part of the embedding vector for the first token of the first sequence:"</span>)</span>
<span id="cb44-32"><a href="#cb44-32" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(embedded[<span class="dv">0</span>, <span class="dv">0</span>, :<span class="dv">10</span>])  <span class="co"># Print only the first 10 dimensions</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Input shape: torch.Size([2, 4])
Shape after embedding: torch.Size([2, 4, 768])

Part of the embedding vector for the first token of the first sequence:
tensor([-0.7838, -0.9194,  0.4240, -0.8408, -0.0876,  2.0239,  1.3892, -0.4484,
        -0.6902,  1.1443], grad_fn=&lt;SliceBackward0&gt;)</code></pre>
</div>
</div>
<p><strong>설정 클래스</strong></p>
<p><code>TransformerConfig</code> 클래스는 모델의 모든 하이퍼파라미터를 정의합니다.</p>
<div id="cell-83" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> TransformerConfig:</span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.vocab_size <span class="op">=</span> <span class="dv">30000</span>          <span class="co"># Vocabulary size</span></span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hidden_size <span class="op">=</span> <span class="dv">768</span>           <span class="co"># Hidden layer dimension</span></span>
<span id="cb46-5"><a href="#cb46-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.num_hidden_layers <span class="op">=</span> <span class="dv">12</span>      <span class="co"># Number of encoder/decoder layers</span></span>
<span id="cb46-6"><a href="#cb46-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.num_attention_heads <span class="op">=</span> <span class="dv">12</span>    <span class="co"># Number of attention heads</span></span>
<span id="cb46-7"><a href="#cb46-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.intermediate_size <span class="op">=</span> <span class="dv">3072</span>    <span class="co"># FFN intermediate layer dimension</span></span>
<span id="cb46-8"><a href="#cb46-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hidden_dropout_prob <span class="op">=</span> <span class="fl">0.1</span>   <span class="co"># Hidden layer dropout probability</span></span>
<span id="cb46-9"><a href="#cb46-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.attention_probs_dropout_prob <span class="op">=</span> <span class="fl">0.1</span>  <span class="co"># Attention dropout probability</span></span>
<span id="cb46-10"><a href="#cb46-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.max_position_embeddings <span class="op">=</span> <span class="dv">512</span>  <span class="co"># Maximum sequence length</span></span>
<span id="cb46-11"><a href="#cb46-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.layer_norm_eps <span class="op">=</span> <span class="fl">1e-12</span>      <span class="co"># Layer normalization epsilon</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><code>vocab_size</code>는 모델이 처리할 수 있는 고유 토큰의 총 개수입니다. 여기서는 간단한 구현을 위해 단어 단위 토큰화를 가정하여 30,000개로 설정했습니다. 실제 언어 모델에서는 BPE(Byte Pair Encoding), Unigram, WordPiece 등 다양한 서브워드 토크나이저를 사용하며, 이 경우 <code>vocab_size</code>는 더 작아질 수 있습니다. 예를 들어, ’playing’이라는 단어를 ’play’와 ’ing’로 분리하면 두 개의 서브워드만으로 표현 가능합니다.</p>
<p><strong>어텐션의 텐서 차원 변화</strong></p>
<p>멀티 헤드 어텐션에서는 각 헤드가 독립적으로 어텐션을 계산하기 위해 입력 텐서의 차원을 재배열합니다.</p>
<div id="cell-85" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MultiHeadAttention(nn.Module):</span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, query, key, value, mask<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a>        batch_size <span class="op">=</span> query.size(<span class="dv">0</span>)</span>
<span id="cb47-4"><a href="#cb47-4" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb47-5"><a href="#cb47-5" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Linear transformations and head splitting</span></span>
<span id="cb47-6"><a href="#cb47-6" aria-hidden="true" tabindex="-1"></a>        query <span class="op">=</span> <span class="va">self</span>.linears[<span class="dv">0</span>](query).view(batch_size, <span class="op">-</span><span class="dv">1</span>, <span class="va">self</span>.h, <span class="va">self</span>.d_k).transpose(<span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb47-7"><a href="#cb47-7" aria-hidden="true" tabindex="-1"></a>        key <span class="op">=</span> <span class="va">self</span>.linears[<span class="dv">1</span>](key).view(batch_size, <span class="op">-</span><span class="dv">1</span>, <span class="va">self</span>.h, <span class="va">self</span>.d_k).transpose(<span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb47-8"><a href="#cb47-8" aria-hidden="true" tabindex="-1"></a>        value <span class="op">=</span> <span class="va">self</span>.linears[<span class="dv">2</span>](value).view(batch_size, <span class="op">-</span><span class="dv">1</span>, <span class="va">self</span>.h, <span class="va">self</span>.d_k).transpose(<span class="dv">1</span>, <span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>차원 변환 과정은 다음과 같습니다.</p>
<ol type="1">
<li>입력: (batch_size, seq_len, d_model)</li>
<li>선형 변환: (batch_size, seq_len, d_model)</li>
<li><code>view</code>: (batch_size, seq_len, h, d_k)</li>
<li><code>transpose</code>: (batch_size, h, seq_len, d_k)</li>
</ol>
<p>여기서 h는 헤드 수, d_k는 각 헤드의 차원(d_model / h)입니다. 이러한 차원 재배열을 통해 각 헤드가 독립적으로 어텐션을 계산합니다.</p>
<p><strong>트랜스포머의 통합 구조</strong></p>
<p>마지막으로 모든 구성 요소를 통합하는 <code>Transformer</code> 클래스를 살펴보겠습니다.</p>
<div id="cell-87" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Transformer(nn.Module):</span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config: TransformerConfig):</span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.encoder <span class="op">=</span> TransformerEncoder(config)</span>
<span id="cb48-5"><a href="#cb48-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.decoder <span class="op">=</span> TransformerDecoder(config)</span>
<span id="cb48-6"><a href="#cb48-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.generator <span class="op">=</span> nn.Linear(config.hidden_size, config.vocab_size)</span>
<span id="cb48-7"><a href="#cb48-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._init_weights()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>트랜스포머는 세 가지 주요 구성 요소로 이루어집니다.</p>
<ol type="1">
<li>인코더: 입력 시퀀스를 처리합니다.</li>
<li>디코더: 출력 시퀀스를 생성합니다.</li>
<li>생성기: 디코더 출력을 어휘 확률로 변환합니다.</li>
</ol>
<p><code>forward</code> 메서드는 다음 순서로 데이터를 처리합니다.</p>
<div id="cell-89" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> forward(<span class="va">self</span>, src, tgt, src_mask<span class="op">=</span><span class="va">None</span>, tgt_mask<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Encoder-decoder processing</span></span>
<span id="cb49-3"><a href="#cb49-3" aria-hidden="true" tabindex="-1"></a>    encoder_output <span class="op">=</span> <span class="va">self</span>.encode(src, src_mask)</span>
<span id="cb49-4"><a href="#cb49-4" aria-hidden="true" tabindex="-1"></a>    decoder_output <span class="op">=</span> <span class="va">self</span>.decode(encoder_output, src_mask, tgt, tgt_mask)</span>
<span id="cb49-5"><a href="#cb49-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb49-6"><a href="#cb49-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Generate final output</span></span>
<span id="cb49-7"><a href="#cb49-7" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> <span class="va">self</span>.generator(decoder_output)</span>
<span id="cb49-8"><a href="#cb49-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> F.log_softmax(output, dim<span class="op">=-</span><span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>텐서의 차원 변화는 다음과 같습니다.</p>
<ol type="1">
<li>입력 (<code>src</code>, <code>tgt</code>): (batch_size, seq_len)</li>
<li>인코더 출력: (batch_size, src_len, hidden_size)</li>
<li>디코더 출력: (batch_size, tgt_len, hidden_size)</li>
<li>최종 출력: (batch_size, tgt_len, vocab_size)</li>
</ol>
<p>다음 섹션에서는 이 구조를 실제 예제에 적용해 보겠습니다.</p>
</section>
</section>
<section id="트랜스포머-예제" class="level2">
<h2 class="anchored" data-anchor-id="트랜스포머-예제">8.5 트랜스포머 예제</h2>
<p>지금까지 트랜스포머의 구조와 작동 원리를 살펴보았습니다. 이제 실제 예제를 통해 트랜스포머의 동작을 확인해보겠습니다. 예제는 난이도 순으로 구성했으며, 각각의 예제는 트랜스포머의 특정 기능을 이해할 수 있도록 합니다. 예제들은 실제 프로젝트에서 마주치는 다양한 데이터 처리와 모델 설계 문제를 단계적으로 해결하는 방법을 보여줍니다. 특히 데이터 전처리, 손실함수 설계, 평가 지표 설정 등 실무에서 필요한 내용을 다룹니다. 예제의 위치는 transformer/examples에 있습니다.</p>
<pre class="text"><code>examples
├── addition_task.py  # 8.5.2 덧셈 문제 태스크
├── copy_task.py      # 8.5.1 단순 복사 태스크
└── parser_task.py    # 8.5.3 파서 태스크</code></pre>
<p>각 예제에서 배우는 내용은 다음과 같습니다.</p>
<p><strong>단순 복사 태스크</strong>는 트랜스포머의 기본 기능을 이해할 수 있습니다. 어텐션 패턴 시각화를 통해 모델의 동작 원리를 명확히 이해할 수 있습니다. 더불어 시퀀스 데이터의 기본적인 처리 방법, 배치 처리를 위한 텐서 차원 설계, 기본적인 패딩과 마스킹 전략, 태스크에 특화된 손실함수 설계를 학습할 수 있습니다.</p>
<p><strong>자릿수 덧셈 문제</strong>는 자기회귀적 생성을 어떻게 가능하게 하는지 보여줍니다. 디코더의 순차적 생성 과정과 교차 어텐션의 역할을 명확히 관찰할 수 있습니다. 이와 함께 숫자 데이터의 토큰화, 유효한 데이터셋 생성 방법, 부분/전체 정확도 평가, 자릿수 확장에 따른 일반화 성능 테스트 등 실무적 경험을 제공합니다.</p>
<p><strong>파서 태스크</strong>는 트랜스포머가 구조적 관계를 어떻게 학습하고 표현하는지 보여줍니다. 입력 시퀀스의 계층적 구조를 어텐션 메커니즘이 어떻게 포착하는지 이해할 수 있습니다. 또한 구조적 데이터의 시퀀스 변환, 토큰 사전 설계, 트리 구조의 선형화 전략, 구조적 정확성 평가 방법 등 실제 파싱 문제에서 필요한 다양한 기술을 익힐 수 있습니다.</p>
<p>다음은 각 예제에서 학습할 내용을 정리한 표입니다.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 71%">
<col style="width: 28%">
</colgroup>
<thead>
<tr class="header">
<th>예제</th>
<th>학습 내용</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>8.5.1 단순 복사 태스크 (copy_task.py)</td>
<td>- 트랜스포머 기본 기능 및 동작 원리 이해<br> - 어텐션 패턴 시각화를 통한 직관적 이해<br> - 시퀀스 데이터 처리 및 배치 처리를 위한 텐서 차원 설계<br> - 패딩 및 마스킹 전략<br> - 태스크에 적합한 손실 함수 설계</td>
</tr>
<tr class="even">
<td>8.5.2 덧셈 문제 태스크 (addition_task.py)</td>
<td>- 트랜스포머의 자기 회귀적(autoregressive) 생성 과정 학습<br> - 디코더의 순차적 생성 및 교차 어텐션(cross-attention) 역할 관찰<br> - 숫자 데이터 토큰화, 유효 데이터셋 생성 방법<br> - 부분/전체 정확도 평가, 자릿수 확장에 따른 일반화 성능 테스트</td>
</tr>
<tr class="odd">
<td>8.5.3 파서 태스크 (parser_task.py)</td>
<td>- 트랜스포머가 구조적 관계를 학습하고 표현하는 방법 이해<br> - 입력 시퀀스의 계층적 구조를 포착하는 어텐션 메커니즘 이해<br> - 구조적 데이터의 시퀀스 변환, 토큰 사전 설계<br> - 트리 구조의 선형화 전략, 구조적 정확성 평가 방법</td>
</tr>
</tbody>
</table>
<section id="단순-복사-태스크" class="level3">
<h3 class="anchored" data-anchor-id="단순-복사-태스크">8.5.1 단순 복사 태스크</h3>
<p>첫 번째 예제는 입력 시퀀스를 그대로 출력하는 복사 태스크입니다. 이 태스크는 트랜스포머의 기본 동작을 확인하고 어텐션 패턴을 시각화하기에 적합하며, 간단해 보이지만 트랜스포머의 핵심 메커니즘을 이해하는 데 매우 유용합니다.</p>
<p><strong>데이터 준비</strong></p>
<p>복사 태스크의 데이터는 입력과 출력이 동일한 시퀀스로 구성됩니다. 다음은 데이터 생성 예시입니다.</p>
<div id="cell-92" class="cell" data-execution_count="1">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_08.transformer.examples.copy_task <span class="im">import</span> explain_copy_data</span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-3"><a href="#cb51-3" aria-hidden="true" tabindex="-1"></a>explain_copy_data(seq_length<span class="op">=</span><span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
=== Copy Task Data Explanation ===
Sequence Length: 5

1. Input Sequence:
Original Tensor Shape: torch.Size([1, 5])
Input Sequence: [7, 15, 2, 3, 12]

2. Target Sequence:
Original Tensor Shape: torch.Size([1, 5])
Target Sequence: [7, 15, 2, 3, 12]

3. Task Description:
- Basic task of copying the input sequence as is
- Tokens at each position are integer values between 1-19
- Input and output have the same sequence length
- Current Example: [7, 15, 2, 3, 12] → [7, 15, 2, 3, 12]</code></pre>
</div>
</div>
<p>create_copy_data는 입력과 출력이 동일한 텐서를 학습을 위해 생성합니다. 배치 처리를 위한 2차원 텐서 (batch_size, seq_length)를 생성하며, 각 원소는 1부터 19 사이의 정수값입니다</p>
<div id="cell-94" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_copy_data(batch_size: <span class="bu">int</span> <span class="op">=</span> <span class="dv">32</span>, seq_length: <span class="bu">int</span> <span class="op">=</span> <span class="dv">5</span>) <span class="op">-&gt;</span> torch.Tensor:</span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""복사 태스크용 데이터 생성"""</span></span>
<span id="cb53-3"><a href="#cb53-3" aria-hidden="true" tabindex="-1"></a>    sequences <span class="op">=</span> torch.randint(<span class="dv">1</span>, <span class="dv">20</span>, (batch_size, seq_length))</span>
<span id="cb53-4"><a href="#cb53-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> sequences, sequences</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>이 예제의 데이터는 자연어 처리나 시퀀스 모델링에서 사용되는 토큰화된 입력 데이터와 동일합니다. 언어처리에서 각 토큰은 고유한 정수값으로 변환된 후 모델에 입력됩니다.</p>
<p><strong>모델 학습</strong></p>
<p>다음 코드로 모델을 학습시킵니다.</p>
<div id="cell-96" class="cell" data-execution_count="1">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb54"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_08.transformer.config <span class="im">import</span> TransformerConfig</span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_08.transformer.examples.copy_task <span class="im">import</span> train_copy_task</span>
<span id="cb54-3"><a href="#cb54-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-4"><a href="#cb54-4" aria-hidden="true" tabindex="-1"></a>seq_length <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb54-5"><a href="#cb54-5" aria-hidden="true" tabindex="-1"></a>config <span class="op">=</span> TransformerConfig()</span>
<span id="cb54-6"><a href="#cb54-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Modify default values</span></span>
<span id="cb54-7"><a href="#cb54-7" aria-hidden="true" tabindex="-1"></a>config.vocab_size <span class="op">=</span> <span class="dv">20</span>           <span class="co"># Small vocabulary size (minimum size to represent integers 1-19)</span></span>
<span id="cb54-8"><a href="#cb54-8" aria-hidden="true" tabindex="-1"></a>config.hidden_size <span class="op">=</span> <span class="dv">64</span>          <span class="co"># Small hidden dimension (enough representation for a simple task)</span></span>
<span id="cb54-9"><a href="#cb54-9" aria-hidden="true" tabindex="-1"></a>config.num_hidden_layers <span class="op">=</span> <span class="dv">2</span>     <span class="co"># Minimum number of layers (considering the low complexity of the copy task)</span></span>
<span id="cb54-10"><a href="#cb54-10" aria-hidden="true" tabindex="-1"></a>config.num_attention_heads <span class="op">=</span> <span class="dv">2</span>   <span class="co"># Minimum number of heads (minimum configuration for attention from various perspectives)</span></span>
<span id="cb54-11"><a href="#cb54-11" aria-hidden="true" tabindex="-1"></a>config.intermediate_size <span class="op">=</span> <span class="dv">128</span>   <span class="co"># Small FFN dimension (set to twice the hidden dimension to ensure adequate transformation capacity)</span></span>
<span id="cb54-12"><a href="#cb54-12" aria-hidden="true" tabindex="-1"></a>config.max_position_embeddings <span class="op">=</span> seq_length  <span class="co"># Short sequence length (set to the same length as the input sequence)</span></span>
<span id="cb54-13"><a href="#cb54-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-14"><a href="#cb54-14" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> train_copy_task(config, num_epochs<span class="op">=</span><span class="dv">50</span>, batch_size<span class="op">=</span><span class="dv">40</span>, steps_per_epoch<span class="op">=</span><span class="dv">100</span>, seq_length<span class="op">=</span>seq_length)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
=== Start Training ==== 
Device: cuda:0
Model saved to saved_models/transformer_copy_task.pth</code></pre>
</div>
</div>
<p><strong>모델 테스트</strong></p>
<p>저장된 훈련 모델을 읽어와 테스트를 수행합니다.</p>
<div id="cell-98" class="cell" data-execution_count="1">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb56"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_08.transformer.examples.copy_task <span class="im">import</span> test_copy</span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-3"><a href="#cb56-3" aria-hidden="true" tabindex="-1"></a>test_copy(seq_length<span class="op">=</span><span class="dv">20</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
=== Copy Test ===
Input: [10, 10, 2, 12, 1, 5, 3, 1, 8, 18, 2, 19, 2, 2, 8, 14, 7, 19, 5, 4]
Output: [10, 10, 2, 12, 1, 5, 3, 1, 8, 18, 2, 19, 2, 2, 8, 14, 7, 19, 5, 4]
Accuracy: True</code></pre>
</div>
</div>
<p><strong>모델 설정</strong></p>
<ul>
<li><code>hidden_size</code>: 64 (모델의 설계 차원, d_model).
<ul>
<li>트랜스포머에서 설계 차원(d_model)과 같은 값:
<ol type="1">
<li>단어 임베딩 차원</li>
<li>포지셔널 임베딩 차원</li>
<li>어텐션의 Q, K, V 벡터 차원</li>
<li>인코더/디코더의 각 서브층 출력 차원</li>
</ol></li>
</ul></li>
<li><code>intermediate_size</code>: FFN의 크기로, d_model보다 충분히 커야 합니다.</li>
</ul>
<p><strong>마스킹 구현</strong></p>
<p>트랜스포머는 두 가지 종류의 마스크를 사용합니다.</p>
<ol type="1">
<li><strong>패딩 마스크 (Padding Mask)</strong>: 배치 처리를 위해 추가된 패딩 토큰을 무시합니다.
<ul>
<li>이 예제는 <code>seq_length</code>로 길이가 동일하여 패딩이 불필요하지만, 트랜스포머의 일반적인 사용을 위해 포함.</li>
<li><code>create_pad_mask</code> 함수를 직접 구현 (PyTorch의 <code>nn.Transformer</code>나 Hugging Face의 <code>transformers</code> 라이브러리에서는 내부적으로 구현).</li>
</ul></li>
</ol>
<div id="cell-100" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb58"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a>src_mask <span class="op">=</span> create_pad_mask(src).to(device)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<ol start="2" type="1">
<li><strong>후속 마스크 (Subsequent Mask)</strong>: 디코더의 자기 회귀적 생성을 위해 사용됩니다.
<ul>
<li><code>create_subsequent_mask</code> 함수는 현재 위치 이후의 토큰을 가리는 상삼각 행렬 형태의 마스크를 생성.</li>
<li>디코더가 이전에 생성된 토큰만 참조하여 다음 토큰을 예측하도록 합니다.</li>
</ul></li>
</ol>
<div id="cell-102" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb59"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a>tgt_mask <span class="op">=</span> create_subsequent_mask(decoder_input.size(<span class="dv">1</span>)).to(device)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>이러한 마스킹은 배치 처리의 효율성과 시퀀스 인과성(causality)을 보장합니다.</p>
<p><strong>손실 함수의 설계</strong></p>
<p><code>CopyLoss</code> 클래스는 복사 태스크를 위한 손실 함수를 구현합니다.</p>
<ul>
<li>각 토큰 위치별 정확도와 전체 시퀀스의 완전 일치 여부를 모두 고려.</li>
<li>정확도, 손실값, 예측/실제 값을 상세히 모니터링하여 학습 진행 상황을 세밀하게 파악.</li>
</ul>
<div id="cell-104" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb60"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> CopyLoss(nn.Module):</span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, outputs: torch.Tensor, target: torch.Tensor, </span>
<span id="cb60-3"><a href="#cb60-3" aria-hidden="true" tabindex="-1"></a>                print_details: <span class="bu">bool</span> <span class="op">=</span> <span class="va">False</span>) <span class="op">-&gt;</span> Tuple[torch.Tensor, <span class="bu">float</span>]:</span>
<span id="cb60-4"><a href="#cb60-4" aria-hidden="true" tabindex="-1"></a>        batch_size <span class="op">=</span> outputs.size(<span class="dv">0</span>)</span>
<span id="cb60-5"><a href="#cb60-5" aria-hidden="true" tabindex="-1"></a>        predictions <span class="op">=</span> F.softmax(outputs, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb60-6"><a href="#cb60-6" aria-hidden="true" tabindex="-1"></a>        target_one_hot <span class="op">=</span> F.one_hot(target, num_classes<span class="op">=</span>outputs.size(<span class="op">-</span><span class="dv">1</span>)).<span class="bu">float</span>()</span>
<span id="cb60-7"><a href="#cb60-7" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb60-8"><a href="#cb60-8" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> <span class="op">-</span>torch.<span class="bu">sum</span>(target_one_hot <span class="op">*</span> torch.log(predictions <span class="op">+</span> <span class="fl">1e-10</span>)) <span class="op">/</span> batch_size</span>
<span id="cb60-9"><a href="#cb60-9" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb60-10"><a href="#cb60-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb60-11"><a href="#cb60-11" aria-hidden="true" tabindex="-1"></a>            pred_tokens <span class="op">=</span> predictions.argmax(dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb60-12"><a href="#cb60-12" aria-hidden="true" tabindex="-1"></a>            exact_match <span class="op">=</span> (pred_tokens <span class="op">==</span> target).<span class="bu">all</span>(dim<span class="op">=</span><span class="dv">1</span>).<span class="bu">float</span>()</span>
<span id="cb60-13"><a href="#cb60-13" aria-hidden="true" tabindex="-1"></a>            match_rate <span class="op">=</span> exact_match.mean().item()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<ul>
<li>크로스 엔트로피만으로는 부족: 개별 토큰 정확도 + 전체 시퀀스 일치 여부 평가.</li>
<li>모델이 순서를 정확하게 학습하도록 유도.</li>
</ul>
<p><strong>동작 예시</strong> (<code>batch_size=2</code>, <code>sequence_length=3</code>, <code>vocab_size=5</code>):</p>
<ol type="1">
<li><strong>모델 출력 (logits)</strong></li>
</ol>
<div id="cell-106" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb61"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Example: batch_size=2, sequence_length=3, vocab_size=5 (example is vocab_size=20)</span></span>
<span id="cb61-2"><a href="#cb61-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-3"><a href="#cb61-3" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Model Output (logits)</span></span>
<span id="cb61-4"><a href="#cb61-4" aria-hidden="true" tabindex="-1"></a>outputs <span class="op">=</span> [</span>
<span id="cb61-5"><a href="#cb61-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># First batch</span></span>
<span id="cb61-6"><a href="#cb61-6" aria-hidden="true" tabindex="-1"></a>    [[<span class="fl">0.9</span>, <span class="fl">0.1</span>, <span class="fl">0.0</span>, <span class="fl">0.0</span>, <span class="fl">0.0</span>],  <span class="co"># First position: token 0 has the highest probability</span></span>
<span id="cb61-7"><a href="#cb61-7" aria-hidden="true" tabindex="-1"></a>     [<span class="fl">0.1</span>, <span class="fl">0.8</span>, <span class="fl">0.1</span>, <span class="fl">0.0</span>, <span class="fl">0.0</span>],  <span class="co"># Second position: token 1 has the highest probability</span></span>
<span id="cb61-8"><a href="#cb61-8" aria-hidden="true" tabindex="-1"></a>     [<span class="fl">0.0</span>, <span class="fl">0.1</span>, <span class="fl">0.9</span>, <span class="fl">0.0</span>, <span class="fl">0.0</span>]], <span class="co"># Third position: token 2 has the highest probability</span></span>
<span id="cb61-9"><a href="#cb61-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Second batch</span></span>
<span id="cb61-10"><a href="#cb61-10" aria-hidden="true" tabindex="-1"></a>    [[<span class="fl">0.8</span>, <span class="fl">0.2</span>, <span class="fl">0.0</span>, <span class="fl">0.0</span>, <span class="fl">0.0</span>],</span>
<span id="cb61-11"><a href="#cb61-11" aria-hidden="true" tabindex="-1"></a>     [<span class="fl">0.1</span>, <span class="fl">0.7</span>, <span class="fl">0.2</span>, <span class="fl">0.0</span>, <span class="fl">0.0</span>],</span>
<span id="cb61-12"><a href="#cb61-12" aria-hidden="true" tabindex="-1"></a>     [<span class="fl">0.1</span>, <span class="fl">0.1</span>, <span class="fl">0.8</span>, <span class="fl">0.0</span>, <span class="fl">0.0</span>]]</span>
<span id="cb61-13"><a href="#cb61-13" aria-hidden="true" tabindex="-1"></a>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<ol start="2" type="1">
<li><strong>실제 타겟</strong></li>
</ol>
<div id="cell-108" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb62"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Actual Target</span></span>
<span id="cb62-2"><a href="#cb62-2" aria-hidden="true" tabindex="-1"></a>target <span class="op">=</span> [</span>
<span id="cb62-3"><a href="#cb62-3" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">2</span>],  <span class="co"># Correct sequence for the first batch</span></span>
<span id="cb62-4"><a href="#cb62-4" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">2</span>]   <span class="co"># Correct sequence for the second batch</span></span>
<span id="cb62-5"><a href="#cb62-5" aria-hidden="true" tabindex="-1"></a>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<ol start="3" type="1">
<li><strong>손실 계산 과정</strong>
<ul>
<li><code>predictions = softmax(outputs)</code> (이미 위에서 확률로 변환)</li>
<li><code>target</code>을 원-핫 벡터로 변환</li>
</ul></li>
</ol>
<div id="cell-110" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb63"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. Loss Calculation Process</span></span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a><span class="co"># predictions = softmax(outputs) (already converted to probabilities above)</span></span>
<span id="cb63-3"><a href="#cb63-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert target to one-hot vectors:</span></span>
<span id="cb63-4"><a href="#cb63-4" aria-hidden="true" tabindex="-1"></a>target_one_hot <span class="op">=</span> [</span>
<span id="cb63-5"><a href="#cb63-5" aria-hidden="true" tabindex="-1"></a>    [[<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>], [<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>], [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>]],  <span class="co"># First batch</span></span>
<span id="cb63-6"><a href="#cb63-6" aria-hidden="true" tabindex="-1"></a>    [[<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>], [<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>], [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>]]   <span class="co"># Second batch</span></span>
<span id="cb63-7"><a href="#cb63-7" aria-hidden="true" tabindex="-1"></a>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<ol start="4" type="1">
<li><strong>정확도 계산</strong></li>
</ol>
<div id="cell-112" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb64"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 4. Accuracy Calculation</span></span>
<span id="cb64-2"><a href="#cb64-2" aria-hidden="true" tabindex="-1"></a>pred_tokens <span class="op">=</span> [</span>
<span id="cb64-3"><a href="#cb64-3" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">2</span>],  <span class="co"># First batch prediction</span></span>
<span id="cb64-4"><a href="#cb64-4" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">2</span>]   <span class="co"># Second batch prediction</span></span>
<span id="cb64-5"><a href="#cb64-5" aria-hidden="true" tabindex="-1"></a>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<pre><code>-   시퀀스 전체 일치 여부: `exact_match = [True, True]` (두 배치 모두 정확)
-   평균 정확도: `match_rate = 1.0` (100%)</code></pre>
<ol start="5" type="1">
<li><strong>최종 손실값</strong>: 크로스 엔트로피의 평균</li>
</ol>
<div id="cell-114" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb66"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Exact sequence match</span></span>
<span id="cb66-2"><a href="#cb66-2" aria-hidden="true" tabindex="-1"></a>exact_match <span class="op">=</span> [<span class="va">True</span>, <span class="va">True</span>]  <span class="co"># Both batches match exactly</span></span>
<span id="cb66-3"><a href="#cb66-3" aria-hidden="true" tabindex="-1"></a>match_rate <span class="op">=</span> <span class="fl">1.0</span>  <span class="co"># Average accuracy 100%</span></span>
<span id="cb66-4"><a href="#cb66-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-5"><a href="#cb66-5" aria-hidden="true" tabindex="-1"></a><span class="co"># The final loss value is the average of the cross-entropy</span></span>
<span id="cb66-6"><a href="#cb66-6" aria-hidden="true" tabindex="-1"></a><span class="co"># loss = -1/2 * (log(0.9) + log(0.8) + log(0.9) + log(0.8) + log(0.7) + log(0.8))</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><strong>어텐션 시각화</strong></p>
<p>어텐션 시각화를 통해 트랜스포머의 동작을 직관적으로 이해합니다.</p>
<div id="cell-116" class="cell" data-execution_count="1">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb67"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_08.transformer.examples.copy_task <span class="im">import</span> visualize_attention</span>
<span id="cb67-2"><a href="#cb67-2" aria-hidden="true" tabindex="-1"></a>visualize_attention(seq_length<span class="op">=</span><span class="dv">20</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="08_트랜스포머의 탄생_files/figure-html/cell-52-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>각 입력 토큰이 다른 위치의 토큰과 어떻게 상호작용하는지 확인합니다.</p>
<p>이 복사 태스크 예제를 통해 트랜스포머의 핵심 메커니즘을 확인했습니다. 다음 예제(덧셈 문제)에서는 트랜스포머가 숫자 간의 관계, 자리 올림 등의 산술 규칙을 어떻게 학습하는지 살펴보겠습니다.</p>
</section>
<section id="자릿수-덧셈-문제" class="level3">
<h3 class="anchored" data-anchor-id="자릿수-덧셈-문제">8.5.2 자릿수 덧셈 문제</h3>
<p>두 번째 예제는 두 개의 숫자를 더하는 덧셈 태스크입니다. 이 태스크는 트랜스포머의 자기 회귀적(autoregressive) 생성 능력과 디코더의 순차적 계산 과정을 이해하는 데 적합합니다. 자리 올림이 있는 계산을 통해 트랜스포머가 숫자 간의 관계를 어떻게 학습하는지 관찰할 수 있습니다.</p>
<p><strong>데이터 준비</strong></p>
<p>덧셈 태스크의 데이터는 <code>create_addition_data()</code>에서 생성합니다.</p>
<div id="cell-119" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb68"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_addition_data(batch_size: <span class="bu">int</span> <span class="op">=</span> <span class="dv">32</span>, max_digits: <span class="bu">int</span> <span class="op">=</span> <span class="dv">3</span>) <span class="op">-&gt;</span> Tuple[torch.Tensor, torch.Tensor]:</span>
<span id="cb68-2"><a href="#cb68-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Create addition dataset"""</span></span>
<span id="cb68-3"><a href="#cb68-3" aria-hidden="true" tabindex="-1"></a>    max_value <span class="op">=</span> <span class="dv">10</span> <span class="op">**</span> max_digits <span class="op">-</span> <span class="dv">1</span></span>
<span id="cb68-4"><a href="#cb68-4" aria-hidden="true" tabindex="-1"></a>    num1 <span class="op">=</span> torch.randint(<span class="dv">0</span>, max_value <span class="op">//</span> <span class="dv">2</span> <span class="op">+</span> <span class="dv">1</span>, (batch_size,))</span>
<span id="cb68-5"><a href="#cb68-5" aria-hidden="true" tabindex="-1"></a>    num2 <span class="op">=</span> torch.randint(<span class="dv">0</span>, max_value <span class="op">//</span> <span class="dv">2</span> <span class="op">+</span> <span class="dv">1</span>, (batch_size,))</span>
<span id="cb68-6"><a href="#cb68-6" aria-hidden="true" tabindex="-1"></a>    result <span class="op">=</span> num1 <span class="op">+</span> num2</span>
<span id="cb68-7"><a href="#cb68-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-8"><a href="#cb68-8" aria-hidden="true" tabindex="-1"></a>    [See source below]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<ul>
<li>합이 지정한 자릿수를 넘지 않도록 두 숫자를 생성.</li>
<li>입력: 두 숫자 + ‘+’ 기호.</li>
<li>자릿수 제한 유효성 검사 포함.</li>
</ul>
<p><strong>학습 데이터 설명</strong></p>
<div id="cell-121" class="cell" data-execution_count="4">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb69"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_08.transformer.config <span class="im">import</span> TransformerConfig</span>
<span id="cb69-2"><a href="#cb69-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_08.transformer.examples.addition_task <span class="im">import</span> explain_addition_data</span>
<span id="cb69-3"><a href="#cb69-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-4"><a href="#cb69-4" aria-hidden="true" tabindex="-1"></a>explain_addition_data()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
=== Addition Data Explanation ====
Maximum Digits: 3

1. Input Sequence:
Original Tensor Shape: torch.Size([1, 7])
First Number: 153 (Indices [np.int64(1), np.int64(5), np.int64(3)])
Plus Sign: '+' (Index 10)
Second Number: 391 (Indices [np.int64(3), np.int64(9), np.int64(1)])
Full Input: [1, 5, 3, 10, 3, 9, 1]

2. Target Sequence:
Original Tensor Shape: torch.Size([1, 3])
Actual Sum: 544
Target Sequence: [5, 4, 4]</code></pre>
</div>
</div>
<p><strong>모델 학습 및 테스트</strong></p>
<div id="cell-123" class="cell" data-execution_count="1">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb71"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_08.transformer.config <span class="im">import</span> TransformerConfig</span>
<span id="cb71-2"><a href="#cb71-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_08.transformer.examples.addition_task <span class="im">import</span> train_addition_task</span>
<span id="cb71-3"><a href="#cb71-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-4"><a href="#cb71-4" aria-hidden="true" tabindex="-1"></a>config <span class="op">=</span> TransformerConfig()</span>
<span id="cb71-5"><a href="#cb71-5" aria-hidden="true" tabindex="-1"></a>config.vocab_size <span class="op">=</span> <span class="dv">11</span>       </span>
<span id="cb71-6"><a href="#cb71-6" aria-hidden="true" tabindex="-1"></a>config.hidden_size <span class="op">=</span> <span class="dv">256</span></span>
<span id="cb71-7"><a href="#cb71-7" aria-hidden="true" tabindex="-1"></a>config.num_hidden_layers <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb71-8"><a href="#cb71-8" aria-hidden="true" tabindex="-1"></a>config.num_attention_heads <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb71-9"><a href="#cb71-9" aria-hidden="true" tabindex="-1"></a>config.intermediate_size <span class="op">=</span> <span class="dv">512</span></span>
<span id="cb71-10"><a href="#cb71-10" aria-hidden="true" tabindex="-1"></a>config.max_position_embeddings <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb71-11"><a href="#cb71-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-12"><a href="#cb71-12" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> train_addition_task(config, num_epochs<span class="op">=</span><span class="dv">10</span>, batch_size<span class="op">=</span><span class="dv">128</span>, steps_per_epoch<span class="op">=</span><span class="dv">300</span>, max_digits<span class="op">=</span><span class="dv">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 0, Average Loss: 6.1352, Final Accuracy: 0.0073, Learning Rate: 0.000100
Epoch 5, Average Loss: 0.0552, Final Accuracy: 0.9852, Learning Rate: 0.000100

=== Loss Calculation Details (Step: 3000) ===
Predicted Sequences (First 10): tensor([[6, 5, 4],
        [5, 3, 3],
        [1, 7, 5],
        [6, 0, 6],
        [7, 5, 9],
        [5, 2, 8],
        [2, 8, 1],
        [3, 5, 8],
        [0, 7, 1],
        [6, 2, 1]], device='cuda:0')

Actual Target Sequences (First 10): tensor([[6, 5, 4],
        [5, 3, 3],
        [1, 7, 5],
        [6, 0, 6],
        [7, 5, 9],
        [5, 2, 8],
        [2, 8, 1],
        [3, 5, 8],
        [0, 7, 1],
        [6, 2, 1]], device='cuda:0')

Exact Match per Sequence (First 10): tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')

Calculated Loss: 0.0106
Calculated Accuracy: 1.0000
========================================
Model saved to saved_models/transformer_addition_task.pth</code></pre>
</div>
</div>
<p>학습이 끝나면 저장된 모델을 불러와서 테스트를 수행합니다.</p>
<div id="cell-125" class="cell" data-execution_count="3">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb73"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_08.transformer.examples.addition_task <span class="im">import</span> test_addition</span>
<span id="cb73-2"><a href="#cb73-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-3"><a href="#cb73-3" aria-hidden="true" tabindex="-1"></a>test_addition(max_digits<span class="op">=</span><span class="dv">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
Addition Test (Digits: 3):
310 + 98 = 408 (Actual Answer: 408)
Correct: True</code></pre>
</div>
</div>
<p><strong>모델 설정</strong></p>
<p>덧셈 태스크를 위한 트랜스포머 설정은 다음과 같습니다.</p>
<div id="cell-127" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb75"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb75-1"><a href="#cb75-1" aria-hidden="true" tabindex="-1"></a>config <span class="op">=</span> TransformerConfig()</span>
<span id="cb75-2"><a href="#cb75-2" aria-hidden="true" tabindex="-1"></a>config.vocab_size <span class="op">=</span> <span class="dv">11</span>          <span class="co"># 0-9 digits + '+' symbol</span></span>
<span id="cb75-3"><a href="#cb75-3" aria-hidden="true" tabindex="-1"></a>config.hidden_size <span class="op">=</span> <span class="dv">256</span>        <span class="co"># Larger hidden dimension than copy task (sufficient capacity for learning arithmetic operations)</span></span>
<span id="cb75-4"><a href="#cb75-4" aria-hidden="true" tabindex="-1"></a>config.num_hidden_layers <span class="op">=</span> <span class="dv">3</span>    <span class="co"># Deeper layers (hierarchical feature extraction for handling carry operations)</span></span>
<span id="cb75-5"><a href="#cb75-5" aria-hidden="true" tabindex="-1"></a>config.num_attention_heads <span class="op">=</span> <span class="dv">4</span>  <span class="co"># Increased number of heads (learning relationships between different digit positions)</span></span>
<span id="cb75-6"><a href="#cb75-6" aria-hidden="true" tabindex="-1"></a>config.intermediate_size <span class="op">=</span> <span class="dv">512</span>  <span class="co">#  FFN dimension: should be larger than hidden_size.</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><strong>마스킹 구현</strong></p>
<p>덧셈 태스크에서 패딩 마스크는 <em>필수적</em> 입니다. 입력 숫자 자릿수가 다를 수 있으므로, 패딩 위치를 무시해야 정확한 계산이 가능합니다.</p>
<div id="cell-129" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb76"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb76-1"><a href="#cb76-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> _number_to_digits(number: torch.Tensor, max_digits: <span class="bu">int</span>) <span class="op">-&gt;</span> torch.Tensor:</span>
<span id="cb76-2"><a href="#cb76-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""숫자를 자릿수 시퀀스로 변환하며 패딩 적용"""</span></span>
<span id="cb76-3"><a href="#cb76-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> torch.tensor([[<span class="bu">int</span>(d) <span class="cf">for</span> d <span class="kw">in</span> <span class="bu">str</span>(n.item()).zfill(max_digits)] </span>
<span id="cb76-4"><a href="#cb76-4" aria-hidden="true" tabindex="-1"></a>                        <span class="cf">for</span> n <span class="kw">in</span> number])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>위 메쏘드의 동작은 구체적으로 다음과 같습니다.</p>
<div id="cell-131" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb77"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb77-1"><a href="#cb77-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-2"><a href="#cb77-2" aria-hidden="true" tabindex="-1"></a>number <span class="op">=</span> torch.tensor([<span class="dv">7</span>, <span class="dv">25</span>, <span class="dv">348</span>])</span>
<span id="cb77-3"><a href="#cb77-3" aria-hidden="true" tabindex="-1"></a>max_digits <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb77-4"><a href="#cb77-4" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> _number_to_digits(number, max_digits)</span>
<span id="cb77-5"><a href="#cb77-5" aria-hidden="true" tabindex="-1"></a><span class="co"># 입력: [7, 25, 348]</span></span>
<span id="cb77-6"><a href="#cb77-6" aria-hidden="true" tabindex="-1"></a><span class="co"># 과정: </span></span>
<span id="cb77-7"><a href="#cb77-7" aria-hidden="true" tabindex="-1"></a><span class="co">#   7   -&gt; "7"   -&gt; "007" -&gt; [0,0,7]</span></span>
<span id="cb77-8"><a href="#cb77-8" aria-hidden="true" tabindex="-1"></a><span class="co">#   25  -&gt; "25"  -&gt; "025" -&gt; [0,2,5]</span></span>
<span id="cb77-9"><a href="#cb77-9" aria-hidden="true" tabindex="-1"></a><span class="co">#   348 -&gt; "348" -&gt; "348" -&gt; [3,4,8]</span></span>
<span id="cb77-10"><a href="#cb77-10" aria-hidden="true" tabindex="-1"></a><span class="co"># 결과: tensor([[0, 0, 7],</span></span>
<span id="cb77-11"><a href="#cb77-11" aria-hidden="true" tabindex="-1"></a><span class="co">#               [0, 2, 5],</span></span>
<span id="cb77-12"><a href="#cb77-12" aria-hidden="true" tabindex="-1"></a><span class="co">#               [3, 4, 8]])</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><strong>손실 함수의 설계</strong></p>
<p><code>AdditionLoss</code> 클래스는 덧셈 태스크의 손실 함수를 구현합니다.</p>
<ul>
<li>복사 태스크와 달리, <em>자릿수별 정확도</em>와 <em>전체 답의 정확도</em>를 구분하여 평가.</li>
</ul>
<div id="cell-133" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb78"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb78-1"><a href="#cb78-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> AdditionLoss(nn.Module):</span>
<span id="cb78-2"><a href="#cb78-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, outputs: torch.Tensor, target: torch.Tensor, </span>
<span id="cb78-3"><a href="#cb78-3" aria-hidden="true" tabindex="-1"></a>                print_details: <span class="bu">bool</span> <span class="op">=</span> <span class="va">False</span>) <span class="op">-&gt;</span> Tuple[torch.Tensor, <span class="bu">float</span>]:</span>
<span id="cb78-4"><a href="#cb78-4" aria-hidden="true" tabindex="-1"></a>        batch_size <span class="op">=</span> outputs.size(<span class="dv">0</span>)</span>
<span id="cb78-5"><a href="#cb78-5" aria-hidden="true" tabindex="-1"></a>        predictions <span class="op">=</span> F.softmax(outputs, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb78-6"><a href="#cb78-6" aria-hidden="true" tabindex="-1"></a>        target_one_hot <span class="op">=</span> F.one_hot(target, num_classes<span class="op">=</span>outputs.size(<span class="op">-</span><span class="dv">1</span>)).<span class="bu">float</span>()</span>
<span id="cb78-7"><a href="#cb78-7" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb78-8"><a href="#cb78-8" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> <span class="op">-</span>torch.<span class="bu">sum</span>(target_one_hot <span class="op">*</span> torch.log(predictions <span class="op">+</span> <span class="fl">1e-10</span>)) <span class="op">/</span> batch_size</span>
<span id="cb78-9"><a href="#cb78-9" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb78-10"><a href="#cb78-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb78-11"><a href="#cb78-11" aria-hidden="true" tabindex="-1"></a>            pred_digits <span class="op">=</span> predictions.argmax(dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb78-12"><a href="#cb78-12" aria-hidden="true" tabindex="-1"></a>            exact_match <span class="op">=</span> (pred_digits <span class="op">==</span> target).<span class="bu">all</span>(dim<span class="op">=</span><span class="dv">1</span>).<span class="bu">float</span>()</span>
<span id="cb78-13"><a href="#cb78-13" aria-hidden="true" tabindex="-1"></a>            match_rate <span class="op">=</span> exact_match.mean().item()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<ul>
<li>손실 계산: 각 자릿수 예측 정확도 + <em>자리 올림</em> 정확성 확인.</li>
<li>단순 자릿수 매핑이 아닌, 덧셈 규칙을 학습하도록 유도.</li>
</ul>
<p><code>AdditionLoss</code> 동작 예시 (<code>batch_size=2</code>, <code>sequence_length=3</code>, <code>vocab_size=10</code>)</p>
<div id="cell-135" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb79"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb79-1"><a href="#cb79-1" aria-hidden="true" tabindex="-1"></a>outputs <span class="op">=</span> [</span>
<span id="cb79-2"><a href="#cb79-2" aria-hidden="true" tabindex="-1"></a>    [[<span class="fl">0.1</span>, <span class="fl">0.8</span>, <span class="fl">0.1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>],  <span class="co"># 첫 번째 자리</span></span>
<span id="cb79-3"><a href="#cb79-3" aria-hidden="true" tabindex="-1"></a>     [<span class="fl">0.1</span>, <span class="fl">0.1</span>, <span class="fl">0.7</span>, <span class="fl">0.1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>], <span class="co"># 두 번째 자리</span></span>
<span id="cb79-4"><a href="#cb79-4" aria-hidden="true" tabindex="-1"></a>     [<span class="fl">0.8</span>, <span class="fl">0.1</span>, <span class="fl">0.1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>]]   <span class="co"># 세 번째 자리</span></span>
<span id="cb79-5"><a href="#cb79-5" aria-hidden="true" tabindex="-1"></a>]  <span class="co"># 첫 번째 배치</span></span>
<span id="cb79-6"><a href="#cb79-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-7"><a href="#cb79-7" aria-hidden="true" tabindex="-1"></a>target <span class="op">=</span> [</span>
<span id="cb79-8"><a href="#cb79-8" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">0</span>]  <span class="co"># 실제 정답: "120"</span></span>
<span id="cb79-9"><a href="#cb79-9" aria-hidden="true" tabindex="-1"></a>]  <span class="co"># 첫 번째 배치</span></span>
<span id="cb79-10"><a href="#cb79-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-11"><a href="#cb79-11" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. softmax는 이미 적용되어 있다고 가정 (outputs)</span></span>
<span id="cb79-12"><a href="#cb79-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-13"><a href="#cb79-13" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. target을 원-핫 인코딩으로 변환</span></span>
<span id="cb79-14"><a href="#cb79-14" aria-hidden="true" tabindex="-1"></a>target_one_hot <span class="op">=</span> [</span>
<span id="cb79-15"><a href="#cb79-15" aria-hidden="true" tabindex="-1"></a>    [[<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>],  <span class="co"># 1</span></span>
<span id="cb79-16"><a href="#cb79-16" aria-hidden="true" tabindex="-1"></a>     [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>],  <span class="co"># 2</span></span>
<span id="cb79-17"><a href="#cb79-17" aria-hidden="true" tabindex="-1"></a>     [<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>]]  <span class="co"># 0</span></span>
<span id="cb79-18"><a href="#cb79-18" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb79-19"><a href="#cb79-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-20"><a href="#cb79-20" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. 손실 계산</span></span>
<span id="cb79-21"><a href="#cb79-21" aria-hidden="true" tabindex="-1"></a><span class="co"># -log(0.8) - log(0.7) - log(0.8) = 0.223 + 0.357 + 0.223 = 0.803</span></span>
<span id="cb79-22"><a href="#cb79-22" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> <span class="fl">0.803</span> <span class="op">/</span> batch_size</span>
<span id="cb79-23"><a href="#cb79-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-24"><a href="#cb79-24" aria-hidden="true" tabindex="-1"></a><span class="co"># 4. 정확도 계산</span></span>
<span id="cb79-25"><a href="#cb79-25" aria-hidden="true" tabindex="-1"></a>pred_digits <span class="op">=</span> [<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">0</span>]  <span class="co"># argmax 적용</span></span>
<span id="cb79-26"><a href="#cb79-26" aria-hidden="true" tabindex="-1"></a>exact_match <span class="op">=</span> <span class="va">True</span>  <span class="co"># 모든 자릿수가 일치</span></span>
<span id="cb79-27"><a href="#cb79-27" aria-hidden="true" tabindex="-1"></a>match_rate <span class="op">=</span> <span class="fl">1.0</span>  <span class="co"># 배치의 평균 정확도</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>트랜스포머 디코더의 출력은 마지막 레이어에서 <code>vocab_size</code>로 선형 변환되므로, 로짓(logit)은 <code>vocab_size</code> 차원을 가집니다.</p>
<p>다음 섹션에서는 파서 태스크를 통해 트랜스포머가 더 복잡한 구조적 관계를 어떻게 학습하는지 살펴보겠습니다.</p>
</section>
<section id="파서-태스크" class="level3">
<h3 class="anchored" data-anchor-id="파서-태스크">8.5.3 파서 태스크</h3>
<p>마지막 예제는 파서(Parser) 태스크 구현입니다. 이 태스크는 수식을 입력받아 파스 트리(parse tree)로 변환하는 것으로, 트랜스포머가 구조적인 정보를 얼마나 잘 처리하는지 확인할 수 있는 예제입니다.</p>
<p><strong>데이터 준비 과정 설명</strong></p>
<p>파서 태스크의 훈련 데이터는 다음 단계를 거쳐 생성됩니다.</p>
<ol type="1">
<li><strong>수식 생성</strong>:
<ul>
<li><code>generate_random_expression()</code> 함수를 사용하여 변수(x, y, z), 연산자(+, -, *, /), 숫자(0-9)를 조합해 “x=1+2”와 같은 간단한 수식을 만듭니다.</li>
</ul></li>
<li><strong>파스 트리 변환</strong>:
<ul>
<li><code>parse_to_tree()</code> 함수를 이용해 생성된 수식을 <code>['ASSIGN', 'x', ['ADD', '1', '2']]</code> 와 같은 중첩 리스트 형태의 파스 트리로 변환합니다. 이 트리는 수식의 계층적 구조를 나타냅니다.</li>
</ul></li>
<li><strong>토큰화 처리</strong>:
<ul>
<li>수식과 파스 트리는 각각 정수 시퀀스로 변환됩니다.</li>
<li>미리 정의된 <code>TOKEN_DICT</code>에 따라 각 토큰은 고유한 정수 ID로 매핑됩니다.</li>
</ul></li>
</ol>
<div id="cell-138" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb80"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb80-1"><a href="#cb80-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_addition_data(batch_size: <span class="bu">int</span> <span class="op">=</span> <span class="dv">32</span>, max_digits: <span class="bu">int</span> <span class="op">=</span> <span class="dv">3</span>) <span class="op">-&gt;</span> Tuple[torch.Tensor, torch.Tensor]:</span>
<span id="cb80-2"><a href="#cb80-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Create addition dataset"""</span></span>
<span id="cb80-3"><a href="#cb80-3" aria-hidden="true" tabindex="-1"></a>    max_value <span class="op">=</span> <span class="dv">10</span> <span class="op">**</span> max_digits <span class="op">-</span> <span class="dv">1</span></span>
<span id="cb80-4"><a href="#cb80-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb80-5"><a href="#cb80-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Generate input numbers</span></span>
<span id="cb80-6"><a href="#cb80-6" aria-hidden="true" tabindex="-1"></a>    num1 <span class="op">=</span> torch.randint(<span class="dv">0</span>, max_value <span class="op">//</span> <span class="dv">2</span> <span class="op">+</span> <span class="dv">1</span>, (batch_size,))</span>
<span id="cb80-7"><a href="#cb80-7" aria-hidden="true" tabindex="-1"></a>    num2 <span class="op">=</span> torch.randint(<span class="dv">0</span>, max_value <span class="op">//</span> <span class="dv">2</span> <span class="op">+</span> <span class="dv">1</span>, (batch_size,))</span>
<span id="cb80-8"><a href="#cb80-8" aria-hidden="true" tabindex="-1"></a>    result <span class="op">=</span> num1 <span class="op">+</span> num2</span>
<span id="cb80-9"><a href="#cb80-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-10"><a href="#cb80-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># [이하 생략]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<ul>
<li>합이 지정한 자릿수를 넘지 않도록 두 숫자를 생성.</li>
<li>입력: 두 숫자 + ‘+’ 기호.</li>
<li>자릿수 제한 유효성 검사 포함.</li>
</ul>
<p><strong>학습 데이터 설명</strong> 다음은 학습데이터 구조를 설명합니다. 표현과 토큰화 되면 어떤 값으로 변화는지를 보여줍니다.</p>
<div id="cell-140" class="cell" data-execution_count="2">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb81"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb81-1"><a href="#cb81-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_08.transformer.examples.parser_task <span class="im">import</span> explain_parser_data</span>
<span id="cb81-2"><a href="#cb81-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-3"><a href="#cb81-3" aria-hidden="true" tabindex="-1"></a>explain_parser_data()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
=== Parsing Data Explanation ===
Max Tokens: 5

1. Input Sequence:
Original Tensor Shape: torch.Size([1, 5])
Expression: x = 4 + 9
Tokenized Input: [11, 1, 17, 2, 22]

2. Target Sequence:
Original Tensor Shape: torch.Size([1, 5])
Parse Tree: ['ASSIGN', 'x', 'ADD', '4', '9']
Tokenized Output: [6, 11, 7, 17, 22]</code></pre>
</div>
</div>
<p><strong>파싱 예제 설명</strong></p>
<p>다음 코드를 실행하면 파싱 예제 데이터가 어떻게 구성되는지 쉽게 이해할 수 있도록 설명이 차례대로 표시됩니다.</p>
<div id="cell-142" class="cell" data-execution_count="4">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb83"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb83-1"><a href="#cb83-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_08.transformer.examples.parser_task <span class="im">import</span> show_parser_examples</span>
<span id="cb83-2"><a href="#cb83-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-3"><a href="#cb83-3" aria-hidden="true" tabindex="-1"></a>show_parser_examples(num_examples<span class="op">=</span><span class="dv">3</span> )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
=== Generating 3 Parsing Examples ===

Example 1:
Generated Expression: y=7/7
Parse Tree: ['ASSIGN', 'y', ['DIV', '7', '7']]
Expression Tokens: [12, 1, 21, 5, 21]
Tree Tokens: [6, 12, 10, 21, 21]
Padded Expression Tokens: [12, 1, 21, 5, 21]
Padded Tree Tokens: [6, 12, 10, 21, 21]

Example 2:
Generated Expression: x=4/3
Parse Tree: ['ASSIGN', 'x', ['DIV', '4', '3']]
Expression Tokens: [11, 1, 18, 5, 17]
Tree Tokens: [6, 11, 10, 18, 17]
Padded Expression Tokens: [11, 1, 18, 5, 17]
Padded Tree Tokens: [6, 11, 10, 18, 17]

Example 3:
Generated Expression: x=1*4
Parse Tree: ['ASSIGN', 'x', ['MUL', '1', '4']]
Expression Tokens: [11, 1, 15, 4, 18]
Tree Tokens: [6, 11, 9, 15, 18]
Padded Expression Tokens: [11, 1, 15, 4, 18]
Padded Tree Tokens: [6, 11, 9, 15, 18]
</code></pre>
</div>
</div>
<p><strong>모델 학습 및 테스트</strong></p>
<div id="cell-144" class="cell" data-execution_count="2">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb85"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb85-1"><a href="#cb85-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_08.transformer.config <span class="im">import</span> TransformerConfig</span>
<span id="cb85-2"><a href="#cb85-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_08.transformer.examples.parser_task <span class="im">import</span> train_parser_task</span>
<span id="cb85-3"><a href="#cb85-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-4"><a href="#cb85-4" aria-hidden="true" tabindex="-1"></a>config <span class="op">=</span> TransformerConfig()</span>
<span id="cb85-5"><a href="#cb85-5" aria-hidden="true" tabindex="-1"></a>config.vocab_size <span class="op">=</span> <span class="dv">25</span>  <span class="co"># Adjusted to match the token dictionary size</span></span>
<span id="cb85-6"><a href="#cb85-6" aria-hidden="true" tabindex="-1"></a>config.hidden_size <span class="op">=</span> <span class="dv">128</span></span>
<span id="cb85-7"><a href="#cb85-7" aria-hidden="true" tabindex="-1"></a>config.num_hidden_layers <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb85-8"><a href="#cb85-8" aria-hidden="true" tabindex="-1"></a>config.num_attention_heads <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb85-9"><a href="#cb85-9" aria-hidden="true" tabindex="-1"></a>config.intermediate_size <span class="op">=</span> <span class="dv">512</span></span>
<span id="cb85-10"><a href="#cb85-10" aria-hidden="true" tabindex="-1"></a>config.max_position_embeddings <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb85-11"><a href="#cb85-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-12"><a href="#cb85-12" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> train_parser_task(config, num_epochs<span class="op">=</span><span class="dv">6</span>, batch_size<span class="op">=</span><span class="dv">64</span>, steps_per_epoch<span class="op">=</span><span class="dv">100</span>, max_tokens<span class="op">=</span><span class="dv">5</span>, print_progress<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
=== Start Training ===
Device: cuda:0
Batch Size: 64
Steps per Epoch: 100
Max Tokens: 5

Epoch 0, Average Loss: 6.3280, Final Accuracy: 0.2309, Learning Rate: 0.000100

=== Prediction Result Samples ===
Input: y = 8 * 8
Prediction: ['ASSIGN', 'y', 'MUL', '8', '8']
Truth: ['ASSIGN', 'y', 'MUL', '8', '8']
Result: Correct

Input: z = 6 / 5
Prediction: ['ASSIGN', 'z', 'DIV', '8', 'a']
Truth: ['ASSIGN', 'z', 'DIV', '6', '5']
Result: Incorrect

Epoch 5, Average Loss: 0.0030, Final Accuracy: 1.0000, Learning Rate: 0.000100

=== Prediction Result Samples ===
Input: z = 5 - 6
Prediction: ['ASSIGN', 'z', 'SUB', '5', '6']
Truth: ['ASSIGN', 'z', 'SUB', '5', '6']
Result: Correct

Input: y = 9 + 9
Prediction: ['ASSIGN', 'y', 'ADD', '9', '9']
Truth: ['ASSIGN', 'y', 'ADD', '9', '9']
Result: Correct

Model saved to saved_models/transformer_parser_task.pth</code></pre>
</div>
</div>
<p>테스를 수행합니다.</p>
<div id="cell-146" class="cell" data-execution_count="3">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb87"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb87-1"><a href="#cb87-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_08.transformer.config <span class="im">import</span> TransformerConfig</span>
<span id="cb87-2"><a href="#cb87-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_08.transformer.examples.parser_task <span class="im">import</span> test_parser</span>
<span id="cb87-3"><a href="#cb87-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-4"><a href="#cb87-4" aria-hidden="true" tabindex="-1"></a>test_parser()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
=== Parser Test ===
Input Expression: x = 8 * 3
Predicted Parse Tree: ['ASSIGN', 'x', 'MUL', '8', '3']
Actual Parse Tree: ['ASSIGN', 'x', 'MUL', '8', '3']
Correct: True

=== Additional Tests ===

Input: x=1+2
Predicted Parse Tree: ['ASSIGN', 'x', 'ADD', '2', '3']

Input: y=3*4
Predicted Parse Tree: ['ASSIGN', 'y', 'MUL', '4', '5']

Input: z=5-1
Predicted Parse Tree: ['ASSIGN', 'z', 'SUB', '6', '2']

Input: x=2/3
Predicted Parse Tree: ['ASSIGN', 'x', 'DIV', '3', '4']</code></pre>
</div>
</div>
<p><strong>모델 설정</strong> - <code>vocab_size</code>: 25 (토큰 사전의 크기) - <code>hidden_size</code>: 128 - <code>num_hidden_layers</code>: 3 - <code>num_attention_heads</code>: 4 - <code>intermediate_size</code>: 512 - <code>max_position_embeddings</code>: 10 (최대 토큰 수)</p>
<p><strong>손실 함수 설계</strong></p>
<p>파서 태스크의 손실 함수는 크로스 엔트로피 손실을 사용합니다.</p>
<ol type="1">
<li><strong>출력 변환</strong>: 모델의 출력을 소프트맥스 함수를 이용해 확률로 변환합니다.</li>
<li><strong>타겟 변환</strong>: 타겟(정답) 시퀀스를 원-핫 인코딩합니다.</li>
<li><strong>손실 계산</strong>: 로그 확률의 음수 평균을 계산하여 손실을 구합니다.</li>
<li><strong>정확도</strong>: 예측 시퀀스와 정답 시퀀스가 완전히 일치하는지 여부로 정확도를 계산합니다. 이는 파스 트리가 정확하게 생성되어야 하는 이 태스크의 특성을 반영합니다.</li>
</ol>
<p><strong>손실 함수 동작 예시</strong></p>
<div id="cell-148" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb89"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb89-1"><a href="#cb89-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Example input values (batch_size=2, sequence_length=4, vocab_size=5)</span></span>
<span id="cb89-2"><a href="#cb89-2" aria-hidden="true" tabindex="-1"></a><span class="co"># vocab = {'=':0, 'x':1, '+':2, '1':3, '2':4}</span></span>
<span id="cb89-3"><a href="#cb89-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb89-4"><a href="#cb89-4" aria-hidden="true" tabindex="-1"></a>outputs <span class="op">=</span> [</span>
<span id="cb89-5"><a href="#cb89-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># First batch: prediction probabilities for "x=1+2"</span></span>
<span id="cb89-6"><a href="#cb89-6" aria-hidden="true" tabindex="-1"></a>    [[<span class="fl">0.1</span>, <span class="fl">0.7</span>, <span class="fl">0.1</span>, <span class="fl">0.1</span>, <span class="fl">0.0</span>],  <span class="co"># predicting x</span></span>
<span id="cb89-7"><a href="#cb89-7" aria-hidden="true" tabindex="-1"></a>     [<span class="fl">0.8</span>, <span class="fl">0.1</span>, <span class="fl">0.0</span>, <span class="fl">0.1</span>, <span class="fl">0.0</span>],  <span class="co"># predicting =</span></span>
<span id="cb89-8"><a href="#cb89-8" aria-hidden="true" tabindex="-1"></a>     [<span class="fl">0.1</span>, <span class="fl">0.0</span>, <span class="fl">0.1</span>, <span class="fl">0.7</span>, <span class="fl">0.1</span>],  <span class="co"># predicting 1</span></span>
<span id="cb89-9"><a href="#cb89-9" aria-hidden="true" tabindex="-1"></a>     [<span class="fl">0.0</span>, <span class="fl">0.1</span>, <span class="fl">0.8</span>, <span class="fl">0.0</span>, <span class="fl">0.1</span>]], <span class="co"># predicting +</span></span>
<span id="cb89-10"><a href="#cb89-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb89-11"><a href="#cb89-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Second batch: prediction probabilities for "x=2+1"</span></span>
<span id="cb89-12"><a href="#cb89-12" aria-hidden="true" tabindex="-1"></a>    [[<span class="fl">0.1</span>, <span class="fl">0.8</span>, <span class="fl">0.0</span>, <span class="fl">0.1</span>, <span class="fl">0.0</span>],  <span class="co"># predicting x</span></span>
<span id="cb89-13"><a href="#cb89-13" aria-hidden="true" tabindex="-1"></a>     [<span class="fl">0.7</span>, <span class="fl">0.1</span>, <span class="fl">0.1</span>, <span class="fl">0.0</span>, <span class="fl">0.1</span>],  <span class="co"># predicting =</span></span>
<span id="cb89-14"><a href="#cb89-14" aria-hidden="true" tabindex="-1"></a>     [<span class="fl">0.1</span>, <span class="fl">0.0</span>, <span class="fl">0.1</span>, <span class="fl">0.1</span>, <span class="fl">0.7</span>],  <span class="co"># predicting 2</span></span>
<span id="cb89-15"><a href="#cb89-15" aria-hidden="true" tabindex="-1"></a>     [<span class="fl">0.0</span>, <span class="fl">0.0</span>, <span class="fl">0.9</span>, <span class="fl">0.1</span>, <span class="fl">0.0</span>]]  <span class="co"># predicting +</span></span>
<span id="cb89-16"><a href="#cb89-16" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb89-17"><a href="#cb89-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb89-18"><a href="#cb89-18" aria-hidden="true" tabindex="-1"></a>target <span class="op">=</span> [</span>
<span id="cb89-19"><a href="#cb89-19" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">3</span>, <span class="dv">2</span>],  <span class="co"># Actual answer: "x=1+"</span></span>
<span id="cb89-20"><a href="#cb89-20" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">4</span>, <span class="dv">2</span>]   <span class="co"># Actual answer: "x=2+"</span></span>
<span id="cb89-21"><a href="#cb89-21" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb89-22"><a href="#cb89-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb89-23"><a href="#cb89-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert target to one-hot encoding</span></span>
<span id="cb89-24"><a href="#cb89-24" aria-hidden="true" tabindex="-1"></a>target_one_hot <span class="op">=</span> [</span>
<span id="cb89-25"><a href="#cb89-25" aria-hidden="true" tabindex="-1"></a>    [[<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>],  <span class="co"># x</span></span>
<span id="cb89-26"><a href="#cb89-26" aria-hidden="true" tabindex="-1"></a>     [<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>],  <span class="co"># =</span></span>
<span id="cb89-27"><a href="#cb89-27" aria-hidden="true" tabindex="-1"></a>     [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>],  <span class="co"># 1</span></span>
<span id="cb89-28"><a href="#cb89-28" aria-hidden="true" tabindex="-1"></a>     [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>]], <span class="co"># +</span></span>
<span id="cb89-29"><a href="#cb89-29" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb89-30"><a href="#cb89-30" aria-hidden="true" tabindex="-1"></a>    [[<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>],  <span class="co"># x</span></span>
<span id="cb89-31"><a href="#cb89-31" aria-hidden="true" tabindex="-1"></a>     [<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>],  <span class="co"># =</span></span>
<span id="cb89-32"><a href="#cb89-32" aria-hidden="true" tabindex="-1"></a>     [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>],  <span class="co"># 2</span></span>
<span id="cb89-33"><a href="#cb89-33" aria-hidden="true" tabindex="-1"></a>     [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>]]  <span class="co"># +</span></span>
<span id="cb89-34"><a href="#cb89-34" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb89-35"><a href="#cb89-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb89-36"><a href="#cb89-36" aria-hidden="true" tabindex="-1"></a><span class="co"># Loss calculation (first batch)</span></span>
<span id="cb89-37"><a href="#cb89-37" aria-hidden="true" tabindex="-1"></a><span class="co"># -log(0.7) - log(0.8) - log(0.7) - log(0.8) = 0.357 + 0.223 + 0.357 + 0.223 = 1.16</span></span>
<span id="cb89-38"><a href="#cb89-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb89-39"><a href="#cb89-39" aria-hidden="true" tabindex="-1"></a><span class="co"># Loss calculation (second batch)</span></span>
<span id="cb89-40"><a href="#cb89-40" aria-hidden="true" tabindex="-1"></a><span class="co"># -log(0.8) - log(0.7) - log(0.7) - log(0.9) = 0.223 + 0.357 + 0.357 + 0.105 = 1.042</span></span>
<span id="cb89-41"><a href="#cb89-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb89-42"><a href="#cb89-42" aria-hidden="true" tabindex="-1"></a><span class="co"># Total loss</span></span>
<span id="cb89-43"><a href="#cb89-43" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> (<span class="fl">1.16</span> <span class="op">+</span> <span class="fl">1.042</span>) <span class="op">/</span> <span class="dv">2</span> <span class="op">=</span> <span class="fl">1.101</span></span>
<span id="cb89-44"><a href="#cb89-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb89-45"><a href="#cb89-45" aria-hidden="true" tabindex="-1"></a><span class="co"># Accuracy calculation</span></span>
<span id="cb89-46"><a href="#cb89-46" aria-hidden="true" tabindex="-1"></a>pred_tokens <span class="op">=</span> [</span>
<span id="cb89-47"><a href="#cb89-47" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">3</span>, <span class="dv">2</span>],  <span class="co"># First batch prediction</span></span>
<span id="cb89-48"><a href="#cb89-48" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">4</span>, <span class="dv">2</span>]   <span class="co"># Second batch prediction</span></span>
<span id="cb89-49"><a href="#cb89-49" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb89-50"><a href="#cb89-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb89-51"><a href="#cb89-51" aria-hidden="true" tabindex="-1"></a>exact_match <span class="op">=</span> [<span class="va">True</span>, <span class="va">True</span>]  <span class="co"># Both batches match exactly</span></span>
<span id="cb89-52"><a href="#cb89-52" aria-hidden="true" tabindex="-1"></a>match_rate <span class="op">=</span> <span class="fl">1.0</span>  <span class="co"># Overall accuracy</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>지금까지 예제를 통해 트랜스포머가 구조적 정보를 효과적으로 처리할 수 있음을 알 수 있었습니다.</p>
</section>
</section>
<section id="맺음말" class="level2">
<h2 class="anchored" data-anchor-id="맺음말">맺음말</h2>
<p>8장에서는 트랜스포머의 탄생 배경과 그 핵심 구성 요소들을 깊이 있게 탐구했습니다. RNN 기반 모델의 한계를 극복하고자 했던 연구자들의 고민, 어텐션 메커니즘의 발견과 발전, 그리고 Q, K, V 벡터 공간 분리와 멀티 헤드 어텐션을 통한 병렬 처리와 다양한 관점에서의 문맥 정보 포착 등 트랜스포머를 구성하는 핵심 아이디어들이 어떻게 점진적으로 구체화되었는지 살펴보았습니다. 또한, 위치 정보를 효과적으로 표현하기 위한 포지셔널 인코딩, 정보 유출을 막기 위한 정교한 마스킹 전략, 그리고 인코더-디코더 구조와 각 구성 요소의 역할 및 작동 방식을 자세히 분석했습니다.</p>
<p>세 가지 예제(단순 복사, 자릿수 덧셈, 파서)를 통해 트랜스포머가 실제로 어떻게 동작하는지, 그리고 각 구성 요소가 어떤 역할을 하는지 직관적으로 이해할 수 있었습니다. 이 예제들은 트랜스포머의 기본 기능, 자기 회귀적 생성 능력, 구조적 정보 처리 능력을 보여주며, 실제 자연어 처리 문제에 트랜스포머를 적용하기 위한 기반 지식을 제공합니다.</p>
<p>이제 9장에서는 트랜스포머가 “Attention is All You Need” 논문 발표 이후 어떻게 발전해 왔는지, 그 진화의 여정을 따라가 보겠습니다. BERT, GPT 등 트랜스포머 기반의 다양한 모델들이 어떻게 등장하게 되었는지, 그리고 이러한 모델들이 자연어 처리를 넘어 컴퓨터 비전, 음성 인식 등 다양한 분야에서 어떤 혁신을 가져왔는지 살펴볼 것입니다.</p>
</section>
<section id="연습-문제" class="level2">
<h2 class="anchored" data-anchor-id="연습-문제">연습 문제</h2>
<section id="기본-문제" class="level3">
<h3 class="anchored" data-anchor-id="기본-문제">기본 문제</h3>
<ol type="1">
<li>트랜스포머가 RNN에 비해 가지는 가장 큰 장점 두 가지는 무엇인가?</li>
<li>어텐션 메커니즘의 핵심 아이디어는 무엇이며, 이를 통해 얻을 수 있는 효과는 무엇인가?</li>
<li>멀티헤드 어텐션은 셀프 어텐션과 비교하여 어떤 장점을 제공하는가?</li>
<li>포지셔널 인코딩은 왜 필요하며, 어떤 방식으로 위치 정보를 표현하는가?</li>
<li>트랜스포머에서 인코더와 디코더는 각각 어떤 역할을 수행하는가?</li>
</ol>
</section>
<section id="응용-문제" class="level3">
<h3 class="anchored" data-anchor-id="응용-문제">응용 문제</h3>
<ol type="1">
<li><strong>텍스트 요약 과제</strong>: 주어진 긴 텍스트를 입력으로 받아, 핵심 내용을 담은 짧은 요약문을 생성하는 트랜스포머 모델을 설계하고, 어떤 평가 지표를 사용하여 모델의 성능을 측정할 수 있는지 설명하시오.</li>
<li><strong>질의응답 시스템 분석</strong>: 트랜스포머 기반 질의응답 시스템이 주어진 질문에 대해 올바른 답변을 찾는 과정을 단계별로 설명하고, 어텐션 메커니즘이 이 과정에서 어떤 핵심적인 역할을 하는지 분석하시오.</li>
<li><strong>다른 도메인 적용 사례 조사</strong>: 이미지, 음성, 그래프 등 자연어 처리 외의 다른 도메인에서 트랜스포머가 성공적으로 적용된 사례를 2가지 이상 조사하고, 각 사례에서 트랜스포머가 어떻게 활용되었는지, 어떤 장점을 제공했는지 설명하시오.</li>
</ol>
</section>
<section id="심화-문제" class="level3">
<h3 class="anchored">심화 문제</h3>
<ol type="1">
<li><strong>계산 복잡도 개선 방안 비교 분석</strong>: 트랜스포머의 계산 복잡도를 개선하기 위해 제안된 방법들(예: Reformer, Performer, Longformer)을 2가지 이상 조사하고, 각 방법의 핵심 아이디어, 장단점, 적용 가능한 시나리오를 비교 분석하시오.</li>
<li><strong>새로운 아키텍처 제안 및 평가</strong>: 특정 문제(예: 장문 텍스트 분류, 다국어 번역)에 특화된 트랜스포머 기반 새로운 아키텍처를 제안하고, 기존 트랜스포머 모델과 비교하여 어떤 장점이 있는지 이론적으로 설명하고, 실험적으로 검증할 수 있는 방법을 제시하시오.</li>
<li><strong>윤리적, 사회적 영향 분석 및 대응 방안</strong>: 트랜스포머 기반 대규모 언어 모델(예: GPT-3, BERT)의 발전이 사회에 미칠 수 있는 긍정적, 부정적 영향을 분석하고, 특히 편향성, 가짜 뉴스 생성, 일자리 감소 등 부정적인 영향을 완화하기 위한 기술적, 정책적 대응 방안을 제시하시오.</li>
</ol>
<div class="callout callout-style-default callout-note callout-titled" title="클릭하여 내용 보기 (연습문제 해답)">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
클릭하여 내용 보기 (연습문제 해답)
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<section id="연습-문제-해답" class="level2 callout-body-container callout-body">
<h2 class="anchored" data-anchor-id="연습-문제-해답">연습 문제 해답</h2>
<section id="기본-문제-1" class="level3">
<h3 class="anchored" data-anchor-id="기본-문제-1">기본 문제</h3>
<ol type="1">
<li><p><strong>RNN 대비 트랜스포머 장점:</strong> 트랜스포머는 RNN과 비교하여 <strong>병렬 처리</strong>와 <strong>장기 의존성 문제 해결</strong>이라는 두 가지 큰 장점을 가집니다. RNN은 순차 처리로 속도가 느리지만, 트랜스포머는 어텐션으로 모든 단어를 동시에 처리하여 GPU 병렬 계산이 가능하고 학습 속도가 빠릅니다. 또한, RNN은 긴 시퀀스에서 정보 손실이 발생하지만, 트랜스포머는 셀프 어텐션으로 단어 간 관계를 직접 계산하여 거리와 무관하게 중요 정보를 보존합니다.</p></li>
<li><p><strong>어텐션 메커니즘 핵심 &amp; 효과:</strong> 어텐션은 <strong>입력 시퀀스의 각 부분이 출력 시퀀스 생성에 얼마나 중요한지</strong> 계산합니다. 디코더는 출력 단어 예측 시 입력 전체를 동일하게 보지 않고, 관련성이 높은 부분에 “주목(attention)”하여 문맥을 더 잘 이해하고, 더 정확하게 예측합니다.</p></li>
<li><p><strong>멀티헤드 어텐션 장점:</strong> 멀티헤드 어텐션은 <strong>셀프 어텐션을 여러 개 병렬 수행</strong>합니다. 각 헤드는 서로 다른 관점에서 입력 시퀀스 내 단어 간 관계를 학습하여, 모델이 더 풍부하고 다양한 문맥 정보를 포착할 수 있도록 돕습니다. (마치 여러 명의 탐정이 각자 전문 분야를 가지고 협력하는 것과 유사)</p></li>
<li><p><strong>포지셔널 인코딩 필요성 &amp; 방식:</strong> 트랜스포머는 순차 처리 방식이 아니므로 단어의 <strong>위치 정보</strong>를 알려줘야 합니다. 포지셔널 인코딩은 각 단어의 위치 정보를 담은 벡터를 단어 임베딩에 더하는 방식으로 작동합니다. 이를 통해 트랜스포머는 단어의 의미뿐 아니라 문장 내 위치 정보까지 함께 고려하여 문맥을 파악합니다. 주로 사인-코사인 함수를 사용하여 위치 정보를 표현합니다.</p></li>
<li><p><strong>인코더 &amp; 디코더 역할:</strong> 트랜스포머는 인코더-디코더 구조입니다. <strong>인코더</strong>는 입력 시퀀스를 받아 각 단어의 문맥을 반영한 표현(문맥 벡터)을 생성합니다. <strong>디코더</strong>는 인코더가 생성한 문맥 벡터와 이전 단계에서 생성된 출력 단어를 바탕으로 다음 단어를 예측하는 과정을 반복하여, 최종 출력 시퀀스를 생성합니다.</p></li>
</ol>
</section>
<section id="응용-문제-1" class="level3">
<h3 class="anchored" data-anchor-id="응용-문제-1">응용 문제</h3>
<ol type="1">
<li><strong>텍스트 요약 과제:</strong>
<ul>
<li><strong>모델 설계:</strong> 인코더-디코더 구조의 트랜스포머 모델을 사용합니다. 인코더는 긴 텍스트를 입력받아 문맥 벡터를 생성하고, 디코더는 이 문맥 벡터를 기반으로 요약문을 생성합니다. 디코더에서는 마스크드 셀프 어텐션을 사용하여 생성 과정에서 미래 시점의 단어를 참조하지 않도록 제약합니다.</li>
<li><strong>평가 지표:</strong> 모델의 성능은 ROUGE (Recall-Oriented Understudy for Gisting Evaluation) 스코어를 주로 사용하여 평가할 수 있습니다. ROUGE는 생성된 요약문과 정답 요약문 간에 겹치는 n-gram(연속된 n개의 단어)의 수를 기반으로 유사도를 측정하며, ROUGE-N, ROUGE-L, ROUGE-S 등 다양한 종류가 있습니다. 또한, BLEU (Bilingual Evaluation Understudy) 스코어도 참고할 수 있습니다.</li>
</ul></li>
<li><strong>질의응답 시스템 분석:</strong> 트랜스포머 기반 질의응답 시스템은 주어진 질문에 대해 문서에서 올바른 답변을 찾는 과정을 다음과 같이 수행합니다.
<ol type="1">
<li>질문과 문서를 각각 트랜스포머 인코더에 입력하여 임베딩 벡터를 얻습니다.</li>
<li>질문 임베딩과 문서 임베딩 간의 어텐션 가중치를 계산합니다. (질문의 각 단어가 문서의 어떤 단어와 관련 있는지 파악)</li>
<li>어텐션 가중치를 사용하여 문서 임베딩의 가중 평균을 구하고, 이를 질문에 대한 문맥 벡터로 활용합니다.</li>
<li>문맥 벡터를 기반으로 답변의 시작 위치와 끝 위치를 예측하여 최종 답변을 추출합니다. 이 과정에서 <strong>어텐션 메커니즘</strong>은 질문과 문서 간의 의미적 관련성을 파악하여, 질문에 대한 답변을 찾는 데 가장 중요한 문서 부분을 식별하는 핵심적인 역할을 수행합니다.</li>
</ol></li>
<li><strong>다른 도메인 적용 사례 조사:</strong>
<ul>
<li><strong>이미지:</strong> Vision Transformer (ViT)는 이미지를 여러 개의 패치(patch)로 나누고, 각 패치를 트랜스포머의 입력 시퀀스처럼 처리하여 이미지 분류, 객체 탐지 등의 태스크에서 뛰어난 성능을 보입니다. 이는 트랜스포머가 순차적인 데이터뿐 아니라 이미지와 같은 2차원 데이터에도 효과적으로 적용될 수 있음을 보여줍니다.</li>
<li><strong>음성:</strong> Conformer는 CNN과 트랜스포머를 결합하여 음성 인식에서 높은 정확도를 달성했습니다. 음성 신호의 지역적 특징(local features)과 전역적 특징(global features)을 모두 효과적으로 모델링하여, 음성 인식 성능을 향상시켰습니다.</li>
</ul></li>
</ol>
</section>
<section id="심화-문제-1" class="level3">
<h3 class="anchored" data-anchor-id="심화-문제-1">심화 문제</h3>
<ol type="1">
<li><p><strong>계산 복잡도 개선 방안 비교 분석:</strong></p>
<p>트랜스포머는 셀프 어텐션 때문에 입력 시퀀스 길이에 대해 제곱의 계산 복잡도를 가집니다. 이를 개선하기 위해 다양한 방법들이 제안되었습니다.</p>
<ul>
<li><strong>Reformer:</strong> Locality-Sensitive Hashing (LSH) 어텐션을 사용하여 쿼리와 키 간의 유사도를 근사적으로 계산합니다. LSH는 유사한 벡터들을 같은 버킷에 할당하는 해싱 기법으로, 이를 통해 전체 시퀀스에 대한 어텐션 계산을 피하고 가까운 토큰에만 집중하여 계산 복잡도를 줄입니다. Reformer는 메모리 사용량과 계산 시간을 크게 줄일 수 있지만, LSH의 특성상 근사적인 계산으로 인해 정확도가 약간 떨어질 수 있습니다.</li>
<li><strong>Longformer:</strong> 슬라이딩 윈도우(sliding window) 어텐션과 글로벌 어텐션(global attention)을 결합하여 긴 시퀀스를 효율적으로 처리합니다. 각 토큰은 주변의 고정된 크기의 윈도우 내의 토큰에만 어텐션을 수행하고, 일부 토큰(예: 문장 시작 토큰)은 전체 시퀀스에 대한 어텐션을 수행합니다. Longformer는 긴 시퀀스에 대한 처리 속도가 빠르고 메모리 사용량이 적지만, 윈도우 크기에 따라 성능이 달라질 수 있습니다.</li>
</ul></li>
<li><p><strong>새로운 아키텍처 제안 및 평가:</strong></p>
<ul>
<li><strong>문제 정의:</strong> 긴 텍스트를 분류할 때, 기존 트랜스포머는 계산 복잡도가 높고 장기 의존성을 포착하기 어렵습니다.</li>
<li><strong>아키텍처 제안:</strong> 텍스트를 여러 개의 세그먼트(segment)로 나누고, 각 세그먼트에 대해 트랜스포머 인코더를 적용하여 세그먼트 임베딩을 얻습니다. 그 후, 세그먼트 임베딩을 다시 트랜스포머 인코더에 입력하여 텍스트 전체의 표현을 얻고, 이를 기반으로 텍스트를 분류합니다.</li>
<li><strong>이론적 장점:</strong> 계층적 구조를 통해 장기 의존성을 효과적으로 포착하고 계산 복잡도를 줄일 수 있습니다.</li>
<li><strong>실험 설계:</strong> IMDB 영화 리뷰 데이터셋 등 장문 텍스트 분류 데이터셋을 사용하여 제안하는 아키텍처와 기존 트랜스포머 모델(예: BERT)의 성능(정확도, F1-score)을 비교합니다. 또한, 텍스트 길이, 세그먼트 크기 등 하이퍼파라미터를 변경하면서 성능 변화를 분석하여 제안하는 아키텍처의 효과를 검증합니다.</li>
</ul></li>
<li><p><strong>윤리적, 사회적 영향 분석 및 대응 방안:</strong></p>
<p>트랜스포머 기반 대규모 언어 모델(예: GPT-3, BERT)의 발전은 사회에 다양한 긍정적, 부정적 영향을 미칠 수 있습니다.</p>
<ul>
<li><strong>긍정적 영향:</strong> 자동 번역, 챗봇, 가상 비서 등을 통해 의사소통 장벽을 낮추고 정보 접근성을 높일 수 있습니다. 또한, 콘텐츠 생성, 코드 생성, 자동 요약 등을 통해 생산성을 향상시키고, 과학 연구(예: 단백질 구조 예측), 의료 진단 등 새로운 분야에 적용되어 혁신을 가속화할 수 있습니다.</li>
<li><strong>부정적 영향:</strong> 훈련 데이터에 존재하는 편향(성별, 인종, 종교 등)을 학습하여 차별적인 결과를 초래할 수 있습니다. 악의적인 사용자가 가짜 뉴스를 대량으로 생성하여 여론을 조작하거나 특정 개인/집단에 대한 명예를 훼손할 수 있습니다. 또한, 자동화된 글쓰기, 번역, 고객 응대 등으로 인해 관련 직종의 일자리가 감소할 수 있으며, 개인 정보 침해, 저작권 침해 등의 문제도 발생할 수 있습니다.</li>
<li><strong>대응 방안:</strong> 이러한 부정적 영향을 완화하기 위해 데이터 편향 제거, 가짜 뉴스 탐지 기술 개발, 자동화에 따른 일자리 변화에 대한 사회적 논의 및 재교육 프로그램 마련, 알고리즘 투명성 및 책임성 강화, 윤리적 가이드라인 제정 등 기술적, 정책적 노력이 필요합니다.</li>
</ul></li>
</ol>
</section>
</section>
</div>
</div>
</section>
</section>
<section id="참고-자료" class="level2">
<h2 class="anchored" data-anchor-id="참고-자료">참고 자료</h2>
<ol type="1">
<li><a href="https://arxiv.org/abs/1706.03762">Attention is All You Need</a> (Vaswani et al., 2017) - 트랜스포머 원논문</li>
<li><a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html">The Annotated Transformer</a> (Harvard NLP) - PyTorch 구현과 함께 트랜스포머를 자세히 설명</li>
<li><a href="http://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a> (Jay Alammar) - 트랜스포머를 시각적으로 설명</li>
<li><a href="https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html">Transformer: A Novel Neural Network Architecture for Language Understanding</a> (Google AI Blog) - 트랜스포머 소개</li>
<li><a href="https://lilianweng.github.io/posts/2020-04-07-the-transformer-family/">The Transformer Family</a> (Lilian Weng) - 트랜스포머의 다양한 변형 모델 소개</li>
<li><a href="https://arxiv.org/abs/1810.04805">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a> (Devlin et al., 2018) - BERT 소개</li>
<li><a href="https://arxiv.org/abs/2005.14165">GPT-3: Language Models are Few-Shot Learners</a> (Brown et al., 2020) - GPT-3 소개</li>
<li><a href="https://huggingface.co/transformers/">Hugging Face Transformers</a> - 다양한 트랜스포머 모델과 툴 제공</li>
<li><a href="https://www.tensorflow.org/text/tutorials/transformer">TensorFlow Transformer Tutorial</a> - TensorFlow를 사용한 트랜스포머 구현 튜토리얼</li>
<li><a href="https://pytorch.org/docs/stable/nn.html#transformer-layers">PyTorch Transformer Documentation</a> - PyTorch의 트랜스포머 모듈 설명</li>
<li><a href="https://arxiv.org/abs/1904.02679">Visualizing Attention in Transformer-Based Language Representation Models</a> - 트랜스포머 어텐션 시각화</li>
<li><a href="https://arxiv.org/abs/2107.03789">A Survey of Long-Term Context in Transformers</a> - 긴 문맥을 처리하기 위한 트랜스포머 연구 동향</li>
<li><a href="https://arxiv.org/abs/2001.04451">Reformer: The Efficient Transformer</a> - 트랜스포머의 효율성을 개선한 Reformer 모델</li>
<li><a href="https://arxiv.org/abs/2009.06732">Efficient Transformers: A Survey</a> - 효율적인 트랜스포머 모델 연구 동향</li>
<li><a href="https://arxiv.org/abs/2011.04006">Long Range Arena: A Benchmark for Efficient Transformers</a> - 긴 문맥을 처리하기 위한 트랜스포머 벤치마크</li>
</ol>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>