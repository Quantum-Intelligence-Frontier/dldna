<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>optimization-and-visualization – Deep Learning DNA: Surviving Architectures and Essential Principles</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap-f507c7d0488cb7630e20aad62ad8c2aa.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>
<script src="https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js" crossorigin="anonymous"></script>
<script>window.MathJax = {loader: {load: ['[tex]/boldsymbol']},tex: {packages: {'[+]': ['boldsymbol']}}};</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../../notebooks/en/part_1/01_The Beginning of Deep Learning.html">part_1</a></li><li class="breadcrumb-item"><a href="../../../notebooks/en/part_1/05_Optimization and Visualization.html">5. Optimization and Visualization</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../../../">English</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Language</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index_de.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Deutsch</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index_en.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">English</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index_es.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Español</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">한국어</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index_zh.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">中文</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/en/part_1/00_Introduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">part_1</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/en/part_1/01_The Beginning of Deep Learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">1. The Beginning of Deep Learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/en/part_1/02_Mathematics of Deep Learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">2. Mathematics of Deep Learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/en/part_1/03_Deep Learning Framework.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">3. Deep Learning Framework</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/en/part_1/04_Activation Function.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">4. Activation Function</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/en/part_1/05_Optimization and Visualization.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">5. Optimization and Visualization</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/en/part_1/06_Overfitting and Development of Solution Techniques.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">6. Overfitting and Development of Solution Techniques</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/en/part_1/07_Evolution of Convolutional Neural Networks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">7. Evolution of Convolutional Neural Networks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/en/part_1/08_The Birth of Transformer.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">8. The Birth of Transformer</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/en/part_1/09_The Evolution of Transformers.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">9. The Evolution of Transformers</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/en/part_1/10_Multimodal Deep Learning: The Beginning of Multisensory Convergence.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">10. Multimodal Deep Learning: The Beginning of Multisensory Convergence</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/en/part_1/11_Multimodal Deep Learning: Intelligence Beyond Limits.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">11. Multimodal Deep Learning: Intelligence Beyond Limits</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Deep Learning Frontier</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/en/Deep Learning Frontier/01_SLM: Small but Powerful Language Model.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">1. SLM: Small but Powerful Language Model</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/en/Deep Learning Frontier/02_Autonomous Driving.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">2. Autonomous Driving</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#optimization-and-visualization" id="toc-optimization-and-visualization" class="nav-link active" data-scroll-target="#optimization-and-visualization">5. Optimization and Visualization</a>
  <ul class="collapse">
  <li><a href="#evolution-and-modern-approaches-to-parameter-initialization" id="toc-evolution-and-modern-approaches-to-parameter-initialization" class="nav-link" data-scroll-target="#evolution-and-modern-approaches-to-parameter-initialization">5.1 Evolution and Modern Approaches to Parameter Initialization</a>
  <ul class="collapse">
  <li><a href="#mathematical-principles-of-initialization-methods" id="toc-mathematical-principles-of-initialization-methods" class="nav-link" data-scroll-target="#mathematical-principles-of-initialization-methods">5.1.1 Mathematical Principles of Initialization Methods</a></li>
  <li><a href="#initialization-methods-comparative-analysis-in-practice" id="toc-initialization-methods-comparative-analysis-in-practice" class="nav-link" data-scroll-target="#initialization-methods-comparative-analysis-in-practice">5.1.2 Initialization Methods: Comparative Analysis in Practice</a></li>
  <li><a href="#practical-recommendations-and-additional-considerations" id="toc-practical-recommendations-and-additional-considerations" class="nav-link" data-scroll-target="#practical-recommendations-and-additional-considerations">5.1.3 Practical Recommendations and Additional Considerations</a></li>
  </ul></li>
  <li><a href="#optimization-algorithms-the-core-engine-of-deep-learning" id="toc-optimization-algorithms-the-core-engine-of-deep-learning" class="nav-link" data-scroll-target="#optimization-algorithms-the-core-engine-of-deep-learning">5.2 Optimization Algorithms: The Core Engine of Deep Learning</a>
  <ul class="collapse">
  <li><a href="#evolution-and-implementation-of-optimization-algorithms---ongoing-evolution" id="toc-evolution-and-implementation-of-optimization-algorithms---ongoing-evolution" class="nav-link" data-scroll-target="#evolution-and-implementation-of-optimization-algorithms---ongoing-evolution">5.2.1 Evolution and Implementation of Optimization Algorithms - Ongoing Evolution</a></li>
  <li><a href="#optimization-training-comparison" id="toc-optimization-training-comparison" class="nav-link" data-scroll-target="#optimization-training-comparison">5.2.2 Optimization Training Comparison</a></li>
  </ul></li>
  <li><a href="#visualization-and-analysis-of-the-optimization-process-peeking-into-the-black-box-of-deep-learning" id="toc-visualization-and-analysis-of-the-optimization-process-peeking-into-the-black-box-of-deep-learning" class="nav-link" data-scroll-target="#visualization-and-analysis-of-the-optimization-process-peeking-into-the-black-box-of-deep-learning">5.3 Visualization and Analysis of the Optimization Process: Peeking into the Black Box of Deep Learning</a>
  <ul class="collapse">
  <li><a href="#understanding-loss-landscape-a-topological-map-of-deep-learning-models" id="toc-understanding-loss-landscape-a-topological-map-of-deep-learning-models" class="nav-link" data-scroll-target="#understanding-loss-landscape-a-topological-map-of-deep-learning-models">5.3.1 Understanding Loss Landscape: A Topological Map of Deep Learning Models</a></li>
  <li><a href="#in-depth-techniques-for-loss-surface-analysis" id="toc-in-depth-techniques-for-loss-surface-analysis" class="nav-link" data-scroll-target="#in-depth-techniques-for-loss-surface-analysis">5.3.2 In-Depth Techniques for Loss Surface Analysis</a></li>
  </ul></li>
  <li><a href="#visualizing-the-optimization-process-unveiling-the-secrets-of-deep-learning-through-gaussian-functions" id="toc-visualizing-the-optimization-process-unveiling-the-secrets-of-deep-learning-through-gaussian-functions" class="nav-link" data-scroll-target="#visualizing-the-optimization-process-unveiling-the-secrets-of-deep-learning-through-gaussian-functions">5.4 Visualizing the Optimization Process: Unveiling the Secrets of Deep Learning through Gaussian Functions</a>
  <ul class="collapse">
  <li><a href="#approximate-analysis-through-gaussian-functions-hidden-insights-in-simplicity" id="toc-approximate-analysis-through-gaussian-functions-hidden-insights-in-simplicity" class="nav-link" data-scroll-target="#approximate-analysis-through-gaussian-functions-hidden-insights-in-simplicity">5.4.1 Approximate Analysis through Gaussian Functions: Hidden Insights in Simplicity</a></li>
  <li><a href="#path-visualization" id="toc-path-visualization" class="nav-link" data-scroll-target="#path-visualization">5.4.2 Path Visualization</a></li>
  </ul></li>
  <li><a href="#dynamic-analysis-of-the-optimization-process-exploring-learning-trajectories" id="toc-dynamic-analysis-of-the-optimization-process-exploring-learning-trajectories" class="nav-link" data-scroll-target="#dynamic-analysis-of-the-optimization-process-exploring-learning-trajectories">5.5 Dynamic Analysis of the Optimization Process: Exploring Learning Trajectories</a>
  <ul class="collapse">
  <li><a href="#characteristics-of-the-training-process" id="toc-characteristics-of-the-training-process" class="nav-link" data-scroll-target="#characteristics-of-the-training-process">5.5.1 Characteristics of the Training Process</a></li>
  <li><a href="#learning-stability-analysis-and-control" id="toc-learning-stability-analysis-and-control" class="nav-link" data-scroll-target="#learning-stability-analysis-and-control">5.5.2 Learning Stability Analysis and Control</a></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a></li>
  <li><a href="#practice-problems" id="toc-practice-problems" class="nav-link" data-scroll-target="#practice-problems">Practice Problems</a></li>
  <li><a href="#reference-materials" id="toc-reference-materials" class="nav-link" data-scroll-target="#reference-materials">Reference Materials</a></li>
  </ul></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../../notebooks/en/part_1/01_The Beginning of Deep Learning.html">part_1</a></li><li class="breadcrumb-item"><a href="../../../notebooks/en/part_1/05_Optimization and Visualization.html">5. Optimization and Visualization</a></li></ol></nav></header>




<p><a href="https://colab.research.google.com/github/Quantum-Intelligence-Frontier/dldna/blob/main/notebooks/en/part_1/05_Optimization_and_Visualization.ipynb" target="_parent"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"> </a></p>
<section id="optimization-and-visualization" class="level1">
<h1>5. Optimization and Visualization</h1>
<blockquote class="blockquote">
<p>“There is a bigger difference between theory and practice than between theory and theory.” - Yann LeCun, 2018 Turing Award winner</p>
</blockquote>
<p>The success of deep learning models heavily depends on effective optimization algorithms and proper weight initialization strategies. This chapter delves into the core elements of deep learning model training, namely optimization and initialization methods, and presents ways to intuitively understand these processes through visualization. First, it explores the development and mathematical principles of various weight initialization methods that form the foundation of neural network learning. Then, starting with Gradient Descent, it compares and analyzes the working principles and performance of state-of-the-art optimization algorithms such as Adam, Lion, Sophia, and AdaFactor. In particular, it not only discusses theoretical backgrounds but also experimentally verifies how each algorithm operates in actual deep learning model training processes. Finally, it introduces various techniques for visualizing and analyzing high-dimensional loss function spaces (loss landscapes) and provides in-depth insights into the learning dynamics of deep learning models.</p>
<section id="evolution-and-modern-approaches-to-parameter-initialization" class="level2">
<h2 class="anchored" data-anchor-id="evolution-and-modern-approaches-to-parameter-initialization">5.1 Evolution and Modern Approaches to Parameter Initialization</h2>
<p>Neural network parameter initialization is a crucial element that determines the model’s convergence, learning efficiency, and final performance. Incorrect initialization can be a primary cause of training failure. PyTorch provides various initialization methods through the torch.nn.init module, with detailed information available in the official documentation. The evolution of initialization methods reflects the history of deep learning researchers overcoming the difficulties of neural network learning. In particular, inappropriate initialization has been identified as a major obstacle to deep neural network learning, causing vanishing or exploding gradient phenomena. Recently, the emergence of large language models such as GPT-3 and LaMDA has further emphasized the importance of initialization. As model sizes increase, the distribution of initial parameters has a greater impact on the early stages of training. Therefore, selecting an appropriate initialization strategy tailored to the model’s characteristics and size has become an essential step in developing deep learning models.</p>
<section id="mathematical-principles-of-initialization-methods" class="level3">
<h3 class="anchored" data-anchor-id="mathematical-principles-of-initialization-methods">5.1.1 Mathematical Principles of Initialization Methods</h3>
<p>The development of neural network initialization methods is the result of in-depth mathematical theory and numerous experimental validations. Each initialization method was designed to address specific problem situations (e.g., using certain activation functions, network depth, or model type) or improve learning dynamics, evolving over time to respond to new challenges.</p>
<p>The following are the initialization methods that will be compared and analyzed in this book. (The complete implementation code is included in the chapter_04/initialization/base.py file.)</p>
<div id="cell-2" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip install dldna[colab] <span class="co"># in Colab</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="co"># !pip install dldna[all] # in your local</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>load_ext autoreload</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>autoreload <span class="dv">2</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="cell-3" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Set seed</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">7</span>)</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">7</span>)</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_05.initialization.base <span class="im">import</span> init_methods, init_weights_lecun, init_weights_scaled_orthogonal, init_weights_lmomentum <span class="co"># init_weights_emergence, init_weights_dynamic 삭제</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>init_methods <span class="op">=</span> {</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Historical/Educational Significance</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>    <span class="st">'lecun'</span>: init_weights_lecun,        <span class="co"># The first systematic initialization proposed in 1998</span></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>    <span class="st">'xavier_normal'</span>: nn.init.xavier_normal_, <span class="co"># Key to the revival of deep learning in 2010</span></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>    <span class="st">'kaiming_normal'</span>: nn.init.kaiming_normal_, <span class="co"># Standard for the ReLU era, 2015</span></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Modern Standard</span></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>    <span class="st">'orthogonal'</span>: nn.init.orthogonal_,  <span class="co"># Important in RNN/LSTM</span></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>    <span class="st">'scaled_orthogonal'</span>: init_weights_scaled_orthogonal, <span class="co"># Optimization of deep neural networks</span></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 2024 Latest Research</span></span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>    <span class="st">'l-momentum'</span>: init_weights_lmomentum <span class="co"># L-Momentum Initialization</span></span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<section id="traditional-initialization" class="level5">
<h5 class="anchored" data-anchor-id="traditional-initialization">Traditional Initialization</h5>
<ul>
<li><strong>LeCun Initialization (1998):</strong> <span class="math inline">\(std = \sqrt{\frac{1}{n_{in}}}\)</span>
<ul>
<li>Proposed by Yann LeCun in 1998, this method determines the standard deviation of weights based solely on the input dimension (<span class="math inline">\(n_{in}\)</span>). It aimed to prevent the output of each neuron from varying greatly with the number of inputs. However, in deep networks, the variance of activation values tended to decrease as the layers got deeper, especially when using sigmoid and tanh-like activation functions.</li>
</ul></li>
</ul>
</section>
<section id="modern-initialization" class="level5">
<h5 class="anchored" data-anchor-id="modern-initialization">Modern Initialization</h5>
<ul>
<li><strong>Xavier Initialization (Glorot, 2010):</strong> <span class="math inline">\(std = \sqrt{\frac{2}{n_{in} + n_{out}}}\)</span>
<ul>
<li>Proposed by Xavier Glorot and Yoshua Bengio, this method considers both the input (<span class="math inline">\(n_{in}\)</span>) and output (<span class="math inline">\(n_{out}\)</span>) dimensions to mitigate the vanishing/exploding gradient problem. The key is to maintain appropriate variance for activation values and gradients at each layer. It’s particularly effective when used with saturating activation functions like sigmoid and tanh.</li>
</ul></li>
<li><strong>Kaiming Initialization (He, 2015):</strong> <span class="math inline">\(std = \sqrt{\frac{2}{n_{in}}}\)</span>
<ul>
<li>Proposed by Kaiming He et al., this method is designed considering the characteristics of ReLU activation functions (which set negative inputs to zero). Since ReLU tends to halve the variance of activation values, it uses a larger variance (<span class="math inline">\(\sqrt{2}\)</span> times) than Xavier initialization to compensate. This reduces the “dead neuron” problem and enables stable learning in deep networks, making it a de facto standard for ReLU-like activation functions.</li>
</ul></li>
</ul>
</section>
<section id="latest-initialization-2023-and-later" class="level5">
<h5 class="anchored" data-anchor-id="latest-initialization-2023-and-later">Latest Initialization (2023 and later)</h5>
<ul>
<li><p><strong>L-Momentum Initialization (Zhuang, 2024)</strong></p>
<ul>
<li>L-Momentum Initialization is a recent initialization method proposed in 2024, inspired by momentum-based optimization algorithms. It controls the L-momentum of the initial weight matrix.</li>
<li><strong>Formula:</strong></li>
</ul>
<p><span class="math inline">\(W \sim U(-\sqrt{\frac{6}{n_{in}}}, \sqrt{\frac{6}{n_{in}}})\)</span> <span class="math inline">\(W = W \cdot \sqrt{\frac{\alpha}{Var(W)}}\)</span></p>
<p>where <span class="math inline">\(U\)</span> is the uniform distribution, and <span class="math inline">\(\alpha\)</span> represents the L-Momentum value, using the square of the momentum value used in the optimizer.</p>
<ul>
<li>The goal is to reduce gradient variability at the initial stage, providing a stable learning path.</li>
<li>It can be applied to various optimizers and activation functions, with experimental results showing contributions to large learning rates, fast convergence, and improved generalization performance.</li>
</ul></li>
</ul>
</section>
<section id="mathematical-principles" class="level5">
<h5 class="anchored">Mathematical Principles</h5>
<p>Most modern initialization methods follow these three core principles (either explicitly or implicitly):</p>
<ol type="1">
<li><p><strong>Variance Preservation:</strong> The variance of activation values during the forward pass and gradients during the backward pass should be maintained consistently across layers.</p>
<p><span class="math inline">\(Var(y) \approx Var(x)\)</span></p>
<p>This helps in stable learning by preventing signals from becoming too large or too small.</p></li>
<li><p><strong>Spectral Control:</strong> The distribution of singular values of weight matrices should be controlled to ensure numerical stability during training.</p>
<p><span class="math inline">\(\sigma_{max}(W) / \sigma_{min}(W) \leq C\)</span></p>
<p>This is particularly important in structures like recurrent neural networks (RNNs), where weight matrices are repeatedly multiplied.</p></li>
<li><p><strong>Expressivity Optimization:</strong> The effective rank of the weight matrix should be maximized to ensure that the network has sufficient expressiveness. <span class="math inline">\(rank_{eff}(W) = \frac{\sum_i \sigma_i}{\max_i \sigma_i}\)</span> <em>Recent studies have been trying to explicitly satisfy these principles.</em></p></li>
</ol>
<p>In conclusion, the initialization method should be carefully chosen considering the model’s size, structure, activation function, and interaction with the optimization algorithm, as it has a significant impact on the model’s learning speed, stability, and final performance.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Click to view content (Deep Dive: Mathematical Principles and Latest Techniques of Deep Neural Network Initialization)">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Click to view content (Deep Dive: Mathematical Principles and Latest Techniques of Deep Neural Network Initialization)
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<section id="mathematical-principles-and-latest-techniques-of-deep-neural-network-initialization" class="level3">
<h3 class="anchored" data-anchor-id="mathematical-principles-and-latest-techniques-of-deep-neural-network-initialization">Mathematical Principles and Latest Techniques of Deep Neural Network Initialization</h3>
<section id="variance-preservation-principle" class="level4">
<h4 class="anchored" data-anchor-id="variance-preservation-principle">1. Variance Preservation Principle</h4>
<section id="theoretical-foundation" class="level5">
<h5 class="anchored" data-anchor-id="theoretical-foundation">1.1 Theoretical Foundation</h5>
<p>As the depth of a neural network increases, preserving the statistical characteristics (especially variance) of signals during forward propagation and backpropagation is crucial. This prevents signals from vanishing or exploding, enabling stable learning.</p>
<p>Let <span class="math inline">\(h_l\)</span> be the activation value of the <span class="math inline">\(l\)</span>th layer, <span class="math inline">\(W_l\)</span> be the weight matrix, <span class="math inline">\(b_l\)</span> be the bias, and <span class="math inline">\(f\)</span> be the activation function. Then, the forward propagation can be expressed as:</p>
<p><span class="math inline">\(h_l = f(W_l h_{l-1} + b_l)\)</span></p>
<p>Assuming that each element of the input signal <span class="math inline">\(h_{l-1} \in \mathbb{R}^{n_{in}}\)</span> is an independent random variable with a mean of 0 and variance <span class="math inline">\(\sigma^2_{h_{l-1}}\)</span>, each element of the weight matrix <span class="math inline">\(W_l \in \mathbb{R}^{n_{out} \times n_{in}}\)</span> is an independent random variable with a mean of 0 and variance <span class="math inline">\(Var(W_l)\)</span>, and the bias <span class="math inline">\(b_l = 0\)</span>, the following holds <em>when the activation function is linear</em>:</p>
<p><span class="math inline">\(Var(h_l) = n_{in} Var(W_l) Var(h_{l-1})\)</span> (where <span class="math inline">\(n_{in}\)</span> is the input dimension of the <span class="math inline">\(l\)</span>th layer)</p>
<p>For the variance of the activation value to be preserved, <span class="math inline">\(Var(h_l) = Var(h_{l-1})\)</span> must hold, so <span class="math inline">\(Var(W_l) = 1/n_{in}\)</span> must be true.</p>
<p>During backpropagation, for the error signal <span class="math inline">\(\delta_l = \frac{\partial L}{\partial h_l}\)</span> (where <span class="math inline">\(L\)</span> is the loss function), the following relationship holds:</p>
<p><span class="math inline">\(\delta_{l-1} = W_l^T \delta_l\)</span> (assuming the activation function is linear)</p>
<p>Therefore, to preserve variance during backpropagation, <span class="math inline">\(Var(\delta_{l-1}) = n_{out}Var(W_l)Var(\delta_l)\)</span> must hold, so <span class="math inline">\(Var(W_l) = 1/n_{out}\)</span> must be true. (where <span class="math inline">\(n_{out}\)</span> is the output dimension of the <span class="math inline">\(l\)</span>th layer)</p>
</section>
<section id="extension-to-non-linear-activation-functions" class="level5">
<h5 class="anchored" data-anchor-id="extension-to-non-linear-activation-functions">1.2 Extension to Non-Linear Activation Functions</h5>
<p><strong>ReLU Activation Function</strong></p>
<p>The ReLU function (<span class="math inline">\(f(x) = max(0, x)\)</span>) tends to reduce the variance of the activation value because it sets half of the input to 0. Kaiming He proposed a variance preservation formula to compensate for this:</p>
<p><span class="math inline">\(Var(W_l) = \frac{2}{n_{in}} \quad (\text{ReLU-specific})\)</span></p>
<p>This compensates for the reduction in variance by increasing it by a factor of 2.</p>
<p><strong>Leaky ReLU Activation Function</strong></p>
<p>For Leaky ReLU (<span class="math inline">\(f(x) = max(\alpha x, x)\)</span>, where <span class="math inline">\(\alpha\)</span> is a small constant), the generalized formula is:</p>
<p><span class="math inline">\(Var(W_l) = \frac{2}{(1 + \alpha^2) n_{in}}\)</span></p>
</section>
<section id="probabilistic-approach-reference" class="level5">
<h5 class="anchored" data-anchor-id="probabilistic-approach-reference">1.3 Probabilistic Approach (Reference)</h5>
<p>There is also a method to initialize using the inverse of the Fisher Information Matrix (FIM), which contains curvature information in parameter space, allowing for more efficient initialization. (For more details, refer to reference [4] Martens, 2020).</p>
</section>
</section>
<section id="spectral-control" class="level4">
<h4 class="anchored" data-anchor-id="spectral-control">2. Spectral Control</h4>
<section id="singular-value-decomposition-and-learning-dynamics" class="level5">
<h5 class="anchored" data-anchor-id="singular-value-decomposition-and-learning-dynamics">2.1 Singular Value Decomposition and Learning Dynamics</h5>
<p>The singular value decomposition (SVD) of a weight matrix <span class="math inline">\(W \in \mathbb{R}^{m \times n}\)</span> is expressed as <span class="math inline">\(W = U\Sigma V^T\)</span>, where <span class="math inline">\(\Sigma\)</span> is a diagonal matrix and its diagonal elements are the singular values (<span class="math inline">\(\sigma_1 \geq \sigma_2 \geq ... \geq 0\)</span>) of <span class="math inline">\(W\)</span>. If the maximum singular value (<span class="math inline">\(\sigma_{max}\)</span>) of the weight matrix is too large, it can cause an exploding gradient, and if the minimum singular value (<span class="math inline">\(\sigma_{min}\)</span>) is too small, it can cause a vanishing gradient.</p>
<p>Therefore, controlling the ratio of singular values (condition number) <span class="math inline">\(\kappa = \sigma_{max}/\sigma_{min}\)</span> is important. The closer <span class="math inline">\(\kappa\)</span> is to 1, the more stable the gradient flow is guaranteed.</p>
<p><strong>Theorem 2.1 (Saxe et al., 2014)</strong>: In a deep linear neural network with orthogonal initialization, if the weight matrix <span class="math inline">\(W_l\)</span> of each layer is an orthogonal matrix, the Frobenius norm of the Jacobian matrix <span class="math inline">\(J\)</span> of the output with respect to the input is maintained as 1.</p>
<p><span class="math inline">\(||J||_F = 1\)</span></p>
<p>This helps alleviate the problem of gradient disappearance or explosion even in very deep networks.</p>
</section>
<section id="dynamic-spectral-normalization" class="level5">
<h5 class="anchored" data-anchor-id="dynamic-spectral-normalization">2.2 Dynamic Spectral Normalization</h5>
<p>Miyato et al.(2018) proposed a spectral normalization technique that restricts the spectral norm (maximum singular value) of the weight matrix to improve the stability of GAN training.</p>
<p><span class="math inline">\(W_{SN} = \frac{W}{\sigma_{max}(W)}\)</span></p>
<p>This method is particularly effective in GAN training and has recently been applied to other models such as Vision Transformer.</p>
</section>
</section>
<section id="expressivity-optimization" class="level4">
<h4 class="anchored" data-anchor-id="expressivity-optimization">3. Expressivity Optimization</h4>
<section id="effective-rank-theory" class="level5">
<h5 class="anchored" data-anchor-id="effective-rank-theory">3.1 Effective Rank Theory</h5>
<p>The diversity of features that a weight matrix <span class="math inline">\(W\)</span> can express can be measured by the uniformity of the singular value distribution. The effective rank is defined as follows.</p>
<p><span class="math inline">\(\text{rank}_{eff}(W) = \exp\left( -\sum_{i=1}^r p_i \ln p_i \right) \quad \text{where } p_i = \frac{\sigma_i}{\sum_j \sigma_j}\)</span></p>
<p>Here, <span class="math inline">\(r\)</span> is the rank of <span class="math inline">\(W\)</span>, <span class="math inline">\(\sigma_i\)</span> is the <span class="math inline">\(i\)</span>-th singular value, and <span class="math inline">\(p_i\)</span> is the normalized singular value. The effective rank is an indicator of the distribution of singular values, and a larger value means that the singular values are more evenly distributed, indicating higher expressivity.</p>
</section>
<section id="initialization-strategy-comparison-table" class="level5">
<h5 class="anchored" data-anchor-id="initialization-strategy-comparison-table">3.2 Initialization Strategy Comparison Table</h5>
<table class="caption-top table">
<colgroup>
<col style="width: 25%">
<col style="width: 32%">
<col style="width: 17%">
<col style="width: 24%">
</colgroup>
<thead>
<tr class="header">
<th>Initialization Method</th>
<th>Singular Value Distribution</th>
<th>Effective Rank</th>
<th>Suitable Architecture</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Xavier</td>
<td>Decreases relatively quickly</td>
<td>Low</td>
<td>Shallow MLP</td>
</tr>
<tr class="even">
<td>Kaiming</td>
<td>Adjusted for ReLU activation (decreases less)</td>
<td>Medium</td>
<td>CNN</td>
</tr>
<tr class="odd">
<td>Orthogonal</td>
<td>All singular values are 1</td>
<td>High</td>
<td>RNN/Transformer</td>
</tr>
<tr class="even">
<td>Emergence-Promoting</td>
<td>Adjusts according to network size, decreases relatively slowly (close to heavy-tailed distribution)</td>
<td>High</td>
<td>LLM</td>
</tr>
</tbody>
</table>
</section>
<section id="emergence-promoting-initialization" class="level5">
<h5 class="anchored" data-anchor-id="emergence-promoting-initialization">3.3 Emergence-Promoting Initialization</h5>
<p>Emergence-Promoting initialization is a recently proposed technique to promote emergent abilities in large language models (LLMs). This method adjusts the variance of the initial weights according to the network size (especially the depth of the layers), which has the effect of increasing the effective rank.</p>
<p>Chen et al.&nbsp;(2023) proposed the following scaling factor <span class="math inline">\(\nu_l\)</span> for Transformer models:</p>
<p><span class="math inline">\(\nu_l = \frac{1}{\sqrt{d_{in}}} \left( 1 + \frac{\ln l}{\ln d} \right)\)</span></p>
<p>where <span class="math inline">\(d_{in}\)</span> is the input dimension, <span class="math inline">\(l\)</span> is the layer index, and <span class="math inline">\(d\)</span> is the model depth. This scaling factor is multiplied by the standard deviation of the weight matrix to initialize it. Specifically, it samples from a normal distribution with a standard deviation of <span class="math inline">\(\sqrt{2/n_{in}}\)</span> multiplied by <span class="math inline">\(\nu_l\)</span>.</p>
</section>
</section>
<section id="interaction-between-initialization-and-optimization" class="level4">
<h4 class="anchored" data-anchor-id="interaction-between-initialization-and-optimization">4. Interaction between Initialization and Optimization</h4>
<section id="ntk-theory-extension" class="level5">
<h5 class="anchored" data-anchor-id="ntk-theory-extension">4.1 NTK Theory Extension</h5>
<p>The Neural Tangent Kernel (NTK) theory proposed by Jacot et al.&nbsp;(2018) is a useful tool for analyzing the learning dynamics of “infinitely wide” neural networks. According to NTK theory, the expected Hessian matrix of a very wide neural network at initialization is proportional to the identity matrix. Specifically,</p>
<p><span class="math inline">\(\lim_{n_{in} \to \infty} \mathbb{E}[\nabla^2 \mathcal{L}] \propto I\)</span> (at initialization)</p>
<p>This suggests that Xavier initialization provides nearly optimal initialization for wide neural networks.</p>
</section>
<section id="meta-initialization-strategy" class="level5">
<h5 class="anchored" data-anchor-id="meta-initialization-strategy">4.2 Meta Initialization Strategy</h5>
<p>Recent studies such as MetaInit (2023) propose learning the optimal initialization distribution for a given architecture and dataset through meta-learning.</p>
<p><span class="math inline">\(\theta_{init} = \arg\min_\theta \mathbb{E}_{\mathcal{T}}[\mathcal{L}(\phi_{fine-tune}(\theta, \mathcal{T}))]\)</span></p>
<p>where <span class="math inline">\(\theta\)</span> is the initialization parameter, <span class="math inline">\(\mathcal{T}\)</span> is the training task, and <span class="math inline">\(\phi\)</span> represents the process of fine-tuning a model initialized with <span class="math inline">\(\theta\)</span>.</p>
</section>
</section>
<section id="reference-physics-based-initialization-techniques" class="level4">
<h4 class="anchored" data-anchor-id="reference-physics-based-initialization-techniques">5. (Reference) Physics-Based Initialization Techniques</h4>
<p>Recently, initialization methods inspired by principles in physics have also been studied. For example, methods that mimic the Schrödinger equation in quantum mechanics or the Navier-Stokes equations in fluid dynamics to optimize inter-layer information flow have been proposed. However, these methods are still in the early stages of research and their practicality has not been verified.</p>
</section>
<section id="practical-recommendations" class="level4">
<h4 class="anchored" data-anchor-id="practical-recommendations">6. Practical Recommendations</h4>
<ol type="1">
<li><strong>CNN Architecture:</strong> It is generally recommended to use Kaiming initialization (He initialization) and batch normalization together.</li>
<li><strong>Transformer:</strong> Scaled Orthogonal Initialization (singular value adjustment) or Xavier initialization are widely used.</li>
<li><strong>LLM:</strong> Consider initialization methods specialized for large models, such as Emergence-Promoting initialization.</li>
<li><strong>Neural ODE</strong>: Use general methods unless special cases apply.</li>
</ol>
<hr>
</section>
</section>
<section id="references" class="level3">
<h3 class="anchored" data-anchor-id="references">References</h3>
<ol type="1">
<li>He et al.&nbsp;“Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification”, ICCV 2015</li>
<li>Saxe et al.&nbsp;“Exact solutions to the nonlinear dynamics of learning in deep linear neural networks”, ICLR 2014</li>
<li>Jacot et al.&nbsp;“Neural Tangent Kernel: Convergence and Generalization in Neural Networks”, NeurIPS 2018</li>
<li>Martens, J. “New insights and perspectives on the natural gradient method.” The Journal of Machine Learning Research, 2020.</li>
<li>Chen et al.&nbsp;“Towards Understanding Large Language Models: A Transformative Reading List”, arXiv preprint arXiv:2307.12980, 2023. (Related to Emergence-Promoting Initialization)</li>
<li>Miyato et al., “Spectral Normalization for Generative Adversarial Networks”, ICLR 2018</li>
</ol>
</section>
</div>
</div>
</div>
</section>
</section>
<section id="initialization-methods-comparative-analysis-in-practice" class="level3">
<h3 class="anchored" data-anchor-id="initialization-methods-comparative-analysis-in-practice">5.1.2 Initialization Methods: Comparative Analysis in Practice</h3>
<p>To investigate how the various initialization methods actually affect model learning, we will conduct a comparative experiment using a simple model. We will train models with each initialization method under the same conditions and analyze the results. The evaluation metrics are as follows.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 17%">
<col style="width: 45%">
<col style="width: 37%">
</colgroup>
<thead>
<tr class="header">
<th>Evaluation Metric</th>
<th>Meaning</th>
<th>Desirable Characteristics</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Error Rate(%)</td>
<td>Final model’s predictive performance (lower is better)</td>
<td>Lower is better</td>
</tr>
<tr class="even">
<td>Convergence Speed</td>
<td>Learning curve’s slope (learning stability indicator)</td>
<td>Lower (steeper) is faster convergence</td>
</tr>
<tr class="odd">
<td>Average Condition Number</td>
<td>Numerical stability of weight matrix</td>
<td>Lower (closer to 1) is more stable</td>
</tr>
<tr class="even">
<td>Spectral Norm</td>
<td>Size of weight matrix (maximum singular value)</td>
<td>Needs an appropriate value, not too large or small</td>
</tr>
<tr class="odd">
<td>Effective Rank Ratio</td>
<td>Expressiveness of weight matrix (uniformity of singular value distribution)</td>
<td>Higher is better</td>
</tr>
<tr class="even">
<td>Execution Time(s)</td>
<td>Learning time</td>
<td>Lower is better</td>
</tr>
</tbody>
</table>
<div id="cell-7" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_04.models.base <span class="im">import</span> SimpleNetwork</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_04.utils.data <span class="im">import</span> get_data_loaders, get_device</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_05.initialization.base <span class="im">import</span> init_methods</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_05.initialization.analysis <span class="im">import</span> analyze_initialization, create_detailed_analysis_table</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> get_device()</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize data loaders</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>train_dataloader, test_dataloader <span class="op">=</span> get_data_loaders()</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Detailed analysis of initialization methods</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> analyze_initialization(</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>    model_class<span class="op">=</span><span class="kw">lambda</span>: SimpleNetwork(act_func<span class="op">=</span>nn.PReLU()),</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>    init_methods<span class="op">=</span>init_methods,</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>    train_loader<span class="op">=</span>train_dataloader,</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>    test_loader<span class="op">=</span>test_dataloader,</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>    epochs<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>    device<span class="op">=</span>device</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Print detailed analysis results table</span></span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>create_detailed_analysis_table(results)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
Initialization method: lecun</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"f7415089fb524e58a3bc0648f03072a2","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/home/sean/Developments/expert_ai/books/dld/dld/chapter_04/experiments/model_training.py:320: UserWarning: std(): degrees of freedom is &lt;= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at ../aten/src/ATen/native/ReduceOps.cpp:1823.)
  'std': param.data.std().item(),</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Initialization method: xavier_normal</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"29ce93b749b746c7a45214d8a3d878f9","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Initialization method: kaiming_normal</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"fd9357f07d154648991dcd18979512aa","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Initialization method: orthogonal</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"bdc4bd358cf64c8ea5002b02fad46550","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Initialization method: scaled_orthogonal</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"e80c4b3368c249dd8edd6b4f4d125d62","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Initialization method: l-momentum</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"622b899e8de849aba61c8cec8bd7e078","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Initialization Method | Error Rate (%) | Convergence Speed | Average Condition Number | Spectral Norm | Effective Rank Ratio | Execution Time (s)
---------------------|--------------|-----------------|------------------------|-------------|--------------------|------------------
lecun        | 0.48 | 0.33 | 5.86 | 1.42 | 0.89 | 30.5
xavier_normal | 0.49 | 0.33 | 5.53 | 1.62 | 0.89 | 30.2
kaiming_normal | 0.45 | 0.33 | 5.85 | 1.96 | 0.89 | 30.1
orthogonal   | 0.49 | 0.33 | 1.00 | 0.88 | 0.95 | 30.0
scaled_orthogonal | 2.30 | 1.00 | 1.00 | 0.13 | 0.95 | 30.0
l-momentum   | nan | 0.00 | 5.48 | 19.02 | 0.89 | 30.1</code></pre>
</div>
</div>
<p>The results of the experiment are summarized in the following table.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 16%">
<col style="width: 11%">
<col style="width: 13%">
<col style="width: 18%">
<col style="width: 10%">
<col style="width: 15%">
<col style="width: 14%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Initialization Method</th>
<th style="text-align: center;">Error Rate (%)</th>
<th style="text-align: center;">Convergence Speed</th>
<th style="text-align: center;">Average Condition Number</th>
<th style="text-align: center;">Spectral Norm</th>
<th style="text-align: center;">Effective Rank Ratio</th>
<th style="text-align: center;">Execution Time (s)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">lecun</td>
<td style="text-align: center;">0.48</td>
<td style="text-align: center;">0.33</td>
<td style="text-align: center;">5.66</td>
<td style="text-align: center;">1.39</td>
<td style="text-align: center;">0.89</td>
<td style="text-align: center;">23.3</td>
</tr>
<tr class="even">
<td style="text-align: left;">xavier_normal</td>
<td style="text-align: center;">0.48</td>
<td style="text-align: center;">0.33</td>
<td style="text-align: center;">5.60</td>
<td style="text-align: center;">1.64</td>
<td style="text-align: center;">0.89</td>
<td style="text-align: center;">23.2</td>
</tr>
<tr class="odd">
<td style="text-align: left;">kaiming_normal</td>
<td style="text-align: center;">0.45</td>
<td style="text-align: center;">0.33</td>
<td style="text-align: center;">5.52</td>
<td style="text-align: center;">1.98</td>
<td style="text-align: center;">0.89</td>
<td style="text-align: center;">23.2</td>
</tr>
<tr class="even">
<td style="text-align: left;">orthogonal</td>
<td style="text-align: center;">0.49</td>
<td style="text-align: center;">0.33</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">0.88</td>
<td style="text-align: center;">0.95</td>
<td style="text-align: center;">23.3</td>
</tr>
<tr class="odd">
<td style="text-align: left;">scaled_orthogonal</td>
<td style="text-align: center;">2.30</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">0.13</td>
<td style="text-align: center;">0.95</td>
<td style="text-align: center;">23.3</td>
</tr>
<tr class="even">
<td style="text-align: left;">l-momentum</td>
<td style="text-align: center;">nan</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">5.78</td>
<td style="text-align: center;">20.30</td>
<td style="text-align: center;">0.89</td>
<td style="text-align: center;">23.2</td>
</tr>
</tbody>
</table>
<p>Notable points from the experiment results are as follows.</p>
<ol type="1">
<li><p><strong>Excellent performance of Kaiming initialization:</strong> Kaiming initialization showed the lowest error rate at 0.45%. This result demonstrates the optimal combination with ReLU activation functions and reconfirms that Kaiming initialization is effective when used with ReLU series functions.</p></li>
<li><p><strong>Stability of Orthogonal series:</strong> Orthogonal initialization showed the best numerical stability with a condition number of 1.00. This means that gradients are not distorted and propagate well during training, which is particularly important in models such as recurrent neural networks (RNNs) where weight matrices are repeatedly multiplied. <em>However, this experiment showed relatively high error rates, possibly due to the characteristics of the model used (simple MLP).</em></p></li>
<li><p><strong>Problem with Scaled Orthogonal initialization:</strong> Scaled Orthogonal initialization showed a very high error rate at 2.30%. This suggests that this initialization method may not be suitable for the given model and dataset or requires additional hyperparameter adjustments. <em>It is possible that the scaling factor was too small, causing learning to not occur properly.</em></p></li>
<li><p><strong>L-Momentum Initialization Instability</strong>: L-Momentum had an error rate and convergence speed of nan and 0.00, which means that no learning occurred. The spectral norm being 20.30 is very high, which may be due to the weights’ initial values being too large, causing divergence.</p></li>
</ol>
</section>
<section id="practical-recommendations-and-additional-considerations" class="level3">
<h3 class="anchored" data-anchor-id="practical-recommendations-and-additional-considerations">5.1.3 Practical Recommendations and Additional Considerations</h3>
<p>Deep learning model initialization is a <em>hyperparameter</em> that should be carefully chosen considering the model’s architecture, activation functions, optimization algorithms, and characteristics of the dataset. The following are factors to consider when selecting an initialization method in practice.</p>
<section id="basic-principles" class="level5">
<h5 class="anchored" data-anchor-id="basic-principles">Basic Principles</h5>
<ul>
<li><strong>ReLU series activation functions:</strong>
<ul>
<li><strong>Kaiming initialization (He initialization):</strong> This is the most widely used initialization method when using ReLU and its variants (Leaky ReLU, ELU, SELU, etc.). It has a solid theoretical background (variance preservation) as well as experimental results.</li>
<li><strong>L-Momentum Initialization</strong>: If you’re using optimizers like Adam, AdamW, or other Momentum series, this can be considered.</li>
</ul></li>
<li><strong>Sigmoid, Tanh activation functions:</strong>
<ul>
<li><strong>Xavier initialization (Glorot initialization):</strong> These activation functions can suffer from the vanishing gradient problem if the input values are too large or too small. Therefore, Xavier initialization remains a valid choice.</li>
</ul></li>
<li><strong>Recurrent Neural Networks (RNN, LSTM, GRU):</strong>
<ul>
<li><strong>Orthogonal initialization:</strong> For RNN models with recurrent connections, it’s crucial to maintain the singular values of the weight matrix close to 1. Orthogonal initialization guarantees this, mitigating gradient explosion/vanishing problems and helping with long-range dependency learning.</li>
<li><em>Note: Orthogonal initialization is typically applied to the hidden-to-hidden weight matrix of RNNs, while a different initialization method (e.g., Kaiming) is used for the input-to-hidden weight matrix.</em></li>
</ul></li>
</ul>
</section>
<section id="model-scale-and-characteristics" class="level5">
<h5 class="anchored" data-anchor-id="model-scale-and-characteristics">Model Scale and Characteristics</h5>
<ul>
<li><strong>General deep neural networks (less than 50 layers):</strong>
<ul>
<li>Kaiming initialization (for ReLU series) or Xavier initialization (for Sigmoid/Tanh series) often suffices.</li>
</ul></li>
<li><strong>Very deep neural networks (50 layers or more):</strong>
<ul>
<li><strong>Residual connections (ResNet):</strong> When residual connections are present, Kaiming initialization works well.</li>
<li><strong>Without Residual connections:</strong> Initialization requires more careful consideration. Options like Scaled Orthogonal or Fixup Initialization may be explored.</li>
</ul></li>
<li><strong>Large-scale models (1B+ parameters):</strong>
<ul>
<li><strong>L-Momentum Initialization</strong></li>
<li><strong>Zero Initialization (for specific parts):</strong> Initializing certain parts of the model (e.g., output projections in attention layers of Transformer models) to zero can be effective. (Reference: Megatron-LM)</li>
<li><em>Note: Large-scale models can become unstable during training, so aside from initialization, careful combination of learning rate scheduling, gradient clipping, and regularization techniques is necessary.</em></li>
</ul></li>
</ul>
</section>
<section id="additional-considerations" class="level5">
<h5 class="anchored" data-anchor-id="additional-considerations">Additional Considerations</h5>
<ul>
<li><strong>Batch Normalization / Layer Normalization:</strong> Normalization techniques reduce the importance of initialization to <em>some</em> extent but do not completely replace it. It is still a good idea to choose an appropriate initialization.</li>
<li><strong>Transfer Learning:</strong> When using pre-trained models, it’s common to use the pre-trained weights as is or apply Kaiming/Xavier initialization with a small learning rate only to the layers that are fine-tuned.</li>
<li><strong>Optimization Algorithm:</strong> Depending on the optimizer used, there are initialization methods that are well-suited for it. For example, when using the Adam optimizer, L-Momentum initialization can be used.</li>
<li><strong>Experiments and Validation:</strong> The best initialization method may vary depending on the problem and data. Therefore, it’s essential to try several initialization methods and choose the one that performs best on the validation set.</li>
</ul>
<p>Initialization is like a “hidden hero” in deep learning model training. Proper initialization can determine the success or failure of model training and plays a crucial role in maximizing model performance and reducing training time. Based on the guidelines presented in this section and the latest research trends, we hope you find the most suitable initialization strategy for your deep learning model.</p>
</section>
</section>
</section>
<section id="optimization-algorithms-the-core-engine-of-deep-learning" class="level2">
<h2 class="anchored" data-anchor-id="optimization-algorithms-the-core-engine-of-deep-learning">5.2 Optimization Algorithms: The Core Engine of Deep Learning</h2>
<blockquote class="blockquote">
<p><strong>Challenge:</strong> How can we address the issue of Gradient Descent getting stuck in local minima or having a slow learning rate?</p>
<p><strong>Researcher’s Dilemma:</strong> Simply reducing the learning rate was not enough. In some cases, the learning became too slow and took a long time, while in other cases, it diverged and failed to learn. The path to finding the optimal point was as difficult as groping one’s way down a foggy mountain road. Various optimization algorithms such as Momentum, RMSProp, and Adam emerged, but there is still no single solution that perfectly fits all problems.</p>
</blockquote>
<p>The remarkable progress in deep learning has been achieved not only through innovations in model architecture but also through the development of <em>efficient optimization algorithms</em>. The optimization algorithm is like a core engine that automates and accelerates the process of finding the minimum value of the loss function. <em>How efficiently and stably this engine operates determines the learning speed and final performance of the deep learning model.</em></p>
<section id="evolution-and-implementation-of-optimization-algorithms---ongoing-evolution" class="level3">
<h3 class="anchored" data-anchor-id="evolution-and-implementation-of-optimization-algorithms---ongoing-evolution">5.2.1 Evolution and Implementation of Optimization Algorithms - Ongoing Evolution</h3>
<p>Optimization algorithms have evolved over the past few decades, solving three core challenges, much like living organisms evolve.</p>
<ol type="1">
<li><strong>Computational Efficiency:</strong> Completing learning as quickly as possible with limited computing resources.</li>
<li><strong>Generalization Performance:</strong> Performing well not only on training data but also on new data.</li>
<li><strong>Scalability:</strong> Operating stably even when the model and data size increase.</li>
</ol>
<p>Each challenge has led to the birth of new algorithms, and the competition to find better algorithms continues to this day.</p>
<section id="history-of-optimization-algorithms" class="level5">
<h5 class="anchored" data-anchor-id="history-of-optimization-algorithms">History of Optimization Algorithms</h5>
<ul>
<li><strong>1847, Cauchy:</strong> Proposed Gradient Descent. This simple yet powerful idea of adjusting parameters slightly along the gradient of the loss function has become the foundation of modern deep learning optimization.</li>
<li><strong>1951, Robbins and Monro:</strong> Established the mathematical basis for Stochastic Gradient Descent (SGD). SGD significantly improved computational efficiency by using mini-batches instead of the entire dataset.</li>
<li><strong>1986, Rumelhart:</strong> Proposed the Momentum method along with the Backpropagation algorithm. Momentum added inertia to the optimization process, mitigating the oscillation problem in SGD and improving convergence speed.</li>
<li><strong>2011, Duchi:</strong> Published the AdaGrad (Adaptive Gradient) algorithm. AdaGrad was the beginning of adaptive learning rate methods, which adjust the learning rate differently for each parameter.</li>
<li><strong>2012, Hinton:</strong> Proposed RMSProp. (Introduced in lecture notes, paper publication is unknown) RMSProp improved upon AdaGrad’s learning rate reduction issue, enabling more stable learning.</li>
<li><strong>2014, Kingma and Ba:</strong> Published Adam (Adaptive Moment Estimation). Adam combined the advantages of Momentum and RMSProp, becoming one of the most widely used optimization algorithms today.</li>
</ul>
<p>Recent optimization algorithms have evolved in the following three main directions. 1. <strong>Memory Efficiency:</strong> Lion, AdaFactor, etc., focus on reducing the memory usage required for training large models (especially Transformer-based ones). 2. <strong>Distributed Learning Optimization:</strong> LAMB, LARS, etc., increase efficiency when training large models in parallel using multiple GPUs/TPUs. 3. <strong>Domain/Task-Specific Optimization:</strong> Sophia, AdaBelief, etc., provide optimized performance for specific problem domains (e.g., natural language processing, computer vision) or specific model structures.</p>
<p><em>In particular, with the emergence of large language models (LLMs) and multimodal models, it has become even more crucial to efficiently optimize tens of billions of parameters, train in limited memory environments, and converge stably in distributed environments. These challenges have led to the development of new technologies such as 8-bit optimization, ZeRO optimization, and gradient checkpointing.</em></p>
</section>
<section id="basic-optimization-algorithms" class="level5">
<h5 class="anchored" data-anchor-id="basic-optimization-algorithms">Basic Optimization Algorithms</h5>
<p>In deep learning, optimization algorithms play a core role in finding the minimum value of the loss function, i.e., finding the optimal parameters of the model. Each algorithm has its unique characteristics and pros and cons, and it is essential to choose the appropriate algorithm based on the characteristics of the problem and the structure of the model.</p>
<p><strong>SGD and Momentum</strong></p>
<p>Stochastic Gradient Descent (SGD) is the most basic and widely used optimization algorithm. At each step, it calculates the gradient of the loss function using mini-batch data and updates the parameters in the opposite direction.</p>
<ul>
<li><p><strong>Parameter Update Formula:</strong></p>
<p><span class="math display">\[w^{(t)} = w^{(t-1)} - \eta \cdot g^{(t)}\]</span></p>
<ul>
<li><span class="math inline">\(w^{(t)}\)</span>: parameter (weight) at step <span class="math inline">\(t\)</span></li>
<li><span class="math inline">\(\eta\)</span>: learning rate</li>
<li><span class="math inline">\(g^{(t)}\)</span>: gradient calculated at step <span class="math inline">\(t\)</span></li>
</ul></li>
</ul>
<p>Momentum is an improved method that introduces the concept of momentum from physics to SGD. By using the exponential moving average of past gradients, it gives inertia to the optimization path, mitigating the oscillation problem of SGD and increasing the convergence speed.</p>
<ul>
<li><p><strong>Momentum Update Formula:</strong></p>
<p><span class="math display">\[v^{(t)} = \mu \cdot v^{(t-1)} + g^{(t)}\]</span></p>
<p><span class="math display">\[w^{(t)} = w^{(t-1)} - \eta \cdot v^{(t)}\]</span></p>
<ul>
<li><span class="math inline">\(\mu\)</span>: momentum coefficient (usually 0.9 or 0.99)</li>
<li><span class="math inline">\(v^{(t)}\)</span>: velocity at step <span class="math inline">\(t\)</span></li>
</ul></li>
</ul>
<p><em>The implementation code for the main optimization algorithms used in learning is included in the <code>chapter_05/optimizer/</code> directory.</em> The following is an example implementation of the SGD (including momentum) algorithm for learning purposes. All optimization algorithm classes inherit from the <code>BaseOptimizer</code> class and are implemented simply for learning purposes. (In actual libraries like PyTorch, they are implemented more complexly for efficiency and generalization.)</p>
<div id="cell-11" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> typing <span class="im">import</span> Iterable, List, Optional</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_05.optimizers.basic <span class="im">import</span> BaseOptimizer</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SGD(BaseOptimizer):</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Implements SGD with momentum."""</span></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, params: Iterable[nn.Parameter], lr: <span class="bu">float</span>, </span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>                 maximize: <span class="bu">bool</span> <span class="op">=</span> <span class="va">False</span>, momentum: <span class="bu">float</span> <span class="op">=</span> <span class="fl">0.0</span>):</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(params, lr)</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.maximize <span class="op">=</span> maximize</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.momentum <span class="op">=</span> momentum</span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.momentum_buffer_list: List[Optional[torch.Tensor]] <span class="op">=</span> [<span class="va">None</span>] <span class="op">*</span> <span class="bu">len</span>(<span class="va">self</span>.params)</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>    <span class="at">@torch.no_grad</span>()</span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> step(<span class="va">self</span>) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i, p <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="va">self</span>.params):</span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a>            grad <span class="op">=</span> p.grad <span class="cf">if</span> <span class="kw">not</span> <span class="va">self</span>.maximize <span class="cf">else</span> <span class="op">-</span>p.grad</span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="va">self</span>.momentum <span class="op">!=</span> <span class="fl">0.0</span>:</span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a>                buf <span class="op">=</span> <span class="va">self</span>.momentum_buffer_list[i]</span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> buf <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a>                    buf <span class="op">=</span> torch.clone(grad).detach()</span>
<span id="cb12-22"><a href="#cb12-22" aria-hidden="true" tabindex="-1"></a>                <span class="cf">else</span>:</span>
<span id="cb12-23"><a href="#cb12-23" aria-hidden="true" tabindex="-1"></a>                    buf.mul_(<span class="va">self</span>.momentum).add_(grad, alpha<span class="op">=</span><span class="dv">1</span><span class="op">-</span><span class="va">self</span>.momentum)</span>
<span id="cb12-24"><a href="#cb12-24" aria-hidden="true" tabindex="-1"></a>                grad <span class="op">=</span> buf</span>
<span id="cb12-25"><a href="#cb12-25" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.momentum_buffer_list[i] <span class="op">=</span> buf</span>
<span id="cb12-26"><a href="#cb12-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-27"><a href="#cb12-27" aria-hidden="true" tabindex="-1"></a>            p.add_(grad, alpha<span class="op">=-</span><span class="va">self</span>.lr)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><strong>Adaptive Learning Rate Algorithms</strong></p>
<p><em>Deep learning model parameters are updated with different frequencies and importance. Adaptive learning rate algorithms are methods that adjust the learning rate individually according to these parameter-specific characteristics.</em></p>
<ul>
<li><p><strong>AdaGrad (Adaptive Gradient, 2011):</strong></p>
<ul>
<li><p><strong>Key Idea:</strong> Apply a small learning rate to frequently updated parameters and a large learning rate to infrequently updated parameters.</p></li>
<li><p><strong>Formula:</strong></p>
<p><span class="math inline">\(w^{(t)} = w^{(t-1)} - \frac{\eta}{\sqrt{G^{(t)} + \epsilon}} \cdot g^{(t)}\)</span></p>
<ul>
<li><span class="math inline">\(G^{(t)}\)</span>: Accumulated sum of past gradient squares</li>
<li><span class="math inline">\(\epsilon\)</span>: Small constant to prevent division by zero (e.g., <span class="math inline">\(10^{-8}\)</span>)</li>
</ul></li>
<li><p><strong>Advantage:</strong> Effective when dealing with sparse data.</p></li>
<li><p><strong>Disadvantage:</strong> The learning rate decreases monotonically as learning progresses, which can cause learning to stop prematurely.</p></li>
</ul></li>
<li><p><strong>RMSProp (Root Mean Square Propagation, 2012):</strong></p>
<ul>
<li><p><strong>Key Idea:</strong> To solve the learning rate decrease problem of AdaGrad, use an exponential moving average instead of the sum of past gradient squares.</p></li>
<li><p><strong>Formula:</strong></p>
<p><span class="math inline">\(v^{(t)} = \beta \cdot v^{(t-1)} + (1-\beta) \cdot (g^{(t)})^2\)</span></p>
<p><span class="math inline">\(w^{(t)} = w^{(t-1)} - \frac{\eta}{\sqrt{v^{(t)} + \epsilon}} \cdot g^{(t)}\)</span></p>
<ul>
<li><span class="math inline">\(\beta\)</span>: Decay rate that controls the influence of past gradient squares (usually 0.9)</li>
</ul></li>
<li><p><strong>Advantage:</strong> The learning rate decrease problem is alleviated compared to AdaGrad, allowing for effective learning over a longer period.</p></li>
</ul></li>
</ul>
<p><strong>Adam (Adaptive Moment Estimation, 2014):</strong></p>
<p><em>Adam is one of the most widely used optimization algorithms today, combining the ideas of Momentum and RMSProp.</em></p>
<ul>
<li><p><strong>Key Idea:</strong></p>
<ul>
<li>Momentum: Uses the exponential moving average of past gradients (first moment) to give an inertia effect.</li>
<li>RMSProp: Uses the exponential moving average of past gradient squares (second moment) to adjust the learning rate for each parameter.</li>
<li>Bias Correction: Corrects the bias where the first and second moments are zero in the initial stages.</li>
</ul></li>
<li><p><strong>Formula:</strong></p>
<p><span class="math inline">\(m^{(t)} = \beta_1 \cdot m^{(t-1)} + (1-\beta_1) \cdot g^{(t)}\)</span></p>
<p><span class="math inline">\(v^{(t)} = \beta_2 \cdot v^{(t-1)} + (1-\beta_2) \cdot (g^{(t)})^2\)</span></p>
<p><span class="math inline">\(\hat{m}^{(t)} = \frac{m^{(t)}}{1-\beta_1^t}\)</span></p>
<p><span class="math inline">\(\hat{v}^{(t)} = \frac{v^{(t)}}{1-\beta_2^t}\)</span></p>
<p><span class="math inline">\(w^{(t)} = w^{(t-1)} - \eta \cdot \frac{\hat{m}^{(t)}}{\sqrt{\hat{v}^{(t)}} + \epsilon}\)</span></p>
<ul>
<li><span class="math inline">\(\beta_1\)</span>: Decay rate for the first moment (momentum) (usually 0.9)</li>
<li><span class="math inline">\(\beta_2\)</span>: Decay rate for the second moment (RMSProp) (usually 0.999) The optimization algorithms presented above have their own strengths and weaknesses, and the appropriate algorithm should be chosen according to the characteristics of the problem, model structure, data, etc. Adam shows good performance in many cases, but sometimes SGD + Momentum combination shows better generalization performance, or other adaptive learning rate algorithms (e.g.&nbsp;RMSProp) may be more effective for specific problems. Therefore, it is important to find the optimal algorithm through experimentation.</li>
</ul></li>
</ul>
</section>
<section id="modern-optimization-algorithms-faster-more-efficient-and-for-larger-models" class="level5">
<h5 class="anchored">Modern Optimization Algorithms: Faster, More Efficient, and for Larger Models</h5>
<p>As the scale of deep learning models and datasets has exploded in recent years, there is a growing need for new optimization algorithms that support <em>memory efficiency, fast convergence rates, and large-scale distributed learning</em>. The following are some of the latest algorithms that have emerged to meet these demands.</p>
<ul>
<li><p><strong>Lion (Evolved Sign Momentum, 2023):</strong></p>
<ul>
<li><strong>Key Idea:</strong> An algorithm discovered by Google Research through program search, which uses momentum similar to Adam but updates using only the sign of the gradient, ignoring its magnitude.</li>
<li><strong>Advantages:</strong>
<ul>
<li>Uses less memory than Adam (no need to store second moments).</li>
<li>Performs updates of the same size for all parameters, making it effective for problems with sparse gradients (e.g., natural language processing).</li>
<li>Can use larger learning rates than Adam.</li>
<li>Empirically shows better performance than AdamW in many cases.</li>
</ul></li>
<li><strong>Disadvantages:</strong>
<ul>
<li>May converge slower or perform worse than Adam on certain problems because it ignores gradient magnitude information.</li>
<li>May be more sensitive to learning rate tuning.</li>
<li>For more details, refer to Deep Dive.</li>
</ul></li>
</ul></li>
<li><p><strong>Sophia (Second-order Clipped Stochastic Optimization, 2023):</strong></p>
<ul>
<li><strong>Key Idea:</strong> Utilizes second-order derivative information (Hessian matrix) but estimates only the diagonal elements of the Hessian to reduce computational cost and applies clipping to updates for increased stability.</li>
<li><strong>Advantages:</strong> Faster convergence and more stable training than Adam.</li>
<li><strong>Disadvantages:</strong> Requires tuning more hyperparameters (e.g., Hessian estimation frequency, clipping threshold) than Adam.</li>
<li>For more details, refer to Deep Dive.</li>
</ul></li>
<li><p><strong>AdaFactor (2018):</strong></p>
<ul>
<li><strong>Key Idea:</strong> Proposed to reduce memory usage for large models (especially Transformers), this algorithm approximates the second-moment matrix in Adam as a product of low-dimensional matrices.</li>
<li><strong>Advantages:</strong> Significantly less memory usage compared to Adam.</li>
<li><strong>Disadvantages:</strong> May perform worse than Adam on certain problems because it approximates second-moment information.</li>
<li>For more details, refer to Deep Dive.</li>
</ul></li>
</ul>
<p>Recent studies suggest that the algorithms introduced above (Lion, Sophia, AdaFactor) can exhibit superior performance to traditional Adam/AdamW under specific conditions.</p>
<ul>
<li><strong>Lion:</strong> Tends to be faster and use less memory than AdamW for large batch size training, with better generalization performance.</li>
<li><strong>Sophia:</strong> Can converge faster and achieve lower perplexity (or higher accuracy) than Adam in the pre-training stage of large language models.</li>
<li><strong>AdaFactor:</strong> Can be a good alternative to Adam for training large Transformer models in memory-constrained environments. However, there is no “universal” optimization algorithm that guarantees the best performance for all problems. Therefore, when applying it to actual problems, you should comprehensively consider the size of the model, the characteristics of the training data, available resources (memory, computing power), and whether distributed learning is used to select an appropriate algorithm, <em>and you must find the optimal hyperparameters through experiments and verification.</em></li>
</ul>
<p>Now, let’s try an experiment with 1 epoch to see if it works.</p>
<div id="cell-14" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_04.models.base <span class="im">import</span> SimpleNetwork</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_04.utils.data <span class="im">import</span> get_data_loaders, get_device</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_05.optimizers.basic <span class="im">import</span> Adam, SGD</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_05.optimizers.advanced <span class="im">import</span> Lion, Sophia</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_04.experiments.model_training <span class="im">import</span> train_model  <span class="co"># Corrected import</span></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> get_device()</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> SimpleNetwork(act_func<span class="op">=</span>nn.ReLU(), hidden_shape<span class="op">=</span>[<span class="dv">512</span>, <span class="dv">64</span>]).to(device)</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize SGD optimizer</span></span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> SGD(params<span class="op">=</span>model.parameters(), lr<span class="op">=</span><span class="fl">1e-3</span>, momentum<span class="op">=</span><span class="fl">0.9</span>)</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a><span class="co"># # Initialize Adam optimizer</span></span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a><span class="co"># optimizer = Adam(params=model.parameters(), lr=1e-3, beta1=0.9, beta2=0.999, eps=1e-8)</span></span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a><span class="co"># # Initialize AdaGrad optimizer</span></span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a><span class="co"># optimizer = AdaGrad(params=model.parameters(), lr=1e-2, eps=1e-10)</span></span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a><span class="co"># # Initialize Lion optimizer</span></span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a><span class="co"># optimizer = Lion(params=model.parameters(), lr=1e-4,  betas=(0.9, 0.99), weight_decay=0.0)</span></span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize Sophia optimizer</span></span>
<span id="cb13-25"><a href="#cb13-25" aria-hidden="true" tabindex="-1"></a><span class="co"># optimizer = Sophia(params=model.parameters(), lr=1e-3, betas=(0.965, 0.99), rho=0.04, weight_decay=0.0, k=10)</span></span>
<span id="cb13-26"><a href="#cb13-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-27"><a href="#cb13-27" aria-hidden="true" tabindex="-1"></a>train_dataloader, test_dataloader <span class="op">=</span> get_data_loaders()</span>
<span id="cb13-28"><a href="#cb13-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-29"><a href="#cb13-29" aria-hidden="true" tabindex="-1"></a>train_model(model, train_dataloader, test_dataloader, device, optimizer<span class="op">=</span>optimizer, epochs<span class="op">=</span><span class="dv">1</span>, batch_size<span class="op">=</span><span class="dv">256</span>, save_dir<span class="op">=</span><span class="st">"./tmp/opts/ReLU"</span>, retrain<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
Starting training for SimpleNetwork-ReLU.</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"c406d6c56f5d409e87cd8759b19fd284","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Execution completed for SimpleNetwork-ReLU, Execution time = 7.4 secs</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="5">
<pre><code>{'epochs': [1],
 'train_losses': [2.2232478597005207],
 'train_accuracies': [0.20635],
 'test_losses': [2.128580910873413],
 'test_accuracies': [0.3466]}</code></pre>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled" title="Click to view content (Deep Dive: In-Depth Analysis of Modern Optimization Algorithms)">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Click to view content (Deep Dive: In-Depth Analysis of Modern Optimization Algorithms)
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<section id="in-depth-analysis-of-modern-optimization-algorithms" class="level3 callout-body-container callout-body">
<h3 class="anchored" data-anchor-id="in-depth-analysis-of-modern-optimization-algorithms">In-Depth Analysis of Modern Optimization Algorithms</h3>
<section id="lion-evolved-sign-momentum" class="level4">
<h4 class="anchored" data-anchor-id="lion-evolved-sign-momentum">Lion (EvoLved Sign Momentum)</h4>
<p>Lion is an optimization algorithm discovered by Google Research using AutoML techniques. Similar to Adam, it uses momentum, but its key feature is that it only utilizes the sign of the gradient, discarding the size information.</p>
<p><strong>Key Ideas:</strong></p>
<ul>
<li><strong>Sign Descent:</strong> Determines the update direction using only the sign of the gradient. This forces all parameters to be updated with the same magnitude, making it effective for problems with sparse gradients (e.g., natural language processing).</li>
<li><strong>Momentum:</strong> Considers previous update directions to improve learning stability and speed.</li>
</ul>
<p><strong>Mathematical Principles:</strong></p>
<ol type="1">
<li><p><strong>Update Calculation:</strong></p>
<p><span class="math inline">\(c_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t\)</span></p>
<ul>
<li><span class="math inline">\(c_t\)</span>: The current update vector, which is a weighted average of the momentum (<span class="math inline">\(m_{t-1}\)</span>) and the current gradient (<span class="math inline">\(g_t\)</span>).</li>
<li><span class="math inline">\(\beta_1\)</span>: The exponential decay rate for momentum (typically 0.9 or 0.99).</li>
</ul></li>
<li><p><strong>Weight Update:</strong></p>
<p><span class="math inline">\(w_{t+1} = w_t - \eta \cdot \text{sign}(c_t)\)</span></p>
<ul>
<li><span class="math inline">\(\eta\)</span>: The learning rate.</li>
<li><span class="math inline">\(\text{sign}(c_t)\)</span>: The sign of each element in <span class="math inline">\(c_t\)</span> (+1 or -1), with 0 if the element is 0.</li>
</ul></li>
<li><p><strong>Momentum Update:</strong></p>
<p><span class="math inline">\(m_t = c_t\)</span></p>
<ul>
<li>The value used in the update calculation is directly used as the momentum for the next step.</li>
</ul></li>
</ol>
<p><strong>Advantages:</strong></p>
<ul>
<li><strong>Memory Efficiency:</strong> Unlike Adam, it does not need to store the second moment (variance), reducing memory usage.</li>
<li><strong>Computational Efficiency:</strong> Sign operations are cheaper than multiplications.</li>
<li><strong>Robustness to Sparsity:</strong> Updating all parameters with the same magnitude makes it effective for problems with sparse gradients.</li>
</ul>
<p><strong>Disadvantages:</strong></p>
<ul>
<li>Ignoring the size information of the gradient can lead to slower convergence or lower performance compared to Adam in certain problems.</li>
<li>It may be more sensitive to learning rate tuning.</li>
</ul>
<p><strong>References:</strong></p>
<ul>
<li>There is an analysis suggesting that Lion has effects similar to L1 regularization. (Further research is needed for details)</li>
<li>The paper by <a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://arxiv.org/abs/2302.06675">Chen et al., 2023</a> reported that when training the BERT-Large model, Lion achieved up to twice the convergence speed of AdamW and reduced memory usage. However, this is a specific experimental result and may not generalize to all cases.</li>
</ul>
</section>
<section id="sophia-second-order-clipped-stochastic-optimization" class="level4">
<h4 class="anchored" data-anchor-id="sophia-second-order-clipped-stochastic-optimization">Sophia (Second-order Clipped Stochastic Optimization)</h4>
<p>Sophia is an optimization algorithm that utilizes second derivative information (Hessian matrix) to improve learning speed and stability. Since directly computing the Hessian matrix is computationally expensive, Sophia estimates only the diagonal elements of the Hessian using an improved version of Hutchinson’s method.</p>
<p><strong>Key Ideas:</strong></p>
<ul>
<li><strong>Lightweight Hessian Estimation:</strong> Improves Hutchinson’s method to efficiently estimate only the diagonal elements of the Hessian matrix.
<ul>
<li>Traditional Hutchinson’s method uses <span class="math inline">\(h_t = \mathbb{E}[z_t z_t^T H_t] = diag(H_t)\)</span>, where <span class="math inline">\(z\)</span> is a random vector.</li>
<li>Improvement: Uses covariance to reduce variance.</li>
</ul></li>
<li><strong>Clipping:</strong> Before updating the gradient using the estimated Hessian, clips the update size to enhance learning stability.</li>
</ul>
<p><strong>Mathematical Principles:</strong></p>
<table class="caption-top table">
<colgroup>
<col style="width: 50%">
<col style="width: 50%">
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(h_t\)</span></td>
<td>Estimate of the diagonal of the Hessian matrix at step <span class="math inline">\(t\)</span>.</td>
</tr>
<tr class="even">
<td><span class="math inline">\(z_t\)</span></td>
<td>Random vector used in Hutchinson’s method.</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(H_t\)</span></td>
<td>Hessian matrix at step <span class="math inline">\(t\)</span>.</td>
</tr>
<tr class="even">
<td><span class="math inline">\(\mathbb{E}[z_t z_t^T H_t]\)</span></td>
<td>Expected value of the product of <span class="math inline">\(z_t\)</span>, its transpose, and the Hessian <span class="math inline">\(H_t\)</span>.</td>
</tr>
</tbody>
</table>
<ol type="1">
<li><p><strong>Hessian Diagonal Estimation:</strong></p>
<ul>
<li><p>At each step, a random vector <span class="math inline">\(z_t\)</span> is sampled (<span class="math inline">\(z_t\)</span>’s elements are chosen uniformly from {-1, +1}).</p></li>
<li><p>The estimate of the Hessian diagonal <span class="math inline">\(h_t\)</span> is calculated as follows.</p>
<p><span class="math inline">\(h_t = \beta_2 h_{t-1} + (1 - \beta_2) \text{diag}(H_t z_t) z_t^T\)</span></p>
<p>(where <span class="math inline">\(H_t\)</span> is the Hessian at step t)</p></li>
<li><p>Sophia uses an exponential moving average (EMA) that leverages past estimates (<span class="math inline">\(h_{t-1}\)</span>) to reduce the variance of Hutchinson’s estimator.</p></li>
</ul></li>
<li><p><strong>Update Calculation:</strong></p>
<ul>
<li><span class="math inline">\(m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t\)</span> (momentum)</li>
<li><span class="math inline">\(u_t = \text{clip}(m_t / (h_t + \epsilon), \rho)\)</span>
<ul>
<li><span class="math inline">\(u_t\)</span>: the update clipped after dividing by the Hessian.</li>
<li><span class="math inline">\(\text{clip}(x, \rho) = \text{sign}(x) \cdot \min(|x|, \rho)\)</span>.</li>
<li><span class="math inline">\(\rho\)</span>: clipping threshold (hyperparameter)</li>
<li><span class="math inline">\(h_t + \epsilon\)</span> is an operation that adds <span class="math inline">\(\epsilon\)</span> to each element of <span class="math inline">\(h_t\)</span></li>
</ul></li>
</ul></li>
<li><p><strong>Weight Update:</strong></p>
<p><span class="math inline">\(w_{t+1} = w_t - \eta \cdot u_t\)</span></p>
<ul>
<li><span class="math inline">\(\eta\)</span>: learning rate</li>
</ul></li>
</ol>
<p><strong>Advantages:</strong></p>
<ul>
<li><strong>Fast Convergence:</strong> Utilizes second-order derivative information to converge faster than Adam.</li>
<li><strong>Stability:</strong> Enhances stability through clipping.</li>
</ul>
<p><strong>Disadvantages:</strong></p>
<ul>
<li>Requires tuning more hyperparameters (<span class="math inline">\(\beta_1\)</span>, <span class="math inline">\(\beta_2\)</span>, <span class="math inline">\(\rho\)</span>) compared to Adam.</li>
<li>Performance may vary depending on the accuracy of Hessian estimation.</li>
</ul>
<p><strong>References:</strong></p>
<ul>
<li><a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://arxiv.org/abs/2305.14342">Li et al., 2023</a> reported that Sophia achieved lower loss with fewer steps than Adam in language model pre-training (using metrics such as accuracy/perplexity).</li>
</ul>
</section>
<section id="adafactor" class="level4">
<h4 class="anchored" data-anchor-id="adafactor">AdaFactor</h4>
<p>AdaFactor is a memory-efficient optimization algorithm used for training large models, especially transformer models. It uses adaptive learning rates similar to Adam but improves the storage of second moments (variances) to significantly reduce memory usage.</p>
<p><strong>Key Idea:</strong></p>
<ul>
<li><strong>Matrix Factorization:</strong> Approximates the second-moment matrix as the product of two low-rank matrices to reduce memory usage.</li>
</ul>
<p><strong>Mathematical Principle:</strong></p>
<p>In Adam, the second-moment matrix <span class="math inline">\(v_t\)</span> for an <span class="math inline">\(n \times m\)</span> weight matrix requires <span class="math inline">\(O(nm)\)</span> memory. AdaFactor approximates this matrix as follows.</p>
<ol type="1">
<li><p><strong>Second-Moment Estimation:</strong></p>
<pre><code>$v_t = u_t v_{t-1} + (1 - u_t) g_t g_t^T$

where $u_t$ is the decay rate at step t</code></pre>
<ul>
<li>This approximation reduces memory usage by storing only two vectors (<span class="math inline">\(g_t\)</span> and <span class="math inline">\(v_{t-1}\)</span>) instead of the full matrix.</li>
</ul></li>
<li><p><strong>Update Calculation:</strong></p>
<ul>
<li><p><span class="math inline">\(m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t\)</span></p></li>
<li><p><span class="math inline">\(v_t = u_t v_{t-1} + (1 - u_t) g_t g_t^T\)</span></p>
<p>where <span class="math inline">\(\beta_1\)</span> is the momentum decay rate and <span class="math inline">\(u_t\)</span> is the second-moment decay rate</p></li>
<li><p>The update is calculated as <span class="math inline">\(u_t = m_t / \sqrt{v_t + \epsilon}\)</span>, where <span class="math inline">\(\epsilon\)</span> is a small value for numerical stability.</p></li>
</ul></li>
<li><p><strong>Weight Update:</strong></p>
<p><span class="math inline">\(w_{t+1} = w_t - \eta \cdot u_t\)</span></p>
<ul>
<li><span class="math inline">\(\eta\)</span>: learning rate</li>
</ul></li>
</ol>
<p><strong>Advantages:</strong></p>
<ul>
<li><strong>Memory Efficiency:</strong> Reduces memory usage significantly compared to Adam, making it suitable for large models.</li>
<li><strong>Fast Convergence:</strong> Adaptive learning rates help in faster convergence.</li>
</ul>
<p><strong>Disadvantages:</strong></p>
<ul>
<li><strong>Complexity:</strong> The algorithm involves more complex calculations due to matrix factorization.</li>
<li><strong>Hyperparameter Tuning:</strong> Requires tuning of additional hyperparameters (<span class="math inline">\(u_t\)</span>) compared to Adam.</li>
<li>Instead of <span class="math inline">\(v\_t\)</span>, maintain two vectors <span class="math inline">\(R\_t\)</span> (<span class="math inline">\(n \times 1\)</span>) and <span class="math inline">\(C\_t\)</span> (<span class="math inline">\(m \times 1\)</span>), representing the sum of each row and column of <span class="math inline">\(v\_t\)</span>. * <span class="math inline">\(R\_t = \beta\_{2t} R\_{t-1} + (1 - \beta\_{2t}) (\text{row\_sum}(g\_t^2)/m)\)</span> * <span class="math inline">\(C\_t = \beta\_{2t} C\_{t-1} + (1 - \beta\_{2t}) (\text{col\_sum}(g\_t^2)/n)\)</span> * <span class="math inline">\(R\_t\)</span> and <span class="math inline">\(C\_t\)</span> are the exponential moving averages of the row and column sums of <span class="math inline">\(g\_t^2\)</span>, respectively. (<span class="math inline">\(\beta\_{2t}\)</span> is the schedule) * Approximate via <span class="math inline">\(\hat{v\_t} = R\_t C\_t^T / (\text{sum}(R\_t) \cdot \text{sum}(C\_t))\)</span></li>
</ul>
<ol start="2" type="1">
<li><p><strong>Update Calculation:</strong></p>
<p><span class="math inline">\(u\_t =  g\_t / \sqrt{\hat{v\_t}}\)</span></p></li>
<li><p><strong>Weight Update</strong> <span class="math inline">\(w\_{t+1} = w\_t - \eta \cdot u\_t\)</span></p></li>
</ol>
<p><strong>Advantages:</strong></p>
<ul>
<li><strong>Memory Efficiency:</strong> Instead of storing a 2nd moment matrix of size <span class="math inline">\(O(nm)\)</span>, only vectors of size <span class="math inline">\(O(n+m)\)</span> are stored, greatly reducing memory usage.</li>
<li><strong>Large Model Training:</strong> Suitable for large model training due to its memory efficiency.</li>
</ul>
<p><strong>Disadvantages:</strong></p>
<ul>
<li>Since it approximates 2nd moment information, it may perform worse than Adam on certain problems.</li>
<li>Additional computational costs due to matrix decomposition may occur.</li>
</ul>
<p><strong>References:</strong></p>
<ul>
<li>The paper by <a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://arxiv.org/abs/1804.04235">Shazeer &amp; Stern, 2018</a> reported that AdaFactor achieved similar performance to Adam while reducing memory usage when training transformer models.</li>
</ul>
</section>
<section id="other-notable-recent-optimization-algorithms" class="level4">
<h4 class="anchored" data-anchor-id="other-notable-recent-optimization-algorithms">Other Notable Recent Optimization Algorithms</h4>
<ul>
<li><strong>LAMB (Layer-wise Adaptive Moments optimizer for Batch training):</strong> An algorithm specialized for large batch training, which adjusts the learning rate for each layer to enable stable training even with large batch sizes. (Reference: <a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://arxiv.org/abs/1904.00962">You et al., 2019</a>)</li>
<li><strong>LARS (Layer-wise Adaptive Rate Scaling):</strong> Similar to LAMB, it uses layer-wise learning rate adjustment and is effective for large batch training, commonly used in image classification models like ResNet. (Reference: <a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://arxiv.org/abs/1708.03888">You et al., 2017</a>)</li>
</ul>
</section>
</section>
</div>
</div>
</section>
</section>
<section id="optimization-training-comparison" class="level3">
<h3 class="anchored" data-anchor-id="optimization-training-comparison">5.2.2 Optimization Training Comparison</h3>
<p>The performance of optimization algorithms varies greatly depending on the task and model structure. Let’s analyze these characteristics through experiments.</p>
<section id="basic-task-analysis" class="level5">
<h5 class="anchored" data-anchor-id="basic-task-analysis">Basic Task Analysis</h5>
<p>We compare basic performance using the FashionMNIST dataset. This dataset is a simplified version of actual clothing image classification problems, making it suitable for analyzing the basic characteristics of deep learning algorithms.</p>
<div id="cell-17" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_05.experiments.basic <span class="im">import</span> run_basic_experiment</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_05.visualization.optimization <span class="im">import</span> plot_training_results</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_04.utils.data <span class="im">import</span> get_data_loaders</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_05.optimizers.basic <span class="im">import</span> SGD, Adam</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_05.optimizers.advanced <span class="im">import</span> Lion</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Device configuration</span></span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> torch.device(<span class="st">"cuda:0"</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">"cpu"</span>)</span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Data loaders</span></span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a>train_loader, test_loader <span class="op">=</span> get_data_loaders()</span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Optimizer dictionary</span></span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a>optimizers <span class="op">=</span> {</span>
<span id="cb18-17"><a href="#cb18-17" aria-hidden="true" tabindex="-1"></a>    <span class="st">'SGD'</span>: SGD,</span>
<span id="cb18-18"><a href="#cb18-18" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Adam'</span>: Adam,</span>
<span id="cb18-19"><a href="#cb18-19" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Lion'</span>: Lion</span>
<span id="cb18-20"><a href="#cb18-20" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb18-21"><a href="#cb18-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-22"><a href="#cb18-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Optimizer configurations</span></span>
<span id="cb18-23"><a href="#cb18-23" aria-hidden="true" tabindex="-1"></a>optimizer_configs <span class="op">=</span> {</span>
<span id="cb18-24"><a href="#cb18-24" aria-hidden="true" tabindex="-1"></a>    <span class="st">'SGD'</span>: {<span class="st">'lr'</span>: <span class="fl">0.01</span>, <span class="st">'momentum'</span>: <span class="fl">0.9</span>},</span>
<span id="cb18-25"><a href="#cb18-25" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Adam'</span>: {<span class="st">'lr'</span>: <span class="fl">0.001</span>},</span>
<span id="cb18-26"><a href="#cb18-26" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Lion'</span>: {<span class="st">'lr'</span>: <span class="fl">1e-4</span>}</span>
<span id="cb18-27"><a href="#cb18-27" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb18-28"><a href="#cb18-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-29"><a href="#cb18-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Run experiments</span></span>
<span id="cb18-30"><a href="#cb18-30" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> {}</span>
<span id="cb18-31"><a href="#cb18-31" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> name, config <span class="kw">in</span> optimizer_configs.items():</span>
<span id="cb18-32"><a href="#cb18-32" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Starting experiment with </span><span class="sc">{</span>name<span class="sc">}</span><span class="ss"> optimizer..."</span>)</span>
<span id="cb18-33"><a href="#cb18-33" aria-hidden="true" tabindex="-1"></a>    results[name] <span class="op">=</span> run_basic_experiment(</span>
<span id="cb18-34"><a href="#cb18-34" aria-hidden="true" tabindex="-1"></a>        optimizer_class<span class="op">=</span>optimizers[name],</span>
<span id="cb18-35"><a href="#cb18-35" aria-hidden="true" tabindex="-1"></a>        train_loader<span class="op">=</span>train_loader,</span>
<span id="cb18-36"><a href="#cb18-36" aria-hidden="true" tabindex="-1"></a>        test_loader<span class="op">=</span>test_loader,</span>
<span id="cb18-37"><a href="#cb18-37" aria-hidden="true" tabindex="-1"></a>        config<span class="op">=</span>config,</span>
<span id="cb18-38"><a href="#cb18-38" aria-hidden="true" tabindex="-1"></a>        device<span class="op">=</span>device,</span>
<span id="cb18-39"><a href="#cb18-39" aria-hidden="true" tabindex="-1"></a>        epochs<span class="op">=</span><span class="dv">20</span></span>
<span id="cb18-40"><a href="#cb18-40" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb18-41"><a href="#cb18-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-42"><a href="#cb18-42" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize training curves</span></span>
<span id="cb18-43"><a href="#cb18-43" aria-hidden="true" tabindex="-1"></a>plot_training_results(</span>
<span id="cb18-44"><a href="#cb18-44" aria-hidden="true" tabindex="-1"></a>    results,</span>
<span id="cb18-45"><a href="#cb18-45" aria-hidden="true" tabindex="-1"></a>    metrics<span class="op">=</span>[<span class="st">'loss'</span>, <span class="st">'accuracy'</span>, <span class="st">'gradient_norm'</span>, <span class="st">'memory'</span>],</span>
<span id="cb18-46"><a href="#cb18-46" aria-hidden="true" tabindex="-1"></a>    mode<span class="op">=</span><span class="st">"train"</span>,  <span class="co"># Changed mode to "train"</span></span>
<span id="cb18-47"><a href="#cb18-47" aria-hidden="true" tabindex="-1"></a>    title<span class="op">=</span><span class="st">'Optimizer Comparison on FashionMNIST'</span></span>
<span id="cb18-48"><a href="#cb18-48" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
Starting experiment with SGD optimizer...

==================================================
Optimizer: SGD
Initial CUDA Memory Status (GPU 0):
Allocated: 23.0MB
Reserved: 48.0MB
Model Size: 283.9K parameters
==================================================
</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"ada5e53909f54c17bd9780217a7549fc","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
==================================================
Final CUDA Memory Status (GPU 0):
Peak Allocated: 27.2MB
Peak Reserved: 48.0MB
Current Allocated: 25.2MB
Current Reserved: 48.0MB
==================================================


Starting experiment with Adam optimizer...

==================================================
Optimizer: Adam
Initial CUDA Memory Status (GPU 0):
Allocated: 25.2MB
Reserved: 48.0MB
Model Size: 283.9K parameters
==================================================
</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"672f6e9f6a2c4916aa04a216cf6fa722","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
==================================================
Final CUDA Memory Status (GPU 0):
Peak Allocated: 28.9MB
Peak Reserved: 50.0MB
Current Allocated: 26.3MB
Current Reserved: 50.0MB
==================================================


Starting experiment with Lion optimizer...

==================================================
Optimizer: Lion
Initial CUDA Memory Status (GPU 0):
Allocated: 24.1MB
Reserved: 50.0MB
Model Size: 283.9K parameters
==================================================
</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"df577f7a7c8042899f6580786f12b819","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
==================================================
Final CUDA Memory Status (GPU 0):
Peak Allocated: 27.2MB
Peak Reserved: 50.0MB
Current Allocated: 25.2MB
Current Reserved: 50.0MB
==================================================
</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="05_Optimization and Visualization_files/figure-html/cell-7-output-8.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The experiment results show the characteristics of each algorithm. The main observations from the experiments using the FashionMNIST dataset and the MLP model are as follows:</p>
<ol type="1">
<li><strong>Convergence Speed:</strong>
<ul>
<li>Adam and Lion converge very quickly in the early stages of training. (Rapid loss decrease, rapid accuracy increase within the first few epochs)</li>
<li>SGD shows a relatively slow and steady convergence pattern.</li>
</ul></li>
<li><strong>Learning Curve Stability:</strong>
<ul>
<li>Adam shows a very smooth and stable learning curve.</li>
<li>Lion is similar to Adam in stability but has some fluctuations in the accuracy curve.</li>
<li>SGD has large fluctuations in both loss and accuracy curves.</li>
</ul></li>
<li><strong>Memory Usage:</strong>
<ul>
<li>Lion uses slightly less memory than Adam, but the difference is not significant (Adam: approximately 26.2MB, Lion: approximately 25.2MB).</li>
<li>SGD uses the least amount of memory among the three.</li>
</ul></li>
<li><strong>Gradient Norm:</strong>
<ul>
<li>Lion: The initial gradient norm is very large (approximately 4.0) and decreases rapidly, stabilizing at a low value (approximately 1.5). (Initial large step exploration, rapid movement to the vicinity of the optimal point)</li>
<li>Adam: The initial gradient norm is smaller than Lion’s (approximately 2.0), decreases rapidly, and stabilizes at an even lower value (approximately 1.0). (Adaptive learning rate adjustment)</li>
<li>SGD: The initial gradient norm is the smallest (approximately 0.3) and shows large fluctuations, oscillating at a higher value (approximately 2.0-2.5) than the other algorithms. (Wide-area exploration, possibility of flat minima)</li>
</ul></li>
</ol>
<p>In the basic experiment, Adam and Lion showed rapid initial convergence speeds, Adam had the most stable learning, Lion used slightly less memory, and SGD tended to explore a wide range.</p>
</section>
<section id="advanced-task-evaluation" class="level5">
<h5 class="anchored" data-anchor-id="advanced-task-evaluation">Advanced Task Evaluation</h5>
<p>In CIFAR-100 and CNN/Transformer models, the differences between optimization algorithms become even more pronounced.</p>
<div id="cell-19" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_05.experiments.advanced <span class="im">import</span> run_advanced_experiment</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_05.visualization.optimization <span class="im">import</span> plot_training_results</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_04.utils.data <span class="im">import</span> get_data_loaders</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_05.optimizers.basic <span class="im">import</span> SGD, Adam</span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_05.optimizers.advanced <span class="im">import</span> Lion</span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Device configuration</span></span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> torch.device(<span class="st">"cuda:0"</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">"cpu"</span>)</span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Data loaders</span></span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a>train_loader, test_loader <span class="op">=</span> get_data_loaders(dataset<span class="op">=</span><span class="st">"CIFAR100"</span>)</span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Optimizer dictionary</span></span>
<span id="cb23-15"><a href="#cb23-15" aria-hidden="true" tabindex="-1"></a>optimizers <span class="op">=</span> {</span>
<span id="cb23-16"><a href="#cb23-16" aria-hidden="true" tabindex="-1"></a>    <span class="st">'SGD'</span>: SGD,</span>
<span id="cb23-17"><a href="#cb23-17" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Adam'</span>: Adam,</span>
<span id="cb23-18"><a href="#cb23-18" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Lion'</span>: Lion</span>
<span id="cb23-19"><a href="#cb23-19" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb23-20"><a href="#cb23-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-21"><a href="#cb23-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Optimizer configurations</span></span>
<span id="cb23-22"><a href="#cb23-22" aria-hidden="true" tabindex="-1"></a>optimizer_configs <span class="op">=</span> {</span>
<span id="cb23-23"><a href="#cb23-23" aria-hidden="true" tabindex="-1"></a>    <span class="st">'SGD'</span>: {<span class="st">'lr'</span>: <span class="fl">0.01</span>, <span class="st">'momentum'</span>: <span class="fl">0.9</span>},</span>
<span id="cb23-24"><a href="#cb23-24" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Adam'</span>: {<span class="st">'lr'</span>: <span class="fl">0.001</span>},</span>
<span id="cb23-25"><a href="#cb23-25" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Lion'</span>: {<span class="st">'lr'</span>: <span class="fl">1e-4</span>}</span>
<span id="cb23-26"><a href="#cb23-26" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb23-27"><a href="#cb23-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-28"><a href="#cb23-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Run experiments</span></span>
<span id="cb23-29"><a href="#cb23-29" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> {}</span>
<span id="cb23-30"><a href="#cb23-30" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> name, config <span class="kw">in</span> optimizer_configs.items():</span>
<span id="cb23-31"><a href="#cb23-31" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Starting experiment with </span><span class="sc">{</span>name<span class="sc">}</span><span class="ss"> optimizer..."</span>)</span>
<span id="cb23-32"><a href="#cb23-32" aria-hidden="true" tabindex="-1"></a>    results[name] <span class="op">=</span> run_advanced_experiment(</span>
<span id="cb23-33"><a href="#cb23-33" aria-hidden="true" tabindex="-1"></a>        optimizer_class<span class="op">=</span>optimizers[name],</span>
<span id="cb23-34"><a href="#cb23-34" aria-hidden="true" tabindex="-1"></a>        model_type<span class="op">=</span><span class="st">'cnn'</span>,</span>
<span id="cb23-35"><a href="#cb23-35" aria-hidden="true" tabindex="-1"></a>        train_loader<span class="op">=</span>train_loader,</span>
<span id="cb23-36"><a href="#cb23-36" aria-hidden="true" tabindex="-1"></a>        test_loader<span class="op">=</span>test_loader,</span>
<span id="cb23-37"><a href="#cb23-37" aria-hidden="true" tabindex="-1"></a>        config<span class="op">=</span>config,</span>
<span id="cb23-38"><a href="#cb23-38" aria-hidden="true" tabindex="-1"></a>        device<span class="op">=</span>device,</span>
<span id="cb23-39"><a href="#cb23-39" aria-hidden="true" tabindex="-1"></a>        epochs<span class="op">=</span><span class="dv">40</span></span>
<span id="cb23-40"><a href="#cb23-40" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb23-41"><a href="#cb23-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-42"><a href="#cb23-42" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize training curves</span></span>
<span id="cb23-43"><a href="#cb23-43" aria-hidden="true" tabindex="-1"></a>plot_training_results(</span>
<span id="cb23-44"><a href="#cb23-44" aria-hidden="true" tabindex="-1"></a>    results,</span>
<span id="cb23-45"><a href="#cb23-45" aria-hidden="true" tabindex="-1"></a>    metrics<span class="op">=</span>[<span class="st">'loss'</span>, <span class="st">'accuracy'</span>, <span class="st">'gradient_norm'</span>, <span class="st">'memory'</span>],</span>
<span id="cb23-46"><a href="#cb23-46" aria-hidden="true" tabindex="-1"></a>    mode<span class="op">=</span><span class="st">"train"</span>,</span>
<span id="cb23-47"><a href="#cb23-47" aria-hidden="true" tabindex="-1"></a>    title<span class="op">=</span><span class="st">'Optimizer Comparison on CIFAR100'</span></span>
<span id="cb23-48"><a href="#cb23-48" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Files already downloaded and verified
Files already downloaded and verified

Starting experiment with SGD optimizer...

==================================================
Optimizer: SGD
Initial CUDA Memory Status (GPU 0):
Allocated: 26.5MB
Reserved: 50.0MB
Model Size: 1194.1K parameters
==================================================
</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"f9006d867491454a9db530180a287b44","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
==================================================
Final CUDA Memory Status (GPU 0):
Peak Allocated: 120.4MB
Peak Reserved: 138.0MB
Current Allocated: 35.6MB
Current Reserved: 138.0MB
==================================================

Results saved to: SGD_cnn_20250225_161620.csv

Starting experiment with Adam optimizer...

==================================================
Optimizer: Adam
Initial CUDA Memory Status (GPU 0):
Allocated: 35.6MB
Reserved: 138.0MB
Model Size: 1194.1K parameters
==================================================
</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"dcd60bbfe3b64789828b28ff1a1c305e","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
==================================================
Final CUDA Memory Status (GPU 0):
Peak Allocated: 124.9MB
Peak Reserved: 158.0MB
Current Allocated: 40.2MB
Current Reserved: 158.0MB
==================================================

Results saved to: Adam_cnn_20250225_162443.csv

Starting experiment with Lion optimizer...

==================================================
Optimizer: Lion
Initial CUDA Memory Status (GPU 0):
Allocated: 31.0MB
Reserved: 158.0MB
Model Size: 1194.1K parameters
==================================================
</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"31f67588627840bd8e354958b5028454","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
==================================================
Final CUDA Memory Status (GPU 0):
Peak Allocated: 120.4MB
Peak Reserved: 158.0MB
Current Allocated: 35.6MB
Current Reserved: 158.0MB
==================================================

Results saved to: Lion_cnn_20250225_163259.csv</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="05_Optimization and Visualization_files/figure-html/cell-8-output-8.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The experimental results compare the SGD, Adam, and Lion optimization algorithms using the CIFAR-100 dataset and CNN model, showing the characteristics of each algorithm.</p>
<ol type="1">
<li><p><strong>Convergence Speed and Accuracy:</strong></p>
<ul>
<li>SGD shows low accuracy (less than about 50%) even after 40 epochs and converges slowly.</li>
<li>Adam reaches an accuracy of around 50% near 20 epochs and converges relatively quickly.</li>
<li>Lion converges faster than Adam and achieves the highest accuracy of around 55% at 40 epochs.</li>
</ul></li>
<li><p><strong>Learning Curve Stability:</strong></p>
<ul>
<li>Adam is stable in both Loss and Accuracy curves.</li>
<li>Lion is similarly stable to Adam, but with slight fluctuations in the Accuracy curve.</li>
<li>SGD has high variability in both Loss and Accuracy curves.</li>
</ul></li>
<li><p><strong>Memory Usage:</strong></p>
<ul>
<li>Lion (about 31MB) and SGD (about 31MB) use slightly less memory than Adam (about 34MB).</li>
</ul></li>
<li><p><strong>Gradient Norm:</strong></p>
<ul>
<li>Lion: The initial gradient norm is large (around 3.56), rapidly increases, then decreases and stabilizes around 10 (initial large step exploration).</li>
<li>Adam: The initial gradient norm is smaller than Lion’s (around 3.26) and increases gently before stabilizing (stable exploration).</li>
<li>SGD: The initial gradient norm is the smallest (around 3.13), has high variability, and is maintained at a higher value than the other algorithms.</li>
</ul></li>
</ol>
<p>Under the given experimental conditions, <strong>Lion</strong> showed the fastest convergence speed and highest accuracy. <strong>Adam</strong> demonstrated stable learning curves, while <strong>SGD</strong> was slow and had high variability. The memory usage was slightly lower for Lion and SGD compared to Adam.</p>
<div id="cell-21" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_05.experiments.advanced <span class="im">import</span> run_advanced_experiment</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_05.visualization.optimization <span class="im">import</span> plot_training_results</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_04.utils.data <span class="im">import</span> get_data_loaders</span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_05.optimizers.basic <span class="im">import</span> SGD, Adam</span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_05.optimizers.advanced <span class="im">import</span> Lion</span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Device configuration</span></span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> torch.device(<span class="st">"cuda:0"</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">"cpu"</span>)</span>
<span id="cb28-10"><a href="#cb28-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-11"><a href="#cb28-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Data loaders</span></span>
<span id="cb28-12"><a href="#cb28-12" aria-hidden="true" tabindex="-1"></a>train_loader, test_loader <span class="op">=</span> get_data_loaders(dataset<span class="op">=</span><span class="st">"CIFAR100"</span>)</span>
<span id="cb28-13"><a href="#cb28-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-14"><a href="#cb28-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Optimizer dictionary</span></span>
<span id="cb28-15"><a href="#cb28-15" aria-hidden="true" tabindex="-1"></a>optimizers <span class="op">=</span> {</span>
<span id="cb28-16"><a href="#cb28-16" aria-hidden="true" tabindex="-1"></a>    <span class="st">'SGD'</span>: SGD,</span>
<span id="cb28-17"><a href="#cb28-17" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Adam'</span>: Adam,</span>
<span id="cb28-18"><a href="#cb28-18" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Lion'</span>: Lion</span>
<span id="cb28-19"><a href="#cb28-19" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb28-20"><a href="#cb28-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-21"><a href="#cb28-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Optimizer configurations</span></span>
<span id="cb28-22"><a href="#cb28-22" aria-hidden="true" tabindex="-1"></a>optimizer_configs <span class="op">=</span> {</span>
<span id="cb28-23"><a href="#cb28-23" aria-hidden="true" tabindex="-1"></a>    <span class="st">'SGD'</span>: {<span class="st">'lr'</span>: <span class="fl">0.01</span>, <span class="st">'momentum'</span>: <span class="fl">0.9</span>},</span>
<span id="cb28-24"><a href="#cb28-24" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Adam'</span>: {<span class="st">'lr'</span>: <span class="fl">0.001</span>},</span>
<span id="cb28-25"><a href="#cb28-25" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Lion'</span>: {<span class="st">'lr'</span>: <span class="fl">1e-4</span>}</span>
<span id="cb28-26"><a href="#cb28-26" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb28-27"><a href="#cb28-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-28"><a href="#cb28-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Run experiments</span></span>
<span id="cb28-29"><a href="#cb28-29" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> {}</span>
<span id="cb28-30"><a href="#cb28-30" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> name, config <span class="kw">in</span> optimizer_configs.items():</span>
<span id="cb28-31"><a href="#cb28-31" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Starting experiment with </span><span class="sc">{</span>name<span class="sc">}</span><span class="ss"> optimizer..."</span>)</span>
<span id="cb28-32"><a href="#cb28-32" aria-hidden="true" tabindex="-1"></a>    results[name] <span class="op">=</span> run_advanced_experiment(</span>
<span id="cb28-33"><a href="#cb28-33" aria-hidden="true" tabindex="-1"></a>        optimizer_class<span class="op">=</span>optimizers[name],</span>
<span id="cb28-34"><a href="#cb28-34" aria-hidden="true" tabindex="-1"></a>        model_type<span class="op">=</span><span class="st">'transformer'</span>,</span>
<span id="cb28-35"><a href="#cb28-35" aria-hidden="true" tabindex="-1"></a>        train_loader<span class="op">=</span>train_loader,</span>
<span id="cb28-36"><a href="#cb28-36" aria-hidden="true" tabindex="-1"></a>        test_loader<span class="op">=</span>test_loader,</span>
<span id="cb28-37"><a href="#cb28-37" aria-hidden="true" tabindex="-1"></a>        config<span class="op">=</span>config,</span>
<span id="cb28-38"><a href="#cb28-38" aria-hidden="true" tabindex="-1"></a>        device<span class="op">=</span>device,</span>
<span id="cb28-39"><a href="#cb28-39" aria-hidden="true" tabindex="-1"></a>        epochs<span class="op">=</span><span class="dv">40</span></span>
<span id="cb28-40"><a href="#cb28-40" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb28-41"><a href="#cb28-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-42"><a href="#cb28-42" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize training curves</span></span>
<span id="cb28-43"><a href="#cb28-43" aria-hidden="true" tabindex="-1"></a>plot_training_results(</span>
<span id="cb28-44"><a href="#cb28-44" aria-hidden="true" tabindex="-1"></a>    results,</span>
<span id="cb28-45"><a href="#cb28-45" aria-hidden="true" tabindex="-1"></a>    metrics<span class="op">=</span>[<span class="st">'loss'</span>, <span class="st">'accuracy'</span>, <span class="st">'gradient_norm'</span>, <span class="st">'memory'</span>],</span>
<span id="cb28-46"><a href="#cb28-46" aria-hidden="true" tabindex="-1"></a>    mode<span class="op">=</span><span class="st">"train"</span>,</span>
<span id="cb28-47"><a href="#cb28-47" aria-hidden="true" tabindex="-1"></a>    title<span class="op">=</span><span class="st">'Optimizer Comparison on CIFAR100'</span></span>
<span id="cb28-48"><a href="#cb28-48" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Files already downloaded and verified
Files already downloaded and verified

Starting experiment with SGD optimizer...</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/home/sean/anaconda3/envs/DL/lib/python3.10/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True
  warnings.warn(</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
==================================================
Optimizer: SGD
Initial CUDA Memory Status (GPU 0):
Allocated: 274.5MB
Reserved: 318.0MB
Model Size: 62099.8K parameters
==================================================
</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"0194a84568dd47cc87bcbd8b8efcf1c7","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
==================================================
Final CUDA Memory Status (GPU 0):
Peak Allocated: 836.8MB
Peak Reserved: 906.0MB
Current Allocated: 749.5MB
Current Reserved: 906.0MB
==================================================

Results saved to: SGD_transformer_20250225_164652.csv

Starting experiment with Adam optimizer...

==================================================
Optimizer: Adam
Initial CUDA Memory Status (GPU 0):
Allocated: 748.2MB
Reserved: 906.0MB
Model Size: 62099.8K parameters
==================================================
</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"a67a0697ce6e40169da7fa60aa2dd45f","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
==================================================
Final CUDA Memory Status (GPU 0):
Peak Allocated: 1073.0MB
Peak Reserved: 1160.0MB
Current Allocated: 985.1MB
Current Reserved: 1160.0MB
==================================================

Results saved to: Adam_transformer_20250225_170159.csv

Starting experiment with Lion optimizer...

==================================================
Optimizer: Lion
Initial CUDA Memory Status (GPU 0):
Allocated: 511.4MB
Reserved: 1160.0MB
Model Size: 62099.8K parameters
==================================================
</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"7cdfacd5f0ef4c57b845cc96581dcd2e","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
==================================================
Final CUDA Memory Status (GPU 0):
Peak Allocated: 985.1MB
Peak Reserved: 1160.0MB
Current Allocated: 748.2MB
Current Reserved: 1160.0MB
==================================================

Results saved to: Lion_transformer_20250225_171625.csv</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="05_Optimization and Visualization_files/figure-html/cell-9-output-10.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Typically, transformers are not used directly for image classification tasks, but rather in a structure modified to suit image characteristics, such as Vision Transformer (ViT). This experiment is conducted as an example for comparing optimization algorithms. The results of the transformer model experiment are as follows:</p>
<ol type="1">
<li>Convergence performance: Adam showed the fastest initial convergence, followed by Lion and then SGD.</li>
<li>Stability and generalization: Adam achieved 30.5% and showed the most stable performance. Lion had a test accuracy of 28.88%, with some performance degradation in the later stages of training. SGD had an accuracy of 31.1%, showing the best generalization performance.</li>
<li>Memory usage: Lion and SGD used similar amounts of memory, while Adam used relatively more memory.</li>
<li>Gradient dynamics: The gradient norm of Adam gradually decreased from 1.98 to 0.92. Lion’s started at 2.81 and decreased to 1.21, and SGD’s started at 8.41 and decreased to 5.92, showing the largest change.</li>
</ol>
<p><strong>Conclusion</strong> In the experiment on the CIFAR-100 dataset, SGD showed the best generalization performance but had the slowest learning speed. Adam showed the fastest convergence and stable learning, but used a lot of memory, while Lion showed balanced performance in terms of memory efficiency and convergence speed.</p>
</section>
</section>
</section>
<section id="visualization-and-analysis-of-the-optimization-process-peeking-into-the-black-box-of-deep-learning" class="level2">
<h2 class="anchored" data-anchor-id="visualization-and-analysis-of-the-optimization-process-peeking-into-the-black-box-of-deep-learning">5.3 Visualization and Analysis of the Optimization Process: Peeking into the Black Box of Deep Learning</h2>
<blockquote class="blockquote">
<p><strong>Challenge:</strong> How can we effectively visualize and understand the deep learning optimization process that occurs in high-dimensional spaces with millions or tens of millions of dimensions?</p>
<p><strong>Researcher’s Concern:</strong> The parameter space of deep learning models is a high-dimensional space that is difficult for humans to intuitively imagine. Researchers have developed various dimension reduction techniques and visualization tools to open this “black box,” but many parts still remain veiled.</p>
</blockquote>
<p>Understanding the learning process of neural networks is essential for effective model design, optimization algorithm selection, and hyperparameter tuning. In particular, visualizing and analyzing the geometric properties of the loss function and the optimization path provide important insights into the dynamics and stability of the learning process. <em>In recent years, research on loss surface visualization has provided a key to unlocking the secrets of neural network learning, contributing to the development of more efficient and stable learning algorithms and model structures.</em></p>
<p>This section examines the basic concepts and latest techniques of loss surface visualization and analyzes various phenomena that occur during the deep learning learning process (e.g., local minima, saddle points, characteristics of optimization paths). In particular, we focus on the impact of model structure (e.g., residual connections) on the loss surface and the differences in optimization paths according to optimization algorithms.</p>
<section id="understanding-loss-landscape-a-topological-map-of-deep-learning-models" class="level3">
<h3 class="anchored" data-anchor-id="understanding-loss-landscape-a-topological-map-of-deep-learning-models">5.3.1 Understanding Loss Landscape: A Topological Map of Deep Learning Models</h3>
<p>Loss surface visualization is a key tool for understanding the learning process of deep learning models. <em>Just as a topographic map helps us understand the heights and valleys of mountains, loss surface visualization allows us to visually grasp the changes in the loss function in the parameter space.</em></p>
<p>In 2017, Goodfellow et al.’s study showed that the flatness of the loss surface is closely related to the model’s generalization performance. (Wide and flat minima tend to have better generalization performance than narrow and sharp minima.) In 2018, Li et al.&nbsp;used 3D visualization to show that residual connections make the loss surface flat, facilitating learning. These findings have become a core foundation for designing modern neural network architectures such as ResNet.</p>
<section id="basic-visualization-techniques" class="level5">
<h5 class="anchored" data-anchor-id="basic-visualization-techniques">Basic Visualization Techniques</h5>
<ol type="1">
<li><p><strong>Linear Interpolation:</strong></p>
<ul>
<li><p><em>Concept:</em> Linearly combine the weights of two different models (e.g., pre-/post-training models, models converged to different local minima) and calculate the loss function value between them.</p></li>
<li><p><em>Formula:</em></p>
<p><span class="math inline">\(w(\alpha) = (1-\alpha)w_1 + \alpha w_2\)</span></p>
<ul>
<li><span class="math inline">\(w_1\)</span>, <span class="math inline">\(w_2\)</span>: Weights of the two models</li>
<li><span class="math inline">\(\alpha \in [0,1]\)</span>: Interpolation coefficient (0 for <span class="math inline">\(w_1\)</span>, 1 for <span class="math inline">\(w_2\)</span>, and a linear combination of the two weights for values in between)</li>
<li><span class="math inline">\(L(w(\alpha))\)</span>: Loss value at the interpolated weight <span class="math inline">\(w(\alpha)\)</span></li>
</ul></li>
</ul></li>
</ol>
<div id="cell-24" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> DataLoader, Subset</span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_05.visualization.loss_surface <span class="im">import</span> linear_interpolation, visualize_linear_interpolation</span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_04.utils.data <span class="im">import</span> get_dataset</span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_04.utils.metrics <span class="im">import</span> load_model</span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Linear Interpolation</span></span>
<span id="cb35-9"><a href="#cb35-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-10"><a href="#cb35-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Device configuration</span></span>
<span id="cb35-11"><a href="#cb35-11" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> torch.device(<span class="st">"cuda"</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">"cpu"</span>)</span>
<span id="cb35-12"><a href="#cb35-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-13"><a href="#cb35-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the dataset</span></span>
<span id="cb35-14"><a href="#cb35-14" aria-hidden="true" tabindex="-1"></a>_, test_dataset <span class="op">=</span> get_dataset(dataset<span class="op">=</span><span class="st">"FashionMNIST"</span>)</span>
<span id="cb35-15"><a href="#cb35-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a small dataset</span></span>
<span id="cb35-16"><a href="#cb35-16" aria-hidden="true" tabindex="-1"></a>small_dataset <span class="op">=</span> Subset(test_dataset, torch.arange(<span class="dv">0</span>, <span class="dv">256</span>))</span>
<span id="cb35-17"><a href="#cb35-17" aria-hidden="true" tabindex="-1"></a>data_loader <span class="op">=</span> DataLoader(small_dataset, batch_size<span class="op">=</span><span class="dv">256</span>, shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb35-18"><a href="#cb35-18" aria-hidden="true" tabindex="-1"></a>loss_func <span class="op">=</span> nn.CrossEntropyLoss()</span>
<span id="cb35-19"><a href="#cb35-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-20"><a href="#cb35-20" aria-hidden="true" tabindex="-1"></a><span class="co"># model1, _ = load_model(model_file="SimpleNetwork-ReLU.pth", path="tmp/models/")</span></span>
<span id="cb35-21"><a href="#cb35-21" aria-hidden="true" tabindex="-1"></a><span class="co"># model2, _ = load_model(model_file="SimpleNetwork-Tanh.pth", path="tmp/models/")</span></span>
<span id="cb35-22"><a href="#cb35-22" aria-hidden="true" tabindex="-1"></a>model1, _ <span class="op">=</span> load_model(model_file<span class="op">=</span><span class="st">"SimpleNetwork-ReLU-epoch1.pth"</span>, path<span class="op">=</span><span class="st">"tmp/models/"</span>)</span>
<span id="cb35-23"><a href="#cb35-23" aria-hidden="true" tabindex="-1"></a>model2, _ <span class="op">=</span> load_model(model_file<span class="op">=</span><span class="st">"SimpleNetwork-ReLU-epoch15.pth"</span>, path<span class="op">=</span><span class="st">"tmp/models/"</span>)</span>
<span id="cb35-24"><a href="#cb35-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-25"><a href="#cb35-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-26"><a href="#cb35-26" aria-hidden="true" tabindex="-1"></a>model1 <span class="op">=</span> model1.to(device)</span>
<span id="cb35-27"><a href="#cb35-27" aria-hidden="true" tabindex="-1"></a>model2 <span class="op">=</span> model2.to(device)</span>
<span id="cb35-28"><a href="#cb35-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Linear interpolation</span></span>
<span id="cb35-29"><a href="#cb35-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-30"><a href="#cb35-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Test with a small dataset</span></span>
<span id="cb35-31"><a href="#cb35-31" aria-hidden="true" tabindex="-1"></a>_, test_dataset <span class="op">=</span> get_dataset(dataset<span class="op">=</span><span class="st">"FashionMNIST"</span>)</span>
<span id="cb35-32"><a href="#cb35-32" aria-hidden="true" tabindex="-1"></a>small_dataset <span class="op">=</span> Subset(test_dataset, torch.arange(<span class="dv">0</span>, <span class="dv">256</span>))</span>
<span id="cb35-33"><a href="#cb35-33" aria-hidden="true" tabindex="-1"></a>data_loader <span class="op">=</span> DataLoader(small_dataset, batch_size<span class="op">=</span><span class="dv">256</span>, shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb35-34"><a href="#cb35-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-35"><a href="#cb35-35" aria-hidden="true" tabindex="-1"></a>alphas, losses,  accuracies <span class="op">=</span> linear_interpolation(model1, model2, data_loader, loss_func, device)</span>
<span id="cb35-36"><a href="#cb35-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-37"><a href="#cb35-37" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> visualize_linear_interpolation(alphas, losses, accuracies,  <span class="st">"ReLU(1)-ReLU(15)"</span>,  size<span class="op">=</span>(<span class="dv">6</span>, <span class="dv">4</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="05_Optimization and Visualization_files/figure-html/cell-10-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>In linear interpolation, α=0 represents the weights of the first model (trained for 1 epoch), and α=1 represents the weights of the second model (trained for 15 epochs), while intermediate values represent a linear combination of the two models’ weights. The graph shows that as the value of α increases, the loss function value decreases, indicating that as training progresses, the model moves to a better optimum. However, linear interpolation has the limitation that it only shows a very limited cross-section of the high-dimensional weight space. The actual optimal path between the two models is likely to be nonlinear, and extending the range of α outside [0,1] makes interpretation difficult.</p>
<p>Using Bézier curves or splines for nonlinear path exploration, or PCA or t-SNE for visualizing high-dimensional structures, can provide more comprehensive information. In practice, it is recommended to use linear interpolation as an initial analysis tool and limit α to the range [0,1] or slight extrapolation. It should be analyzed comprehensively with other visualization techniques, and if there are large differences in model performance, further analysis is needed.</p>
<p>The following is the result of PCA and t-SNE analysis.</p>
<div id="cell-26" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_05.visualization.loss_surface <span class="im">import</span> analyze_weight_space, visualize_weight_space</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_04.utils.metrics <span class="im">import</span> load_model, load_models_by_pattern</span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a>models, labels <span class="op">=</span> load_models_by_pattern(</span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a>    activation_types<span class="op">=</span>[<span class="st">'ReLU'</span>],</span>
<span id="cb36-8"><a href="#cb36-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># activation_types=['Tanh'],</span></span>
<span id="cb36-9"><a href="#cb36-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># activation_types=['GELU'],</span></span>
<span id="cb36-10"><a href="#cb36-10" aria-hidden="true" tabindex="-1"></a>    epochs<span class="op">=</span>[<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>,<span class="dv">5</span>,<span class="dv">6</span>,<span class="dv">7</span>,<span class="dv">8</span>,<span class="dv">9</span>,<span class="dv">10</span>,<span class="dv">11</span>,<span class="dv">12</span>,<span class="dv">13</span>,<span class="dv">14</span>,<span class="dv">15</span>]</span>
<span id="cb36-11"><a href="#cb36-11" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb36-12"><a href="#cb36-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-13"><a href="#cb36-13" aria-hidden="true" tabindex="-1"></a><span class="co"># PCA analysis</span></span>
<span id="cb36-14"><a href="#cb36-14" aria-hidden="true" tabindex="-1"></a>embedded_pca <span class="op">=</span> analyze_weight_space(models, labels, method<span class="op">=</span><span class="st">'pca'</span>)</span>
<span id="cb36-15"><a href="#cb36-15" aria-hidden="true" tabindex="-1"></a>visualize_weight_space(embedded_pca, labels, method<span class="op">=</span><span class="st">'PCA'</span>)</span>
<span id="cb36-16"><a href="#cb36-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"embedded_pca = </span><span class="sc">{</span>embedded_pca<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb36-17"><a href="#cb36-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-18"><a href="#cb36-18" aria-hidden="true" tabindex="-1"></a><span class="co"># t-SNE analysis</span></span>
<span id="cb36-19"><a href="#cb36-19" aria-hidden="true" tabindex="-1"></a>embedded_tsne <span class="op">=</span> analyze_weight_space(models, labels, method<span class="op">=</span><span class="st">'tsne'</span>, perplexity<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb36-20"><a href="#cb36-20" aria-hidden="true" tabindex="-1"></a>visualize_weight_space(embedded_tsne, labels, method<span class="op">=</span><span class="st">'t-SNE'</span>)</span>
<span id="cb36-21"><a href="#cb36-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"embedded_tsne = </span><span class="sc">{</span>embedded_tsne<span class="sc">}</span><span class="ss">"</span>) <span class="co"># Corrected: Print embedded_tsne, not embedded_pca</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="05_Optimization and Visualization_files/figure-html/cell-11-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>embedded_pca = [[ 9.8299894e+00  2.1538167e+00]
 [-1.1609798e+01 -9.0169059e-03]
 [-1.1640446e+01 -1.2218434e-02]
 [-1.1667191e+01 -1.3469303e-02]
 [-1.1691980e+01 -1.5136327e-02]
 [-1.1714937e+01 -1.6765745e-02]
 [-1.1735878e+01 -1.8110925e-02]
 [ 9.9324265e+00  1.5862983e+00]
 [ 1.0126298e+01  4.7935897e-01]
 [ 1.0256655e+01 -2.8844318e-01]
 [ 1.0319887e+01 -6.6510278e-01]
 [ 1.0359785e+01 -8.9812231e-01]
 [ 1.0392080e+01 -1.0731999e+00]
 [ 1.0418671e+01 -1.2047548e+00]
 [-1.1575559e+01 -5.1336871e-03]]</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="05_Optimization and Visualization_files/figure-html/cell-11-output-3.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>embedded_tsne = [[ 119.4719    -99.78837 ]
 [ 100.26558    66.285835]
 [  94.79294    62.795162]
 [  89.221085   59.253677]
 [  83.667984   55.70297 ]
 [  77.897224   52.022995]
 [  74.5897     49.913578]
 [ 123.20351  -100.34615 ]
 [ -70.45423   -65.66194 ]
 [ -65.55417   -68.90429 ]
 [ -60.166885  -72.466805]
 [ -54.70004   -76.077   ]
 [ -49.00131   -79.833694]
 [ -45.727974  -81.99213 ]
 [ 105.22419    69.45333 ]]</code></pre>
</div>
</div>
<p>PCA and t-SNE visualizations project the change in the model weight space during training into a lower dimension (2D) to show it.</p>
<ul>
<li><strong>PCA Visualization:</strong>
<ul>
<li>The points represent the model weights of each epoch. (Purple (epoch 1) -&gt; Red (epoch 9) -&gt; Green series (after epoch 10))</li>
<li>The weights that were initially spread out gather in a specific area as training progresses.</li>
<li>In particular, a large change is observed when transitioning from epoch 9 to epoch 10.</li>
<li>PCA shows the direction of the largest change in the weight space (principal component).</li>
</ul></li>
<li><strong>t-SNE Visualization:</strong>
<ul>
<li>Similar to PCA, the color of the points changes with the epoch, showing the distribution change of weights during early/middle/late training.</li>
<li>t-SNE is a <em>non-linear</em> dimension reduction technique that focuses on preserving the <em>local neighborhood relationship</em> in high-dimensional space.</li>
<li>The epoch 1-9 group and the epoch 10-15 group are relatively clearly separated, supporting the PCA results.</li>
</ul></li>
</ul>
<p>Through these visualizations, we can gain an intuitive understanding of the change in model weights during training and the weight space exploration of the optimization algorithm. <em>In particular, using PCA and t-SNE together allows us to grasp both global changes (PCA) and local structures (t-SNE) simultaneously.</em></p>
<ol start="2" type="1">
<li><strong>Contour Plot</strong></li>
</ol>
<p>A contour plot is a method of visualizing the shape of the loss surface by drawing lines (contours) that connect points with the same loss function value on a 2D plane. <em>Like the contours on a topographic map, it represents the “height” of the loss function.</em></p>
<p><em>The general procedure is as follows.</em></p>
<ol type="1">
<li><p><strong>Setting the reference point:</strong> Select a reference model parameter (<span class="math inline">\(w_0\)</span>). (e.g., parameters of a trained model)</p></li>
<li><p><strong>Selecting direction vectors:</strong> Select two direction vectors (<span class="math inline">\(d_1\)</span>, <span class="math inline">\(d_2\)</span>). <em>These vectors form the basis of the 2D plane.</em></p>
<ul>
<li><em>Typical choices:</em> random directions, principal component directions obtained through PCA, or the top two eigenvectors corresponding to the largest eigenvalues of the Hessian matrix using libraries like PyHessian. <em>In the latter case, it represents the direction in which the loss function value changes most rapidly.</em></li>
</ul></li>
<li><p><strong>Perturbing parameters:</strong> Perturb the parameters around the reference point <span class="math inline">\(w_0\)</span> along the selected two direction vectors <span class="math inline">\(d_1\)</span>, <span class="math inline">\(d_2\)</span>.</p>
<p><span class="math inline">\(w(\lambda_1, \lambda_2) = w_0 + \lambda_1 d_1 + \lambda_2 d_2\)</span></p>
<ul>
<li><span class="math inline">\(\lambda_1\)</span>, <span class="math inline">\(\lambda_2\)</span>: scalar coefficients for each direction vector (e.g., select values at regular intervals in the range -0.2 to 0.2)</li>
</ul></li>
<li><p><strong>Calculating loss values:</strong> For each combination of <span class="math inline">\((\lambda_1, \lambda_2)\)</span>, apply the perturbed parameters <span class="math inline">\(w(\lambda_1, \lambda_2)\)</span> to the model and calculate the loss function value.</p></li>
<li><p><strong>Contour plot:</strong> Use the <span class="math inline">\((\lambda_1, \lambda_2, L(w(\lambda_1, \lambda_2)))\)</span> data to draw a 2D contour plot. (using functions like <code>contour</code> or <code>tricontourf</code> from matplotlib)</p></li>
</ol>
<p><em>The contour map visually shows the local geometry of the loss surface and can also be used to analyze the behavior of optimization algorithms by displaying their trajectories.</em></p>
<div id="cell-29" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> DataLoader, Subset</span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_05.visualization.loss_surface <span class="im">import</span> hessian_eigenvectors, xy_perturb_loss, visualize_loss_surface, linear_interpolation</span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_04.utils.data <span class="im">import</span> get_dataset</span>
<span id="cb39-7"><a href="#cb39-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_04.utils.metrics <span class="im">import</span> load_model</span>
<span id="cb39-8"><a href="#cb39-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_05.optimizers.basic <span class="im">import</span> SGD, Adam</span>
<span id="cb39-9"><a href="#cb39-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-10"><a href="#cb39-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Device configuration</span></span>
<span id="cb39-11"><a href="#cb39-11" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> torch.device(<span class="st">"cuda"</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">"cpu"</span>)</span>
<span id="cb39-12"><a href="#cb39-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-13"><a href="#cb39-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the dataset</span></span>
<span id="cb39-14"><a href="#cb39-14" aria-hidden="true" tabindex="-1"></a>_, test_dataset <span class="op">=</span> get_dataset(dataset<span class="op">=</span><span class="st">"FashionMNIST"</span>)</span>
<span id="cb39-15"><a href="#cb39-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a small dataset</span></span>
<span id="cb39-16"><a href="#cb39-16" aria-hidden="true" tabindex="-1"></a>small_dataset <span class="op">=</span> Subset(test_dataset, torch.arange(<span class="dv">0</span>, <span class="dv">256</span>))</span>
<span id="cb39-17"><a href="#cb39-17" aria-hidden="true" tabindex="-1"></a>data_loader <span class="op">=</span> DataLoader(small_dataset, batch_size<span class="op">=</span><span class="dv">256</span>, shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb39-18"><a href="#cb39-18" aria-hidden="true" tabindex="-1"></a>loss_func <span class="op">=</span> nn.CrossEntropyLoss()</span>
<span id="cb39-19"><a href="#cb39-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-20"><a href="#cb39-20" aria-hidden="true" tabindex="-1"></a>trained_model, _ <span class="op">=</span> load_model(model_file<span class="op">=</span><span class="st">"SimpleNetwork-ReLU.pth"</span>, path<span class="op">=</span><span class="st">"tmp/models/"</span>)</span>
<span id="cb39-21"><a href="#cb39-21" aria-hidden="true" tabindex="-1"></a><span class="co"># trained_model, _ = load_model(model_file="SimpleNetwork-Tanh.pth", path="tmp/models/")</span></span>
<span id="cb39-22"><a href="#cb39-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-23"><a href="#cb39-23" aria-hidden="true" tabindex="-1"></a>trained_model <span class="op">=</span> trained_model.to(device)</span>
<span id="cb39-24"><a href="#cb39-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-25"><a href="#cb39-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-26"><a href="#cb39-26" aria-hidden="true" tabindex="-1"></a><span class="co"># pyhessian</span></span>
<span id="cb39-27"><a href="#cb39-27" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> []  <span class="co"># List to store the calculated result sets</span></span>
<span id="cb39-28"><a href="#cb39-28" aria-hidden="true" tabindex="-1"></a>top_n <span class="op">=</span> <span class="dv">4</span>  <span class="co"># Must be an even number.  Each pair of eigenvectors is used.  2 is the minimum.  10 means 5 graphs.</span></span>
<span id="cb39-29"><a href="#cb39-29" aria-hidden="true" tabindex="-1"></a>top_eigenvalues, top_eignevectors <span class="op">=</span> hessian_eigenvectors(model<span class="op">=</span>trained_model, loss_func<span class="op">=</span>loss_func, data_loader<span class="op">=</span>data_loader, top_n<span class="op">=</span>top_n, is_cuda<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb39-30"><a href="#cb39-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-31"><a href="#cb39-31" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the scale with lambda.</span></span>
<span id="cb39-32"><a href="#cb39-32" aria-hidden="true" tabindex="-1"></a>lambda1, lambda2 <span class="op">=</span> np.linspace(<span class="op">-</span><span class="fl">0.2</span>, <span class="fl">0.2</span>, <span class="dv">40</span>).astype(np.float32), np.linspace(<span class="op">-</span><span class="fl">0.2</span>, <span class="fl">0.2</span>, <span class="dv">40</span>).astype(np.float32)</span>
<span id="cb39-33"><a href="#cb39-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-34"><a href="#cb39-34" aria-hidden="true" tabindex="-1"></a><span class="co"># If top_n=10, a total of 5 pairs of graphs can be drawn.</span></span>
<span id="cb39-35"><a href="#cb39-35" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(top_n <span class="op">//</span> <span class="dv">2</span>):</span>
<span id="cb39-36"><a href="#cb39-36" aria-hidden="true" tabindex="-1"></a>    x, y, z <span class="op">=</span> xy_perturb_loss(model<span class="op">=</span>trained_model, top_eigenvectors<span class="op">=</span>top_eignevectors[i<span class="op">*</span><span class="dv">2</span>:(i<span class="op">+</span><span class="dv">1</span>)<span class="op">*</span><span class="dv">2</span>], data_loader<span class="op">=</span>data_loader, loss_func<span class="op">=</span>loss_func, lambda1<span class="op">=</span>lambda1, lambda2<span class="op">=</span>lambda2, device<span class="op">=</span>device)</span>
<span id="cb39-37"><a href="#cb39-37" aria-hidden="true" tabindex="-1"></a>    data.append((x, y, z))</span>
<span id="cb39-38"><a href="#cb39-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-39"><a href="#cb39-39" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> visualize_loss_surface(data, <span class="st">"ReLU"</span>, color<span class="op">=</span><span class="st">"C0"</span>, alpha<span class="op">=</span><span class="fl">0.6</span>, plot_3d<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb39-40"><a href="#cb39-40" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> visualize_loss_surface(data, <span class="st">"ReLU"</span>, color<span class="op">=</span><span class="st">"C0"</span>, alpha<span class="op">=</span><span class="fl">0.6</span>, plot_3d<span class="op">=</span><span class="va">False</span>) <span class="co"># Changed "ReLu" to "ReLU" for consistency</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stderr">
<pre><code>/home/sean/anaconda3/envs/DL/lib/python3.10/site-packages/torch/autograd/graph.py:825: UserWarning: Using backward() with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak. (Triggered internally at ../torch/csrc/autograd/engine.cpp:1201.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="05_Optimization and Visualization_files/figure-html/cell-12-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="05_Optimization and Visualization_files/figure-html/cell-12-output-3.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Contour maps provide more detailed information about local areas than simple linear interpolation. While linear interpolation shows the change in loss function values along a one-dimensional path between two models, contour maps visualize the change in loss functions on a two-dimensional plane with the selected two directions (<span class="math inline">\(\lambda_1\)</span>, <span class="math inline">\(\lambda_2\)</span>) as axes. This allows us to identify subtle changes in the optimization path, local minima in surrounding areas that could not be identified by linear interpolation, the existence of saddle points, and barriers between them.</p>
</section>
</section>
<section id="in-depth-techniques-for-loss-surface-analysis" class="level3">
<h3 class="anchored" data-anchor-id="in-depth-techniques-for-loss-surface-analysis">5.3.2 In-Depth Techniques for Loss Surface Analysis</h3>
<p>Beyond simple visualization (linear interpolation, contour maps), advanced analysis techniques are being researched to gain a deeper understanding of the loss landscape of deep learning models.</p>
<ol type="1">
<li><p><strong>Topological Data Analysis (TDA):</strong></p>
<ul>
<li><em>Key idea:</em> Analyze the “shape” of the loss surface, such as its connectivity, using tools from topology.</li>
<li><em>Main techniques:</em> Persistent homology, Mapper algorithm, etc.</li>
<li><em>Applications:</em> Understand the complexity of the loss surface, the connection structure of local minima, and the characteristics of saddle points to gain insights into learning dynamics and generalization performance. <em>(For more details, see “Deep Dive: Topological Loss Surface Analysis”)</em></li>
</ul></li>
<li><p><strong>Multi-scale Analysis:</strong></p>
<ul>
<li><em>Key idea:</em> Analyze the loss surface at various scales to capture both macroscopic and microscopic structures.</li>
<li><em>Main techniques:</em> Wavelet transform, scale-space theory, etc.</li>
<li><em>Applications:</em> Analyze the roughness of the loss surface and the scale-by-scale distribution of major features to understand the behavior of optimization algorithms and the difficulty of learning. <em>(For more details, see “Deep Dive: Multi-scale Loss Surface Analysis”)</em></li>
</ul></li>
</ol>
<p><em>These advanced analysis techniques provide more abstract and quantitative information about the loss surface, contributing to a deeper understanding of the learning process of deep learning models and the establishment of better model design and optimization strategies.</em></p>
<div id="cell-32" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn  <span class="co"># Import the nn module</span></span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> DataLoader, Subset  <span class="co"># Import DataLoader and Subset</span></span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_05.visualization.loss_surface <span class="im">import</span>  analyze_loss_surface_multiscale</span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_04.utils.data <span class="im">import</span> get_dataset  <span class="co"># Import get_dataset</span></span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_04.utils.metrics <span class="im">import</span> load_model  <span class="co"># Import load_model</span></span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-8"><a href="#cb41-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Device configuration</span></span>
<span id="cb41-9"><a href="#cb41-9" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> torch.device(<span class="st">"cuda"</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">"cpu"</span>)</span>
<span id="cb41-10"><a href="#cb41-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-11"><a href="#cb41-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Load dataset and create a small subset</span></span>
<span id="cb41-12"><a href="#cb41-12" aria-hidden="true" tabindex="-1"></a>_, test_dataset <span class="op">=</span> get_dataset(dataset<span class="op">=</span><span class="st">"FashionMNIST"</span>)</span>
<span id="cb41-13"><a href="#cb41-13" aria-hidden="true" tabindex="-1"></a>small_dataset <span class="op">=</span> Subset(test_dataset, torch.arange(<span class="dv">0</span>, <span class="dv">256</span>))</span>
<span id="cb41-14"><a href="#cb41-14" aria-hidden="true" tabindex="-1"></a>data_loader <span class="op">=</span> DataLoader(small_dataset, batch_size<span class="op">=</span><span class="dv">256</span>, shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb41-15"><a href="#cb41-15" aria-hidden="true" tabindex="-1"></a>loss_func <span class="op">=</span> nn.CrossEntropyLoss()</span>
<span id="cb41-16"><a href="#cb41-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-17"><a href="#cb41-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Load model (example: SimpleNetwork-ReLU)</span></span>
<span id="cb41-18"><a href="#cb41-18" aria-hidden="true" tabindex="-1"></a>model, _ <span class="op">=</span> load_model(model_file<span class="op">=</span><span class="st">"SimpleNetwork-ReLU.pth"</span>, path<span class="op">=</span><span class="st">"tmp/models/"</span>)</span>
<span id="cb41-19"><a href="#cb41-19" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> model.to(device)</span>
<span id="cb41-20"><a href="#cb41-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-21"><a href="#cb41-21" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> analyze_loss_surface_multiscale(model, data_loader, loss_func, device)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="05_Optimization and Visualization_files/figure-html/cell-13-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The <code>analyze_loss_surface_multiscale</code> function was used to analyze and visualize the loss surface of the <code>SimpleNetwork-ReLU</code> model trained on the FashionMNIST dataset from a multi-scale perspective.</p>
<p><strong>Graph Interpretation (Wavelet Transform-Based):</strong></p>
<ul>
<li><p><strong>Approx. Coefficients:</strong> Represents the overall shape (global structure) of the loss surface, likely with the minimum value at the center (low loss values).</p></li>
<li><p><strong>Detail Coeff Level 1/2:</strong> Shows smaller scale changes. “Level 1” represents medium-scale and “Level 2” represents the finest-scale undulations (local minima, saddle points, noise, etc.).</p></li>
<li><p><strong>Color:</strong> Dark colors (low loss), bright colors (high loss)</p></li>
<li><p>The results may vary depending on the implementation of the <code>analyze_loss_surface_multiscale</code> function (wavelet function, decomposition level, etc.).</p></li>
<li><p>This visualization shows only <em>part</em> of the loss surface and it is difficult to fully grasp the complexity of high-dimensional space.</p></li>
</ul>
<p>Multi-scale analysis decomposes the loss surface into multiple scales, revealing a multi-layered structure that is difficult to discern through simple visualization. By understanding large-scale tendencies and small-scale local changes, it helps to understand optimization algorithm behavior, learning difficulty, generalization performance, etc.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Click to view contents (Phase-based loss surface analysis)">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Click to view contents (Phase-based loss surface analysis)
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<section id="topology-based-loss-surface-analysis" class="level3 callout-body-container callout-body">
<h3 class="anchored" data-anchor-id="topology-based-loss-surface-analysis">Topology-Based Loss Surface Analysis</h3>
<p>Topology is a field of study that examines geometric properties that do not change under continuous transformations. In deep learning, topology-based analysis is used to analyze topological features such as connectivity, holes, and voids of the loss surface, providing insights into learning dynamics and generalization performance.</p>
<p><strong>Key Concepts:</strong></p>
<ul>
<li><p><strong>Sublevel Set:</strong> For a given function <span class="math inline">\(f: \mathbb{R}^n \rightarrow \mathbb{R}\)</span> and threshold <span class="math inline">\(c\)</span>, it is defined as <span class="math inline">\(f^{-1}((-\infty, c]) = {x \in \mathbb{R}^n | f(x) \leq c}\)</span>. In the context of loss functions, it represents the region of parameter space with a loss value below a certain threshold.</p></li>
<li><p><strong>Persistent Homology:</strong> Tracks changes in sublevel sets and records the creation and destruction of topological features (0th: connected components, 1st: loops, 2nd: voids, …).</p>
<ul>
<li><strong>0th-Order Feature (Connected Components):</strong> The number of connected regions. In loss surfaces, it is related to the number of local minima.</li>
<li><strong>1st-Order Feature (Loops):</strong> The number of closed loops. In loss surfaces, it is related to the existence of paths surrounding saddle points.</li>
</ul></li>
<li><p><strong>Persistence Diagram:</strong> A plot of the birth and death values of each topological feature on a coordinate plane. The <span class="math inline">\(y\)</span>-coordinate (<span class="math inline">\(\text{death} - \text{birth}\)</span>) represents the “lifetime” or “persistence” of the feature, with higher values indicating more stable features.</p></li>
<li><p><strong>Bottleneck Distance:</strong> A method for measuring the distance between two persistence diagrams. It finds the optimal matching between points in the two diagrams and calculates the maximum distance between matched points.</p></li>
</ul>
<p><strong>Mathematical Background (Brief):</strong></p>
<ul>
<li><strong>Simplicial Complex:</strong> A generalized concept that includes points, edges, triangles, tetrahedra, etc., used to approximate topological spaces.</li>
<li><strong>Boundary Operator:</strong> An operator that calculates the boundary of a simplicial complex.</li>
<li><strong>Homology Group:</strong> A group defined using the boundary operator, representing “holes” in a topological space.</li>
<li><strong>Persistent Homology Algorithm:</strong> Constructs a simplicial complex through sublevel set filtration and tracks changes in homology groups to compute persistence diagrams. (See reference [1] for details)</li>
</ul>
<p><strong>Application to Deep Learning Research:</strong> * <strong>Loss Surface Structure Analysis:</strong> Through persistence diagrams, we can understand the complexity of the loss surface, the number of local minima, stability, and the presence of saddle points. * Example: <a href="https://www.google.com/search?q=https://www.google.com/search%3Fq%3Dhttps://arxiv.org/abs/1803.06934">Gur-Ari et al., 2018</a> calculated the persistence diagram of neural network loss surfaces, showing that wide networks have a simpler topological structure than narrow networks. * <strong>Generalization Performance Prediction:</strong> The characteristics of persistence diagrams (e.g., the lifetime of the longest-lived 0-dimensional feature) may be correlated with the model’s generalization performance. * Example: <a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=%5Bhttps://www.google.com/search?q=https://proceedings.mlr.press/v162/perez22a.html%5D">Perez et al., 2022</a> proposed a method to predict the generalization performance of models using persistence diagram characteristics. * <strong>Mode Connectivity</strong>: We find paths connecting different local minima and analyze the energy barriers on these paths. * Example: <a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://arxiv.org/abs/1802.10026">Garipov et al., 2018</a></p>
<p><strong>References:</strong></p>
<ol type="1">
<li>Edelsbrunner, H., &amp; Harer, J. (2010). <em>Computational Topology: An Introduction</em>. American Mathematical Society.</li>
<li>Gur-Ari, G., Roberts, D. A., &amp; Dyer, E. (2018). <em>Gradient descent happens in a tiny subspace</em>. arXiv preprint arXiv:1812.04754.</li>
<li>Perez, D., Masoomi, A., DiCecco, J., &amp; Chwialkowski, K. (2022). <em>Relating loss landscape topology to generalization with persistent homology</em>. In International Conference on Machine Learning (pp.&nbsp;17953-17977). PMLR.</li>
<li>Garipov, T., Izmailov, P., Podoprikhin, D., Vetrov, D. P., &amp; Wilson, A. G. (2018). <em>Loss surfaces, mode connectivity, and fast ensembling of dnns.</em> Advances in neural information processing systems, 31.</li>
</ol>
</section>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled" title="Click to view contents (Deep Dive: Multi-Scale Loss Surface Analysis)">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Click to view contents (Deep Dive: Multi-Scale Loss Surface Analysis)
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<section id="multi-scale-loss-surface-analysis" class="level3 callout-body-container callout-body">
<h3 class="anchored" data-anchor-id="multi-scale-loss-surface-analysis">Multi-Scale Loss Surface Analysis</h3>
<p>The loss surface of deep learning models has various scale features. From large-scale valleys and ridges to small-scale bumps and holes, various geometric structures of different sizes affect the learning process. Multi-scale analysis is a method for separating and analyzing these various scale features.</p>
<p><strong>Key Idea:</strong></p>
<ul>
<li><p><strong>Wavelet Transform:</strong> The wavelet transform is a mathematical tool that decomposes a signal into frequency components of various frequencies. When applied to the loss function, it can separate features of different scales.</p>
<ul>
<li><p><strong>Continuous Wavelet Transform (CWT):</strong></p>
<p><span class="math inline">\(W(a, b) = \int_{-\infty}^{\infty} f(x) \psi_{a,b}(x) dx\)</span></p>
<ul>
<li><span class="math inline">\(f(x)\)</span>: The function to be analyzed (loss function)</li>
<li><span class="math inline">\(\psi_{a,b}(x) = \frac{1}{\sqrt{a}}\psi(\frac{x-b}{a})\)</span>: Wavelet function (mother wavelet <span class="math inline">\(\psi\)</span> scaled (<span class="math inline">\(a\)</span>) and shifted (<span class="math inline">\(b\)</span>))</li>
<li><span class="math inline">\(W(a, b)\)</span>: Wavelet coefficient at scale <span class="math inline">\(a\)</span>, position <span class="math inline">\(b\)</span></li>
</ul></li>
<li><p><strong>Mother Wavelet:</strong> A function that satisfies certain conditions (e.g., Mexican hat wavelet, Morlet wavelet) (see reference [2] for details)</p></li>
</ul></li>
<li><p><strong>Multi-Resolution Analysis (MRA):</strong> A method for discretizing the CWT to decompose a signal into different resolution levels.</p></li>
</ul>
<p><strong>Mathematical Background (Brief):</strong></p>
<ul>
<li><strong>Scaling Function:</strong> A function representing low-frequency components.</li>
<li><strong>Wavelet Function:</strong> A function representing high-frequency components.</li>
<li><strong>Decomposition:</strong> Decomposes a signal into a combination of scaling functions and wavelet functions.</li>
<li><strong>Reconstruction:</strong> Reconstructs the decomposed signal back into the original signal. <em>(See reference [1] for details)</em></li>
</ul>
<p><strong>Application to Deep Learning Research:</strong></p>
<ul>
<li><p><strong>Loss Surface Roughness Analysis:</strong> Wavelet transforms can be used to quantify the roughness of the loss surface and analyze its effect on learning speed and generalization performance.</p>
<ul>
<li>Example: <a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=%5Bhttps://www.google.com/search?q=https://arxiv.org/abs/1910.00779%5D">Li et al., 2019</a> used wavelet-based multi-resolution analysis to analyze the effect of loss surface roughness on learning dynamics.</li>
</ul></li>
<li><p><strong>Optimization Algorithm Analysis:</strong> Analyzing how optimization algorithms move along features at each scale can help better understand their behavior.</p></li>
</ul>
<p><strong>References:</strong> 1. Mallat, S. (2008). <em>A wavelet tour of signal processing: the sparse way</em>. 2. Daubechies, I. (1992). <em>Ten lectures on wavelets</em>. 3. Li, Y., Hu, W., Zhang, Y., &amp; Gu, Q. (2019). <em>Multiresolution analysis of the loss landscape of deep nets</em>. arXiv preprint arXiv:1910.00779.</p>
</section>
</div>
</div>
</section>
</section>
<section id="visualizing-the-optimization-process-unveiling-the-secrets-of-deep-learning-through-gaussian-functions" class="level2">
<h2 class="anchored" data-anchor-id="visualizing-the-optimization-process-unveiling-the-secrets-of-deep-learning-through-gaussian-functions">5.4 Visualizing the Optimization Process: Unveiling the Secrets of Deep Learning through Gaussian Functions</h2>
<p>The actual loss surface of deep learning models exists in <em>ultra-high-dimensional space</em>, ranging from millions to tens of billions of dimensions, and has a very complex geometric structure. Therefore, directly visualizing and analyzing it is <em>virtually impossible</em>. Additionally, the actual loss surface has various problems such as non-differentiable points, discontinuities, and numerical instability, making theoretical analysis difficult.</p>
<section id="approximate-analysis-through-gaussian-functions-hidden-insights-in-simplicity" class="level3">
<h3 class="anchored" data-anchor-id="approximate-analysis-through-gaussian-functions-hidden-insights-in-simplicity">5.4.1 Approximate Analysis through Gaussian Functions: Hidden Insights in Simplicity</h3>
<p>To overcome these limitations and conceptually understand the optimization process, we use a Gaussian function, which is smooth, continuous, and <em>convex</em>, to approximate the loss surface.</p>
<p><strong>Reasons for using Gaussian functions (advantages of loss surface approximation):</strong></p>
<ol type="1">
<li><strong>Differentiability:</strong> The Gaussian function is infinitely differentiable at all points. This is an essential condition for applying and analyzing gradient descent-based optimization algorithms.</li>
<li><strong>Convexity:</strong> A single Gaussian function is a convex function. Convex functions have only one global minimum, making it easier to analyze the convergence of optimization algorithms.</li>
<li><strong>Symmetry:</strong> The Gaussian function has a symmetrical shape around its center point. This means that there is no bias in the loss surface in specific directions, allowing for simplified assumptions when analyzing the behavior of optimization algorithms.</li>
<li><strong>Mathematical simplicity:</strong> The Gaussian function can be expressed with relatively simple formulas, making mathematical analysis easier. This enables a theoretical understanding of the principles behind optimization algorithms and the derivation of predictable results.</li>
<li><strong>Adjustable complexity:</strong> The complexity can be adjusted using Gaussian mixture models.</li>
</ol>
<p><strong>Gaussian function formula:</strong></p>
<p><span class="math inline">\(z = A \exp\left(-\left(\frac{(x-x_0)^2}{2\sigma_1^2} + \frac{(y-y_0)^2}{2\sigma_2^2}\right)\right)\)</span></p>
<ul>
<li><span class="math inline">\(A\)</span>: amplitude - the maximum height of the loss function</li>
<li><span class="math inline">\(x_0\)</span>, <span class="math inline">\(y_0\)</span>: center point - the location of the minimum value of the loss function</li>
<li><span class="math inline">\(\sigma_1\)</span>, <span class="math inline">\(\sigma_2\)</span>: standard deviations in the x and y axes - the width (broadness and narrowness) of the loss surface</li>
</ul>
<p>Of course, actual loss surfaces can have much more complex shapes than Gaussian functions (e.g., multiple local minima, saddle points, plateaus). However, <em>using a single Gaussian function for approximation provides a useful starting point for understanding the basic behavior of optimization algorithms (e.g., convergence speed, vibration patterns) and comparing different algorithms.</em> To simulate more complex loss surfaces, Gaussian mixture models (GMMs), which combine multiple Gaussian functions, can be used.</p>
<p><em>In this section, we will approximate the loss surface using a single Gaussian function and visualize the learning trajectory by applying various optimization algorithms (e.g., SGD, Adam) to intuitively grasp the dynamic characteristics and pros and cons of each algorithm.</em></p>
<div id="cell-37" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> DataLoader, Subset</span>
<span id="cb42-5"><a href="#cb42-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_05.visualization.loss_surface <span class="im">import</span> hessian_eigenvectors, xy_perturb_loss, visualize_loss_surface, linear_interpolation</span>
<span id="cb42-6"><a href="#cb42-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_04.utils.data <span class="im">import</span> get_dataset  </span>
<span id="cb42-7"><a href="#cb42-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_04.utils.metrics <span class="im">import</span> load_model  </span>
<span id="cb42-8"><a href="#cb42-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_05.optimizers.basic <span class="im">import</span> SGD, Adam</span>
<span id="cb42-9"><a href="#cb42-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_05.visualization.gaussian_loss_surface <span class="im">import</span> (</span>
<span id="cb42-10"><a href="#cb42-10" aria-hidden="true" tabindex="-1"></a>    get_opt_params,  visualize_gaussian_fit, train_loss_surface, visualize_optimization_path</span>
<span id="cb42-11"><a href="#cb42-11" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb42-12"><a href="#cb42-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-13"><a href="#cb42-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-14"><a href="#cb42-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Device configuration</span></span>
<span id="cb42-15"><a href="#cb42-15" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> torch.device(<span class="st">"cuda"</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">"cpu"</span>)</span>
<span id="cb42-16"><a href="#cb42-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-17"><a href="#cb42-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the dataset</span></span>
<span id="cb42-18"><a href="#cb42-18" aria-hidden="true" tabindex="-1"></a>_, test_dataset <span class="op">=</span> get_dataset(dataset<span class="op">=</span><span class="st">"FashionMNIST"</span>)</span>
<span id="cb42-19"><a href="#cb42-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a small dataset</span></span>
<span id="cb42-20"><a href="#cb42-20" aria-hidden="true" tabindex="-1"></a>small_dataset <span class="op">=</span> Subset(test_dataset, torch.arange(<span class="dv">0</span>, <span class="dv">256</span>))</span>
<span id="cb42-21"><a href="#cb42-21" aria-hidden="true" tabindex="-1"></a>data_loader <span class="op">=</span> DataLoader(small_dataset, batch_size<span class="op">=</span><span class="dv">256</span>, shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb42-22"><a href="#cb42-22" aria-hidden="true" tabindex="-1"></a>loss_func <span class="op">=</span> nn.CrossEntropyLoss()</span>
<span id="cb42-23"><a href="#cb42-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-24"><a href="#cb42-24" aria-hidden="true" tabindex="-1"></a>trained_model, _ <span class="op">=</span> load_model(model_file<span class="op">=</span><span class="st">"SimpleNetwork-ReLU.pth"</span>, path<span class="op">=</span><span class="st">"tmp/models/"</span>)</span>
<span id="cb42-25"><a href="#cb42-25" aria-hidden="true" tabindex="-1"></a><span class="co"># trained_model, _ = load_model(model_file="SimpleNetwork-Tanh.pth", path="tmp/models/")</span></span>
<span id="cb42-26"><a href="#cb42-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-27"><a href="#cb42-27" aria-hidden="true" tabindex="-1"></a>trained_model <span class="op">=</span> trained_model.to(device)</span>
<span id="cb42-28"><a href="#cb42-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-29"><a href="#cb42-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Loss surface data generation</span></span>
<span id="cb42-30"><a href="#cb42-30" aria-hidden="true" tabindex="-1"></a>top_n <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb42-31"><a href="#cb42-31" aria-hidden="true" tabindex="-1"></a>top_eigenvalues, top_eignevectors <span class="op">=</span> hessian_eigenvectors(</span>
<span id="cb42-32"><a href="#cb42-32" aria-hidden="true" tabindex="-1"></a>    model<span class="op">=</span>trained_model,</span>
<span id="cb42-33"><a href="#cb42-33" aria-hidden="true" tabindex="-1"></a>    loss_func<span class="op">=</span>loss_func,</span>
<span id="cb42-34"><a href="#cb42-34" aria-hidden="true" tabindex="-1"></a>    data_loader<span class="op">=</span>data_loader,</span>
<span id="cb42-35"><a href="#cb42-35" aria-hidden="true" tabindex="-1"></a>    top_n<span class="op">=</span>top_n,</span>
<span id="cb42-36"><a href="#cb42-36" aria-hidden="true" tabindex="-1"></a>    is_cuda<span class="op">=</span><span class="va">True</span></span>
<span id="cb42-37"><a href="#cb42-37" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb42-38"><a href="#cb42-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-39"><a href="#cb42-39" aria-hidden="true" tabindex="-1"></a><span class="co"># Define lambda range</span></span>
<span id="cb42-40"><a href="#cb42-40" aria-hidden="true" tabindex="-1"></a>d_min, d_max, d_num <span class="op">=</span> <span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">30</span></span>
<span id="cb42-41"><a href="#cb42-41" aria-hidden="true" tabindex="-1"></a>lambda1 <span class="op">=</span> np.linspace(d_min, d_max, d_num).astype(np.float32)</span>
<span id="cb42-42"><a href="#cb42-42" aria-hidden="true" tabindex="-1"></a>lambda2 <span class="op">=</span> np.linspace(d_min, d_max, d_num).astype(np.float32)</span>
<span id="cb42-43"><a href="#cb42-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-44"><a href="#cb42-44" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate loss surface</span></span>
<span id="cb42-45"><a href="#cb42-45" aria-hidden="true" tabindex="-1"></a>x, y, z <span class="op">=</span> xy_perturb_loss(</span>
<span id="cb42-46"><a href="#cb42-46" aria-hidden="true" tabindex="-1"></a>    model<span class="op">=</span>trained_model,</span>
<span id="cb42-47"><a href="#cb42-47" aria-hidden="true" tabindex="-1"></a>    top_eigenvectors<span class="op">=</span>top_eignevectors,</span>
<span id="cb42-48"><a href="#cb42-48" aria-hidden="true" tabindex="-1"></a>    data_loader<span class="op">=</span>data_loader,</span>
<span id="cb42-49"><a href="#cb42-49" aria-hidden="true" tabindex="-1"></a>    loss_func<span class="op">=</span>loss_func,</span>
<span id="cb42-50"><a href="#cb42-50" aria-hidden="true" tabindex="-1"></a>    lambda1<span class="op">=</span>lambda1,</span>
<span id="cb42-51"><a href="#cb42-51" aria-hidden="true" tabindex="-1"></a>    lambda2<span class="op">=</span>lambda2,</span>
<span id="cb42-52"><a href="#cb42-52" aria-hidden="true" tabindex="-1"></a>    device<span class="op">=</span>device</span>
<span id="cb42-53"><a href="#cb42-53" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb42-54"><a href="#cb42-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-55"><a href="#cb42-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-56"><a href="#cb42-56" aria-hidden="true" tabindex="-1"></a><span class="co"># After generating loss surface data</span></span>
<span id="cb42-57"><a href="#cb42-57" aria-hidden="true" tabindex="-1"></a>popt, _, offset <span class="op">=</span> get_opt_params(x, y, z)</span>
<span id="cb42-58"><a href="#cb42-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-59"><a href="#cb42-59" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize Gaussian fitting</span></span>
<span id="cb42-60"><a href="#cb42-60" aria-hidden="true" tabindex="-1"></a>visualize_gaussian_fit(x, y, z, popt, offset, d_min, d_max, d_num)</span>
<span id="cb42-61"><a href="#cb42-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-62"><a href="#cb42-62" aria-hidden="true" tabindex="-1"></a><span class="co"># View from a different angle</span></span>
<span id="cb42-63"><a href="#cb42-63" aria-hidden="true" tabindex="-1"></a>visualize_gaussian_fit(x, y, z, popt, offset, d_min, d_max, d_num,</span>
<span id="cb42-64"><a href="#cb42-64" aria-hidden="true" tabindex="-1"></a>                      elev<span class="op">=</span><span class="dv">30</span>, azim<span class="op">=</span><span class="dv">45</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Function parameters = [29.27164346 -0.0488573  -0.06687705  0.7469189   0.94904458]</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="05_Optimization and Visualization_files/figure-html/cell-14-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="05_Optimization and Visualization_files/figure-html/cell-14-output-3.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The actual loss plane data (blue dots) and the approximated plane using a Gaussian function (red) are visualized together. As can be seen from the graph, the generated Gaussian function relatively well captures the <em>overall trend of the original loss surface data</em> (especially, the concave shape in the center), creating a similar surface. Now, using this approximated loss plane function, we will analyze and visualize how various optimization algorithms (optimizers) find the minimum value and their paths.</p>
</section>
<section id="path-visualization" class="level3">
<h3 class="anchored" data-anchor-id="path-visualization">5.4.2 Path Visualization</h3>
<p>Using the loss surface approximated by a Gaussian function, let’s visualize how the optimizer works in a 2D plane.</p>
<p>Please provide the original Korean text for translation. I will translate it into English according to the given instructions.</p>
<div id="cell-41" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Gaussian fitting</span></span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a>popt, _, offset <span class="op">=</span> get_opt_params(x, y, z)</span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a>gaussian_params <span class="op">=</span> (<span class="op">*</span>popt, offset)</span>
<span id="cb44-4"><a href="#cb44-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-5"><a href="#cb44-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate optimization paths</span></span>
<span id="cb44-6"><a href="#cb44-6" aria-hidden="true" tabindex="-1"></a>points_sgd <span class="op">=</span> train_loss_surface(</span>
<span id="cb44-7"><a href="#cb44-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">lambda</span> params: SGD(params, lr<span class="op">=</span><span class="fl">0.1</span>),</span>
<span id="cb44-8"><a href="#cb44-8" aria-hidden="true" tabindex="-1"></a>    [d_min, d_max], <span class="dv">100</span>, gaussian_params</span>
<span id="cb44-9"><a href="#cb44-9" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb44-10"><a href="#cb44-10" aria-hidden="true" tabindex="-1"></a>points_sgd_m <span class="op">=</span> train_loss_surface(</span>
<span id="cb44-11"><a href="#cb44-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">lambda</span> params: SGD(params, lr<span class="op">=</span><span class="fl">0.05</span>, momentum<span class="op">=</span><span class="fl">0.8</span>),</span>
<span id="cb44-12"><a href="#cb44-12" aria-hidden="true" tabindex="-1"></a>    [d_min, d_max], <span class="dv">100</span>, gaussian_params</span>
<span id="cb44-13"><a href="#cb44-13" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb44-14"><a href="#cb44-14" aria-hidden="true" tabindex="-1"></a>points_adam <span class="op">=</span> train_loss_surface(</span>
<span id="cb44-15"><a href="#cb44-15" aria-hidden="true" tabindex="-1"></a>    <span class="kw">lambda</span> params: Adam(params, lr<span class="op">=</span><span class="fl">0.1</span>),</span>
<span id="cb44-16"><a href="#cb44-16" aria-hidden="true" tabindex="-1"></a>    [d_min, d_max], <span class="dv">100</span>, gaussian_params</span>
<span id="cb44-17"><a href="#cb44-17" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb44-18"><a href="#cb44-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-19"><a href="#cb44-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualization</span></span>
<span id="cb44-20"><a href="#cb44-20" aria-hidden="true" tabindex="-1"></a>visualize_optimization_path(</span>
<span id="cb44-21"><a href="#cb44-21" aria-hidden="true" tabindex="-1"></a>    x, y, z, popt, offset,</span>
<span id="cb44-22"><a href="#cb44-22" aria-hidden="true" tabindex="-1"></a>    [points_sgd, points_sgd_m, points_adam],</span>
<span id="cb44-23"><a href="#cb44-23" aria-hidden="true" tabindex="-1"></a>    act_name<span class="op">=</span><span class="st">"ReLU"</span></span>
<span id="cb44-24"><a href="#cb44-24" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>The graph shows the learning paths of three optimization algorithms, SGD, Momentum SGD, and Adam, on a loss surface approximated by a Gaussian function. The three algorithms show <strong>different</strong> characteristics in both gentle and steep areas.</p>
<ul>
<li><strong>SGD (orange):</strong> In gentle areas, it approaches the lowest point with a relatively <em>wide range of oscillations</em>, but in steep areas, it tends to <em>oscillate with a larger amplitude</em> and has a <em>slower convergence speed</em> near the lowest point.</li>
<li><strong>Momentum SGD (green):</strong> Compared to SGD, it approaches the lowest point with <em>less oscillation</em> and a <em>smoother curve</em>. Thanks to the momentum effect, it can find the lowest point relatively <em>stably</em> even in steep areas.</li>
<li><strong>Adam (red):</strong> With the <em>least oscillation</em>, it responds <em>sensitively to gradient changes</em> and reaches the lowest point along an efficient path. <em>In particular, it shows a relatively fast convergence speed even in steep areas</em>. This is due to Adam’s adaptive learning rate adjustment mechanism.</li>
</ul>
<p>In practice, Momentum SGD is much more preferred than SGD itself, and adaptive optimization algorithms like Adam or AdamW are also widely used. Generally, the loss surface tends to be flat in most areas but has a narrow and deep valley shape near the minimum value. Therefore, a large learning rate can cause overshooting or divergence, so it is common to use a learning rate scheduler that gradually decreases the learning rate. Additionally, it is essential to consider not only the choice of optimization algorithm but also an appropriate learning rate scheduler, batch size, regularization techniques, and more.</p>
<p><img src="../../../assets/images/04_loss_resnet_ft_imagenet_2.png" class="img-fluid"></p>
<p><img src="../../../assets/images/04_loss_resnet_ft_imagenet_1.png" class="img-fluid"></p>
<p>The above loss surface image is a 3D visualization of the loss surface of a ResNet-50 model newly trained on the ImageNet dataset (using the top two eigenvectors of the Hessian matrix calculated by PyHessian as axes). Unlike the Gaussian function approximation, the actual loss surface of deep learning models has a much more complex and irregular shape. Nevertheless, it can be seen that the large tendency for the minimum value to exist in the central area (blue area) is maintained. This visualization helps provide an intuitive understanding of how complex the actual loss surface of deep learning models is and why optimization is a difficult problem.</p>
</section>
</section>
<section id="dynamic-analysis-of-the-optimization-process-exploring-learning-trajectories" class="level2">
<h2 class="anchored" data-anchor-id="dynamic-analysis-of-the-optimization-process-exploring-learning-trajectories">5.5 Dynamic Analysis of the Optimization Process: Exploring Learning Trajectories</h2>
<p>Understanding how optimization algorithms navigate through the loss landscape to find the minimum value in deep learning model training is crucial. <em>Especially with the emergence of large language models (LLMs), analyzing and controlling the learning dynamics of models with billions of parameters has become even more important.</em></p>
<section id="characteristics-of-the-training-process" class="level3">
<h3 class="anchored" data-anchor-id="characteristics-of-the-training-process">5.5.1 Characteristics of the Training Process</h3>
<p>The deep learning model training process can be divided into initial, mid-term, and late stages, each with its own characteristics.</p>
<ol type="1">
<li><p><strong>Stage-wise Characteristics:</strong></p>
<ul>
<li><strong>Initial:</strong> The gradient norm is large and fluctuates significantly, and the loss function value decreases rapidly.</li>
<li><strong>Mid-term:</strong> The gradient stabilizes, and parameters explore the optimal region.</li>
<li><strong>Late:</strong> Parameters fine-tune around local optima. (Early termination is important)</li>
</ul></li>
<li><p><strong>Layer-wise Gradient Characteristics:</strong></p>
<ul>
<li>In deep neural networks, gradients tend to be larger near the input layer and smaller near the output layer. (vanishing gradient problem)</li>
<li>This is due to the chain rule during backpropagation.</li>
<li>Residual connections alleviate this imbalance, helping stabilize learning in deeper layers.</li>
</ul></li>
<li><p><strong>Parameter Dependencies:</strong></p>
<ul>
<li>Neural network parameters are interdependent, making the optimization process nonlinear.</li>
<li>Some parameters may have a greater impact on learning, so balance between parameters is crucial.</li>
</ul></li>
<li><p><strong>Optimization Path Analysis:</strong></p>
<ul>
<li>The path that parameters move along the loss surface during optimization is called the optimization path.</li>
<li>Wide and gentle valley-shaped local minima tend to have better generalization performance than narrow and steep ones.</li>
<li>Saddle points are very common in high-dimensional spaces. (Momentum, Adam, etc., are designed to avoid them)</li>
<li>In flat areas of the loss surface, gradients can become small, slowing down learning. (Adaptive learning rate algorithms help)</li>
</ul></li>
</ol>
</section>
<section id="learning-stability-analysis-and-control" class="level3">
<h3 class="anchored" data-anchor-id="learning-stability-analysis-and-control">5.5.2 Learning Stability Analysis and Control</h3>
<section id="stability-analysis-methodology" class="level4">
<h4 class="anchored" data-anchor-id="stability-analysis-methodology">Stability Analysis Methodology</h4>
<p>To analyze the stability of the optimization process, consider the following:</p>
<ol type="1">
<li><p><strong>Gradient Diagnostics:</strong></p>
<ul>
<li>Detect vanishing or exploding gradient phenomena.</li>
<li>Periodically observe the gradient norm during training.</li>
</ul></li>
<li><p><strong>Hessian-based Analysis:</strong></p>
<ul>
<li>The distribution of Hessian matrix eigenvalues and condition numbers indicate optimization path stability.</li>
<li>(Refer to Hessian-based visualization in Section 5.3)</li>
</ul></li>
<li><p><strong>Real-time Monitoring:</strong></p>
<ul>
<li>Monitor the gradient norm, parameter update size, loss function value, and performance metrics in real-time during learning.</li>
</ul></li>
</ol>
<section id="stabilization-techniques" class="level5">
<h5 class="anchored" data-anchor-id="stabilization-techniques">Stabilization Techniques</h5>
<ul>
<li><p><strong>Gradient Clipping:</strong> Limit the gradient size (norm) to not exceed a threshold.</p>
<p><span class="math inline">\(g \leftarrow \text{clip}(g) = \min(\max(g, -c), c)\)</span></p></li>
<li><p><span class="math inline">\(g\)</span>: gradient, <span class="math inline">\(c\)</span>: threshold</p></li>
<li><p><strong>Adaptive Learning Rate:</strong> Adam, RMSProp, Lion, Sophia, etc. automatically adjust the learning rate based on gradient statistics.</p></li>
<li><p><strong>Learning Rate Scheduler:</strong> gradually decrease the learning rate based on the training epoch or validation loss.</p></li>
<li><p><strong>Hyperparameter Optimization:</strong> automatically search and adjust optimization-related hyperparameters.</p></li>
</ul>
</section>
<section id="latest-research-trends" class="level5">
<h5 class="anchored" data-anchor-id="latest-research-trends">Latest Research Trends</h5>
<p>Recent (2024) studies on learning dynamics are advancing in the following directions:</p>
<ul>
<li><strong>Predictive Stabilization:</strong> analyze model structure, initialization, and dataset characteristics before training to pre-emptively remove or alleviate instability factors.</li>
<li><strong>Unified Analysis:</strong> deeply understand optimization algorithms by jointly analyzing curvature information (Hessian) and gradient statistics.</li>
<li><strong>Automated Control:</strong> use reinforcement learning, etc. to automatically adjust the hyperparameters of optimization algorithms.</li>
</ul>
<p>These studies contribute to making deep learning model training more stable and efficient and help to understand the “black box”.</p>
<p>Now, let’s explore the dynamic analysis of the optimization process through a simple example.</p>
<div id="cell-44" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.optim <span class="im">as</span> optim</span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> DataLoader, Subset  <span class="co"># Import Subset</span></span>
<span id="cb45-5"><a href="#cb45-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_05.visualization.train_dynamics <span class="im">import</span> visualize_training_dynamics</span>
<span id="cb45-6"><a href="#cb45-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_04.utils.data <span class="im">import</span> get_dataset  </span>
<span id="cb45-7"><a href="#cb45-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_04.utils.metrics <span class="im">import</span> load_model  </span>
<span id="cb45-8"><a href="#cb45-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-9"><a href="#cb45-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Device configuration</span></span>
<span id="cb45-10"><a href="#cb45-10" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> torch.device(<span class="st">"cuda"</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">"cpu"</span>)</span>
<span id="cb45-11"><a href="#cb45-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-12"><a href="#cb45-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the FashionMNIST dataset (both training and testing)</span></span>
<span id="cb45-13"><a href="#cb45-13" aria-hidden="true" tabindex="-1"></a>train_dataset, test_dataset <span class="op">=</span> get_dataset(dataset<span class="op">=</span><span class="st">"FashionMNIST"</span>)</span>
<span id="cb45-14"><a href="#cb45-14" aria-hidden="true" tabindex="-1"></a>train_loader <span class="op">=</span> DataLoader(train_dataset, batch_size<span class="op">=</span><span class="dv">256</span>, shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb45-15"><a href="#cb45-15" aria-hidden="true" tabindex="-1"></a>loss_func <span class="op">=</span> nn.CrossEntropyLoss()</span>
<span id="cb45-16"><a href="#cb45-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-17"><a href="#cb45-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Load a pre-trained model (e.g., ReLU-based network)</span></span>
<span id="cb45-18"><a href="#cb45-18" aria-hidden="true" tabindex="-1"></a>trained_model, _ <span class="op">=</span> load_model(model_file<span class="op">=</span><span class="st">"SimpleNetwork-ReLU.pth"</span>, path<span class="op">=</span><span class="st">"tmp/models/"</span>)</span>
<span id="cb45-19"><a href="#cb45-19" aria-hidden="true" tabindex="-1"></a>trained_model <span class="op">=</span> trained_model.to(device)</span>
<span id="cb45-20"><a href="#cb45-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-21"><a href="#cb45-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Choose an optimizer (e.g., Adam)</span></span>
<span id="cb45-22"><a href="#cb45-22" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> optim.Adam(trained_model.parameters(), lr<span class="op">=</span><span class="fl">0.001</span>)</span>
<span id="cb45-23"><a href="#cb45-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-24"><a href="#cb45-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Call the training dynamics visualization function (e.g., train for 10 epochs with the entire training dataset)</span></span>
<span id="cb45-25"><a href="#cb45-25" aria-hidden="true" tabindex="-1"></a>metrics <span class="op">=</span> visualize_training_dynamics(</span>
<span id="cb45-26"><a href="#cb45-26" aria-hidden="true" tabindex="-1"></a>    trained_model, optimizer, train_loader, loss_func, num_epochs<span class="op">=</span><span class="dv">20</span>, device<span class="op">=</span>device</span>
<span id="cb45-27"><a href="#cb45-27" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb45-28"><a href="#cb45-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-29"><a href="#cb45-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the final results for each metric</span></span>
<span id="cb45-30"><a href="#cb45-30" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Final Loss:"</span>, metrics[<span class="st">"loss"</span>][<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb45-31"><a href="#cb45-31" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Final Grad Norm:"</span>, metrics[<span class="st">"grad_norm"</span>][<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb45-32"><a href="#cb45-32" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Final Param Change:"</span>, metrics[<span class="st">"param_change"</span>][<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb45-33"><a href="#cb45-33" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Final Weight Norm:"</span>, metrics[<span class="st">"weight_norm"</span>][<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb45-34"><a href="#cb45-34" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Final Loss Improvement:"</span>, metrics[<span class="st">"loss_improvement"</span>][<span class="op">-</span><span class="dv">1</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>The example above actually shows various aspects of the learning dynamics described. Using a pre-trained SimpleNetwork-ReLU model on the FashionMNIST dataset and continuing to train it with the Adam optimization algorithm, we visualized the following five key metrics for each epoch:</p>
<ul>
<li><strong>Loss (loss):</strong> Shows how the loss function value decreases during training. (blue line)</li>
<li><strong>Grad Norm (gradient norm):</strong> Represents the size of the gradient (L2 norm). (red line)</li>
<li><strong>Param Change (parameter change):</strong> Represents the change in parameters (weights) compared to the previous epoch (L2 norm).</li>
<li><strong>Weight Norm (weight norm):</strong> Represents the size of all model parameters (weights) (L2 norm). (purple line)</li>
<li><strong>Loss Improvement (loss improvement):</strong> Represents how much the loss function value has decreased compared to the previous epoch. (yellow line)</li>
</ul>
<p>The graph shows the following:</p>
<ul>
<li><strong>Loss:</strong> The loss value, which was around 0.51 in the initial epoch (epoch 1), consistently decreases as training progresses and reaches around 0.16 in the final epoch (epoch 20).</li>
<li><strong>Grad Norm:</strong> The gradient norm, which was relatively high in the initial epoch (around 4.5), gradually decreases as training progresses and reaches around 2.0 in the final epoch.</li>
<li><strong>Param Change:</strong> The parameter change is large at the beginning of training but tends to decrease as training progresses. This means that the model’s parameters change less as it gets closer to the optimal point.</li>
<li><strong>Weight Norm:</strong> The weight norm consistently increases throughout the training process. This means that the model’s parameters become “larger” through training. (However, this does not necessarily mean overfitting.)</li>
<li><strong>Loss Improvement:</strong> The loss improvement is large at the beginning of training and tends to decrease as training progresses.</li>
</ul>
<p>Through this example, we can visually confirm the process of the optimization algorithm minimizing the loss function, the change in gradients, and the change in parameters, and gain an intuitive understanding of the learning dynamics.</p>
</section>
</section>
</section>
<section id="conclusion" class="level3">
<h3 class="anchored" data-anchor-id="conclusion">Conclusion</h3>
<p>In this chapter 5, we deeply explored various topics related to optimization, a key element of deep learning model training. We understood the importance of weight initialization methods, the principles and characteristics of various optimization algorithms (SGD, Momentum, Adam, Lion, Sophia, AdaFactor), and loss surface visualization and learning dynamics analysis to better understand the training process of deep learning models.</p>
<p>In chapter 6, we will learn about regularization, a key technique for improving the generalization performance of deep learning models. We will examine the principles and effects of various regularization techniques such as L1/L2 regularization, dropout, batch normalization, and data augmentation, and learn how to apply them through practical examples.</p>
</section>
<section id="practice-problems" class="level3">
<h3 class="anchored" data-anchor-id="practice-problems">Practice Problems</h3>
<section id="basic-problems" class="level4">
<h4 class="anchored" data-anchor-id="basic-problems">Basic Problems</h4>
<ol type="1">
<li><strong>SGD Manual Calculation:</strong> Manually calculate the SGD update rule for more than three steps with a learning rate of 0.1 and momentum of 0.9 on the loss function <span class="math inline">\(L(w) = w^2\)</span>. The initial weight is set to <span class="math inline">\(w_0 = 2\)</span>.</li>
<li><strong>Gradient Descent Convergence Speed Comparison:</strong> Apply gradient descent to a simple two-dimensional function <span class="math inline">\(f(x, y) = x^2 + 2y^2\)</span>, and compare the convergence speeds by changing the learning rate to 0.1, 0.01, and 0.001.</li>
<li><strong>Initialization Method Comparison:</strong> Compare Kaiming initialization and Xavier initialization, and explain why Kaiming initialization is more suitable when used with the ReLU activation function.</li>
</ol>
</section>
<section id="applied-problems" class="level4">
<h4 class="anchored" data-anchor-id="applied-problems">Applied Problems</h4>
<ol type="1">
<li><strong>Adam Optimizer:</strong> Explain the working principle of the Adam optimizer (including formulas) and describe the roles of the <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_2\)</span> parameters.</li>
<li><strong>Batch Normalization and Initialization:</strong> Explain how batch normalization reduces the importance of initialization methods, and provide reasons for this phenomenon.</li>
<li><strong>Gaussian Loss Surface:</strong> In an example that approximates a loss surface using Gaussian functions (Section 5.5.1), explain the effect of the parameters of the Gaussian function (amplitude, center point, variance) on the optimization process, and observe how changing each parameter affects the optimization path.</li>
</ol>
</section>
<section id="advanced-problems" class="level4">
<h4 class="anchored">Advanced Problems</h4>
<ol type="1">
<li><strong>Lion Optimizer Analysis:</strong> Explain the core idea of the Lion optimizer (including formulas) and analyze its advantages and disadvantages compared to Adam.</li>
<li><strong>Initialization Method Experimentation:</strong> For a given dataset (e.g., FashionMNIST) and model (SimpleNetwork in Section 5.1), apply different initialization methods (LeCun, Xavier, Kaiming, Orthogonal) and compare the results (error rate, convergence speed, average condition number, spectral norm, effective rank ratio).</li>
<li><strong>Optimization Path Visualization:</strong> Refer to the optimization path visualization code in Section 5.5, define your own loss function (e.g., multi-modal function, non-convex function), and visualize and compare the optimization paths of various optimizers (SGD, Momentum, Adam, Lion, etc.). Compare at least three optimizers.</li>
</ol>
<div class="callout callout-style-default callout-note callout-titled" title="Click to view contents (Phase-based loss surface analysis)">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-5-contents" aria-controls="callout-5" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Click to view contents (Phase-based loss surface analysis)
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-5" class="callout-5-contents callout-collapse collapse">
<section id="practice-problem-solutions" class="level3 callout-body-container callout-body">
<h3 class="anchored" data-anchor-id="practice-problem-solutions">Practice Problem Solutions</h3>
<section id="basic-problems-1" class="level4">
<h4 class="anchored" data-anchor-id="basic-problems-1">Basic Problems</h4>
<ol type="1">
<li><p><strong>SGD Manual Calculation:</strong></p>
<ul>
<li><strong>Step 1:</strong>
<ul>
<li><span class="math inline">\(g_0 = \frac{dL}{dw}(w_0) = 2w_0 = 4\)</span></li>
<li><span class="math inline">\(v_0 = 0\)</span> (initial momentum)</li>
<li><span class="math inline">\(w_1 = w_0 - \eta v_1 = 2 - 0.1 \cdot (0.9 \cdot 0 + 4) = 1.6\)</span></li>
</ul></li>
<li><strong>Step 2:</strong>
<ul>
<li><span class="math inline">\(g_1 = 2w_1 = 3.2\)</span></li>
<li><span class="math inline">\(v_1 = 0.9 \cdot v_0 + g_0= 0.9 \cdot 0 + 4 = 4\)</span></li>
<li><span class="math inline">\(w_2 = w_1 - \eta \cdot (0.9 \cdot v_1 + g_1) = 1.6 - 0.1 \cdot (0.9 \cdot 4+ 3.2) = 0.92\)</span></li>
</ul></li>
<li><strong>Step 3:</strong>
<ul>
<li><span class="math inline">\(g_2 = 2w_2 = 1.84\)</span></li>
<li><span class="math inline">\(v_2 = 0.9 \cdot 4 + 3.2 = 6.8\)</span></li>
<li><span class="math inline">\(w_3 = w_2 - \eta \cdot (0.9 * v_2 + g_2) = 0.92 - 0.1 \cdot (0.9 \cdot 6.8 + 1.84) = 0.124\)</span></li>
</ul></li>
</ul></li>
<li><p><strong>Gradient Descent Convergence Speed Comparison:</strong></p>
<ul>
<li>The larger the learning rate (0.1), the faster the initial convergence, but it may oscillate near the optimal point.</li>
<li>The smaller the learning rate (0.001), the slower the convergence speed, but it approaches the optimal point more stably.</li>
<li>An appropriate learning rate (0.01) shows a moderate convergence speed and stability.</li>
</ul></li>
<li><p><strong>Initialization Method Comparison:</strong></p>
<ul>
<li><strong>Kaiming Initialization:</strong> Initializes weights with a distribution having a standard deviation of <span class="math inline">\(\sqrt{2/n_{in}}\)</span>, considering the characteristics of ReLU activation functions (making negative inputs 0).</li>
<li><strong>Xavier Initialization:</strong> Uses a standard deviation of <span class="math inline">\(\sqrt{2/(n_{in} + n_{out})}\)</span>, which maintains the variance of inputs and outputs regardless of the type of activation function.</li>
<li><strong>ReLU + Kaiming:</strong> Since ReLU has linear activation in the positive region, Kaiming initialization provides a larger variance, alleviating the “dead neuron” problem and helping with faster learning.</li>
</ul></li>
</ol>
</section>
<section id="application-problems" class="level4">
<h4 class="anchored" data-anchor-id="application-problems">Application Problems</h4>
<ol type="1">
<li><p><strong>Adam Optimizer:</strong></p>
<ul>
<li><strong>Working Principle:</strong> Adam is an optimizer that combines the ideas of Momentum and RMSProp.
<ul>
<li><strong>Momentum:</strong> Gives inertia by using the exponential weighted average of past gradients (first moment, <span class="math inline">\(m_t\)</span>).</li>
<li><strong>RMSProp:</strong> Adjusts the learning rate for each parameter by using the exponential weighted average of squared past gradients (second moment, <span class="math inline">\(v_t\)</span>).</li>
<li><strong>Bias Correction:</strong> Corrects for the bias in <span class="math inline">\(m_t\)</span> and <span class="math inline">\(v_t\)</span> that occurs during initial stages.</li>
</ul></li>
<li><strong>Formula:</strong>
<ul>
<li><span class="math inline">\(m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t\)</span></li>
<li><span class="math inline">\(v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2\)</span></li>
<li><span class="math inline">\(\hat{m_t} = m_t / (1 - \beta_1^t)\)</span></li>
<li><span class="math inline">\(\hat{v_t} = v_t / (1 - \beta_2^t)\)</span></li>
<li><span class="math inline">\(w_{t+1} = w_t - \eta \hat{m_t} / (\sqrt{\hat{v_t}} + \epsilon)\)</span></li>
</ul></li>
<li><strong>Role of <span class="math inline">\(\beta_1\)</span>, <span class="math inline">\(\beta_2\)</span>:</strong>
<ul>
<li><span class="math inline">\(\beta_1\)</span>: Controls the exponential decay rate for the first moment (momentum). (Typically 0.9)</li>
<li><span class="math inline">\(\beta_2\)</span>: Controls the exponential decay rate for the second moment (RMSProp). (Typically 0.999)</li>
</ul></li>
</ul></li>
<li><p><strong>Batch Normalization and Initialization:</strong></p>
<ul>
<li>Batch normalization normalizes the input of each layer, which can improve the stability and speed of training.</li>
<li>Initialization methods like Kaiming and Xavier are designed to work well with batch normalization by initializing weights in a way that maintains the variance of inputs as they flow through the network.</li>
</ul></li>
</ol>
<ul>
<li><strong>Batch Normalization:</strong> normalizes the input of each mini-batch to have a mean of 0 and a variance of 1.</li>
<li><strong>Reducing the Impact of Initialization:</strong> batch normalization reduces the internal covariate shift in the network, lowering the dependence on the initial weight distribution.</li>
<li><strong>Reason:</strong> normalized inputs place the activation functions in an appropriate range (e.g., the positive region of ReLU), mitigating the vanishing/exploding gradient problem and stabilizing learning.</li>
</ul>
<ol start="3" type="1">
<li><strong>Gaussian Loss Surface:</strong>
<ul>
<li><strong>Amplitude (A):</strong> adjusts the overall size of the loss function; a large amplitude can make learning unstable due to large variations in loss values.</li>
<li><strong>Center (<span class="math inline">\(x_0\)</span>, <span class="math inline">\(y_0\)</span>):</strong> determines the location of the minimum value of the loss function, which the optimization algorithm moves towards.</li>
<li><strong>Variance (<span class="math inline">\(\sigma_1\)</span>, <span class="math inline">\(\sigma_2\)</span>):</strong> represents the degree of change of the loss function in each axis direction; small variance results in a narrow and pointed shape, while large variance gives a broad and gentle shape. Different variances require adjusting the learning speed differently for each direction.</li>
</ul></li>
</ol>
</section>
<section id="advanced-problems-1" class="level4">
<h4 class="anchored" data-anchor-id="advanced-problems-1">Advanced Problems</h4>
<ol type="1">
<li><p><strong>Lion Optimizer Analysis:</strong></p>
<ul>
<li><strong>Core Idea:</strong> performs updates using only the sign of the gradient.</li>
<li><strong>Formula:</strong> <code>c_t = β_1 * m_{t-1} + (1 - β_1) * g_t     w_{t+1} = w_t - η * sign(c_t)     m_t = c_t</code>
<ul>
<li>only uses the sign of updates, so it doesn’t need to calculate and store the 2nd moment like Adam</li>
</ul></li>
<li><strong>Advantages:</strong>
<ul>
<li>uses less memory than Adam (doesn’t store 2nd moments)</li>
<li>update size is uniform for all parameters, making it robust to sparse gradients</li>
</ul></li>
<li><strong>Disadvantages:</strong>
<ul>
<li>ignores gradient magnitude information, potentially leading to slower convergence than Adam in certain situations</li>
<li>may be more sensitive to learning rate tuning than Adam</li>
</ul></li>
</ul></li>
<li><p><strong>Initialization Method Experiment:</strong></p>
<ul>
<li><strong>Experiment Design:</strong>
<ul>
<li>uses the same model (SimpleNetwork) and dataset (FashionMNIST)</li>
<li>applies LeCun, Xavier, Kaiming, and Orthogonal initializations</li>
<li>uses the same optimization algorithm (e.g., Adam) and learning rate</li>
<li>trains for a sufficient number of epochs (e.g., 20) and records evaluation metrics (error rate, convergence speed, average condition number, spectral norm, effective rank) at each epoch</li>
</ul></li>
<li><strong>Result Analysis:</strong>
<ul>
<li>when using ReLU activation functions, Kaiming initialization is likely to perform best</li>
<li>Orthogonal initialization may perform well in RNN/LSTM models</li>
<li>Xavier initialization may perform well with tanh and sigmoid activation functions</li>
<li>LeCun initialization may perform poorly in modern networks</li>
</ul></li>
</ul></li>
<li><p><strong>Optimization Path Visualization:</strong></p></li>
</ol>
<ul>
<li><strong>Defining one’s own loss function:</strong></li>
<li>Example: <span class="math inline">\(f(x, y) = (x^2 + y - 11)^2 + (x + y^2 - 7)^2\)</span> (Himmelblau function, multi-modal function)</li>
<li>Example: <span class="math inline">\(f(x, y) = 0.5x^2 - 0.25y^2 + 3\)</span> (non-convex function with a saddle point)</li>
<li><strong>Choosing an optimization algorithm:</strong> SGD, Momentum(SGD with momentum), Adam, Lion</li>
<li><strong>Visualization:</strong> Refer to the code in Section 5.5 to visualize the optimization path of each optimizer on a 2D plane.</li>
<li><strong>Result analysis:</strong>
<ul>
<li>SGD is likely to fall into local minima/saddle points.</li>
<li>Momentum can escape local minima through inertia, but may oscillate.</li>
<li>Adam can reach the optimal point more efficiently thanks to its adaptive learning rate.</li>
<li>Lion may show similar or faster convergence to Adam, but can be sensitive to learning rate tuning.</li>
<li>Compare and analyze optimization results based on the shape of the loss function, such as multi-modality and saddle points.</li>
</ul></li>
</ul>
</section>
</section>
</div>
</div>
</section>
</section>
<section id="reference-materials" class="level3">
<h3 class="anchored" data-anchor-id="reference-materials">Reference Materials</h3>
<ol type="1">
<li><strong><a href="https://arxiv.org/abs/1609.04747">An overview of gradient descent optimization algorithms</a></strong> (Sebastian Ruder, 2016) - An excellent overview paper on deep learning optimization algorithms. It compares and analyzes various algorithms such as SGD, Momentum, AdaGrad, RMSProp, and Adam.</li>
<li><strong><a href="https://arxiv.org/abs/1712.09913">Visualizing the Loss Landscape of Neural Nets</a></strong> (Li et al., 2018) - A pioneering paper on loss surface visualization. It shows how residual connections can flatten the loss surface.</li>
<li><strong><a href="https://www.google.com/search?q=https://ruder.io/deep-learning-optimization-2023/">Optimization for Deep Learning Highlights in 2023</a></strong> (Sebastian Ruder, 2023) - A blog post summarizing the key points of deep learning optimization in 2023. It is useful for understanding the latest research trends.</li>
<li><strong><a href="https://arxiv.org/abs/2110.08536">Improving Deep Learning with Better Initialization</a></strong> (Mishkin &amp; Matas, 2021) - A paper on modern research trends in initialization. It compares various initialization methods and provides practical guidelines.</li>
<li><strong><a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://arxiv.org/abs/2302.06675">Symbolic Discovery of Optimization Algorithms</a></strong> (Chen et al.&nbsp;2023) - A paper on the Lion algorithm discovered by Google Brain.</li>
<li><strong><a href="https://arxiv.org/abs/1912.07145">PyHessian: Neural Networks Through the Lens of the Hessian</a></strong> (Yao et al., 2020) - A paper on PyHessian, a tool for analyzing loss surfaces using Hessian matrices.</li>
<li><strong><a href="https://arxiv.org/abs/1705.08292">The Marginal Value of Adaptive Gradient Methods in Machine Learning</a></strong> (Wilson et al., 2017) - A paper showing that adaptive learning rate methods (such as Adam) are not always better than SGD.</li>
<li><strong><a href="https://www.offconvex.org/2016/03/22/saddlepoints/">How to escape saddle points efficiently</a></strong> (Ge et al., 2015) - A blog post explaining how to efficiently escape saddle points using perturbed gradient descent.</li>
<li><strong><a href="https://www.google.com/search?q=https://openreview.net/forum%3Fid%3DFpgg9h-xO_a">Deep Understanding of Modern Initialization Methods with Block Diagonal Matrices</a></strong> (Huang et al., 2021) - A paper analyzing initialization methods using block diagonal matrices.</li>
<li><strong><a href="https://www.google.com/search?q=https://proceedings.neurips.cc/paper/2020/hash/9c838d2e45b2ad1094d42f4ef36764f6-Abstract.html">AdaHessian: An Adaptive Second Order Optimizer for Machine Learning</a></strong> (Yao et al., 2020) - A paper on AdaHessian, an optimizer that uses the diagonal elements of Hessian matrices to leverage second-order information.</li>
<li><strong><a href="https://arxiv.org/abs/2502.00894">A Closer Look at Smoothness in Deep Learning: A Tensor Decomposition Approach</a></strong> (Defazio &amp; Bottou, 2025) - A paper analyzing the smoothness of deep learning models using tensor decomposition.</li>
<li><strong><a href="https://arxiv.org/abs/2502.00894">Understanding Measures of Efficiency for Stochastic Optimization</a></strong> (Defazio &amp; Bottou, 2025) - A paper proposing methods to measure the efficiency of stochastic optimization algorithms.</li>
<li><strong><a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://www.deeplearningbook.org/">Deep Learning</a></strong> (Goodfellow, Bengio, Courville, 2016) - A deep learning textbook that covers initialization and optimization in chapters 6 (Deep Feedforward Networks) and 8 (Optimization for Training Deep Models).</li>
<li><strong><a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://cs231n.github.io/optimization-1/">Stanford CS231n: Convolutional Neural Networks for Visual Recognition</a></strong> - A Stanford University deep learning course that covers optimization in the Optimization section.</li>
<li><strong><a href="https://www.google.com/search?q=https://paperswithcode.com/methods/category/optimization-methods">Papers with Code - Optimization Methods</a></strong> - A website that collects recent papers on optimization methods.</li>
<li><strong><a href="https://arxiv.org/abs/1609.04747">An overview of gradient descent optimization algorithms</a></strong> (Sebastian Ruder, 2016) - This is an excellent overview paper on deep learning optimization algorithms. It compares and analyzes various algorithms such as SGD, Momentum, AdaGrad, RMSProp, and Adam.</li>
<li><strong><a href="https://arxiv.org/abs/1712.09913">Visualizing the Loss Landscape of Neural Nets</a></strong> (Li et al., 2018) - This is a pioneering paper on visualizing loss surfaces. It shows how residual connections flatten the loss surface.</li>
<li><strong><a href="https://www.google.com/search?q=https://ruder.io/deep-learning-optimization-2023/">Optimization for Deep Learning Highlights in 2023</a></strong> (Sebastian Ruder, 2023) - This is a blog post summarizing the main points of deep learning optimization in 2023. It is useful for understanding the latest research trends.</li>
<li><strong><a href="https://arxiv.org/abs/2110.08536">Improving Deep Learning with Better Initialization</a></strong> (Mishkin &amp; Matas, 2021) - This paper presents modern research trends on initialization. It compares various initialization methods and provides practical guidelines.</li>
<li><strong><a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://arxiv.org/abs/2302.06675">Symbolic Discovery of Optimization Algorithms</a></strong> (Chen et al.&nbsp;2023) - This paper is about the Lion algorithm discovered by Google Brain.</li>
<li><strong><a href="https://arxiv.org/abs/1912.07145">PyHessian: Neural Networks Through the Lens of the Hessian</a></strong> (Yao et al., 2020) - This paper is about PyHessian, a tool for analyzing loss surfaces using the Hessian matrix.</li>
<li><strong><a href="https://arxiv.org/abs/1705.08292">The Marginal Value of Adaptive Gradient Methods in Machine Learning</a></strong> (Wilson et al., 2017) - This paper shows that adaptive learning rate methods (such as Adam) are not always better than SGD.</li>
<li><strong><a href="https://www.offconvex.org/2016/03/22/saddlepoints/">How to escape saddle points efficiently</a></strong> (Ge et al., 2015) - This blog post explains how to efficiently escape saddle points using perturbed gradient descent.</li>
<li><strong><a href="https://www.google.com/search?q=https://openreview.net/forum%3Fid%3DFpgg9h-xO_a">Deep Understanding of Modern Initialization Methods with Block Diagonal Matrices</a></strong> (Huang et al., 2021) - This paper analyzes initialization methods using block diagonal matrices.</li>
<li><strong><a href="https://www.google.com/search?q=https://proceedings.neurips.cc/paper/2020/hash/9c838d2e45b2ad1094d42f4ef36764f6-Abstract.html">AdaHessian: An Adaptive Second Order Optimizer for Machine Learning</a></strong> (Yao et al., 2020) - a paper on the AdaHessian optimizer that uses the diagonal elements of the Hessian matrix to utilize second-order information.</li>
<li><strong><a href="https://arxiv.org/abs/2411.01593">A Closer Look at Smoothness in Deep Learning: A Tensor Decomposition Approach</a></strong> (Li et al., 2024) - a paper analyzing the smoothness of deep learning models using tensor decomposition.</li>
<li><strong><a href="https://arxiv.org/abs/2502.00894">Understanding Measures of Efficiency for Stochastic Optimization</a></strong> (Defazio &amp; Bottou, 2025) - a paper proposing methods to measure the efficiency of stochastic optimization algorithms.</li>
<li><strong><a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://www.deeplearningbook.org/">Deep Learning</a></strong> (Goodfellow, Bengio, Courville, 2016) - a deep learning textbook that covers initialization and optimization-related content in chapters 6 (Deep Feedforward Networks) and 8 (Optimization for Training Deep Models).</li>
<li><strong><a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://cs231n.github.io/optimization-1/">Stanford CS231n: Convolutional Neural Networks for Visual Recognition</a></strong> - a Stanford University deep learning course that covers optimization-related content in the Optimization part.</li>
<li><strong><a href="https://www.google.com/search?q=https://paperswithcode.com/methods/category/optimization-methods">Papers with Code - Optimization Methods</a></strong> - a website collecting recent papers on optimization methods.</li>
</ol>


</section>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>