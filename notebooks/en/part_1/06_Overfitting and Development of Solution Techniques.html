<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>overfitting-and-development-of-solution-techniques – Deep Learning DNA: Surviving Architectures and Essential Principles</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap-f507c7d0488cb7630e20aad62ad8c2aa.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script>window.MathJax = {loader: {load: ['[tex]/boldsymbol']},tex: {packages: {'[+]': ['boldsymbol']}}};</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../../notebooks/en/part_1/01_The Beginning of Deep Learning.html">part_1</a></li><li class="breadcrumb-item"><a href="../../../notebooks/en/part_1/06_Overfitting and Development of Solution Techniques.html">6. Overfitting and Development of Solution Techniques</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../../../">English</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Language</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index_de.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Deutsch</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index_en.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">English</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index_es.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Español</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">한국어</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index_zh.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">中文</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/en/part_1/00_Introduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">part_1</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/en/part_1/01_The Beginning of Deep Learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">1. The Beginning of Deep Learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/en/part_1/02_Mathematics of Deep Learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">2. Mathematics of Deep Learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/en/part_1/03_Deep Learning Framework.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">3. Deep Learning Framework</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/en/part_1/04_Activation Function.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">4. Activation Function</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/en/part_1/05_Optimization and Visualization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">5. Optimization and Visualization</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/en/part_1/06_Overfitting and Development of Solution Techniques.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">6. Overfitting and Development of Solution Techniques</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/en/part_1/07_Evolution of Convolutional Neural Networks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">7. Evolution of Convolutional Neural Networks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/en/part_1/08_The Birth of Transformer.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">8. The Birth of Transformer</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/en/part_1/09_The Evolution of Transformers.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">9. The Evolution of Transformers</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/en/part_1/10_Multimodal Deep Learning: The Beginning of Multisensory Convergence.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">10. Multimodal Deep Learning: The Beginning of Multisensory Convergence</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/en/part_1/11_Multimodal Deep Learning: Intelligence Beyond Limits.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">11. Multimodal Deep Learning: Intelligence Beyond Limits</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Deep Learning Frontier</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/en/Deep Learning Frontier/01_SLM: Small but Powerful Language Model.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">1. SLM: Small but Powerful Language Model</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/en/Deep Learning Frontier/02_Autonomous Driving.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">2. Autonomous Driving</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#chapter-6-overfitting-and-advances-in-mitigation-techniques" id="toc-chapter-6-overfitting-and-advances-in-mitigation-techniques" class="nav-link active" data-scroll-target="#chapter-6-overfitting-and-advances-in-mitigation-techniques">Chapter 6 Overfitting and Advances in Mitigation Techniques</a>
  <ul class="collapse">
  <li><a href="#understanding-overfitting" id="toc-understanding-overfitting" class="nav-link" data-scroll-target="#understanding-overfitting">6.1 Understanding Overfitting</a></li>
  <li><a href="#regularization-techniques" id="toc-regularization-techniques" class="nav-link" data-scroll-target="#regularization-techniques">6.2 Regularization Techniques</a>
  <ul class="collapse">
  <li><a href="#l1-l2-regularization" id="toc-l1-l2-regularization" class="nav-link" data-scroll-target="#l1-l2-regularization">6.2.1 L1, L2 Regularization</a></li>
  <li><a href="#applying-l1-l2-regularization-in-pytorch" id="toc-applying-l1-l2-regularization-in-pytorch" class="nav-link" data-scroll-target="#applying-l1-l2-regularization-in-pytorch">6.2.2 Applying L1, L2 Regularization in PyTorch</a></li>
  <li><a href="#analyzing-the-regularization-effect-on-the-loss-surface" id="toc-analyzing-the-regularization-effect-on-the-loss-surface" class="nav-link" data-scroll-target="#analyzing-the-regularization-effect-on-the-loss-surface">6.2.3 Analyzing the Regularization Effect on the Loss Surface</a></li>
  </ul></li>
  <li><a href="#dropout" id="toc-dropout" class="nav-link" data-scroll-target="#dropout">6.3 Dropout</a>
  <ul class="collapse">
  <li><a href="#principle-of-dropout" id="toc-principle-of-dropout" class="nav-link" data-scroll-target="#principle-of-dropout">6.3.1 Principle of Dropout</a></li>
  <li><a href="#implementing-dropout-in-pytorch" id="toc-implementing-dropout-in-pytorch" class="nav-link" data-scroll-target="#implementing-dropout-in-pytorch">6.3.2 Implementing Dropout in PyTorch</a></li>
  </ul></li>
  <li><a href="#batch-normalization" id="toc-batch-normalization" class="nav-link" data-scroll-target="#batch-normalization">6.4 Batch Normalization</a>
  <ul class="collapse">
  <li><a href="#concept-and-effect-of-batch-normalization" id="toc-concept-and-effect-of-batch-normalization" class="nav-link" data-scroll-target="#concept-and-effect-of-batch-normalization">6.4.1 Concept and Effect of Batch Normalization</a></li>
  <li><a href="#batch-normalization-implementation-in-pytorch" id="toc-batch-normalization-implementation-in-pytorch" class="nav-link" data-scroll-target="#batch-normalization-implementation-in-pytorch">6.4.2 Batch Normalization Implementation in PyTorch</a></li>
  <li><a href="#statistical-tracking-and-inference-application" id="toc-statistical-tracking-and-inference-application" class="nav-link" data-scroll-target="#statistical-tracking-and-inference-application">6.4.3 Statistical Tracking and Inference Application</a></li>
  </ul></li>
  <li><a href="#hyperparameter-optimization" id="toc-hyperparameter-optimization" class="nav-link" data-scroll-target="#hyperparameter-optimization">6.5 Hyperparameter Optimization</a>
  <ul class="collapse">
  <li><a href="#comparison-of-optimization-methodologies" id="toc-comparison-of-optimization-methodologies" class="nav-link" data-scroll-target="#comparison-of-optimization-methodologies">6.5.1 Comparison of Optimization Methodologies</a></li>
  <li><a href="#optimization-using-bayes-opt" id="toc-optimization-using-bayes-opt" class="nav-link" data-scroll-target="#optimization-using-bayes-opt">6.5.2 Optimization Using Bayes-Opt</a></li>
  <li><a href="#optimization-using-botorch" id="toc-optimization-using-botorch" class="nav-link" data-scroll-target="#optimization-using-botorch">6.5.3 Optimization using BoTorch</a></li>
  </ul></li>
  <li><a href="#gaussian-process" id="toc-gaussian-process" class="nav-link" data-scroll-target="#gaussian-process">6.6 Gaussian Process</a>
  <ul class="collapse">
  <li><a href="#mathematical-foundations-of-uncertainty-handling" id="toc-mathematical-foundations-of-uncertainty-handling" class="nav-link" data-scroll-target="#mathematical-foundations-of-uncertainty-handling">6.6.1 Mathematical Foundations of Uncertainty Handling</a></li>
  <li><a href="#modern-applications" id="toc-modern-applications" class="nav-link" data-scroll-target="#modern-applications">6.6.2 Modern Applications</a></li>
  <li><a href="#deep-kernel-learning" id="toc-deep-kernel-learning" class="nav-link" data-scroll-target="#deep-kernel-learning">6.6.3 Deep Kernel Learning</a></li>
  </ul></li>
  <li><a href="#practice-problems" id="toc-practice-problems" class="nav-link" data-scroll-target="#practice-problems">Practice Problems</a>
  <ul class="collapse">
  <li><a href="#basic-problems" id="toc-basic-problems" class="nav-link" data-scroll-target="#basic-problems">Basic Problems</a></li>
  <li><a href="#application-problems" id="toc-application-problems" class="nav-link" data-scroll-target="#application-problems">Application Problems</a></li>
  <li><a href="#advanced-problems" id="toc-advanced-problems" class="nav-link" data-scroll-target="#advanced-problems">Advanced Problems</a></li>
  </ul></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../../notebooks/en/part_1/01_The Beginning of Deep Learning.html">part_1</a></li><li class="breadcrumb-item"><a href="../../../notebooks/en/part_1/06_Overfitting and Development of Solution Techniques.html">6. Overfitting and Development of Solution Techniques</a></li></ol></nav></header>




<p><a href="https://colab.research.google.com/github/Quantum-Intelligence-Frontier/dldna/blob/main/notebooks/en/part_1/06_Overfitting_and_Advances_in_Techniques.ipynb" target="_parent"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"> </a></p>
<section id="chapter-6-overfitting-and-advances-in-mitigation-techniques" class="level1">
<h1>Chapter 6 Overfitting and Advances in Mitigation Techniques</h1>
<blockquote class="blockquote">
<p>“Simplicity is the ultimate sophistication.” - Leonardo da Vinci</p>
</blockquote>
<p>Deep learning models have the powerful ability to express complex functions through numerous parameters. However, this ability can sometimes be a <em>double-edged sword</em>. When a model is overly fitted to the training data, the predictive performance on new data may actually decrease due to the <strong>overfitting</strong> phenomenon.</p>
<p>Since the resurgence of the backpropagation algorithm in 1986, overfitting has been a relentless challenge for deep learning researchers. Initially, they responded to overfitting by simply reducing the model size or increasing the training data. However, these methods were limited by their restriction on the model’s expressiveness or the difficulty of data collection. The emergence of AlexNet in 2012 opened a new era for deep learning but also highlighted the severity of the overfitting problem. AlexNet had many more parameters than previous models, increasing the risk of overfitting. As the scale of deep learning models increased exponentially thereafter, the overfitting problem became a core challenge in deep learning research.</p>
<p>In this chapter, we will explore the nature of overfitting and various techniques that have evolved to address it. Just as explorers navigate uncharted territories and create maps, deep learning researchers have continuously explored and developed new methods to overcome the obstacle of overfitting.</p>
<section id="understanding-overfitting" class="level2">
<h2 class="anchored" data-anchor-id="understanding-overfitting">6.1 Understanding Overfitting</h2>
<p>Overfitting was first mentioned in William Hopkins’ writings in 1670, but its modern meaning began with a mention in the <em>Quarterly Review of Biology</em> in 1935: “Analyzing six variables with 13 observations seems like overfitting.” It was formally studied in statistics starting in the 1950s, particularly in the context of time series analysis in the 1952 paper “Tests of Fit in Time Series.”</p>
<p>In deep learning, the overfitting problem entered a new phase with the emergence of AlexNet in 2012. AlexNet was a large-scale neural network with approximately 60 million parameters, unprecedented in scale compared to previous models. As the size of deep learning models increased exponentially thereafter, the overfitting problem became more severe. For example, modern large language models (LLMs) have billions of parameters, making overfitting prevention a core challenge in model design.</p>
<p>In response to these challenges, innovative solutions such as dropout (2014) and batch normalization (2015) were proposed. More recently, methods utilizing training history for overfitting detection and prevention (2024) are being researched. Especially in large models, various strategies are used in combination, from traditional methods like early stopping to modern techniques like ensemble learning and data augmentation.</p>
<p>Let’s intuitively understand the overfitting phenomenon through a simple example. We’ll apply polynomials of different degrees to noisy sine function data.</p>
<div id="cell-2" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip install dldna[colab] <span class="co"># in Colab</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="co"># !pip install dldna[all] # in your local</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>load_ext autoreload</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>autoreload <span class="dv">2</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="cell-3" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Noisy sin graph</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> real_func(x):</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> np.sin(x) <span class="op">+</span> np.random.uniform(<span class="op">-</span><span class="fl">0.2</span>, <span class="fl">0.2</span>, <span class="bu">len</span>(x))</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> y</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Create x data from 40 to 320 degrees.  Use a step value to avoid making it too dense.</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.array([np.pi<span class="op">/</span><span class="dv">180</span> <span class="op">*</span> i <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">40</span>, <span class="dv">320</span>, <span class="dv">4</span>)])</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> real_func(x)</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>sns.scatterplot(x<span class="op">=</span>x, y<span class="op">=</span>y, label<span class="op">=</span><span class="st">'real function'</span>)</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot with 1st, 3rd, and 21th degree polynomials.  </span></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> deg <span class="kw">in</span> [<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">21</span>]:  </span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the coefficients for the corresponding degree using polyfit, and create the estimated function using poly1d.</span></span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>    params <span class="op">=</span> np.polyfit(x, y, deg) <span class="co"># Get the parameter values</span></span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>    <span class="co"># print(f" {deg} params = {params}")</span></span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>    p <span class="op">=</span> np.poly1d(params) <span class="co"># Get the line function</span></span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>    sns.lineplot(x<span class="op">=</span>x, y<span class="op">=</span>p(x), color<span class="op">=</span><span class="ss">f"C</span><span class="sc">{</span>deg<span class="sc">}</span><span class="ss">"</span>, label<span class="op">=</span><span class="ss">f"deg = </span><span class="sc">{</span>deg<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>The autoreload extension is already loaded. To reload it, use:
  %reload_ext autoreload</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_1362795/2136320363.py:25: RankWarning: Polyfit may be poorly conditioned
  params = np.polyfit(x, y, deg) # Get the parameter values</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="06_Overfitting and Development of Solution Techniques_files/figure-html/cell-3-output-3.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The code above generates noisy sine function data and fits it with 1st, 3rd, and 21st degree polynomials as an example.</p>
<ul>
<li><p><strong>1st degree function (deg = 1):</strong> The result is a simple line that does not follow the overall trend of the data. This shows a state of <em>underfitting</em>, where the model fails to capture the complexity of the data.</p></li>
<li><p><strong>3rd degree function (deg = 3):</strong> It follows the basic pattern of the data relatively well, forming a smooth curve without being overly affected by noise.</p></li>
<li><p><strong>21st degree function (deg = 21):</strong> The result shows <em>overfitting</em>, where the model follows the training data too closely, including its noise, and becomes overly optimized for only the training data.</p></li>
</ul>
<p>Thus, if the complexity of the model (the degree of the polynomial in this case) is too low, underfitting occurs; if it’s too high, overfitting occurs. What we ultimately want to find is a model that generalizes well not just to the training data but also to new data - that is, an approximation function closest to the actual sine function.</p>
<p>Overfitting occurs when the complexity (or capacity) of the model is relatively high compared to the amount of training data. Neural networks are especially prone to overfitting because they have many parameters and high expressiveness. Overfitting can also occur when there’s not enough training data or when the data contains a lot of noise. The characteristics of overfitting include:</p>
<ul>
<li><strong>Training data loss</strong> continues to decrease.</li>
<li><strong>Validation data loss</strong> initially decreases, then starts to increase at some point.</li>
<li>This is because the model learns not just the underlying pattern but also the noise and fine details of the training data, becoming overly specialized in it.</li>
</ul>
<p>As a result, an overfitted model performs well on the training data but shows poor prediction performance on new, unseen data. To prevent such overfitting, we’ll explore various techniques like L1/L2 regularization, dropout, and batch normalization in more detail later.</p>
</section>
<section id="regularization-techniques" class="level2">
<h2 class="anchored" data-anchor-id="regularization-techniques">6.2 Regularization Techniques</h2>
<blockquote class="blockquote">
<p><strong>Challenge:</strong> What methods can effectively control the complexity of a model while improving its generalization performance?</p>
<p><strong>Researcher’s Dilemma:</strong> Reducing the size of a model to prevent overfitting may limit its expressiveness, and simply increasing the training data is not always possible. It was necessary to impose constraints on the model’s structure or learning process to prevent excessive optimization for the training data and improve predictive performance for new data.</p>
</blockquote>
<section id="l1-l2-regularization" class="level3">
<h3 class="anchored" data-anchor-id="l1-l2-regularization">6.2.1 L1, L2 Regularization</h3>
<p>In neural networks, L1 and L2 regularization are commonly used. L1 refers to Lasso regression, while L2 refers to Ridge regression.</p>
<p>Ridge regression and Lasso regression introduce a penalty term to restrict parameter movement. The differences between the two methods can be summarized in the following table:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 27%">
<col style="width: 38%">
<col style="width: 34%">
</colgroup>
<thead>
<tr class="header">
<th>Feature</th>
<th>Ridge Regression</th>
<th>Lasso Regression</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Penalty Type</td>
<td>Imposes an L2 penalty. The penalty term is the sum of the squares of the parameters multiplied by alpha.</td>
<td>Imposes an L1 penalty. The penalty term is the sum of the absolute values of the parameters multiplied by alpha.</td>
</tr>
<tr class="even">
<td>Parameter Impact</td>
<td>Suppresses large parameters, making them close to 0, but not exactly 0</td>
<td>Can set some parameter values to 0 when alpha is large, resulting in a simpler model</td>
</tr>
<tr class="odd">
<td>Overall Impact</td>
<td>Preserves all parameters, including those with minor effects</td>
<td>Selectively retains relevant parameters, allowing for a more straightforward explanation of complex models.</td>
</tr>
<tr class="even">
<td>Optimization Characteristics</td>
<td>Less sensitive to ideal values compared to Lasso</td>
<td>Sensitive to ideal values due to the absolute value penalty term</td>
</tr>
</tbody>
</table>
<p>The equations can be expressed as follows:</p>
<ul>
<li><p>Ridge Regression Objective Function</p>
<p>“Modified Ridge Objective Function” = (Unmodified Linear Regression Function) + <span class="math inline">\(\alpha \cdot \sum (\text{parameter})^2\)</span></p>
<p><span class="math inline">\(f_{\beta} = \sum_{i=1}^{M} (y_i - \hat{y}_i)^2 + \alpha \sum_{j} \beta_{j}^2\)</span></p>
<p>Here, <span class="math inline">\(\beta\)</span> is the parameter vector being sought. <span class="math inline">\(\alpha \sum_{j} \beta_{j}^2\)</span> is the penalty term or regularization term. <span class="math inline">\(\alpha\)</span> is a hyperparameter that controls the size of the regularization term. The formula for obtaining the parameters is as follows.</p>
<p><span class="math inline">\(\beta = \underset{\beta}{\operatorname{argmin}} \left( \sum_{i=1}^{M} (y_i - \hat{y}_i)^2 + \alpha \sum_{j} \beta_{j}^2 \right)\)</span></p></li>
<li><p>Lasso Regression Objective Function</p>
<p>“Modified Lasso Objective Function” = (Unmodified Linear Regression Function) + $ || $</p>
<p><span class="math inline">\(f_{\beta} = \sum_{i=1}^{M} (y_i - \hat{y}_i)^2 + \alpha \sum_{j} |\beta_{j}|\)</span> <span class="math inline">\(\beta = \underset{\beta}{\operatorname{argmin}} \left( \sum_{i=1}^{M} (y_i - \hat{y}_i)^2 + \alpha \sum_{j} |\beta_j| \right)\)</span> Using L2 with the sum of the squares of parameters as a penalty term is often referred to as weight decay in neural networks. Let’s take a look at how ridge (L2) regression differs from simple linear regression using sklearn’s implemented model. To do this, we need to increase the dimension of the input x data to the degree. We will use the following simple utility function to create it.</p></li>
</ul>
<div id="cell-6" class="cell" data-execution_count="15">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_x_powered(x, p<span class="op">=</span><span class="dv">1</span>):</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    size <span class="op">=</span> <span class="bu">len</span>(x)</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># The shape of the created x will be (data size, degree)</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>    new_x <span class="op">=</span> np.zeros((size, p))</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> s <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(x)): <span class="co"># Iterate over data size</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> d <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, p<span class="op">+</span><span class="dv">1</span>): <span class="co"># Iterate over degrees</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>            new_x[s][d<span class="op">-</span><span class="dv">1</span>] <span class="op">=</span> x[s]<span class="op">**</span>d <span class="co"># Raise x to the power of the degree.</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> new_x</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Let's take a quick look at how it works.</span></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>deg <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.array([np.pi<span class="op">/</span><span class="dv">180</span> <span class="op">*</span> i <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">20</span>, <span class="dv">35</span>, <span class="dv">5</span>)])</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> real_func(x)  <span class="co"># real_func는 이전 코드에 정의되어 있다고 가정</span></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"x = </span><span class="sc">{</span>x<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>new_x <span class="op">=</span> get_x_powered(x, p<span class="op">=</span>deg)</span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"new_x = </span><span class="sc">{</span>new_x<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>x = [0.34906585 0.43633231 0.52359878]
new_x = [[0.34906585 0.12184697 0.04253262]
 [0.43633231 0.19038589 0.08307151]
 [0.52359878 0.27415568 0.14354758]]</code></pre>
</div>
</div>
<p>Because it is third-order, the value of <span class="math inline">\(x\)</span> increases to <span class="math inline">\(x^2, x^3\)</span>. For example, 0.3490, 0.1218 (the square of 0.3490), and 0.04253 (the cube of 0.3490) are examples. If it is tenth-order, data up to <span class="math inline">\(x^{10}\)</span> is created. The alpha value of the penalty term can have a value from 0 to infinity. As the alpha value increases, the regulation intensity becomes larger. With the degree fixed at 13, we will compare the linear regression function and ridge regression by changing the alpha value.</p>
<div id="cell-8" class="cell" data-execution_count="18">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> Ridge</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> PolynomialFeatures</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.pipeline <span class="im">import</span> make_pipeline</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a noisy sine wave (increased noise)</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> real_func(x):</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.sin(x) <span class="op">+</span> np.random.normal(<span class="dv">0</span>, <span class="fl">0.4</span>, <span class="bu">len</span>(x))  <span class="co"># Increased noise</span></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Create x data (narrower range)</span></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.array([np.pi <span class="op">/</span> <span class="dv">180</span> <span class="op">*</span> i <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">40</span>, <span class="dv">280</span>, <span class="dv">8</span>)])  <span class="co"># Narrower range, larger step</span></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> real_func(x)</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Degree of the polynomial</span></span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>deg <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a><span class="co"># List of alpha values to compare (adjusted)</span></span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>alpha_list <span class="op">=</span> [<span class="fl">0.0</span>, <span class="fl">0.1</span>, <span class="dv">10</span>]  <span class="co"># Adjusted alpha values</span></span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a>cols <span class="op">=</span> <span class="bu">len</span>(alpha_list)</span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a>fig, axes_list <span class="op">=</span> plt.subplots(<span class="dv">1</span>, cols, figsize<span class="op">=</span>(<span class="dv">20</span>, <span class="dv">5</span>))  <span class="co"># Adjusted figure size</span></span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, alpha <span class="kw">in</span> <span class="bu">enumerate</span>(alpha_list):</span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a>    axes <span class="op">=</span> axes_list[i]</span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Plot the original data</span></span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a>    sns.scatterplot(ax<span class="op">=</span>axes, x<span class="op">=</span>x, y<span class="op">=</span>y, label<span class="op">=</span><span class="st">'real function'</span>, s<span class="op">=</span><span class="dv">50</span>)  <span class="co"># Increased marker size</span></span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-32"><a href="#cb7-32" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Plot linear regression</span></span>
<span id="cb7-33"><a href="#cb7-33" aria-hidden="true" tabindex="-1"></a>    params <span class="op">=</span> np.polyfit(x, y, deg)</span>
<span id="cb7-34"><a href="#cb7-34" aria-hidden="true" tabindex="-1"></a>    p <span class="op">=</span> np.poly1d(params)</span>
<span id="cb7-35"><a href="#cb7-35" aria-hidden="true" tabindex="-1"></a>    sns.lineplot(ax<span class="op">=</span>axes, x<span class="op">=</span>x, y<span class="op">=</span>p(x), label<span class="op">=</span><span class="ss">f"LR deg = </span><span class="sc">{</span>deg<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb7-36"><a href="#cb7-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-37"><a href="#cb7-37" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Ridge regression (using Pipeline, solver='auto')</span></span>
<span id="cb7-38"><a href="#cb7-38" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> make_pipeline(PolynomialFeatures(degree<span class="op">=</span>deg), Ridge(alpha<span class="op">=</span>alpha, solver<span class="op">=</span><span class="st">'auto'</span>))</span>
<span id="cb7-39"><a href="#cb7-39" aria-hidden="true" tabindex="-1"></a>    model.fit(x.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>), y)  <span class="co"># Reshape x for pipeline</span></span>
<span id="cb7-40"><a href="#cb7-40" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> model.predict(x.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)) <span class="co"># Reshape x for prediction</span></span>
<span id="cb7-41"><a href="#cb7-41" aria-hidden="true" tabindex="-1"></a>    sns.lineplot(ax<span class="op">=</span>axes, x<span class="op">=</span>x, y<span class="op">=</span>y_pred, label<span class="op">=</span><span class="ss">f"Ridge alpha=</span><span class="sc">{</span>alpha<span class="sc">:0.1e}</span><span class="ss"> deg=</span><span class="sc">{</span>deg<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb7-42"><a href="#cb7-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-43"><a href="#cb7-43" aria-hidden="true" tabindex="-1"></a>    axes.set_title(<span class="ss">f"Alpha = </span><span class="sc">{</span>alpha<span class="sc">:0.1e}</span><span class="ss">"</span>)</span>
<span id="cb7-44"><a href="#cb7-44" aria-hidden="true" tabindex="-1"></a>    axes.set_ylim(<span class="op">-</span><span class="fl">1.5</span>, <span class="fl">1.5</span>)  <span class="co"># Limit y-axis range</span></span>
<span id="cb7-45"><a href="#cb7-45" aria-hidden="true" tabindex="-1"></a>    axes.legend()</span>
<span id="cb7-46"><a href="#cb7-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-47"><a href="#cb7-47" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb7-48"><a href="#cb7-48" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="06_Overfitting and Development of Solution Techniques_files/figure-html/cell-5-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The above graph is the result of fitting a 10th degree polynomial to noisy sine function data, showing Ridge regression results with different alpha values (regularization strengths). The data range is narrow and noisy, so overfitting easily occurs even at low degrees.</p>
<ul>
<li><strong>Alpha = 0.0:</strong> Ridge regression becomes the same as ordinary least squares linear regression, and the 10th degree polynomial follows the training data noise, showing a severely wavy overfitting shape.</li>
<li><strong>Alpha = 0.1:</strong> Weak regularization is applied, reducing the waviness compared to <code>alpha=0</code>, but still sensitive to noise and far from the sine function.</li>
<li><strong>Alpha = 10:</strong> Strong regularization makes the curve much smoother, well representing the overall trend of the data (sine function). This shows that L2 regularization (Ridge regression) effectively controls overfitting.</li>
</ul>
<p>By selecting an appropriate <code>alpha</code> value, we can control model complexity and improve generalization performance. L2 regularization is useful for stabilizing models by making weights close to 0.</p>
<p>The <code>sklearn.linear_model.Ridge</code> model can have different optimization methods depending on the <code>solver</code>. Especially when the data range is narrow and noisy like this example, the <code>'svd'</code> or <code>'cholesky'</code> solver may be more stable, so careful selection of the <code>solver</code> is necessary (in the code, <code>'cholesky'</code> is specified).</p>
</section>
<section id="applying-l1-l2-regularization-in-pytorch" class="level3">
<h3 class="anchored" data-anchor-id="applying-l1-l2-regularization-in-pytorch">6.2.2 Applying L1, L2 Regularization in PyTorch</h3>
<p>PyTorch and Keras have different ways of implementing L1 and L2 regularization. Keras supports adding regularization terms directly to each layer (<code>kernel_regularizer</code>, <code>bias_regularizer</code>).</p>
<div id="cell-10" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># In Keras, you can specify regularization when declaring a layer.</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>keras.layers.Dense(<span class="dv">64</span>, activation<span class="op">=</span><span class="st">'relu'</span>,</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>                    kernel_regularizer<span class="op">=</span>regularizers.l2(<span class="fl">0.01</span>),</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>                    input_shape<span class="op">=</span>(<span class="dv">784</span>,))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>On the other hand, PyTorch applies L2 regularization by setting weight decay on the optimizer, and L1 regularization is typically implemented through a custom loss function.</p>
<div id="cell-12" class="cell" data-execution_count="5">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> custom_loss(outputs, targets, model, lambda_l1<span class="op">=</span><span class="fl">0.01</span>, lambda_l2<span class="op">=</span><span class="fl">0.01</span>,):</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>    mse_loss <span class="op">=</span> nn.MSELoss()(outputs, targets)</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>    l1_loss <span class="op">=</span> <span class="fl">0.</span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>    l2_loss <span class="op">=</span> <span class="fl">0.</span></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> param <span class="kw">in</span> model.parameters():</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>        l1_loss <span class="op">+=</span> torch.<span class="bu">sum</span>(torch.<span class="bu">abs</span>(param)) <span class="co"># Take the absolute value of the parameters.</span></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>        l2_loss <span class="op">+=</span> torch.<span class="bu">sum</span>(param <span class="op">**</span> <span class="dv">2</span>)  <span class="co"># Square the parameters.</span></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>    total_loss <span class="op">=</span> mse_loss <span class="op">+</span> lambda_l1 <span class="op">*</span> l1_loss <span class="op">+</span> lambda_l2 <span class="op">*</span> l2_loss <span class="co"># Add L1 and L2 penalty terms to the loss.</span></span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> total_loss</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Example usage within a training loop (not runnable as is)</span></span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a><span class="co"># for inputs, targets in dataloader:</span></span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a><span class="co">#     # ... (rest of the training loop)</span></span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a><span class="co">#     loss = custom_loss(outputs, targets, model)</span></span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a><span class="co">#     loss.backward()</span></span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ... (rest of the training loop)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>As in the above example, you can define a custom_loss function to apply both L1 and L2 regularization. However, it is common to set the weight_decay corresponding to L2 regularization in the optimizer. But Adam and SGD optimizers implement weight decay slightly differently than L2 regularization. Traditional L2 regularization is to add a parameter squared term to the loss function.</p>
<p><span class="math inline">\(L_{n+1} = L_{n} + \frac{ \lambda }{2} \sum w^2\)</span></p>
<p>Differentiating this with respect to the parameters gives:</p>
<p><span class="math inline">\(\frac{\partial L_{n+1}}{\partial w} = \frac{\partial L_{n}}{\partial w} +\lambda w\)</span></p>
<p>SGD and Adam are implemented by directly adding the <span class="math inline">\(\lambda w\)</span> term to the gradient. The SGD code in chapter_05/optimizers is as follows.</p>
<div id="cell-14" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="va">self</span>.weight_decay <span class="op">!=</span> <span class="dv">0</span>:</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>    grad <span class="op">=</span> grad.add(p, alpha<span class="op">=</span><span class="va">self</span>.weight_decay)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>This approach does not have exactly the same effect as adding an L2 regularization term to the loss function when combined with momentum or adaptive learning rate.</p>
<p><strong>AdamW and Decoupled Weight Decay</strong></p>
<p>The 2017 ICLR paper “Fixing Weight Decay Regularization in Adam” (https://arxiv.org/abs/1711.05101) pointed out that weight decay in the Adam optimizer works differently from L2 regularization, and proposed the AdamW optimizer to fix this issue. In AdamW, weight decay is decoupled from gradient updates and applied directly in the parameter update step. The code is in the same basic.py.</p>
<div id="cell-16" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># PyTorch AdamW weght decay</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> weight_decay <span class="op">!=</span> <span class="dv">0</span>:</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>    param.data.mul_(<span class="dv">1</span> <span class="op">-</span> lr <span class="op">*</span> weight_decay)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>AdamW multiplies the parameter value by 1 - lr * weight_decay.</p>
<ul>
<li><strong>Weight decay in existing Adam</strong>: The weight decay term (<span class="math inline">\(\lambda w\)</span>) is considered during the gradient update step along with the learning rate (<span class="math inline">\(lr\)</span>) and other gradient adjustment terms (e.g., momentum). This causes the effect of weight decay to vary depending on the learning rate and other hyperparameter settings.</li>
<li><strong>Weight decay in AdamW</strong>: Since weight decay is applied separately at the parameter update step, it is less dependent on the learning rate or other hyperparameters. That is, the effect of weight decay becomes more predictable and consistent.</li>
</ul>
<p>In conclusion, AdamW’s approach is closer to a more accurate implementation of L2 regularization. While it’s common to refer to SGD and Adam’s weight decay as L2 regularization due to historical reasons and similar effects, it’s more accurate to view them as separate regularization techniques, and AdamW clarifies this difference, providing better performance.</p>
</section>
<section id="analyzing-the-regularization-effect-on-the-loss-surface" class="level3">
<h3 class="anchored">6.2.3 Analyzing the Regularization Effect on the Loss Surface</h3>
<p>To visually understand the effect of L1 and L2 regularization on model learning, we will use the loss surface visualization technique introduced in Chapter 4. We compare the change in the loss surface with and without L2 regularization and observe how the location of the optimum changes with the strength of the regularization (<code>weight_decay</code>).</p>
<div id="cell-19" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sys</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_05.visualization.loss_surface <span class="im">import</span> xy_perturb_loss,  hessian_eigenvectors, visualize_loss_surface </span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_04.utils.data <span class="im">import</span> get_dataset, get_device   </span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_04.utils.metrics <span class="im">import</span> load_model  </span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.utils.data <span class="im">as</span> data_utils</span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span>  DataLoader</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> get_device()  <span class="co"># Get the device (CPU or CUDA)</span></span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a>train_dataset, test_dataset <span class="op">=</span> get_dataset()  <span class="co"># Load the datasets.  </span></span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a>act_name <span class="op">=</span> <span class="st">"ReLU"</span></span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a>model_file <span class="op">=</span> <span class="ss">f"SimpleNetwork-</span><span class="sc">{</span>act_name<span class="sc">}</span><span class="ss">.pth"</span></span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a>small_dataset <span class="op">=</span> data_utils.Subset(test_dataset, torch.arange(<span class="dv">0</span>, <span class="dv">256</span>))  <span class="co"># Use a subset of the test dataset</span></span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a>data_loader <span class="op">=</span> DataLoader(small_dataset, batch_size<span class="op">=</span><span class="dv">256</span>, shuffle<span class="op">=</span><span class="va">True</span>)  <span class="co"># Create a data loader</span></span>
<span id="cb12-22"><a href="#cb12-22" aria-hidden="true" tabindex="-1"></a>loss_func <span class="op">=</span> nn.CrossEntropyLoss()  <span class="co"># Define the loss function</span></span>
<span id="cb12-23"><a href="#cb12-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-24"><a href="#cb12-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the trained model.</span></span>
<span id="cb12-25"><a href="#cb12-25" aria-hidden="true" tabindex="-1"></a>trained_model, _ <span class="op">=</span> load_model(model_file<span class="op">=</span>model_file, path<span class="op">=</span><span class="st">"./tmp/opts/ReLU"</span>) <span class="co"># 4장의 load_model 사용</span></span>
<span id="cb12-26"><a href="#cb12-26" aria-hidden="true" tabindex="-1"></a>trained_model <span class="op">=</span> trained_model.to(device)  <span class="co"># Move the model to the device</span></span>
<span id="cb12-27"><a href="#cb12-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-28"><a href="#cb12-28" aria-hidden="true" tabindex="-1"></a>top_n <span class="op">=</span> <span class="dv">2</span>  <span class="co"># Number of top eigenvalues/eigenvectors to compute</span></span>
<span id="cb12-29"><a href="#cb12-29" aria-hidden="true" tabindex="-1"></a>top_eigenvalues, top_eigenvectors <span class="op">=</span>  hessian_eigenvectors(model<span class="op">=</span>trained_model, loss_func<span class="op">=</span>loss_func, data_loader<span class="op">=</span>data_loader, top_n<span class="op">=</span>top_n, is_cuda<span class="op">=</span><span class="va">True</span>)  <span class="co"># 5장의 함수 사용</span></span>
<span id="cb12-30"><a href="#cb12-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-31"><a href="#cb12-31" aria-hidden="true" tabindex="-1"></a>d_min ,d_max, d_num <span class="op">=</span> <span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">50</span>  <span class="co"># Define the range and number of points for the grid</span></span>
<span id="cb12-32"><a href="#cb12-32" aria-hidden="true" tabindex="-1"></a>lambda1, lambda2 <span class="op">=</span> np.linspace(d_min, d_max, d_num).astype(np.float32), np.linspace(d_min, d_max, d_num).astype(np.float32)  <span class="co"># Create the grid of lambda values</span></span>
<span id="cb12-33"><a href="#cb12-33" aria-hidden="true" tabindex="-1"></a>x, y, z <span class="op">=</span> xy_perturb_loss(model<span class="op">=</span>trained_model, top_eigenvectors<span class="op">=</span>top_eigenvectors, data_loader<span class="op">=</span>data_loader, loss_func<span class="op">=</span>loss_func, lambda1<span class="op">=</span>lambda1, lambda2<span class="op">=</span>lambda2, device<span class="op">=</span>device) <span class="co"># 5장의 함수 사용</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Create an approximate function with <code>xy_perturb_loss</code>, then put (x, y) into the approximate function again to obtain a new z value. The reason for doing this is that when drawing contours with the value obtained by <code>xy_perturb_loss</code> as in Chapter 5, the minimum value is slightly different, so the point of convergence of the optimizer deviates slightly. Now, without expressing all the paths the optimizer takes, only the last lowest point is compared while increasing the weight_decay value.</p>
<div id="cell-21" class="cell" data-execution_count="8">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.optim <span class="im">as</span> optim  <span class="co"># Import optim</span></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> DataLoader, Subset</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a><span class="co"># 5장, 4장 함수들 import</span></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_05.visualization.loss_surface <span class="im">import</span> (</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>    hessian_eigenvectors,</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>    xy_perturb_loss,</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>    visualize_loss_surface</span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_04.utils.data <span class="im">import</span> get_dataset, get_device</span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_04.utils.metrics <span class="im">import</span> load_model</span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_05.visualization.gaussian_loss_surface <span class="im">import</span> (</span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>    get_opt_params,</span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>    train_loss_surface,</span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a>    gaussian_func <span class="co"># gaussian_func 추가.</span></span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> get_device()</span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a>_, test_dataset <span class="op">=</span> get_dataset(dataset<span class="op">=</span><span class="st">"FashionMNIST"</span>) </span>
<span id="cb13-25"><a href="#cb13-25" aria-hidden="true" tabindex="-1"></a>small_dataset <span class="op">=</span> Subset(test_dataset, torch.arange(<span class="dv">0</span>, <span class="dv">256</span>))</span>
<span id="cb13-26"><a href="#cb13-26" aria-hidden="true" tabindex="-1"></a>data_loader <span class="op">=</span> DataLoader(small_dataset, batch_size<span class="op">=</span><span class="dv">256</span>, shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb13-27"><a href="#cb13-27" aria-hidden="true" tabindex="-1"></a>loss_func <span class="op">=</span> nn.CrossEntropyLoss()</span>
<span id="cb13-28"><a href="#cb13-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-29"><a href="#cb13-29" aria-hidden="true" tabindex="-1"></a>act_name <span class="op">=</span> <span class="st">"ReLU"</span> <span class="co"># Tanh로 실험하려면 이 부분을 변경</span></span>
<span id="cb13-30"><a href="#cb13-30" aria-hidden="true" tabindex="-1"></a>model_file <span class="op">=</span> <span class="ss">f"SimpleNetwork-</span><span class="sc">{</span>act_name<span class="sc">}</span><span class="ss">.pth"</span></span>
<span id="cb13-31"><a href="#cb13-31" aria-hidden="true" tabindex="-1"></a>trained_model, _ <span class="op">=</span> load_model(model_file<span class="op">=</span>model_file, path<span class="op">=</span><span class="st">"./tmp/opts/ReLU"</span>) </span>
<span id="cb13-32"><a href="#cb13-32" aria-hidden="true" tabindex="-1"></a>trained_model <span class="op">=</span> trained_model.to(device)</span>
<span id="cb13-33"><a href="#cb13-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-34"><a href="#cb13-34" aria-hidden="true" tabindex="-1"></a>top_n <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb13-35"><a href="#cb13-35" aria-hidden="true" tabindex="-1"></a>top_eigenvalues, top_eigenvectors <span class="op">=</span> hessian_eigenvectors(</span>
<span id="cb13-36"><a href="#cb13-36" aria-hidden="true" tabindex="-1"></a>    model<span class="op">=</span>trained_model,</span>
<span id="cb13-37"><a href="#cb13-37" aria-hidden="true" tabindex="-1"></a>    loss_func<span class="op">=</span>loss_func,</span>
<span id="cb13-38"><a href="#cb13-38" aria-hidden="true" tabindex="-1"></a>    data_loader<span class="op">=</span>data_loader,</span>
<span id="cb13-39"><a href="#cb13-39" aria-hidden="true" tabindex="-1"></a>    top_n<span class="op">=</span>top_n,</span>
<span id="cb13-40"><a href="#cb13-40" aria-hidden="true" tabindex="-1"></a>    is_cuda<span class="op">=</span><span class="va">True</span></span>
<span id="cb13-41"><a href="#cb13-41" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb13-42"><a href="#cb13-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-43"><a href="#cb13-43" aria-hidden="true" tabindex="-1"></a>d_min, d_max, d_num <span class="op">=</span> <span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">30</span> <span class="co"># 5장의 30을 사용</span></span>
<span id="cb13-44"><a href="#cb13-44" aria-hidden="true" tabindex="-1"></a>lambda1 <span class="op">=</span> np.linspace(d_min, d_max, d_num).astype(np.float32)</span>
<span id="cb13-45"><a href="#cb13-45" aria-hidden="true" tabindex="-1"></a>lambda2 <span class="op">=</span> np.linspace(d_min, d_max, d_num).astype(np.float32)</span>
<span id="cb13-46"><a href="#cb13-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-47"><a href="#cb13-47" aria-hidden="true" tabindex="-1"></a>x, y, z <span class="op">=</span> xy_perturb_loss(</span>
<span id="cb13-48"><a href="#cb13-48" aria-hidden="true" tabindex="-1"></a>    model<span class="op">=</span>trained_model,</span>
<span id="cb13-49"><a href="#cb13-49" aria-hidden="true" tabindex="-1"></a>    top_eigenvectors<span class="op">=</span>top_eigenvectors,</span>
<span id="cb13-50"><a href="#cb13-50" aria-hidden="true" tabindex="-1"></a>    data_loader<span class="op">=</span>data_loader,</span>
<span id="cb13-51"><a href="#cb13-51" aria-hidden="true" tabindex="-1"></a>    loss_func<span class="op">=</span>loss_func,</span>
<span id="cb13-52"><a href="#cb13-52" aria-hidden="true" tabindex="-1"></a>    lambda1<span class="op">=</span>lambda1,</span>
<span id="cb13-53"><a href="#cb13-53" aria-hidden="true" tabindex="-1"></a>    lambda2<span class="op">=</span>lambda2,</span>
<span id="cb13-54"><a href="#cb13-54" aria-hidden="true" tabindex="-1"></a>    device<span class="op">=</span>device <span class="co"># device 추가</span></span>
<span id="cb13-55"><a href="#cb13-55" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb13-56"><a href="#cb13-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-57"><a href="#cb13-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-58"><a href="#cb13-58" aria-hidden="true" tabindex="-1"></a><span class="co"># --- Optimization and Visualization ---</span></span>
<span id="cb13-59"><a href="#cb13-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-60"><a href="#cb13-60" aria-hidden="true" tabindex="-1"></a><span class="co"># Find the parameters that best fit the data.</span></span>
<span id="cb13-61"><a href="#cb13-61" aria-hidden="true" tabindex="-1"></a>popt, _, offset <span class="op">=</span> get_opt_params(x, y, z)  <span class="co"># offset 사용</span></span>
<span id="cb13-62"><a href="#cb13-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-63"><a href="#cb13-63" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Optimal parameters: </span><span class="sc">{</span>popt<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb13-64"><a href="#cb13-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-65"><a href="#cb13-65" aria-hidden="true" tabindex="-1"></a><span class="co"># Get a new z using the optimized surface function (Gaussian).</span></span>
<span id="cb13-66"><a href="#cb13-66" aria-hidden="true" tabindex="-1"></a><span class="co"># No need for global g_offset, we can use the returned offset.</span></span>
<span id="cb13-67"><a href="#cb13-67" aria-hidden="true" tabindex="-1"></a>z_fitted <span class="op">=</span> gaussian_func((x, y), <span class="op">*</span>popt,offset) <span class="co"># offset을 더해야 함.</span></span>
<span id="cb13-68"><a href="#cb13-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-69"><a href="#cb13-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-70"><a href="#cb13-70" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> [(x, y, z_fitted)]  <span class="co"># Use z_fitted</span></span>
<span id="cb13-71"><a href="#cb13-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-72"><a href="#cb13-72" aria-hidden="true" tabindex="-1"></a>axes <span class="op">=</span> visualize_loss_surface(data, act_name<span class="op">=</span>act_name, color<span class="op">=</span><span class="st">"C0"</span>, size<span class="op">=</span><span class="dv">6</span>, levels<span class="op">=</span><span class="dv">80</span>, alpha<span class="op">=</span><span class="fl">0.7</span>, plot_3d<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb13-73"><a href="#cb13-73" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> axes[<span class="dv">0</span>]</span>
<span id="cb13-74"><a href="#cb13-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-75"><a href="#cb13-75" aria-hidden="true" tabindex="-1"></a><span class="co"># Train with different weight decays and plot trajectories.</span></span>
<span id="cb13-76"><a href="#cb13-76" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> n, weight_decay <span class="kw">in</span> <span class="bu">enumerate</span>([<span class="fl">0.0</span>, <span class="fl">6.0</span>, <span class="fl">10.0</span>, <span class="fl">18.0</span>, <span class="fl">20.0</span>]):</span>
<span id="cb13-77"><a href="#cb13-77" aria-hidden="true" tabindex="-1"></a><span class="co"># for n, weight_decay in enumerate([0.0]):  # For faster testing</span></span>
<span id="cb13-78"><a href="#cb13-78" aria-hidden="true" tabindex="-1"></a>    points_sgd_m <span class="op">=</span> train_loss_surface(</span>
<span id="cb13-79"><a href="#cb13-79" aria-hidden="true" tabindex="-1"></a>        <span class="kw">lambda</span> params: optim.SGD(params, lr<span class="op">=</span><span class="fl">0.1</span>, momentum<span class="op">=</span><span class="fl">0.7</span>, weight_decay<span class="op">=</span>weight_decay),</span>
<span id="cb13-80"><a href="#cb13-80" aria-hidden="true" tabindex="-1"></a>        [d_min, d_max],</span>
<span id="cb13-81"><a href="#cb13-81" aria-hidden="true" tabindex="-1"></a>        <span class="dv">200</span>,</span>
<span id="cb13-82"><a href="#cb13-82" aria-hidden="true" tabindex="-1"></a>        (<span class="op">*</span>popt, offset) <span class="co"># unpack popt and offset</span></span>
<span id="cb13-83"><a href="#cb13-83" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb13-84"><a href="#cb13-84" aria-hidden="true" tabindex="-1"></a>    ax.plot(</span>
<span id="cb13-85"><a href="#cb13-85" aria-hidden="true" tabindex="-1"></a>        points_sgd_m[<span class="op">-</span><span class="dv">1</span>, <span class="dv">0</span>],</span>
<span id="cb13-86"><a href="#cb13-86" aria-hidden="true" tabindex="-1"></a>        points_sgd_m[<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>],</span>
<span id="cb13-87"><a href="#cb13-87" aria-hidden="true" tabindex="-1"></a>        color<span class="op">=</span><span class="ss">f"C</span><span class="sc">{</span>n<span class="sc">}</span><span class="ss">"</span>,</span>
<span id="cb13-88"><a href="#cb13-88" aria-hidden="true" tabindex="-1"></a>        marker<span class="op">=</span><span class="st">"o"</span>,</span>
<span id="cb13-89"><a href="#cb13-89" aria-hidden="true" tabindex="-1"></a>        markersize<span class="op">=</span><span class="dv">10</span>,</span>
<span id="cb13-90"><a href="#cb13-90" aria-hidden="true" tabindex="-1"></a>        zorder<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb13-91"><a href="#cb13-91" aria-hidden="true" tabindex="-1"></a>        label<span class="op">=</span><span class="ss">f"wd=</span><span class="sc">{</span>weight_decay<span class="sc">:0.1f}</span><span class="ss">"</span></span>
<span id="cb13-92"><a href="#cb13-92" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb13-93"><a href="#cb13-93" aria-hidden="true" tabindex="-1"></a>    ax.ticklabel_format(axis<span class="op">=</span><span class="st">'both'</span>, style<span class="op">=</span><span class="st">'scientific'</span>, scilimits<span class="op">=</span>(<span class="dv">0</span>, <span class="dv">0</span>))</span>
<span id="cb13-94"><a href="#cb13-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-95"><a href="#cb13-95" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb13-96"><a href="#cb13-96" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Function parameters = [ 4.59165436  0.34582255 -0.03204057 -1.09810435  1.54530407]
Optimal parameters: [ 4.59165436  0.34582255 -0.03204057 -1.09810435  1.54530407]

train_loss_surface: SGD
SGD: Iter=1 loss=4.7671 w=[-0.8065, 0.9251]
SGD: Iter=200 loss=1.9090 w=[0.3458, -0.0320]

train_loss_surface: SGD
SGD: Iter=1 loss=4.7671 w=[-0.2065, 0.3251]
SGD: Iter=200 loss=1.9952 w=[0.1327, -0.0077]

train_loss_surface: SGD
SGD: Iter=1 loss=4.7671 w=[0.1935, -0.0749]
SGD: Iter=200 loss=2.0293 w=[0.0935, -0.0051]

train_loss_surface: SGD
SGD: Iter=1 loss=4.7671 w=[0.9935, -0.8749]
SGD: Iter=200 loss=2.0641 w=[0.0587, -0.0030]

train_loss_surface: SGD
SGD: Iter=1 loss=4.7671 w=[1.1935, -1.0749]
SGD: Iter=200 loss=2.0694 w=[0.0537, -0.0027]</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="06_Overfitting and Development of Solution Techniques_files/figure-html/cell-11-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>As can be seen in the figure, the larger the L2 regulation (weight decay), the farther the final point reached by the optimizer is from the lowest point of the loss function. This is because L2 regularization helps prevent the model from overfitting by preventing the weights from becoming too large.</p>
<p>L1 regularization creates a sparse model by making some weights 0. It is useful when you want to reduce the complexity of the model and remove unnecessary features. On the other hand, L2 regularization does not make the weights completely 0, but keeps all weights small. L2 regularization is generally more stable and is also called ‘soft regularization’ because it gradually reduces the weights.</p>
<p>L1 and L2 regularizations are applied differently depending on the characteristics of the problem, data, and purpose of the model. While L2 regularization is generally more widely used, it is a good idea to try both and see which one performs better in some cases. Additionally, Elastic Net regularization, which combines L1 and L2 regularization, can also be considered.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Click to view contents (Deep Dive: Elastic Net Regulation - Harmony of L1 and L2)">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Click to view contents (Deep Dive: Elastic Net Regulation - Harmony of L1 and L2)
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<section id="elastic-net-regulation---a-combination-of-l1-and-l2" class="level2 callout-body-container callout-body">
<h2 class="anchored" data-anchor-id="elastic-net-regulation---a-combination-of-l1-and-l2">Elastic Net Regulation - A Combination of L1 and L2</h2>
<p>Elastic Net is a regularization method that combines L1 and L2 regulations. By taking the advantages of each regulation and compensating for their disadvantages, it can create more flexible and effective models.</p>
<p><strong>Key Points:</strong></p>
<ul>
<li><strong>L1 Regulation (Lasso):</strong> Limits the sum of the absolute values of weights. It creates sparse models by making some weights <em>exactly 0</em>. It has a feature selection effect, removing unnecessary features and simplifying the model.</li>
<li><strong>L2 Regulation (Ridge):</strong> Limits the sum of the squares of weights. It prevents overfitting by keeping all weights <em>small</em>. The convergence is stable and smoothly reduces the weights.</li>
<li><strong>Elastic Net:</strong> Applies both L1 and L2 regulations simultaneously. It can achieve the effects of both regulations.</li>
</ul>
<p><strong>Formula:</strong></p>
<p>The cost function of Elastic Net is expressed as follows:</p>
<p><span class="math inline">\(Cost = Loss + \lambda_1 \sum_{i} |w_i| + \lambda_2 \sum_{i} (w_i)^2\)</span></p>
<ul>
<li><code>Loss</code>: The original model’s loss function (e.g., MSE, Cross-Entropy)</li>
<li><code>λ₁</code>: A hyperparameter that controls the strength of L1 regulation</li>
<li><code>λ₂</code>: A hyperparameter that controls the strength of L2 regulation</li>
<li><code>wᵢ</code>: The model’s weights</li>
</ul>
<p><strong>Advantages:</strong></p>
<ul>
<li><strong>Feature Selection + Overfitting Prevention:</strong> It can achieve both the feature selection effect of L1 regulation and the overfitting prevention effect of L2 regulation.</li>
<li><strong>Handling Highly Correlated Features:</strong> L1 regulation tends to select only one feature among highly correlated features and set the others to 0. Elastic Net mitigates this issue through L2 regulation, making it tend to select or remove highly correlated features <em>together</em>.</li>
<li><strong>Flexibility:</strong> The weights of L1 and L2 regulations can be adjusted by controlling <code>λ₁</code> and <code>λ₂</code>. If <code>λ₁=0</code>, it becomes L2 regulation (Ridge), and if <code>λ₂=0</code>, it becomes L1 regulation (Lasso).</li>
</ul>
<p><strong>Disadvantages:</strong></p>
<ul>
<li><strong>Hyperparameter Tuning:</strong> It requires tuning two hyperparameters, <code>λ₁</code> and <code>λ₂</code>, which can be more complex than tuning L1 or L2 regulation.</li>
<li><strong>Computational Cost:</strong> Since it calculates both L1 and L2 regulations simultaneously, the computational cost may increase slightly (although this is not a significant issue in modern machine learning).</li>
</ul>
<p><strong>Applicable Cases:</strong></p>
<ul>
<li>When there are many features, and only some of them are expected to be important (feature selection is necessary)</li>
<li>When there is a high correlation between features</li>
<li>When it is unclear whether L1 or L2 regulation is better (both can be tried)</li>
<li>When preventing overfitting while creating a sparse model is desired</li>
</ul>
<p><strong>Summary:</strong> Elastic Net is a powerful regularization method that combines the advantages of L1 and L2 regulations. Although hyperparameter tuning is required, it can perform well in various problems.</p>
</section>
</div>
</div>
</section>
</section>
<section id="dropout" class="level2">
<h2 class="anchored" data-anchor-id="dropout">6.3 Dropout</h2>
<section id="principle-of-dropout" class="level3">
<h3 class="anchored" data-anchor-id="principle-of-dropout">6.3.1 Principle of Dropout</h3>
<p>Dropout is one of the powerful regularization methods to prevent overfitting in neural networks. During the training process, some neurons are randomly deactivated (dropped out) to prevent specific neurons or combinations of neurons from becoming too dependent on the training data. This has a similar effect to ensemble learning, where multiple people learn different parts and then combine their strengths to solve a problem. Each neuron is encouraged to learn important features independently, improving the model’s generalization performance. It is typically applied to fully connected layers, with a dropout rate of 20% to 50%. Dropout is only applied during training, and all neurons are used during inference.</p>
</section>
<section id="implementing-dropout-in-pytorch" class="level3">
<h3 class="anchored" data-anchor-id="implementing-dropout-in-pytorch">6.3.2 Implementing Dropout in PyTorch</h3>
<p>Dropout can be implemented in PyTorch as follows.</p>
<div id="cell-25" class="cell" data-execution_count="9">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Dropout(nn.Module):</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, dropout_rate):</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(Dropout, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout_rate <span class="op">=</span> dropout_rate</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.training:</span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>            mask <span class="op">=</span> torch.bernoulli(torch.ones_like(x) <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> <span class="va">self</span>.dropout_rate)) <span class="op">/</span> (<span class="dv">1</span> <span class="op">-</span> <span class="va">self</span>.dropout_rate)</span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> x <span class="op">*</span> mask</span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> x</span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Usage example.  Drops out 0.5 (50%).</span></span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a>dropout <span class="op">=</span> Dropout(dropout_rate<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Example input data</span></span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a>inputs <span class="op">=</span> torch.randn(<span class="dv">1000</span>, <span class="dv">100</span>)</span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Forward pass (during training)</span></span>
<span id="cb15-22"><a href="#cb15-22" aria-hidden="true" tabindex="-1"></a>dropout.train()</span>
<span id="cb15-23"><a href="#cb15-23" aria-hidden="true" tabindex="-1"></a>outputs_train <span class="op">=</span> dropout(inputs)</span>
<span id="cb15-24"><a href="#cb15-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-25"><a href="#cb15-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Forward pass (during inference)</span></span>
<span id="cb15-26"><a href="#cb15-26" aria-hidden="true" tabindex="-1"></a>dropout.<span class="bu">eval</span>()</span>
<span id="cb15-27"><a href="#cb15-27" aria-hidden="true" tabindex="-1"></a>outputs_test <span class="op">=</span> dropout(inputs)</span>
<span id="cb15-28"><a href="#cb15-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-29"><a href="#cb15-29" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Input shape:"</span>, inputs.shape)</span>
<span id="cb15-30"><a href="#cb15-30" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Training output shape:"</span>, outputs_train.shape)</span>
<span id="cb15-31"><a href="#cb15-31" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Test output shape"</span>, outputs_test.shape)</span>
<span id="cb15-32"><a href="#cb15-32" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Dropout rate (should be close to 0.5):"</span>, <span class="dv">1</span> <span class="op">-</span> torch.count_nonzero(outputs_train) <span class="op">/</span> outputs_train.numel())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Input shape: torch.Size([1000, 100])
Training output shape: torch.Size([1000, 100])
Test output shape torch.Size([1000, 100])
Dropout rate (should be close to 0.5): tensor(0.4997)</code></pre>
</div>
</div>
<p>The implementation is very simple. It multiplies the <code>mask</code> value to the input tensor and inactivates a certain percentage of neurons. The dropout layer does not have separate learnable parameters, but simply plays the role of randomly making some of the inputs 0. In actual neural networks, the dropout layer is used by inserting it between other layers (e.g., linear layers, convolutional layers).</p>
<p>Dropout removes neurons randomly during training, but uses all neurons during inference. To match the scale of output values between training and inference, the <em>inverted dropout</em> method is used. Inverted dropout performs scaling in advance by dividing by (1 - dropout_rate) during training, so that it can be used as is without additional calculations during inference. This allows achieving an effect similar to ensemble learning during inference, i.e., averaging multiple sub-networks, while also improving computational efficiency.</p>
<p>Let’s take a look at how effective dropout is through a graph using simple data. The source code is <code>chapter_06/plot_dropout.py</code>, and we will omit introducing the unimportant code due to space constraints. Since it has detailed comments, it’s not difficult to look at the source code. When drawing the graph, we can see that the model with dropout applied (blue) has a much higher test accuracy.</p>
<div id="cell-27" class="cell" data-execution_count="11">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_06.plot_dropout <span class="im">import</span> plot_dropout_effect</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>plot_dropout_effect()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="06_Overfitting and Development of Solution Techniques_files/figure-html/cell-13-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The training accuracy of the model with dropout (With Dropout) is lower than that of the model without dropout (Without Dropout), but the validation accuracy is higher. This means that dropout reduces overfitting to the training data and improves the generalization performance of the model.</p>
</section>
</section>
<section id="batch-normalization" class="level2">
<h2 class="anchored" data-anchor-id="batch-normalization">6.4 Batch Normalization</h2>
<section id="concept-and-effect-of-batch-normalization" class="level3">
<h3 class="anchored" data-anchor-id="concept-and-effect-of-batch-normalization">6.4.1 Concept and Effect of Batch Normalization</h3>
<p>Batch normalization plays a role in regularization and also increases the stability of data during training. Batch normalization was first proposed in the 2015 paper by Ioffe and Szegedy [Reference 2]. In deep learning, as data passes through each layer, the distribution of activation values changes, causing an internal covariate shift. This slows down the training speed and makes the model unstable (since the distribution changes, more calculation steps are required). This problem becomes more severe as the number of layers increases. Batch normalization mitigates this by normalizing the data in mini-batch units.</p>
<p>The core idea of batch normalization is to normalize the data in mini-batch units. The following code illustrates this easily.</p>
<div id="cell-30" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the mean and variance of the mini-batch</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>batch_mean <span class="op">=</span> x.mean(dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>batch_var <span class="op">=</span> x.var(dim<span class="op">=</span><span class="dv">0</span>, unbiased<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform normalization</span></span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>x_norm <span class="op">=</span> (x <span class="op">-</span> batch_mean) <span class="op">/</span> torch.sqrt(batch_var <span class="op">+</span> epsilon)</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Apply scale and shift parameters</span></span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> gamma <span class="op">*</span> x_norm <span class="op">+</span> beta</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Generally, batch normalization uses the variance and mean of the data within a single mini-batch to normalize the entire data, bringing about distributional changes. It first performs normalization, then applies a certain scale parameter and shift parameter. The gamma above is the scale parameter, and beta is the shift parameter. It’s helpful to think of it simply as <span class="math inline">\(y = ax + b\)</span>. The epsilon used during normalization is a very small constant value (1e-5 or 1e-7) that often appears in numerical analysis, and is used for numerical stability. Batch normalization provides the following additional effects:</p>
<ul>
<li><strong>Improved learning speed</strong>: By stabilizing the distribution of activation values for each layer, it alleviates the gradient vanishing/exploding problem, allowing the use of larger learning rates.</li>
<li><strong>Reduced dependence on initialization</strong>: It makes the model less sensitive to weight initialization, making it easier to start training.</li>
<li><strong>Regularization effect</strong>: Since statistics are calculated for each mini-batch, it has the effect of adding a bit of noise, which helps prevent overfitting. (Using it with dropout is even more effective.)</li>
</ul>
<p>Let’s compare the case where pure normalization is applied to randomly generated data with two features, and the case where scale and shift parameters are applied, using a graph. Through visualization, we can easily understand the numerical meaning of normalization for mini-batches.</p>
<div id="cell-32" class="cell" data-execution_count="21">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate data</span></span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.random.rand(<span class="dv">50</span>, <span class="dv">2</span>) <span class="op">*</span> <span class="dv">10</span></span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Batch normalization (including scaling parameters)</span></span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> batch_normalize(x, epsilon<span class="op">=</span><span class="fl">1e-5</span>, gamma<span class="op">=</span><span class="fl">1.0</span>, beta<span class="op">=</span><span class="fl">0.0</span>):</span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a>    mean <span class="op">=</span> x.mean(axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a>    var <span class="op">=</span> x.var(axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a>    x_norm <span class="op">=</span> (x <span class="op">-</span> mean) <span class="op">/</span> np.sqrt(var <span class="op">+</span> epsilon)</span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a>    x_scaled <span class="op">=</span> gamma <span class="op">*</span> x_norm <span class="op">+</span> beta</span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x_norm, mean, x_scaled</span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform normalization (gamma=1.0, beta=0.0 is pure normalization)</span></span>
<span id="cb19-18"><a href="#cb19-18" aria-hidden="true" tabindex="-1"></a>x_norm, mean, x_norm_scaled <span class="op">=</span> batch_normalize(x, gamma<span class="op">=</span><span class="fl">1.0</span>, beta<span class="op">=</span><span class="fl">0.0</span>)</span>
<span id="cb19-19"><a href="#cb19-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform normalization and scaling (apply gamma=2.0, beta=1.0)</span></span>
<span id="cb19-20"><a href="#cb19-20" aria-hidden="true" tabindex="-1"></a>_, _, x_scaled <span class="op">=</span> batch_normalize(x, gamma<span class="op">=</span><span class="fl">2.0</span>, beta<span class="op">=</span><span class="fl">1.0</span>)</span>
<span id="cb19-21"><a href="#cb19-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-22"><a href="#cb19-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Set Seaborn style</span></span>
<span id="cb19-23"><a href="#cb19-23" aria-hidden="true" tabindex="-1"></a>sns.set_style(<span class="st">"whitegrid"</span>)</span>
<span id="cb19-24"><a href="#cb19-24" aria-hidden="true" tabindex="-1"></a>sns.set_context(<span class="st">"notebook"</span>, font_scale<span class="op">=</span><span class="fl">1.2</span>)</span>
<span id="cb19-25"><a href="#cb19-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-26"><a href="#cb19-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualization</span></span>
<span id="cb19-27"><a href="#cb19-27" aria-hidden="true" tabindex="-1"></a>fig, (ax1, ax2, ax3) <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">3</span>, figsize<span class="op">=</span>(<span class="dv">18</span>, <span class="dv">5</span>))</span>
<span id="cb19-28"><a href="#cb19-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-29"><a href="#cb19-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Original data</span></span>
<span id="cb19-30"><a href="#cb19-30" aria-hidden="true" tabindex="-1"></a>sns.scatterplot(x<span class="op">=</span>x[:, <span class="dv">0</span>], y<span class="op">=</span>x[:, <span class="dv">1</span>], ax<span class="op">=</span>ax1, color<span class="op">=</span><span class="st">'royalblue'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb19-31"><a href="#cb19-31" aria-hidden="true" tabindex="-1"></a>ax1.scatter(mean[<span class="dv">0</span>], mean[<span class="dv">1</span>], color<span class="op">=</span><span class="st">'red'</span>, marker<span class="op">=</span><span class="st">'*'</span>, s<span class="op">=</span><span class="dv">200</span>, label<span class="op">=</span><span class="st">'Mean'</span>)</span>
<span id="cb19-32"><a href="#cb19-32" aria-hidden="true" tabindex="-1"></a>ax1.<span class="bu">set</span>(title<span class="op">=</span><span class="st">'Original Data'</span>,</span>
<span id="cb19-33"><a href="#cb19-33" aria-hidden="true" tabindex="-1"></a>        xlabel<span class="op">=</span><span class="st">'Feature 1'</span>,</span>
<span id="cb19-34"><a href="#cb19-34" aria-hidden="true" tabindex="-1"></a>        ylabel<span class="op">=</span><span class="st">'Feature 2'</span>,</span>
<span id="cb19-35"><a href="#cb19-35" aria-hidden="true" tabindex="-1"></a>        xlim<span class="op">=</span>(<span class="op">-</span><span class="dv">2</span>, <span class="dv">12</span>),</span>
<span id="cb19-36"><a href="#cb19-36" aria-hidden="true" tabindex="-1"></a>        ylim<span class="op">=</span>(<span class="op">-</span><span class="dv">2</span>, <span class="dv">12</span>))</span>
<span id="cb19-37"><a href="#cb19-37" aria-hidden="true" tabindex="-1"></a>ax1.legend()</span>
<span id="cb19-38"><a href="#cb19-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-39"><a href="#cb19-39" aria-hidden="true" tabindex="-1"></a><span class="co"># After normalization (gamma=1, beta=0)</span></span>
<span id="cb19-40"><a href="#cb19-40" aria-hidden="true" tabindex="-1"></a>sns.scatterplot(x<span class="op">=</span>x_norm[:, <span class="dv">0</span>], y<span class="op">=</span>x_norm[:, <span class="dv">1</span>], ax<span class="op">=</span>ax2, color<span class="op">=</span><span class="st">'crimson'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb19-41"><a href="#cb19-41" aria-hidden="true" tabindex="-1"></a>ax2.scatter(<span class="dv">0</span>, <span class="dv">0</span>, color<span class="op">=</span><span class="st">'blue'</span>, marker<span class="op">=</span><span class="st">'*'</span>, s<span class="op">=</span><span class="dv">200</span>, label<span class="op">=</span><span class="st">'Mean (0,0)'</span>)</span>
<span id="cb19-42"><a href="#cb19-42" aria-hidden="true" tabindex="-1"></a>ax2.axhline(y<span class="op">=</span><span class="dv">0</span>, color<span class="op">=</span><span class="st">'k'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb19-43"><a href="#cb19-43" aria-hidden="true" tabindex="-1"></a>ax2.axvline(x<span class="op">=</span><span class="dv">0</span>, color<span class="op">=</span><span class="st">'k'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb19-44"><a href="#cb19-44" aria-hidden="true" tabindex="-1"></a>ax2.<span class="bu">set</span>(title<span class="op">=</span><span class="st">'After Normalization (γ=1, β=0)'</span>,</span>
<span id="cb19-45"><a href="#cb19-45" aria-hidden="true" tabindex="-1"></a>        xlabel<span class="op">=</span><span class="st">'Normalized Feature 1'</span>,</span>
<span id="cb19-46"><a href="#cb19-46" aria-hidden="true" tabindex="-1"></a>        ylabel<span class="op">=</span><span class="st">'Normalized Feature 2'</span>,</span>
<span id="cb19-47"><a href="#cb19-47" aria-hidden="true" tabindex="-1"></a>        xlim<span class="op">=</span>(<span class="op">-</span><span class="dv">2</span>, <span class="dv">12</span>),</span>
<span id="cb19-48"><a href="#cb19-48" aria-hidden="true" tabindex="-1"></a>        ylim<span class="op">=</span>(<span class="op">-</span><span class="dv">2</span>, <span class="dv">12</span>))</span>
<span id="cb19-49"><a href="#cb19-49" aria-hidden="true" tabindex="-1"></a>ax2.legend()</span>
<span id="cb19-50"><a href="#cb19-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-51"><a href="#cb19-51" aria-hidden="true" tabindex="-1"></a><span class="co"># After scaling and shifting (gamma=2, beta=1)</span></span>
<span id="cb19-52"><a href="#cb19-52" aria-hidden="true" tabindex="-1"></a>sns.scatterplot(x<span class="op">=</span>x_scaled[:, <span class="dv">0</span>], y<span class="op">=</span>x_scaled[:, <span class="dv">1</span>], ax<span class="op">=</span>ax3, color<span class="op">=</span><span class="st">'green'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb19-53"><a href="#cb19-53" aria-hidden="true" tabindex="-1"></a>ax3.scatter(<span class="dv">1</span>, <span class="dv">1</span>, color<span class="op">=</span><span class="st">'purple'</span>, marker<span class="op">=</span><span class="st">'*'</span>, s<span class="op">=</span><span class="dv">200</span>, label<span class="op">=</span><span class="st">'New Mean'</span>)</span>
<span id="cb19-54"><a href="#cb19-54" aria-hidden="true" tabindex="-1"></a>ax3.axhline(y<span class="op">=</span><span class="dv">1</span>, color<span class="op">=</span><span class="st">'k'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb19-55"><a href="#cb19-55" aria-hidden="true" tabindex="-1"></a>ax3.axvline(x<span class="op">=</span><span class="dv">1</span>, color<span class="op">=</span><span class="st">'k'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb19-56"><a href="#cb19-56" aria-hidden="true" tabindex="-1"></a>ax3.<span class="bu">set</span>(title<span class="op">=</span><span class="st">'After Scale &amp; Shift (γ=2, β=1)'</span>,</span>
<span id="cb19-57"><a href="#cb19-57" aria-hidden="true" tabindex="-1"></a>        xlabel<span class="op">=</span><span class="st">'Scaled Feature 1'</span>,</span>
<span id="cb19-58"><a href="#cb19-58" aria-hidden="true" tabindex="-1"></a>        ylabel<span class="op">=</span><span class="st">'Scaled Feature 2'</span>,</span>
<span id="cb19-59"><a href="#cb19-59" aria-hidden="true" tabindex="-1"></a>        xlim<span class="op">=</span>(<span class="op">-</span><span class="dv">2</span>, <span class="dv">12</span>),</span>
<span id="cb19-60"><a href="#cb19-60" aria-hidden="true" tabindex="-1"></a>        ylim<span class="op">=</span>(<span class="op">-</span><span class="dv">2</span>, <span class="dv">12</span>))</span>
<span id="cb19-61"><a href="#cb19-61" aria-hidden="true" tabindex="-1"></a>ax3.legend()</span>
<span id="cb19-62"><a href="#cb19-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-63"><a href="#cb19-63" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb19-64"><a href="#cb19-64" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb19-65"><a href="#cb19-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-66"><a href="#cb19-66" aria-hidden="true" tabindex="-1"></a><span class="co"># Print statistics</span></span>
<span id="cb19-67"><a href="#cb19-67" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Original Data Statistics:"</span>)</span>
<span id="cb19-68"><a href="#cb19-68" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Mean: </span><span class="sc">{</span>mean<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb19-69"><a href="#cb19-69" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Variance: </span><span class="sc">{</span>x<span class="sc">.</span>var(axis<span class="op">=</span><span class="dv">0</span>)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb19-70"><a href="#cb19-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-71"><a href="#cb19-71" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Normalized Data Statistics (γ=1, β=0):"</span>)</span>
<span id="cb19-72"><a href="#cb19-72" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Mean: </span><span class="sc">{</span>x_norm<span class="sc">.</span>mean(axis<span class="op">=</span><span class="dv">0</span>)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb19-73"><a href="#cb19-73" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Variance: </span><span class="sc">{</span>x_norm<span class="sc">.</span>var(axis<span class="op">=</span><span class="dv">0</span>)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb19-74"><a href="#cb19-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-75"><a href="#cb19-75" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Scaled Data Statistics (γ=2, β=1):"</span>)</span>
<span id="cb19-76"><a href="#cb19-76" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Mean: </span><span class="sc">{</span>x_scaled<span class="sc">.</span>mean(axis<span class="op">=</span><span class="dv">0</span>)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb19-77"><a href="#cb19-77" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Variance: </span><span class="sc">{</span>x_scaled<span class="sc">.</span>var(axis<span class="op">=</span><span class="dv">0</span>)<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="06_Overfitting and Development of Solution Techniques_files/figure-html/cell-15-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Original Data Statistics:
Mean: [4.40716778 4.99644709]
Variance: [8.89458134 8.45478364]

Normalized Data Statistics (γ=1, β=0):
Mean: [-2.70894418e-16 -3.59712260e-16]
Variance: [0.99999888 0.99999882]

Scaled Data Statistics (γ=2, β=1):
Mean: [1. 1.]
Variance: [3.9999955  3.99999527]</code></pre>
</div>
</div>
<p>In seed(42), you can often see that the random initial value is set to 42. This is a programmer’s habit, and other numbers can also be used. 42 is a number that appears in Douglas Adams’ novel “The Hitchhiker’s Guide to the Galaxy” as the “answer to life, the universe, and everything”. Therefore, it is often used as an example code among programmers.</p>
</section>
<section id="batch-normalization-implementation-in-pytorch" class="level3">
<h3 class="anchored">6.4.2 Batch Normalization Implementation in PyTorch</h3>
<p>In PyTorch, the implementation typically involves inserting batch normalization layers into neural network layers. The following is an example.</p>
<div id="cell-35" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SimpleNet(nn.Module):</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.network <span class="op">=</span> nn.Sequential(</span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">784</span>, <span class="dv">256</span>),</span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>            nn.BatchNorm1d(<span class="dv">256</span>),  <span class="co"># 배치 정규화 층</span></span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">256</span>, <span class="dv">10</span>)</span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.network(x)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>PyTorch batch normalization implementation based on the original source code can be simplified as follows, and as done in the previous chapter, it is implemented briefly for learning purposes.</p>
<div id="cell-37" class="cell" data-execution_count="23">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> BatchNorm1d(nn.Module):</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, num_features, eps<span class="op">=</span><span class="fl">1e-5</span>, momentum<span class="op">=</span><span class="fl">0.1</span>):</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.num_features <span class="op">=</span> num_features</span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.eps <span class="op">=</span> eps</span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.momentum <span class="op">=</span> momentum</span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Trainable parameters</span></span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.gamma <span class="op">=</span> nn.Parameter(torch.ones(num_features))  <span class="co"># scale</span></span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.beta <span class="op">=</span> nn.Parameter(torch.zeros(num_features))  <span class="co"># shift</span></span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-15"><a href="#cb22-15" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Running statistics to be tracked</span></span>
<span id="cb22-16"><a href="#cb22-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.register_buffer(<span class="st">'running_mean'</span>, torch.zeros(num_features))</span>
<span id="cb22-17"><a href="#cb22-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.register_buffer(<span class="st">'running_var'</span>, torch.ones(num_features))</span>
<span id="cb22-18"><a href="#cb22-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-19"><a href="#cb22-19" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb22-20"><a href="#cb22-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.training:</span>
<span id="cb22-21"><a href="#cb22-21" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Calculate mini-batch statistics</span></span>
<span id="cb22-22"><a href="#cb22-22" aria-hidden="true" tabindex="-1"></a>            batch_mean <span class="op">=</span> x.mean(dim<span class="op">=</span><span class="dv">0</span>)  <span class="co"># Mean per channel</span></span>
<span id="cb22-23"><a href="#cb22-23" aria-hidden="true" tabindex="-1"></a>            batch_var <span class="op">=</span> x.var(dim<span class="op">=</span><span class="dv">0</span>, unbiased<span class="op">=</span><span class="va">False</span>)  <span class="co"># Variance per channel</span></span>
<span id="cb22-24"><a href="#cb22-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-25"><a href="#cb22-25" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Update running statistics (important)</span></span>
<span id="cb22-26"><a href="#cb22-26" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.running_mean <span class="op">=</span> (<span class="dv">1</span> <span class="op">-</span> <span class="va">self</span>.momentum) <span class="op">*</span> <span class="va">self</span>.running_mean <span class="op">+</span> <span class="va">self</span>.momentum <span class="op">*</span> batch_mean</span>
<span id="cb22-27"><a href="#cb22-27" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.running_var <span class="op">=</span> (<span class="dv">1</span> <span class="op">-</span> <span class="va">self</span>.momentum) <span class="op">*</span> <span class="va">self</span>.running_var <span class="op">+</span> <span class="va">self</span>.momentum <span class="op">*</span> batch_var</span>
<span id="cb22-28"><a href="#cb22-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-29"><a href="#cb22-29" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Normalize</span></span>
<span id="cb22-30"><a href="#cb22-30" aria-hidden="true" tabindex="-1"></a>            x_norm <span class="op">=</span> (x <span class="op">-</span> batch_mean) <span class="op">/</span> torch.sqrt(batch_var <span class="op">+</span> <span class="va">self</span>.eps)</span>
<span id="cb22-31"><a href="#cb22-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-32"><a href="#cb22-32" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb22-33"><a href="#cb22-33" aria-hidden="true" tabindex="-1"></a>            <span class="co"># During inference, use the stored statistics</span></span>
<span id="cb22-34"><a href="#cb22-34" aria-hidden="true" tabindex="-1"></a>            x_norm <span class="op">=</span> (x <span class="op">-</span> <span class="va">self</span>.running_mean) <span class="op">/</span> torch.sqrt(<span class="va">self</span>.running_var <span class="op">+</span> <span class="va">self</span>.eps)</span>
<span id="cb22-35"><a href="#cb22-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-36"><a href="#cb22-36" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Apply scale and shift</span></span>
<span id="cb22-37"><a href="#cb22-37" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.gamma <span class="op">*</span> x_norm <span class="op">+</span> <span class="va">self</span>.beta</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>The biggest difference from the basic implementation is the part that updates statistics during execution. During training, it allows you to finally know the overall average and variance by accumulating the statistical values (average and variance) of the mini-batch as it moves. To track the movement, an exponential moving average (Exponential Moving Average) using momentum (default 0.1) is used. By using the average and variance obtained during training for inference, accurate variance and deviation are applied to the inference data, ensuring consistency between learning and inference.</p>
<p>Of course, this implementation is highly simplified for learning purposes. The location of the reference code is (https://github.com/pytorch/pytorch/blob/main/torch/nn/modules/batchnorm.py). The actual implementation of BatchNorm1d is much more complex. This is because frameworks such as PyTorch and TensorFlow typically include various logic beyond basic logic, including CUDA optimization, gradient optimization, handling various settings, and linking with C/C++.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Click to view contents (Deep Dive: Derivation of Batch Normalization Formula and Detailed Analysis of Backpropagation Process)">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Click to view contents (Deep Dive: Derivation of Batch Normalization Formula and Detailed Analysis of Backpropagation Process)
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<section id="derivation-of-batch-normalization-formula-and-detailed-analysis-of-backpropagation-process" class="level2 callout-body-container callout-body">
<h2 class="anchored" data-anchor-id="derivation-of-batch-normalization-formula-and-detailed-analysis-of-backpropagation-process">Derivation of Batch Normalization Formula and Detailed Analysis of Backpropagation Process</h2>
<p>Batch normalization (BN), proposed by Ioffe &amp; Szegedy in 2015, has become a key technique in deep learning model training. BN accelerates the learning speed by normalizing the input of each layer, alleviates the vanishing/exploding gradient problem, and provides some regularization effect. In this deep dive, we will examine the forward and backward pass processes of BN in detail and analyze its effects mathematically.</p>
<section id="derivation-of-batch-normalization-forward-pass-formula" class="level3">
<h3 class="anchored" data-anchor-id="derivation-of-batch-normalization-forward-pass-formula">Derivation of Batch Normalization Forward Pass Formula</h3>
<p>Batch normalization is performed on a mini-batch basis. Given a mini-batch size of <span class="math inline">\(B\)</span> and a feature dimension of <span class="math inline">\(D\)</span>, the mini-batch input data can be represented as a <span class="math inline">\(B \times D\)</span> matrix <span class="math inline">\(\mathbf{X}\)</span>. Since BN operates independently for each feature dimension, we will consider only one feature dimension for explanation.</p>
<ol type="1">
<li><p><strong>Mini-Batch Mean Calculation:</strong></p>
<p><span class="math inline">\(\mu_B = \frac{1}{B} \sum_{i=1}^{B} x_i\)</span></p>
<p>Here, <span class="math inline">\(x_i\)</span> represents the value of the corresponding feature for the <span class="math inline">\(i\)</span>th sample in the mini-batch.</p></li>
<li><p><strong>Mini-Batch Variance Calculation:</strong></p>
<p><span class="math inline">\(\sigma_B^2 = \frac{1}{B} \sum_{i=1}^{B} (x_i - \mu_B)^2\)</span></p></li>
<li><p><strong>Normalization:</strong></p>
<p><span class="math inline">\(\hat{x_i} = \frac{x_i - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}}\)</span></p>
<p>Here, <span class="math inline">\(\epsilon\)</span> is a small constant to prevent the denominator from being zero.</p></li>
<li><p><strong>Scaling and Shifting:</strong></p>
<p><span class="math inline">\(y_i = \gamma \hat{x_i} + \beta\)</span></p>
<p>Here, <span class="math inline">\(\gamma\)</span> and <span class="math inline">\(\beta\)</span> are learnable parameters responsible for scaling and shifting, respectively. These parameters restore the representation power of the normalized data.</p></li>
</ol>
</section>
<section id="derivation-of-batch-normalization-backward-pass-formula---including-computational-graph" class="level3">
<h3 class="anchored" data-anchor-id="derivation-of-batch-normalization-backward-pass-formula---including-computational-graph">Derivation of Batch Normalization Backward Pass Formula - Including Computational Graph</h3>
<p>The backward pass of batch normalization involves calculating the gradients of the loss function with respect to each parameter using the chain rule. The process can be visually represented through a computational graph as follows (simplified using ASCII art):</p>
<pre><code>     x_i   --&gt;   [-]   --&gt;   [/]   --&gt;   [*]   --&gt;   [+]   --&gt;   y_i
      |          ^          ^          ^          ^
      |          |          |          |          |
      |          |          |          |          +---&gt; beta
      |          |          |          +---&gt; gamma
      |          |          +---&gt; sqrt(...) + epsilon
      |          +---&gt; mu_B, sigma_B^2</code></pre>
<ul>
<li><span class="math inline">\(x_i\)</span>: input</li>
<li><span class="math inline">\([-]\)</span>: subtraction (<span class="math inline">\(x_i - \mu_B\)</span>)</li>
<li><span class="math inline">\([/]\)</span>: division (<span class="math inline">\((x_i - \mu_B) / \sqrt{\sigma_B^2 + \epsilon}\)</span>)</li>
<li><span class="math inline">\([*]\)</span>: multiplication (<span class="math inline">\(\gamma \hat{x_i}\)</span>)</li>
<li><span class="math inline">\([+]\)</span>: addition (<span class="math inline">\(\gamma \hat{x_i} + \beta\)</span>)</li>
<li><span class="math inline">\(y_i\)</span>: output</li>
<li><span class="math inline">\(\mu_B\)</span>: mean</li>
<li><span class="math inline">\(\sigma_B^2\)</span>: variance</li>
<li><span class="math inline">\(\epsilon\)</span>: small number to prevent division by zero</li>
<li><span class="math inline">\(\gamma, \beta\)</span>: learnable parameters Now, let’s calculate the backpropagation for each step. Let the loss function be <span class="math inline">\(\mathcal{L}\)</span> and assume that <span class="math inline">\(\frac{\partial \mathcal{L}}{\partial y_i}\)</span> is given.</li>
</ul>
<ol type="1">
<li><p><strong>Calculation of <span class="math inline">\(\frac{\partial \mathcal{L}}{\partial \beta}\)</span> and <span class="math inline">\(\frac{\partial \mathcal{L}}{\partial \gamma}\)</span>:</strong></p>
<p><span class="math inline">\(\frac{\partial \mathcal{L}}{\partial \beta} = \sum_{i=1}^{B} \frac{\partial \mathcal{L}}{\partial y_i} \cdot \frac{\partial y_i}{\partial \beta} = \sum_{i=1}^{B} \frac{\partial \mathcal{L}}{\partial y_i}\)</span></p>
<p><span class="math inline">\(\frac{\partial \mathcal{L}}{\partial \gamma} = \sum_{i=1}^{B} \frac{\partial \mathcal{L}}{\partial y_i} \cdot \frac{\partial y_i}{\partial \gamma} = \sum_{i=1}^{B} \frac{\partial \mathcal{L}}{\partial y_i} \cdot \hat{x_i}\)</span></p></li>
<li><p><strong>Calculation of <span class="math inline">\(\frac{\partial \mathcal{L}}{\partial \hat{x_i}}\)</span>:</strong></p>
<p><span class="math inline">\(\frac{\partial \mathcal{L}}{\partial \hat{x_i}} = \frac{\partial \mathcal{L}}{\partial y_i} \cdot \frac{\partial y_i}{\partial \hat{x_i}} = \frac{\partial \mathcal{L}}{\partial y_i} \cdot \gamma\)</span></p></li>
<li><p><strong>Calculation of <span class="math inline">\(\frac{\partial \mathcal{L}}{\partial \sigma_B^2}\)</span>:</strong></p>
<p><span class="math inline">\(\frac{\partial \mathcal{L}}{\partial \sigma_B^2} = \sum_{i=1}^{B} \frac{\partial \mathcal{L}}{\partial \hat{x_i}} \cdot \frac{\partial \hat{x_i}}{\partial \sigma_B^2} = \sum_{i=1}^{B} \frac{\partial \mathcal{L}}{\partial \hat{x_i}} \cdot (x_i - \mu_B) \cdot (-\frac{1}{2})(\sigma_B^2 + \epsilon)^{-3/2}\)</span></p></li>
<li><p><strong>Calculation of <span class="math inline">\(\frac{\partial \mathcal{L}}{\partial \mu_B}\)</span>:</strong></p>
<p><span class="math inline">\(\frac{\partial \mathcal{L}}{\partial \mu_B} = \sum_{i=1}^{B} \frac{\partial \mathcal{L}}{\partial \hat{x_i}} \cdot \frac{\partial \hat{x_i}}{\partial \mu_B} + \frac{\partial \mathcal{L}}{\partial \sigma_B^2} \cdot \frac{\partial \sigma_B^2}{\partial \mu_B}  = \sum_{i=1}^{B} \frac{\partial \mathcal{L}}{\partial \hat{x_i}} \cdot \frac{-1}{\sqrt{\sigma_B^2 + \epsilon}} + \frac{\partial \mathcal{L}}{\partial \sigma_B^2} \cdot (-2)\frac{1}{B}\sum_{i=1}^B (x_i-\mu_B)\)</span></p>
<p>Since <span class="math inline">\(\sum_{i=1}^B (x_i - \mu_B) = 0\)</span> <span class="math inline">\(\frac{\partial \mathcal{L}}{\partial \mu_B} =  \sum_{i=1}^{B} \frac{\partial \mathcal{L}}{\partial \hat{x_i}} \cdot \frac{-1}{\sqrt{\sigma_B^2 + \epsilon}}\)</span></p></li>
<li><p><strong><span class="math inline">\(\frac{\partial \mathcal{L}}{\partial x_i}\)</span> calculation:</strong></p>
<p><span class="math inline">\(\frac{\partial \mathcal{L}}{\partial x_i} = \frac{\partial \mathcal{L}}{\partial \hat{x_i}} \cdot \frac{\partial \hat{x_i}}{\partial x_i} + \frac{\partial \mathcal{L}}{\partial \mu_B} \cdot \frac{\partial \mu_B}{\partial x_i}  + \frac{\partial \mathcal{L}}{\partial \sigma_B^2} \cdot \frac{\partial \sigma_B^2}{\partial x_i} = \frac{\partial \mathcal{L}}{\partial \hat{x_i}} \cdot \frac{1}{\sqrt{\sigma_B^2 + \epsilon}} + \frac{\partial \mathcal{L}}{\partial \mu_B} \cdot \frac{1}{B} +  \frac{\partial \mathcal{L}}{\partial \sigma_B^2} \cdot \frac{2}{B}(x_i - \mu_B)\)</span></p></li>
</ol>
</section>
<section id="principle-of-batch-normalization-alleviating-gradient-vanishingexploding-problems" class="level3">
<h3 class="anchored" data-anchor-id="principle-of-batch-normalization-alleviating-gradient-vanishingexploding-problems">Principle of Batch Normalization Alleviating Gradient Vanishing/Exploding Problems</h3>
<p>Batch normalization prevents the input of each layer from becoming extremely large or small by normalizing it, which helps alleviate the gradient vanishing/exploding problems that occur in activation functions such as sigmoid or tanh.</p>
<ul>
<li><strong>Gradient Vanishing Problem:</strong> When the input of an activation function becomes very large or small, its gradient approaches 0, causing the gradient to disappear during backpropagation. Batch normalization alleviates this problem by normalizing the input to have a mean of 0 and a variance of 1, keeping the input within an appropriate range.</li>
<li><strong>Gradient Exploding Problem:</strong> When the input of an activation function becomes very large, its gradient becomes extremely large. Batch normalization limits the range of inputs, thereby alleviating the gradient exploding problem.</li>
</ul>
</section>
<section id="calculation-and-inference-of-running-mean-and-running-variance-in-batch-normalization" class="level3">
<h3 class="anchored" data-anchor-id="calculation-and-inference-of-running-mean-and-running-variance-in-batch-normalization">Calculation and Inference of Running Mean and Running Variance in Batch Normalization</h3>
<p>During training, batch normalization calculates the mean and variance for each mini-batch, but during inference, it requires an estimate of the mean and variance of the entire training data. To achieve this, batch normalization calculates the running mean and running variance during training.</p>
<ul>
<li><p><strong>Running Mean Calculation:</strong></p>
<p><span class="math inline">\(\text{running\_mean} = (1 - \text{momentum}) \times \text{running\_mean} + \text{momentum} \times \mu_B\)</span></p></li>
<li><p><strong>Running Variance Calculation:</strong></p>
<p><span class="math inline">\(\text{running\_var} = (1 - \text{momentum}) \times \text{running\_var} + \text{momentum} \times \sigma_B^2\)</span></p></li>
</ul>
<p>Here, <code>momentum</code> is a hyperparameter that is typically set to a small value such as 0.1 or 0.01.</p>
<p>During inference, the <code>running_mean</code> and <code>running_var</code> calculated during training are used to normalize the input.</p>
</section>
<section id="comparison-of-batch-normalization-with-other-normalization-techniques-layer-normalization-instance-normalization-group-normalization" class="level3">
<h3 class="anchored" data-anchor-id="comparison-of-batch-normalization-with-other-normalization-techniques-layer-normalization-instance-normalization-group-normalization">Comparison of Batch Normalization with Other Normalization Techniques (Layer Normalization, Instance Normalization, Group Normalization)</h3>
<ul>
<li><p><strong>Batch Normalization (BN):</strong> uses statistics from within the mini-batch of samples. It is affected by batch size and is difficult to apply to RNNs.</p></li>
<li><p><strong>Layer Normalization (LN):</strong> uses statistics from within each sample over the feature dimension. It is not affected by batch size and is easy to apply to RNNs.</p></li>
<li><p><strong>Instance Normalization (IN):</strong> calculates statistics independently for each sample and each channel. It is mainly used in image generation tasks such as style transfer.</p></li>
<li><p><strong>Group Normalization (GN):</strong> divides channels into groups and calculates statistics within each group. It can be used as an alternative to BN when the batch size is small.</p></li>
</ul>
<p>Each normalization technique has its own strengths and weaknesses under different circumstances, so it’s necessary to choose the right technique according to the characteristics of the problem and model architecture.</p>
</section>
</section>
</div>
</div>
</section>
<section id="statistical-tracking-and-inference-application" class="level3">
<h3 class="anchored" data-anchor-id="statistical-tracking-and-inference-application">6.4.3 Statistical Tracking and Inference Application</h3>
</section>
</section>
<section id="hyperparameter-optimization" class="level2">
<h2 class="anchored" data-anchor-id="hyperparameter-optimization">6.5 Hyperparameter Optimization</h2>
<p>Hyperparameter optimization has a significant impact on model performance. Its importance began to be recognized in the 1990s. In the late 1990s, it was discovered in Support Vector Machines (SVM) that even with the same model, the parameters of the kernel function (C, gamma, etc.) played a crucial role in performance. Around 2015, it was proven that Bayesian optimization yields better results than manual tuning, which became the core foundation for automated tuning methods like Google AutoML (2017).</p>
<section id="comparison-of-optimization-methodologies" class="level3">
<h3 class="anchored" data-anchor-id="comparison-of-optimization-methodologies">6.5.1 Comparison of Optimization Methodologies</h3>
<p>There are several methods to optimize hyperparameters. The representative methods are as follows:</p>
<ol type="1">
<li><p><strong>Grid Search:</strong> The most basic method, where possible values for each hyperparameter are listed and all combinations of these values are tried. It is useful when the number of hyperparameters is small and the range of values each parameter can take is limited, but it incurs high computational costs because all combinations must be tested. It is suitable for testing simple models or when the search space is very small.</p></li>
<li><p><strong>Random Search:</strong> Hyperparameter values are randomly selected to create combinations, and these combinations are used to train the model and evaluate its performance. If some hyperparameters have a significant impact on performance, it can be more effective than grid search (Bergstra &amp; Bengio, 2012).</p></li>
<li><p><strong>Bayesian Optimization:</strong> Based on previous search results, a probabilistic model (usually Gaussian process) is used to intelligently select the next hyperparameter combination to try. It selects the next exploration point by maximizing an acquisition function. By efficiently exploring the hyperparameter search space, it can find better combinations with fewer trials than grid search or random search.</p></li>
</ol>
<p>There are also other methods such as evolutionary algorithms using genetic algorithms and gradient-based optimization.</p>
<p>The following is an example of optimizing hyperparameters for a simple neural network model using Bayesian optimization.</p>
</section>
<section id="optimization-using-bayes-opt" class="level3">
<h3 class="anchored" data-anchor-id="optimization-using-bayes-opt">6.5.2 Optimization Using Bayes-Opt</h3>
<p>Bayesian optimization began to gain attention in the 2010s. Especially after the publication of the paper “Practical Bayesian Optimization of Machine Learning Algorithms” in 2012, it has become one of the main methods for optimizing hyperparameters of deep learning models. Unlike grid search or random search, its significant advantage is that it intelligently selects the next parameter to explore based on previous trial results.</p>
<p>Bayesian optimization largely repeats the following three steps:</p>
<ol type="1">
<li><strong>Initialization:</strong> Randomly select hyperparameter combinations and train the model to evaluate its performance for the number of times specified by <code>init_points</code>.</li>
<li><strong>Surrogate Model Construction:</strong> Based on the experimental results so far, construct a surrogate model (usually a Gaussian process) that models the relationship between hyperparameters and performance.</li>
<li><strong>Acquisition Function Optimization:</strong> Based on the surrogate model, select the most promising hyperparameter combination to try next. At this time, an acquisition function is used, which determines the next exploration point by balancing “exploration” and “exploitation” based on the current information (surrogate model).</li>
<li>Repeat steps 2-3</li>
</ol>
<div id="cell-42" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.optim <span class="im">as</span> optim</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_04.models.base <span class="im">import</span> SimpleNetwork  </span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_04.utils.data <span class="im">import</span> get_data_loaders, get_device  </span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> bayes_opt <span class="im">import</span> BayesianOptimization</span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_04.experiments.model_training <span class="im">import</span> train_model, eval_loop  </span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train_simple_net(hidden_layers, learning_rate, batch_size, epochs):</span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Trains a SimpleNetwork model with given hyperparameters.</span></span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true" tabindex="-1"></a><span class="co">       Uses CIFAR100 dataset and train_model from Chapter 4.</span></span>
<span id="cb24-13"><a href="#cb24-13" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb24-14"><a href="#cb24-14" aria-hidden="true" tabindex="-1"></a>    device <span class="op">=</span> get_device()  <span class="co"># Use the utility function to get device</span></span>
<span id="cb24-15"><a href="#cb24-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-16"><a href="#cb24-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get data loaders for CIFAR100</span></span>
<span id="cb24-17"><a href="#cb24-17" aria-hidden="true" tabindex="-1"></a>    train_loader, test_loader <span class="op">=</span> get_data_loaders(dataset<span class="op">=</span><span class="st">"CIFAR100"</span>, batch_size<span class="op">=</span>batch_size)</span>
<span id="cb24-18"><a href="#cb24-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-19"><a href="#cb24-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Instantiate the model with specified activation and hidden layers.</span></span>
<span id="cb24-20"><a href="#cb24-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># CIFAR100 images are 3x32x32, so the input size is 3*32*32 = 3072.</span></span>
<span id="cb24-21"><a href="#cb24-21" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> SimpleNetwork(act_func<span class="op">=</span>nn.ReLU(), input_shape<span class="op">=</span><span class="dv">3</span><span class="op">*</span><span class="dv">32</span><span class="op">*</span><span class="dv">32</span>, hidden_shape<span class="op">=</span>hidden_layers, num_labels<span class="op">=</span><span class="dv">100</span>).to(device)</span>
<span id="cb24-22"><a href="#cb24-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-23"><a href="#cb24-23" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Optimizer: Use Adam</span></span>
<span id="cb24-24"><a href="#cb24-24" aria-hidden="true" tabindex="-1"></a>    optimizer <span class="op">=</span> optim.Adam(model.parameters(), lr<span class="op">=</span>learning_rate)</span>
<span id="cb24-25"><a href="#cb24-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-26"><a href="#cb24-26" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Train the model using the training function from Chapter 4</span></span>
<span id="cb24-27"><a href="#cb24-27" aria-hidden="true" tabindex="-1"></a>    results <span class="op">=</span> train_model(model, train_loader, test_loader, device, optimizer<span class="op">=</span>optimizer, epochs<span class="op">=</span>epochs, save_dir<span class="op">=</span><span class="st">"./tmp/tune"</span>,</span>
<span id="cb24-28"><a href="#cb24-28" aria-hidden="true" tabindex="-1"></a>                         retrain<span class="op">=</span><span class="va">True</span>) <span class="co"># retrain=True로 설정</span></span>
<span id="cb24-29"><a href="#cb24-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-30"><a href="#cb24-30" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Return the final test accuracy</span></span>
<span id="cb24-31"><a href="#cb24-31" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> results[<span class="st">'test_accuracies'</span>][<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb24-32"><a href="#cb24-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-33"><a href="#cb24-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-34"><a href="#cb24-34" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train_wrapper(learning_rate, batch_size, hidden1, hidden2):</span>
<span id="cb24-35"><a href="#cb24-35" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Wrapper function for Bayesian optimization."""</span></span>
<span id="cb24-36"><a href="#cb24-36" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> train_simple_net(</span>
<span id="cb24-37"><a href="#cb24-37" aria-hidden="true" tabindex="-1"></a>        hidden_layers<span class="op">=</span>[<span class="bu">int</span>(hidden1), <span class="bu">int</span>(hidden2)],</span>
<span id="cb24-38"><a href="#cb24-38" aria-hidden="true" tabindex="-1"></a>        learning_rate<span class="op">=</span>learning_rate,</span>
<span id="cb24-39"><a href="#cb24-39" aria-hidden="true" tabindex="-1"></a>        batch_size<span class="op">=</span><span class="bu">int</span>(batch_size),</span>
<span id="cb24-40"><a href="#cb24-40" aria-hidden="true" tabindex="-1"></a>        epochs<span class="op">=</span><span class="dv">10</span></span>
<span id="cb24-41"><a href="#cb24-41" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb24-42"><a href="#cb24-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-43"><a href="#cb24-43" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> optimize_hyperparameters():</span>
<span id="cb24-44"><a href="#cb24-44" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Runs hyperparameter optimization."""</span></span>
<span id="cb24-45"><a href="#cb24-45" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Set the parameter ranges to be optimized.</span></span>
<span id="cb24-46"><a href="#cb24-46" aria-hidden="true" tabindex="-1"></a>    pbounds <span class="op">=</span> {</span>
<span id="cb24-47"><a href="#cb24-47" aria-hidden="true" tabindex="-1"></a>        <span class="st">"learning_rate"</span>: (<span class="fl">1e-4</span>, <span class="fl">1e-2</span>),</span>
<span id="cb24-48"><a href="#cb24-48" aria-hidden="true" tabindex="-1"></a>        <span class="st">"batch_size"</span>: (<span class="dv">64</span>, <span class="dv">256</span>),</span>
<span id="cb24-49"><a href="#cb24-49" aria-hidden="true" tabindex="-1"></a>        <span class="st">"hidden1"</span>: (<span class="dv">64</span>, <span class="dv">512</span>),  <span class="co"># First hidden layer</span></span>
<span id="cb24-50"><a href="#cb24-50" aria-hidden="true" tabindex="-1"></a>        <span class="st">"hidden2"</span>: (<span class="dv">32</span>, <span class="dv">256</span>)   <span class="co"># Second hidden layer</span></span>
<span id="cb24-51"><a href="#cb24-51" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb24-52"><a href="#cb24-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-53"><a href="#cb24-53" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create a Bayesian optimization object.</span></span>
<span id="cb24-54"><a href="#cb24-54" aria-hidden="true" tabindex="-1"></a>    optimizer <span class="op">=</span> BayesianOptimization(</span>
<span id="cb24-55"><a href="#cb24-55" aria-hidden="true" tabindex="-1"></a>        f<span class="op">=</span>train_wrapper,</span>
<span id="cb24-56"><a href="#cb24-56" aria-hidden="true" tabindex="-1"></a>        pbounds<span class="op">=</span>pbounds,</span>
<span id="cb24-57"><a href="#cb24-57" aria-hidden="true" tabindex="-1"></a>        random_state<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb24-58"><a href="#cb24-58" aria-hidden="true" tabindex="-1"></a>        allow_duplicate_points<span class="op">=</span><span class="va">True</span></span>
<span id="cb24-59"><a href="#cb24-59" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb24-60"><a href="#cb24-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-61"><a href="#cb24-61" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Run optimization</span></span>
<span id="cb24-62"><a href="#cb24-62" aria-hidden="true" tabindex="-1"></a>    optimizer.maximize(</span>
<span id="cb24-63"><a href="#cb24-63" aria-hidden="true" tabindex="-1"></a>        init_points<span class="op">=</span><span class="dv">4</span>,</span>
<span id="cb24-64"><a href="#cb24-64" aria-hidden="true" tabindex="-1"></a>        n_iter<span class="op">=</span><span class="dv">10</span>,</span>
<span id="cb24-65"><a href="#cb24-65" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb24-66"><a href="#cb24-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-67"><a href="#cb24-67" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Print the best parameters and accuracy</span></span>
<span id="cb24-68"><a href="#cb24-68" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Best parameters found:"</span>)</span>
<span id="cb24-69"><a href="#cb24-69" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Learning Rate: </span><span class="sc">{</span>optimizer<span class="sc">.</span><span class="bu">max</span>[<span class="st">'params'</span>][<span class="st">'learning_rate'</span>]<span class="sc">:.6f}</span><span class="ss">"</span>)</span>
<span id="cb24-70"><a href="#cb24-70" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Batch Size: </span><span class="sc">{</span><span class="bu">int</span>(optimizer.<span class="bu">max</span>[<span class="st">'params'</span>][<span class="st">'batch_size'</span>])<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb24-71"><a href="#cb24-71" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Hidden Layer 1: </span><span class="sc">{</span><span class="bu">int</span>(optimizer.<span class="bu">max</span>[<span class="st">'params'</span>][<span class="st">'hidden1'</span>])<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb24-72"><a href="#cb24-72" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Hidden Layer 2: </span><span class="sc">{</span><span class="bu">int</span>(optimizer.<span class="bu">max</span>[<span class="st">'params'</span>][<span class="st">'hidden2'</span>])<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb24-73"><a href="#cb24-73" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Best accuracy: </span><span class="sc">{</span>optimizer<span class="sc">.</span><span class="bu">max</span>[<span class="st">'target'</span>]<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb24-74"><a href="#cb24-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-75"><a href="#cb24-75" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="va">__name__</span> <span class="op">==</span> <span class="st">"__main__"</span>:</span>
<span id="cb24-76"><a href="#cb24-76" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Starting hyperparameter optimization..."</span>)</span>
<span id="cb24-77"><a href="#cb24-77" aria-hidden="true" tabindex="-1"></a>    optimize_hyperparameters()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>The above example performs hyperparameter optimization using the <code>BayesOpt</code> package. The <code>SimpleNetwork</code> (defined in Chapter 4) is used as the training target, and the CIFAR100 dataset is used. The <code>train_wrapper</code> function serves as the objective function for <code>BayesOpt</code>, training the model with a given combination of hyperparameters and returning the final test accuracy.</p>
<p><code>pbounds</code> specifies the search range for each hyperparameter. In <code>optimizer.maximize</code>, <code>init_points</code> is the number of initial random searches, and <code>n_iter</code> is the number of Bayesian optimization iterations. Therefore, the total number of experiments is <code>init_points + n_iter</code>.</p>
<p>When searching for hyperparameters, note the following:</p>
<ol type="1">
<li><strong>Parameter range:</strong> A range that is too wide increases the search time, while a range that is too narrow may miss the optimal point. Typically, the learning rate is set on a log scale (1e-4 ~ 1e-2), and the number of neurons is set in units of powers of 2.</li>
<li><strong>Number of iterations:</strong> The total number of attempts is roughly set to (number of parameters) x 20, which empirically gives good results. In the above example, there are 4 parameters, so a total of 14 attempts (init_points=4, n_iter=10) may be slightly insufficient. To get better results, consider increasing <code>n_iter</code>.</li>
</ol>
</section>
<section id="optimization-using-botorch" class="level3">
<h3 class="anchored" data-anchor-id="optimization-using-botorch">6.5.3 Optimization using BoTorch</h3>
<p>The BoTorch framework has recently gained attention in the field of deep learning hyperparameter optimization. BoTorch is a Bayesian optimization framework based on PyTorch, developed by FAIR (Facebook AI Research, now Meta AI) in 2019. Bayes-Opt is an older Bayesian optimization library that has been developed since 2016 and provides an intuitive and simple interface (scikit-learn style API), making it widely used.</p>
<p>The advantages and disadvantages of the two libraries are clear.</p>
<ul>
<li><strong>BoTorch:</strong>
<ul>
<li><strong>Advantages:</strong> Provides features specialized for deep learning model hyperparameter optimization, such as integration with deep learning models, GPU acceleration, high sampling efficiency, various advanced Bayesian optimization techniques (multi-fidelity, multi-task, constrained optimization, etc.), and automatic differentiation support. It is particularly suitable for large-scale models, high-dimensional parameter spaces, and experiments with high computational costs.</li>
<li><strong>Disadvantages:</strong> Requires relatively more learning compared to Bayes-Opt, and the initial setup can be complex.</li>
</ul></li>
<li><strong>Bayes-Opt:</strong>
<ul>
<li><strong>Advantages:</strong> Provides a simple and intuitive API, making it easy to use. Installation is straightforward, and there are many tutorials and example codes available.</li>
<li><strong>Disadvantages:</strong> Lacks advanced features compared to BoTorch, and integration with deep learning models is relatively less smooth. Performance may degrade for large-scale/high-dimensional problems.</li>
</ul></li>
</ul>
<p>Therefore, Bayes-Opt is suitable for simple problems or rapid prototyping, while BoTorch is recommended for complex hyperparameter optimization of deep learning models, large-scale/high-dimensional problems, or advanced Bayesian optimization techniques (e.g., multi-task, constrained optimization).</p>
<p>To use BoTorch, unlike Bayes-Opt, you need to understand a few key concepts required for initial setup: surrogate models, input data normalization, and acquisition functions.</p>
<ol type="1">
<li><p><strong>Surrogate Model:</strong></p>
<p>A surrogate model is a model that approximates the actual objective function (in this case, the validation accuracy of a deep learning model). Typically, a Gaussian process (GP) is used. GP is used to predict results quickly and cheaply instead of the actual objective function, which can be computationally expensive. BoTorch provides the following GP models:</p>
<ul>
<li><code>SingleTaskGP</code>: The most basic Gaussian process model, suitable for single-objective optimization problems and effective with relatively small datasets (less than 1000 data points).</li>
<li><code>MultiTaskGP</code>: Used for multi-objective optimization problems where multiple objective functions are optimized simultaneously. For example, you can optimize both the accuracy and inference time of a model.</li>
<li><code>SAASBO</code> (Sparsity-Aware Adaptive Subspace Bayesian Optimization): A model specialized for high-dimensional parameter spaces. It assumes sparsity in high-dimensional spaces and performs efficient exploration.</li>
</ul></li>
<li><p><strong>Input Data Normalization:</strong></p>
<p>Gaussian processes are sensitive to the scale of the data, so normalizing the input data (hyperparameters) is crucial. Typically, all hyperparameters are transformed to the range [0, 1]. BoTorch provides <code>Normalize</code> and <code>Standardize</code> transformations.</p></li>
<li><p><strong>Acquisition Function:</strong></p>
<p>The acquisition function determines the next point to evaluate based on the current surrogate model. It balances exploration (searching for new areas) and exploitation (focusing on areas with high predicted performance). Common acquisition functions include probability of improvement (PI), expected improvement (EI), and Thompson sampling. BoTorch provides a variety of acquisition functions, including <code>ProbabilityOfImprovement</code>, <code>ExpectedImprovement</code>, and <code>UpperConfidenceBound</code>. The acquisition function is based on a surrogate model (GP) and is used to determine the next hyperparameter combination to experiment with. The acquisition function balances the trade-off between “exploration” and “exploitation”. BoTorch provides the following various acquisition functions.</p></li>
</ol>
<ul>
<li><code>ExpectedImprovement (EI)</code>: One of the most common acquisition functions, considering both the probability of getting a better result than the current optimum and the magnitude of improvement.</li>
<li><code>LogExpectedImprovement (LogEI)</code>: A log-transformed version of EI, which is numerically more stable and sensitive to small changes.</li>
<li><code>UpperConfidenceBound (UCB)</code>: An acquisition function that focuses more on exploration, actively exploring areas with high uncertainty.</li>
<li><code>ProbabilityOfImprovement (PI)</code>: Represents the probability of improvement over the current optimum.</li>
<li><code>qExpectedImprovement (qEI)</code>: Also known as q-batch EI, used for parallel optimization, selecting multiple candidates at once.</li>
<li><code>qNoisyExpectedImprovement (qNEI)</code>: Q-batch Noisy EI, used in noisy environments.</li>
</ul>
<p>The entire code is located in <code>package/botorch_optimization.py</code> and can be run directly from the command line. Since the full code contains detailed comments, we will only explain the important parts here.</p>
<div id="cell-45" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, max_trials: <span class="bu">int</span> <span class="op">=</span> <span class="dv">80</span>, init_samples: <span class="bu">int</span> <span class="op">=</span> <span class="dv">10</span>):</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.param_bounds <span class="op">=</span> torch.tensor([</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>        [<span class="fl">1e-4</span>, <span class="fl">64.0</span>, <span class="fl">32.0</span>, <span class="fl">32.0</span>],      <span class="co"># 최소값</span></span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>        [<span class="fl">1e-2</span>, <span class="fl">256.0</span>, <span class="fl">512.0</span>, <span class="fl">512.0</span>]    <span class="co"># 최대값</span></span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>    ], dtype<span class="op">=</span>torch.float64)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>In the initialization part, the minimum and maximum values of each hyperparameter are set. max_trials is the total number of attempts, and init_samples is the initial number of random experiments (same as init_points in Bayes-Opt). init_samples is typically set to 2-3 times the number of parameters. In the above example, since there are 4 hyperparameters, around 8-12 would be suitable. torch.float64 is used for numerical stability. Bayesian optimization, especially Gaussian process, uses Cholesky decomposition when calculating kernel matrices, and in this process, float32 can cause errors due to precision issues.</p>
<div id="cell-47" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> tune(<span class="va">self</span>):</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 가우시안 프로세스 모델 학습</span></span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> SingleTaskGP(configs, accuracies)</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>    mll <span class="op">=</span> ExactMarginalLogLikelihood(model.likelihood, model)</span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>    fit_gpytorch_mll(mll)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>We use <code>SingleTaskGP</code> as a surrogate model based on Gaussian processes. <code>ExactMarginalLogLikelihood</code> is the loss function for model training, and <code>fit_gpytorch_mll</code> trains the model using this loss function.</p>
<div id="cell-49" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>acq_func <span class="op">=</span> LogExpectedImprovement(</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>    model, </span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>    best_f<span class="op">=</span>accuracies.<span class="bu">max</span>().item()</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>The acquisition function used is <code>LogExpectedImprovement</code>. It uses a log, so it has high numerical stability and is sensitive to small changes.</p>
<div id="cell-51" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>candidate, _ <span class="op">=</span> optimize_acqf(                                   <span class="co"># 획득 함수 최적화로 다음 실험할 파라미터 선택</span></span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>    acq_func, bounds<span class="op">=</span>bounds,                                    <span class="co"># 획득 함수와 파라미터 범위 지정</span></span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>    q<span class="op">=</span><span class="dv">1</span>,                                                        <span class="co"># 한 번에 하나의 설정만 선택</span></span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>    num_restarts<span class="op">=</span><span class="dv">10</span>,                                            <span class="co"># 최적화 재시작 횟수</span></span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a>    raw_samples<span class="op">=</span><span class="dv">512</span>                                             <span class="co"># 초기 샘플링 수</span></span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>The <code>optimize_acqf</code> function optimizes the acquisition function to select the next hyperparameter combination (<code>candidate</code>) for experimentation.</p>
<ul>
<li><code>q=1</code>: Selects one candidate at a time (not q-batch optimization).</li>
<li><code>num_restarts=10</code>: Performs each optimization step 10 times with different starting points to prevent getting stuck in local optima.</li>
<li><code>raw_samples=512</code>: Draws 512 samples from the Gaussian process to estimate the acquisition function value.</li>
</ul>
<p><code>num_restarts</code> and <code>raw_samples</code> significantly impact the exploration-exploitation trade-off in Bayesian optimization. <code>num_restarts</code> determines the thoroughness of the optimization, while <code>raw_samples</code> determines the accuracy of the acquisition function estimation. Larger values increase computational cost but provide a higher chance of better results. The following values can be used:</p>
<ul>
<li>Fast execution: <code>num_restarts=5</code>, <code>raw_samples=256</code></li>
<li>Balanced setting: <code>num_restarts=10</code>, <code>raw_samples=512</code></li>
<li>Accuracy-focused: <code>num_restarts=20</code>, <code>raw_samples=1024</code></li>
</ul>
<div id="cell-53" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_06.botorch_optimizer <span class="im">import</span> run_botorch_optimization</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>run_botorch_optimization(max_trials<span class="op">=</span><span class="dv">80</span>, init_samples<span class="op">=</span><span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><strong>Results</strong> Dataset: FashionMNIST Epochs: 20 Initial experiments: 5 times Repeated experiments: 80 times</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Optimal parameters</th>
<th>Bayes-Opt</th>
<th>Botorch</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Learning rate</td>
<td>6e-4</td>
<td>1e-4</td>
</tr>
<tr class="even">
<td>Batch size</td>
<td>173</td>
<td>158</td>
</tr>
<tr class="odd">
<td>hid 1</td>
<td>426</td>
<td>512</td>
</tr>
<tr class="even">
<td>hid 2</td>
<td>197</td>
<td>512</td>
</tr>
<tr class="odd">
<td>Accuracy</td>
<td>0.7837</td>
<td>0.8057</td>
</tr>
</tbody>
</table>
<p>Although this is a simple comparison, Botorch’s accuracy is higher. For simple parameter tuning, Bayes-Opt is recommended, while for professional tuning, Botorch is recommended.</p>
</section>
</section>
<section id="gaussian-process" class="level2">
<h2 class="anchored" data-anchor-id="gaussian-process">6.6 Gaussian Process</h2>
<blockquote class="blockquote">
<p><strong>Challenge:</strong> What methods can quantify the predictive uncertainty of a model and utilize it for active learning?</p>
<p><strong>Researcher’s Concern:</strong> Traditional deep learning models provide point estimates as predictions, but in real-world applications, knowing the uncertainty of predictions is crucial. For example, an autonomous vehicle needs to know how uncertain its prediction of a pedestrian’s next location is to drive safely. Gaussian processes were powerful tools for quantifying predictive uncertainty based on Bayesian probability, but they had limitations such as high computational complexity and difficulty in applying them to large-scale data.</p>
</blockquote>
<p>Gaussian Process (GP) is a core model in Bayesian machine learning that provides predictions with uncertainty. Previously, we briefly looked at using Gaussian processes as surrogate models in Bayesian optimization; here, we will delve deeper into the basic principles and importance of Gaussian processes themselves.</p>
<p>GP is defined as a “probability distribution over a set of function values.” Unlike deterministic functions like <span class="math inline">\(y = f(x)\)</span>, GP does not predict a single output value <span class="math inline">\(y\)</span> for a given input <span class="math inline">\(x\)</span>, but instead predicts a <em>distribution</em> of possible output values. For instance, instead of predicting “tomorrow’s high temperature will be 25 degrees” deterministically, it predicts “there is a 95% chance that tomorrow’s high temperature will be between 23 and 27 degrees,” providing uncertainty along with the prediction. If you are riding a bike back home, the general route is already determined, but the actual route will vary each time. This requires predictions that include uncertainty rather than deterministic ones.</p>
<p>The mathematical basis for tools that handle predictions with uncertainty is the normal distribution (Gaussian distribution) proposed by the 19th-century mathematician Gauss. Based on this, GP developed in the 1940s. At that time, during World War II, scientists had to deal with more uncertain data than ever before, such as radar signal processing, code decryption, and weather information processing. A notable example is Norbert Wiener’s effort to improve anti-aircraft gun accuracy by predicting the future position of an aircraft. He conceived the “Wiener process,” which views the movement of an airplane as a stochastic process (current location - predictable to some extent after a while - increasing uncertainty over time), laying important groundwork for GP. Around the same period, Harald Cramér was working on the mathematical foundations of GP in time series analysis, and Andrey Kolmogorov in probability theory. In 1951, Daniel Krige created a practical application of GP for predicting ore distributions. Subsequently, in the 1970s, statisticians systematized GP for applications in spatial statistics, computer experimental design, and Bayesian optimization in ML. Today, GP plays a core role in almost every field that deals with uncertainty, including AI, robotics, climate prediction, and more. Especially in deep learning, deep kernel GP through meta-learning has recently gained attention and shows excellent performance, particularly in fields like molecular property prediction.</p>
<p>Today, GP is being utilized in various fields such as AI, robotics, climate modeling, and more. Especially in deep learning, recent advancements in meta-learning through deep kernel GP and applications like molecular property prediction have shown outstanding performance.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Click to view contents (Deep Dive: Mathematical Foundations of Gaussian Processes and Machine Learning Applications)">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Click to view contents (Deep Dive: Mathematical Foundations of Gaussian Processes and Machine Learning Applications)
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<section id="mathematical-foundations-of-gaussian-processes-and-machine-learning-applications" class="level2 callout-body-container callout-body">
<h2 class="anchored" data-anchor-id="mathematical-foundations-of-gaussian-processes-and-machine-learning-applications">Mathematical Foundations of Gaussian Processes and Machine Learning Applications</h2>
<p>Gaussian Process (GP) is a probabilistic model based on kernel methods, widely used for regression and classification problems. GP has the advantage of defining a distribution over functions themselves, allowing for quantification of prediction uncertainty. In this deep dive, we explore the mathematical foundations of Gaussian Processes from multivariate normal distributions to stochastic processes, and delve into various machine learning applications.</p>
<section id="multivariate-normal-distribution" class="level3">
<h3 class="anchored" data-anchor-id="multivariate-normal-distribution">1. Multivariate Normal Distribution</h3>
<p>The first step in understanding Gaussian Processes is understanding multivariate normal distributions. A <span class="math inline">\(d\)</span>-dimensional random vector <span class="math inline">\(\mathbf{x} = (x_1, x_2, ..., x_d)^T\)</span> following a multivariate normal distribution means it has the following probability density function:</p>
<p><span class="math inline">\(p(\mathbf{x}) = \frac{1}{(2\pi)^{d/2}|\mathbf{\Sigma}|^{1/2}} \exp\left(-\frac{1}{2}(\mathbf{x} - \boldsymbol{\mu})^T \mathbf{\Sigma}^{-1} (\mathbf{x} - \boldsymbol{\mu})\right)\)</span></p>
<p>where <span class="math inline">\(\boldsymbol{\mu} \in \mathbb{R}^d\)</span> is the mean vector and <span class="math inline">\(\mathbf{\Sigma} \in \mathbb{R}^{d \times d}\)</span> is the covariance matrix. The covariance matrix must be a positive definite matrix.</p>
<p><strong>Key Properties:</strong></p>
<ul>
<li><p><strong>Linear Transformation:</strong> A linear transformation of a random variable following a multivariate normal distribution still follows a multivariate normal distribution. That is, if <span class="math inline">\(\mathbf{x} \sim \mathcal{N}(\boldsymbol{\mu}, \mathbf{\Sigma})\)</span> and <span class="math inline">\(\mathbf{y} = \mathbf{A}\mathbf{x} + \mathbf{b}\)</span>, then <span class="math inline">\(\mathbf{y} \sim \mathcal{N}(\mathbf{A}\boldsymbol{\mu} + \mathbf{b}, \mathbf{A}\mathbf{\Sigma}\mathbf{A}^T)\)</span>.</p></li>
<li><p><strong>Conditional Distribution:</strong> The conditional distribution of a multivariate normal distribution is also normal. By partitioning <span class="math inline">\(\mathbf{x}\)</span> into <span class="math inline">\(\mathbf{x} = (\mathbf{x}_1, \mathbf{x}_2)^T\)</span> and similarly partitioning the mean and covariance matrix:</p>
<p><span class="math inline">\(\boldsymbol{\mu} = \begin{pmatrix} \boldsymbol{\mu}_1 \\ \boldsymbol{\mu}_2 \end{pmatrix}, \quad \mathbf{\Sigma} = \begin{pmatrix} \mathbf{\Sigma}_{11} &amp; \mathbf{\Sigma}_{12} \\ \mathbf{\Sigma}_{21} &amp; \mathbf{\Sigma}_{22} \end{pmatrix}\)</span></p>
<p>The conditional distribution of <span class="math inline">\(\mathbf{x}_2\)</span> given <span class="math inline">\(\mathbf{x}_1\)</span> is:</p>
<p><span class="math inline">\(p(\mathbf{x}_2 | \mathbf{x}_1) = \mathcal{N}(\boldsymbol{\mu}_{2|1}, \mathbf{\Sigma}_{2|1})\)</span></p>
<p><span class="math inline">\(\boldsymbol{\mu}_{2|1} = \boldsymbol{\mu}_2 + \mathbf{\Sigma}_{21}\mathbf{\Sigma}_{11}^{-1}(\mathbf{x}_1 - \boldsymbol{\mu}_1)\)</span> <span class="math inline">\(\mathbf{\Sigma}_{2|1} = \mathbf{\Sigma}_{22} - \mathbf{\Sigma}_{21}\mathbf{\Sigma}_{11}^{-1}\mathbf{\Sigma}_{12}\)</span></p></li>
<li><p><strong>Marginal Distribution:</strong> The marginal distribution of a multivariate normal distribution is also a normal distribution. In the partition above, the marginal distribution of <span class="math inline">\(\mathbf{x}_1\)</span> is as follows. <span class="math inline">\(p(\mathbf{x}_1) = \mathcal{N}(\boldsymbol{\mu_1}, \mathbf{\Sigma}_{11})\)</span></p></li>
</ul>
</section>
<section id="definition-and-interpretation-of-gaussian-process" class="level3">
<h3 class="anchored" data-anchor-id="definition-and-interpretation-of-gaussian-process">2. Definition and Interpretation of Gaussian Process</h3>
<p>A Gaussian process is a probability distribution over <em>functions</em>. That is, if some function <span class="math inline">\(f(x)\)</span> follows a Gaussian process, it means that the vector of function values at any finite set of input points <span class="math inline">\(\{x_1, x_2, ..., x_n\}\)</span>, <span class="math inline">\((f(x_1), f(x_2), ..., f(x_n))^T\)</span>, follows a multivariate normal distribution.</p>
<p><strong>Definition:</strong> A Gaussian process is defined by a mean function <span class="math inline">\(m(x)\)</span> and a covariance function (or kernel function) <span class="math inline">\(k(x, x')\)</span>.</p>
<p><span class="math inline">\(f(x) \sim \mathcal{GP}(m(x), k(x, x'))\)</span></p>
<ul>
<li><strong>Mean Function:</strong> <span class="math inline">\(m(x) = \mathbb{E}[f(x)]\)</span></li>
<li><strong>Covariance Function:</strong> <span class="math inline">\(k(x, x') = \mathbb{E}[(f(x) - m(x))(f(x') - m(x'))]\)</span></li>
</ul>
<p><strong>Stochastic Process Perspective:</strong> A Gaussian process is a type of stochastic process that assigns a random variable to each element in the index set (here, the input space). In a Gaussian process, these random variables form a joint Gaussian distribution.</p>
</section>
<section id="role-and-introduction-of-kernel-function" class="level3">
<h3 class="anchored" data-anchor-id="role-and-introduction-of-kernel-function">3. Role and Introduction of Kernel Function</h3>
<p>The kernel function is one of the most important elements in a Gaussian process. The kernel function represents the similarity between two inputs <span class="math inline">\(x\)</span> and <span class="math inline">\(x'\)</span> and determines the properties of the Gaussian process.</p>
<p><strong>Key Roles:</strong></p>
<ul>
<li><strong>Covariance Definition:</strong> The kernel function defines the covariance between function values. That is, <span class="math inline">\(k(x, x')\)</span> represents the covariance between <span class="math inline">\(f(x)\)</span> and <span class="math inline">\(f(x')\)</span>.</li>
<li><strong>Function Smoothness Determination:</strong> The kernel function determines the smoothness of the generated function. For example, the RBF kernel generates infinitely differentiable functions, while the Matern kernel can adjust differentiability.</li>
<li><strong>Positive Definiteness:</strong> To be a valid covariance function, the kernel function must satisfy positive definiteness. That is, for any input points, the resulting kernel matrix (Gram matrix) must be a positive definite matrix.</li>
</ul>
<p><strong>Various Kernel Functions:</strong></p>
<ul>
<li><p><strong>RBF (Radial Basis Function) Kernel (or Squared Exponential Kernel):</strong></p>
<p><span class="math inline">\(k(x, x') = \sigma^2 \exp\left(-\frac{\|x - x'\|^2}{2l^2}\right)\)</span></p>
<ul>
<li><span class="math inline">\(\sigma^2\)</span>: variance</li>
<li><span class="math inline">\(l\)</span>: length scale</li>
<li>Generates very smooth functions.</li>
</ul></li>
<li><p><strong>Matern Kernel:</strong></p>
<p><span class="math inline">\(k(x, x') = \sigma^2 \frac{2^{1-\nu}}{\Gamma(\nu)}\left(\sqrt{2\nu}\frac{\|x - x'\|}{l}\right)^\nu K_\nu\left(\sqrt{2\nu}\frac{\|x - x'\|}{l}\right)\)</span></p></li>
<li><p><span class="math inline">\(\nu\)</span>: smoothness parameter</p></li>
<li><p><span class="math inline">\(K_\nu\)</span>: modified Bessel function</p></li>
<li><p><span class="math inline">\(\nu = 1/2, 3/2, 5/2\)</span> and other half-integer values are commonly used.</p></li>
<li><p>As <span class="math inline">\(\nu\)</span> increases, it approaches the RBF kernel.</p></li>
<li><p><strong>Periodic kernel:</strong></p>
<p><span class="math inline">\(k(x, x') = \sigma^2 \exp\left(-\frac{2\sin^2(\pi|x-x'|/p)}{l^2}\right)\)</span></p>
<ul>
<li><span class="math inline">\(p\)</span>: period</li>
</ul></li>
<li><p><strong>Linear kernel:</strong></p>
<p><span class="math inline">\(k(x,x') = \sigma_b^2 + \sigma_v^2(x - c)(x' -c)\)</span></p></li>
</ul>
</section>
<section id="solving-regression-and-classification-problems-using-gaussian-processes" class="level3">
<h3 class="anchored" data-anchor-id="solving-regression-and-classification-problems-using-gaussian-processes">4. Solving Regression and Classification Problems using Gaussian Processes</h3>
<p><strong>Regression:</strong></p>
<p>Gaussian process regression is the problem of predicting the output <span class="math inline">\(f(\mathbf{x}_*)\)</span> for a new input <span class="math inline">\(\mathbf{x}_*\)</span> given the training data <span class="math inline">\(\mathcal{D} = \{(\mathbf{x}_i, y_i)\}_{i=1}^n\)</span>. The prior distribution of the Gaussian process and the training data are combined to compute the posterior distribution, which is then used to obtain the predictive distribution.</p>
<ul>
<li><strong>Prior distribution:</strong> <span class="math inline">\(f(\mathbf{x}) \sim \mathcal{GP}(0, k(\mathbf{x}, \mathbf{x}'))\)</span> (assuming the mean function is 0 for simplicity)</li>
<li><strong>Training data:</strong> <span class="math inline">\(\mathbf{y} = (y_1, y_2, ..., y_n)^T\)</span></li>
<li><strong>Kernel matrix:</strong> <span class="math inline">\(\mathbf{K} = k(\mathbf{X}, \mathbf{X})\)</span>, where <span class="math inline">\(\mathbf{X} = [\mathbf{x}_1, \mathbf{x}_2, ..., \mathbf{x}_n]\)</span></li>
<li><strong>Predictive distribution:</strong> <span class="math inline">\(p(f(\mathbf{x}_*) | \mathbf{y}, \mathbf{X}, \mathbf{x}_*) = \mathcal{N}(\mu_*, \sigma_*^2)\)</span>
<ul>
<li><span class="math inline">\(\mu_* = \mathbf{k}_*^T \mathbf{K}^{-1} \mathbf{y}\)</span></li>
<li><span class="math inline">\(\sigma_*^2 = k(\mathbf{x}_*, \mathbf{x}_*) - \mathbf{k}_*^T \mathbf{K}^{-1} \mathbf{k}_*\)</span></li>
<li><span class="math inline">\(\mathbf{k}_* = [k(\mathbf{x}_*, \mathbf{x}_1), k(\mathbf{x}_*, \mathbf{x}_2), ..., k(\mathbf{x}_*, \mathbf{x}_n)]^T\)</span></li>
</ul></li>
</ul>
<p><strong>Classification:</strong></p>
<p>Gaussian process classification models the latent function <span class="math inline">\(f(\mathbf{x})\)</span> as a Gaussian process and defines the classification probability through this latent function. For example, in binary classification problems, the logistic function or probit function is used to transform the latent function value into a probability.</p>
<ul>
<li><strong>Latent function:</strong> <span class="math inline">\(f(\mathbf{x}) \sim \mathcal{GP}(0, k(\mathbf{x}, \mathbf{x}'))\)</span></li>
<li><strong>Binary classification:</strong> <span class="math inline">\(p(y = 1 | f(\mathbf{x})) = \sigma(f(\mathbf{x}))\)</span> (where <span class="math inline">\(\sigma\)</span> is the logistic function)</li>
</ul>
<p>In classification problems, the posterior distribution does not have a closed form, so approximate inference methods such as Laplace approximation or variational inference are used. ### 5. Gaussian Process’s Advantages and Disadvantages and Comparison with Deep Learning</p>
<p><strong>Advantages:</strong></p>
<ul>
<li><strong>Quantification of Uncertainty:</strong> Provides the uncertainty of predictions in the form of predictive variance.</li>
<li><strong>Data Efficiency:</strong> Can perform well with a relatively small amount of data.</li>
<li><strong>Flexibility in Kernel Selection:</strong> Allows for the design of models tailored to the characteristics of the problem by using various kernel functions.</li>
<li><strong>Bayesian Interpretation:</strong> Naturally interpretable within the Bayesian framework.</li>
</ul>
<p><strong>Disadvantages:</strong></p>
<ul>
<li><strong>Computational Complexity:</strong> Has a computational complexity of <span class="math inline">\(O(n^3)\)</span> for training data size <span class="math inline">\(n\)</span>. (Due to inverse matrix calculation)</li>
<li><strong>Model Selection:</strong> Selecting an appropriate kernel function and hyperparameters is crucial, which can be challenging.</li>
<li><strong>High-Dimensional Input:</strong> Performance may degrade in high-dimensional input spaces.</li>
</ul>
<p><strong>Comparison with Deep Learning:</strong></p>
<ul>
<li><strong>Data Requirements:</strong> Deep learning models generally require much more data than Gaussian processes.</li>
<li><strong>Computational Cost:</strong> Deep learning models are computationally expensive to train but relatively fast for inference. Gaussian processes are fast to train (especially with small datasets) but incur a computational cost proportional to the data size for inference.</li>
<li><strong>Uncertainty:</strong> Deep learning models typically do not provide uncertainty of predictions. (Though there are exceptions, such as Bayesian deep learning.)</li>
<li><strong>Expressiveness:</strong> Deep learning models can express very complex functions, whereas Gaussian processes have their expressiveness limited by the kernel function.</li>
<li><strong>Interpretability:</strong> Gaussian processes can explicitly state model assumptions through kernel functions and quantify the uncertainty of prediction results.</li>
</ul>
<p>Recently, models combining deep learning and Gaussian processes (e.g., Deep Kernel Learning) are also being researched.</p>
</section>
</section>
</div>
</div>
<section id="mathematical-foundations-of-uncertainty-handling" class="level3">
<h3 class="anchored" data-anchor-id="mathematical-foundations-of-uncertainty-handling">6.6.1 Mathematical Foundations of Uncertainty Handling</h3>
<p>Generally, we think of a function as a single line, but the Gaussian process considers it as “a set of possible lines”. Mathematically, this is represented as:</p>
<p><span class="math inline">\(f(t) \sim \mathcal{GP}(m(t), k(t,t'))\)</span></p>
<p>For example, if we consider the location of a bicycle, <span class="math inline">\(m(t)\)</span> is the mean function that represents the prediction “it will mostly follow this path”. <span class="math inline">\(k(t,t')\)</span> is the covariance function (or kernel) that represents “how related are the locations at different times?” There are several representative kernels. One of the most commonly used kernel functions is the RBF (Radial Basis Function).</p>
<p><span class="math inline">\(k(t,t') = \sigma^2 \exp\left(-\frac{(t-t')^2}{2l^2}\right)\)</span></p>
<p>This formula is very intuitive. The closer the two time points <span class="math inline">\(t\)</span> and <span class="math inline">\(t'\)</span> are, the larger the value, and the farther apart they are, the smaller the value. It’s similar to “if I know the current location, I can predict the location after a while, but I don’t know the location in the distant future”.</p>
<p>Let’s assume the kernel (<span class="math inline">\(K\)</span>) is RBF and look at an actual example. Suppose you have a bicycle-sharing service (or an autonomous vehicle). We want to estimate the entire moving path of the bicycle based on only a few observed GPS data points.</p>
<p><strong>Basic Prediction Formula</strong></p>
<p><span class="math inline">\(f_* | X, y, X_* \sim \mathcal{N}(\mu_*, \Sigma_*)\)</span></p>
<p>This formula represents “based on the GPS records (<span class="math inline">\(X\)</span>, <span class="math inline">\(y\)</span>) we have, the location of the bicycle at an unknown time (<span class="math inline">\(X_*\)</span>) follows a normal distribution with mean <span class="math inline">\(\mu_*\)</span> and uncertainty <span class="math inline">\(\Sigma_*\)</span>”.</p>
<p><strong>Location Prediction Calculation</strong></p>
<p><span class="math inline">\(\mu_* = K_*K^{-1}y\)</span></p>
<p>This formula shows how to predict the location of the bicycle. <span class="math inline">\(K_*\)</span> represents the ‘temporal correlation’ between the predicted time point and the GPS recorded time points, <span class="math inline">\(K^{-1}\)</span> represents the ‘weight adjustment’ considering the relationship between the GPS records, and <span class="math inline">\(y\)</span> represents the actual GPS-recorded locations. For example, when predicting the location at 2:15 PM: 1. Refer to the GPS records (<span class="math inline">\(y\)</span>) at 2:10 PM and 2:20 PM, 2. Consider the time difference (<span class="math inline">\(K_*\)</span>) between each time point, 3. Reflect the temporal continuity (<span class="math inline">\(K^{-1}\)</span>) of the GPS records.</p>
<p><strong>Uncertainty Estimation</strong></p>
<p><span class="math inline">\(\Sigma_* = K_{**} - K_*K^{-1}K_*^T\)</span></p>
<p>This formula calculates the uncertainty of the location prediction. <span class="math inline">\(K_{**}\)</span> calculates the ‘basic uncertainty’ of the predicted time point, and <span class="math inline">\(K_*K^{-1}K_*^T\)</span> calculates the ‘decreasing uncertainty’ due to the GPS records. <span class="math inline">\(K\)</span> is a matrix representing the relationship between existing observation data, so the value increases as the data becomes more dense. <span class="math inline">\(K_*\)</span> represents the relationship between the new prediction point and the existing data, so it can consider more surrounding data as the data becomes more dense.</p>
<p>In real-world situations: 1. Initially, assume the bicycle can go anywhere (<span class="math inline">\(K_{**}\)</span> is large) 2. As there are more GPS records (<span class="math inline">\(K_*\)</span> increases) 3. And the records are consistent (<span class="math inline">\(K^{-1}\)</span> becomes stable) 4. The uncertainty of the location estimation decreases.</p>
<p><strong>Effect of Data on Formulas</strong> The prediction with uncertainty, depending on the amount of GPS data, appears as follows: 1. Frequent GPS recording sections: low uncertainty - <span class="math inline">\(K_*\)</span> is large and there is a lot of data, so <span class="math inline">\(K_*K^{-1}K_*^T\)</span> is large - Therefore, <span class="math inline">\(\Sigma_*\)</span> is small, making the path estimation accurate 2. Infrequent GPS recording sections: high uncertainty - <span class="math inline">\(K_*\)</span> is small and there is little data, so <span class="math inline">\(K_*K^{-1}K_*^T\)</span> is small - Therefore, <span class="math inline">\(\Sigma_*\)</span> is large, making the path estimation uncertain</p>
<p>Simply put, as the amount of dense time-interval data increases, <span class="math inline">\(K\)</span> becomes larger, and the uncertainty decreases.</p>
<p>Let’s understand this using an example of predicting a bicycle path.</p>
<div id="cell-58" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a><span class="co"># 시각화 스타일 설정</span></span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a>sns.set_style(<span class="st">"whitegrid"</span>)</span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a>plt.rcParams[<span class="st">'font.size'</span>] <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-9"><a href="#cb30-9" aria-hidden="true" tabindex="-1"></a><span class="co"># 데이터셋 1: 5개 관측점</span></span>
<span id="cb30-10"><a href="#cb30-10" aria-hidden="true" tabindex="-1"></a>time1 <span class="op">=</span> np.array([<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">5</span>, <span class="dv">8</span>, <span class="dv">10</span>]).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb30-11"><a href="#cb30-11" aria-hidden="true" tabindex="-1"></a>position1 <span class="op">=</span> np.array([<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">1</span>, <span class="dv">4</span>])</span>
<span id="cb30-12"><a href="#cb30-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-13"><a href="#cb30-13" aria-hidden="true" tabindex="-1"></a><span class="co"># 데이터셋 2: 8개 관측점</span></span>
<span id="cb30-14"><a href="#cb30-14" aria-hidden="true" tabindex="-1"></a>time2 <span class="op">=</span> np.array([<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">4</span>, <span class="dv">5</span>, <span class="dv">6</span>, <span class="dv">8</span>, <span class="dv">10</span>]).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb30-15"><a href="#cb30-15" aria-hidden="true" tabindex="-1"></a>position2 <span class="op">=</span> np.array([<span class="dv">0</span>, <span class="dv">1</span>, <span class="fl">2.5</span>, <span class="fl">1.5</span>, <span class="dv">3</span>, <span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">4</span>])  <span class="co"># 더 큰 변동성 추가</span></span>
<span id="cb30-16"><a href="#cb30-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-17"><a href="#cb30-17" aria-hidden="true" tabindex="-1"></a><span class="co"># 예측할 시간점 생성: 0~10분 구간을 100개로 분할</span></span>
<span id="cb30-18"><a href="#cb30-18" aria-hidden="true" tabindex="-1"></a>time_pred <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">10</span>, <span class="dv">100</span>).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb30-19"><a href="#cb30-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-20"><a href="#cb30-20" aria-hidden="true" tabindex="-1"></a><span class="co"># RBF 커널 함수 정의</span></span>
<span id="cb30-21"><a href="#cb30-21" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> kernel(T1, T2, l<span class="op">=</span><span class="fl">2.0</span>):</span>
<span id="cb30-22"><a href="#cb30-22" aria-hidden="true" tabindex="-1"></a>    sqdist <span class="op">=</span> np.<span class="bu">sum</span>(T1<span class="op">**</span><span class="dv">2</span>, <span class="dv">1</span>).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>) <span class="op">+</span> np.<span class="bu">sum</span>(T2<span class="op">**</span><span class="dv">2</span>, <span class="dv">1</span>) <span class="op">-</span> <span class="dv">2</span> <span class="op">*</span> np.dot(T1, T2.T)</span>
<span id="cb30-23"><a href="#cb30-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.exp(<span class="op">-</span><span class="fl">0.5</span> <span class="op">*</span> sqdist <span class="op">/</span> l<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb30-24"><a href="#cb30-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-25"><a href="#cb30-25" aria-hidden="true" tabindex="-1"></a><span class="co"># 가우시안 프로세스 예측 함수</span></span>
<span id="cb30-26"><a href="#cb30-26" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> predict_gp(time, position, time_pred):</span>
<span id="cb30-27"><a href="#cb30-27" aria-hidden="true" tabindex="-1"></a>    K <span class="op">=</span> kernel(time, time)</span>
<span id="cb30-28"><a href="#cb30-28" aria-hidden="true" tabindex="-1"></a>    K_star <span class="op">=</span> kernel(time_pred, time)</span>
<span id="cb30-29"><a href="#cb30-29" aria-hidden="true" tabindex="-1"></a>    K_star_star <span class="op">=</span> kernel(time_pred, time_pred)</span>
<span id="cb30-30"><a href="#cb30-30" aria-hidden="true" tabindex="-1"></a>    mu_star <span class="op">=</span> K_star.dot(np.linalg.inv(K)).dot(position)</span>
<span id="cb30-31"><a href="#cb30-31" aria-hidden="true" tabindex="-1"></a>    sigma_star <span class="op">=</span> K_star_star <span class="op">-</span> K_star.dot(np.linalg.inv(K)).dot(K_star.T)</span>
<span id="cb30-32"><a href="#cb30-32" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> mu_star, sigma_star</span>
<span id="cb30-33"><a href="#cb30-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-34"><a href="#cb30-34" aria-hidden="true" tabindex="-1"></a><span class="co"># 두 데이터셋에 대한 예측 수행</span></span>
<span id="cb30-35"><a href="#cb30-35" aria-hidden="true" tabindex="-1"></a>mu1, sigma1 <span class="op">=</span> predict_gp(time1, position1, time_pred)</span>
<span id="cb30-36"><a href="#cb30-36" aria-hidden="true" tabindex="-1"></a>mu2, sigma2 <span class="op">=</span> predict_gp(time2, position2, time_pred)</span>
<span id="cb30-37"><a href="#cb30-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-38"><a href="#cb30-38" aria-hidden="true" tabindex="-1"></a><span class="co"># 2개의 서브플롯 생성</span></span>
<span id="cb30-39"><a href="#cb30-39" aria-hidden="true" tabindex="-1"></a>fig, (ax1, ax2) <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">16</span>, <span class="dv">4</span>))</span>
<span id="cb30-40"><a href="#cb30-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-41"><a href="#cb30-41" aria-hidden="true" tabindex="-1"></a><span class="co"># 첫 번째 그래프 (5개 데이터)</span></span>
<span id="cb30-42"><a href="#cb30-42" aria-hidden="true" tabindex="-1"></a>ax1.fill_between(time_pred.flatten(),</span>
<span id="cb30-43"><a href="#cb30-43" aria-hidden="true" tabindex="-1"></a>                mu1 <span class="op">-</span> <span class="dv">2</span><span class="op">*</span>np.sqrt(np.diag(sigma1)),</span>
<span id="cb30-44"><a href="#cb30-44" aria-hidden="true" tabindex="-1"></a>                mu1 <span class="op">+</span> <span class="dv">2</span><span class="op">*</span>np.sqrt(np.diag(sigma1)),</span>
<span id="cb30-45"><a href="#cb30-45" aria-hidden="true" tabindex="-1"></a>                color<span class="op">=</span><span class="st">'blue'</span>, alpha<span class="op">=</span><span class="fl">0.2</span>, label<span class="op">=</span><span class="st">'95</span><span class="sc">% c</span><span class="st">onfidence interval'</span>)</span>
<span id="cb30-46"><a href="#cb30-46" aria-hidden="true" tabindex="-1"></a>ax1.plot(time_pred, mu1, <span class="st">'b-'</span>, linewidth<span class="op">=</span><span class="fl">1.5</span>, label<span class="op">=</span><span class="st">'Predicted path'</span>)</span>
<span id="cb30-47"><a href="#cb30-47" aria-hidden="true" tabindex="-1"></a>ax1.plot(time1, position1, <span class="st">'ro'</span>, markersize<span class="op">=</span><span class="dv">6</span>, label<span class="op">=</span><span class="st">'GPS records'</span>)</span>
<span id="cb30-48"><a href="#cb30-48" aria-hidden="true" tabindex="-1"></a>ax1.set_xlabel(<span class="st">'Time (min)'</span>)</span>
<span id="cb30-49"><a href="#cb30-49" aria-hidden="true" tabindex="-1"></a>ax1.set_ylabel(<span class="st">'Position (km)'</span>)</span>
<span id="cb30-50"><a href="#cb30-50" aria-hidden="true" tabindex="-1"></a>ax1.set_title(<span class="st">'Route Estimation (5 GPS points)'</span>)</span>
<span id="cb30-51"><a href="#cb30-51" aria-hidden="true" tabindex="-1"></a>ax1.legend(fontsize<span class="op">=</span><span class="dv">8</span>)</span>
<span id="cb30-52"><a href="#cb30-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-53"><a href="#cb30-53" aria-hidden="true" tabindex="-1"></a><span class="co"># 두 번째 그래프 (8개 데이터)</span></span>
<span id="cb30-54"><a href="#cb30-54" aria-hidden="true" tabindex="-1"></a>ax2.fill_between(time_pred.flatten(),</span>
<span id="cb30-55"><a href="#cb30-55" aria-hidden="true" tabindex="-1"></a>                mu2 <span class="op">-</span> <span class="dv">2</span><span class="op">*</span>np.sqrt(np.diag(sigma2)),</span>
<span id="cb30-56"><a href="#cb30-56" aria-hidden="true" tabindex="-1"></a>                mu2 <span class="op">+</span> <span class="dv">2</span><span class="op">*</span>np.sqrt(np.diag(sigma2)),</span>
<span id="cb30-57"><a href="#cb30-57" aria-hidden="true" tabindex="-1"></a>                color<span class="op">=</span><span class="st">'blue'</span>, alpha<span class="op">=</span><span class="fl">0.2</span>, label<span class="op">=</span><span class="st">'95</span><span class="sc">% c</span><span class="st">onfidence interval'</span>)</span>
<span id="cb30-58"><a href="#cb30-58" aria-hidden="true" tabindex="-1"></a>ax2.plot(time_pred, mu2, <span class="st">'b-'</span>, linewidth<span class="op">=</span><span class="fl">1.5</span>, label<span class="op">=</span><span class="st">'Predicted path'</span>)</span>
<span id="cb30-59"><a href="#cb30-59" aria-hidden="true" tabindex="-1"></a>ax2.plot(time2, position2, <span class="st">'ro'</span>, markersize<span class="op">=</span><span class="dv">6</span>, label<span class="op">=</span><span class="st">'GPS records'</span>)</span>
<span id="cb30-60"><a href="#cb30-60" aria-hidden="true" tabindex="-1"></a>ax2.set_xlabel(<span class="st">'Time (min)'</span>)</span>
<span id="cb30-61"><a href="#cb30-61" aria-hidden="true" tabindex="-1"></a>ax2.set_ylabel(<span class="st">'Position (km)'</span>)</span>
<span id="cb30-62"><a href="#cb30-62" aria-hidden="true" tabindex="-1"></a>ax2.set_title(<span class="st">'Route Estimation (8 GPS points)'</span>)</span>
<span id="cb30-63"><a href="#cb30-63" aria-hidden="true" tabindex="-1"></a>ax2.legend(fontsize<span class="op">=</span><span class="dv">8</span>)</span>
<span id="cb30-64"><a href="#cb30-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-65"><a href="#cb30-65" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb30-66"><a href="#cb30-66" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="06_Overfitting and Development of Solution Techniques_files/figure-html/cell-24-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The code above is an example of using GP to estimate bicycle routes in two scenarios (5 observation points, 8 observation points). In each graph, the blue solid line represents the predicted average route, and the blue shaded area represents the 95% confidence interval.</p>
<ul>
<li><strong>When there is little data</strong> (left graph): Because GPS records are sparse, the uncertainty of prediction (the width of the confidence interval) is large.</li>
<li><strong>When there is a lot of data</strong> (right graph): As GPS records become denser, the uncertainty of prediction decreases and the predicted route becomes closer to the actual route.</li>
</ul>
<p>Thus, GP provides not only predictive results but also uncertainty, making it useful in various fields where uncertainty needs to be considered in decision-making processes (e.g., autonomous driving, robot control, medical diagnosis).</p>
</section>
<section id="modern-applications" class="level3">
<h3 class="anchored" data-anchor-id="modern-applications">6.6.2 Modern Applications</h3>
<p>Gaussian processes are applied in various scientific and engineering fields such as robot control, sensor network optimization, molecular structure prediction, climate modeling, and astrophysical data analysis. A representative application in machine learning is hyperparameter optimization, which we have already discussed. Another field that requires predictive uncertainty is autonomous driving. By predicting the future position of surrounding vehicles and driving more defensively in areas with high uncertainty, autonomous vehicles can improve safety. Additionally, Gaussian processes are widely used in medical fields to predict patient status changes and in asset markets to predict stock prices and manage risks based on uncertainty. Recently, GP applications have been actively researched in combination with reinforcement learning, generative models, causal inference, and meta-learning.</p>
</section>
<section id="deep-kernel-learning" class="level3">
<h3 class="anchored" data-anchor-id="deep-kernel-learning">6.6.3 Deep Kernel Learning</h3>
<p>The most important aspect of Gaussian processes is the kernel (covariance function). Deep learning has strengths in learning representations from data. Combining the predictive power of GP and the representation learning ability of deep learning is a natural research direction. One representative method is deep kernel learning (DKL), which uses neural networks to directly learn kernels from data instead of predefining them like RBF kernels.</p>
<p>The general structure of DKL is as follows:</p>
<ol type="1">
<li><strong>Feature Extraction</strong>: Input data passes through a deep neural network (usually CNN or Transformer) and is transformed into a low-dimensional feature vector.</li>
<li><strong>Kernel Computation</strong>: The extracted feature vector is used as input to calculate the kernel matrix using a Gaussian process kernel function (e.g., RBF kernel).</li>
<li><strong>Gaussian Process</strong>: The calculated kernel matrix and training data are used to learn a Gaussian process model, which performs predictions (mean and variance) on new inputs. DKL has the advantage of learning useful feature representations and data similarities simultaneously through neural networks. This enables uncertainty-aware predictions for complex data (e.g., images, graphs, text).</li>
</ol>
<p>DKL is being applied in various fields. * <strong>Image Classification</strong>: Uses CNN to extract image features and GP to perform classification. * <strong>Graph Classification</strong>: Uses Graph Neural Network (GNN) to extract features from graph structures and GP to perform graph classification. * <strong>Molecular Property Prediction</strong>: Takes molecular graphs as input and predicts molecular properties (e.g., solubility, toxicity). * <strong>Time Series Forecasting</strong>: Uses RNN to extract features from time series data and GP to predict future values. Here, we will run a simple example of DKL and examine more detailed content and application cases in Part 2.</p>
<p><strong>Deep Kernel Network</strong></p>
<p>First, we define the deep kernel network. The kernel network is a neural network that learns kernel functions. This neural network takes input data and outputs feature representations. These feature representations are used to calculate the kernel matrix.</p>
<div id="cell-60" class="cell" data-execution_count="2">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.optim <span class="im">as</span> optim</span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.distributions <span class="im">import</span> Normal</span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Set seed for reproducibility</span></span>
<span id="cb31-10"><a href="#cb31-10" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">42</span>)</span>
<span id="cb31-11"><a href="#cb31-11" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb31-12"><a href="#cb31-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-13"><a href="#cb31-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Define a neural network to learn the kernel</span></span>
<span id="cb31-14"><a href="#cb31-14" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> DeepKernel(nn.Module):</span>
<span id="cb31-15"><a href="#cb31-15" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, input_dim, hidden_dim, output_dim):</span>
<span id="cb31-16"><a href="#cb31-16" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(DeepKernel, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb31-17"><a href="#cb31-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc1 <span class="op">=</span> nn.Linear(input_dim, hidden_dim)</span>
<span id="cb31-18"><a href="#cb31-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc2 <span class="op">=</span> nn.Linear(hidden_dim, hidden_dim)</span>
<span id="cb31-19"><a href="#cb31-19" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc3 <span class="op">=</span> nn.Linear(hidden_dim, output_dim)</span>
<span id="cb31-20"><a href="#cb31-20" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.activation <span class="op">=</span> nn.ReLU()</span>
<span id="cb31-21"><a href="#cb31-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-22"><a href="#cb31-22" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb31-23"><a href="#cb31-23" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.activation(<span class="va">self</span>.fc1(x))</span>
<span id="cb31-24"><a href="#cb31-24" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.activation(<span class="va">self</span>.fc2(x))</span>
<span id="cb31-25"><a href="#cb31-25" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.fc3(x)  <span class="co"># No activation on the final layer</span></span>
<span id="cb31-26"><a href="#cb31-26" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>The input to a deep kernel network is typically a 2D tensor, where the first dimension is the batch size and the second dimension is the dimension of the input data. The output is a 2D tensor of shape (batch size, feature dimension).</p>
<p><strong>GP Layer Definition</strong></p>
<p>The GP layer takes the output of the deep kernel network and computes the kernel matrix, then calculates the predictive distribution.</p>
<div id="cell-62" class="cell" data-execution_count="3">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the Gaussian Process layer</span></span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> GaussianProcessLayer(nn.Module):</span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, num_dim, num_data):</span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(GaussianProcessLayer, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.num_dim <span class="op">=</span> num_dim</span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.num_data <span class="op">=</span> num_data</span>
<span id="cb32-11"><a href="#cb32-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.lengthscale <span class="op">=</span> nn.Parameter(torch.ones(num_dim))  <span class="co"># Length-scale for each dimension</span></span>
<span id="cb32-12"><a href="#cb32-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.noise_var <span class="op">=</span> nn.Parameter(torch.ones(<span class="dv">1</span>))  <span class="co"># Noise variance</span></span>
<span id="cb32-13"><a href="#cb32-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.outputscale <span class="op">=</span> nn.Parameter(torch.ones(<span class="dv">1</span>))  <span class="co"># Output scale</span></span>
<span id="cb32-14"><a href="#cb32-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-15"><a href="#cb32-15" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x, y):</span>
<span id="cb32-16"><a href="#cb32-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculate the kernel matrix (using RBF kernel)</span></span>
<span id="cb32-17"><a href="#cb32-17" aria-hidden="true" tabindex="-1"></a>        dist_matrix <span class="op">=</span> torch.cdist(x, x)  <span class="co"># Pairwise distances between inputs</span></span>
<span id="cb32-18"><a href="#cb32-18" aria-hidden="true" tabindex="-1"></a>        kernel_matrix <span class="op">=</span> <span class="va">self</span>.outputscale <span class="op">*</span> torch.exp(<span class="op">-</span><span class="fl">0.5</span> <span class="op">*</span> dist_matrix<span class="op">**</span><span class="dv">2</span> <span class="op">/</span> <span class="va">self</span>.lengthscale<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb32-19"><a href="#cb32-19" aria-hidden="true" tabindex="-1"></a>        kernel_matrix <span class="op">+=</span> <span class="va">self</span>.noise_var <span class="op">*</span> torch.eye(<span class="va">self</span>.num_data)</span>
<span id="cb32-20"><a href="#cb32-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-21"><a href="#cb32-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculate the predictive distribution (using Cholesky decomposition)</span></span>
<span id="cb32-22"><a href="#cb32-22" aria-hidden="true" tabindex="-1"></a>        L <span class="op">=</span> torch.linalg.cholesky(kernel_matrix)</span>
<span id="cb32-23"><a href="#cb32-23" aria-hidden="true" tabindex="-1"></a>        alpha <span class="op">=</span> torch.cholesky_solve(y.unsqueeze(<span class="op">-</span><span class="dv">1</span>), L)  <span class="co"># Add unsqueeze for correct shape</span></span>
<span id="cb32-24"><a href="#cb32-24" aria-hidden="true" tabindex="-1"></a>        predictive_mean <span class="op">=</span> torch.matmul(kernel_matrix, alpha).squeeze(<span class="op">-</span><span class="dv">1</span>) <span class="co"># Remove extra dimension</span></span>
<span id="cb32-25"><a href="#cb32-25" aria-hidden="true" tabindex="-1"></a>        v <span class="op">=</span> torch.linalg.solve_triangular(L, kernel_matrix, upper<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb32-26"><a href="#cb32-26" aria-hidden="true" tabindex="-1"></a>        predictive_var <span class="op">=</span> kernel_matrix <span class="op">-</span> torch.matmul(v.T, v)</span>
<span id="cb32-27"><a href="#cb32-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-28"><a href="#cb32-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-29"><a href="#cb32-29" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> predictive_mean, predictive_var</span>
<span id="cb32-30"><a href="#cb32-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-31"><a href="#cb32-31" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> predictive_mean, predictive_var</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>The input to the GP layer is a 2D tensor of shape (batch size, feature dimension). The output is a tuple containing the predicted mean and variance. The RBF kernel is used for computing the kernel matrix, and Cholesky decomposition is utilized for calculating the predictive distribution to improve computational efficiency. <code>y.unsqueeze(-1)</code> and <code>.squeeze(-1)</code> are used to match the dimensions between y and the kernel matrix.</p>
<div id="cell-64" class="cell" data-execution_count="4">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 데이터를 생성</span></span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">10</span>, <span class="dv">10</span>, <span class="dv">100</span>)</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.sin(x) <span class="op">+</span> <span class="fl">0.1</span> <span class="op">*</span> np.random.randn(<span class="dv">100</span>)</span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a><span class="co"># 데이터를 텐서로 변환</span></span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a>x_tensor <span class="op">=</span> torch.tensor(x, dtype<span class="op">=</span>torch.float32).unsqueeze(<span class="op">-</span><span class="dv">1</span>)  <span class="co"># (100,) -&gt; (100, 1)</span></span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a>y_tensor <span class="op">=</span> torch.tensor(y, dtype<span class="op">=</span>torch.float32)  <span class="co"># (100,)</span></span>
<span id="cb33-8"><a href="#cb33-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-9"><a href="#cb33-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-10"><a href="#cb33-10" aria-hidden="true" tabindex="-1"></a><span class="co"># 딥 커널과 GP 레이어를 초기화</span></span>
<span id="cb33-11"><a href="#cb33-11" aria-hidden="true" tabindex="-1"></a>deep_kernel <span class="op">=</span> DeepKernel(input_dim<span class="op">=</span><span class="dv">1</span>, hidden_dim<span class="op">=</span><span class="dv">50</span>, output_dim<span class="op">=</span><span class="dv">1</span>)  <span class="co"># output_dim=1로 수정</span></span>
<span id="cb33-12"><a href="#cb33-12" aria-hidden="true" tabindex="-1"></a>gp_layer <span class="op">=</span> GaussianProcessLayer(num_dim<span class="op">=</span><span class="dv">1</span>, num_data<span class="op">=</span><span class="bu">len</span>(x))</span>
<span id="cb33-13"><a href="#cb33-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-14"><a href="#cb33-14" aria-hidden="true" tabindex="-1"></a><span class="co"># 손실 함수와 최적화기를 정의</span></span>
<span id="cb33-15"><a href="#cb33-15" aria-hidden="true" tabindex="-1"></a>loss_fn <span class="op">=</span> nn.MSELoss()  <span class="co"># Use MSE loss</span></span>
<span id="cb33-16"><a href="#cb33-16" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> optim.Adam(<span class="bu">list</span>(deep_kernel.parameters()) <span class="op">+</span> <span class="bu">list</span>(gp_layer.parameters()), lr<span class="op">=</span><span class="fl">0.01</span>)</span>
<span id="cb33-17"><a href="#cb33-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-18"><a href="#cb33-18" aria-hidden="true" tabindex="-1"></a>num_epochs <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb33-19"><a href="#cb33-19" aria-hidden="true" tabindex="-1"></a><span class="co"># 모델을 학습</span></span>
<span id="cb33-20"><a href="#cb33-20" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(num_epochs):</span>
<span id="cb33-21"><a href="#cb33-21" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()</span>
<span id="cb33-22"><a href="#cb33-22" aria-hidden="true" tabindex="-1"></a>    kernel_output <span class="op">=</span> deep_kernel(x_tensor)</span>
<span id="cb33-23"><a href="#cb33-23" aria-hidden="true" tabindex="-1"></a>    predictive_mean, _ <span class="op">=</span> gp_layer(kernel_output, y_tensor) <span class="co"># predictive_var는 사용 안함</span></span>
<span id="cb33-24"><a href="#cb33-24" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> loss_fn(predictive_mean, y_tensor)  <span class="co"># Use predictive_mean here</span></span>
<span id="cb33-25"><a href="#cb33-25" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb33-26"><a href="#cb33-26" aria-hidden="true" tabindex="-1"></a>    optimizer.step()</span>
<span id="cb33-27"><a href="#cb33-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span>(epoch <span class="op">%</span> <span class="dv">10</span> <span class="op">==</span> <span class="dv">0</span>):</span>
<span id="cb33-28"><a href="#cb33-28" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f'Epoch </span><span class="sc">{</span>epoch<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">, Loss: </span><span class="sc">{</span>loss<span class="sc">.</span>item()<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb33-29"><a href="#cb33-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-30"><a href="#cb33-30" aria-hidden="true" tabindex="-1"></a><span class="co"># 예측을 수행</span></span>
<span id="cb33-31"><a href="#cb33-31" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb33-32"><a href="#cb33-32" aria-hidden="true" tabindex="-1"></a>    kernel_output <span class="op">=</span> deep_kernel(x_tensor)</span>
<span id="cb33-33"><a href="#cb33-33" aria-hidden="true" tabindex="-1"></a>    predictive_mean, predictive_var <span class="op">=</span> gp_layer(kernel_output, y_tensor)</span>
<span id="cb33-34"><a href="#cb33-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-35"><a href="#cb33-35" aria-hidden="true" tabindex="-1"></a><span class="co"># 결과를 시각화</span></span>
<span id="cb33-36"><a href="#cb33-36" aria-hidden="true" tabindex="-1"></a>sns.<span class="bu">set</span>()</span>
<span id="cb33-37"><a href="#cb33-37" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb33-38"><a href="#cb33-38" aria-hidden="true" tabindex="-1"></a>plt.plot(x, y, <span class="st">'bo'</span>, label<span class="op">=</span><span class="st">'Training Data'</span>)</span>
<span id="cb33-39"><a href="#cb33-39" aria-hidden="true" tabindex="-1"></a>plt.plot(x, predictive_mean.numpy(), <span class="st">'r-'</span>, label<span class="op">=</span><span class="st">'Predictive Mean'</span>)</span>
<span id="cb33-40"><a href="#cb33-40" aria-hidden="true" tabindex="-1"></a>plt.fill_between(x, predictive_mean.numpy() <span class="op">-</span> <span class="fl">1.96</span> <span class="op">*</span> np.sqrt(predictive_var.numpy().diagonal()),</span>
<span id="cb33-41"><a href="#cb33-41" aria-hidden="true" tabindex="-1"></a>                 predictive_mean.numpy() <span class="op">+</span> <span class="fl">1.96</span> <span class="op">*</span> np.sqrt(predictive_var.numpy().diagonal()),</span>
<span id="cb33-42"><a href="#cb33-42" aria-hidden="true" tabindex="-1"></a>                 alpha<span class="op">=</span><span class="fl">0.2</span>, label<span class="op">=</span><span class="st">'95% CI'</span>)</span>
<span id="cb33-43"><a href="#cb33-43" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb33-44"><a href="#cb33-44" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1, Loss: 4.3467857893837725e-13
Epoch 11, Loss: 3.1288711313699757e-13
Epoch 21, Loss: 3.9212150236903054e-13
Epoch 31, Loss: 4.184870765894244e-13
Epoch 41, Loss: 2.9785689973499396e-13
Epoch 51, Loss: 3.8607078688482344e-13
Epoch 61, Loss: 3.9107123572454383e-13
Epoch 71, Loss: 2.359286811054462e-13
Epoch 81, Loss: 3.4729958167147024e-13
Epoch 91, Loss: 2.7600995490886793e-13</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_1408185/2425174321.py:40: RuntimeWarning: invalid value encountered in sqrt
  plt.fill_between(x, predictive_mean.numpy() - 1.96 * np.sqrt(predictive_var.numpy().diagonal()),
/tmp/ipykernel_1408185/2425174321.py:41: RuntimeWarning: invalid value encountered in sqrt
  predictive_mean.numpy() + 1.96 * np.sqrt(predictive_var.numpy().diagonal()),</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="06_Overfitting and Development of Solution Techniques_files/figure-html/cell-27-output-3.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>For model learning, the Mean Squared Error (MSE) loss function is used, and the Adam optimizer is used to learn the parameters of the deep kernel network and the GP layer simultaneously.</p>
<section id="possibilities-and-limitations-of-dkl" class="level4">
<h4 class="anchored" data-anchor-id="possibilities-and-limitations-of-dkl">6.6.4 Possibilities and Limitations of DKL</h4>
<p>The previous example shows the basic idea of Deep Kernel Learning (DKL). It uses a deep learning model (the <code>DeepKernel</code> class) to extract features from the input data and calculates the Gaussian Process (GP) kernel using these features. Then, it uses GP to calculate the mean and variance (uncertainty) of predictions. In this way, DKL combines the representation learning ability of deep learning and the uncertainty estimation ability of GP, making reliable predictions possible even for complex data.</p>
<p><strong>Possibilities of DKL:</strong></p>
<ul>
<li><strong>Data Efficiency:</strong> GP tends to work well even with small amounts of data. DKL combines the powerful feature extraction capability of deep learning and the data efficiency of GP, allowing it to perform well with small amounts of data.</li>
<li><strong>Uncertainty Estimation:</strong> DKL can quantify the uncertainty of predictions. This is particularly useful in applications where safety is crucial (e.g., medical diagnosis, autonomous driving).</li>
<li><strong>Flexibility:</strong> DKL can be applied to various types of data (images, text, graphs, etc.). By freely selecting neural network architectures, it is possible to design feature extractors tailored to specific problems.</li>
<li><strong>Combination with Bayesian Optimization:</strong> DKL can be combined with Bayesian optimization to efficiently tune the hyperparameters of models.</li>
</ul>
<p><strong>Limitations of DKL:</strong></p>
<ul>
<li><strong>Computational Cost:</strong> GP still has a high computational cost. Especially when there is a large amount of training data, the size of the kernel matrix becomes large, making calculations difficult.</li>
<li><strong>Neural Network Design:</strong> The performance of DKL heavily depends on the design of the feature extractor (neural network). Selecting an appropriate neural network architecture remains a challenging problem.</li>
<li><strong>Lack of Theoretical Understanding:</strong> Theoretical analysis of DKL is still in its early stages. More research is needed to understand why DKL works well and under what conditions it performs well.</li>
</ul>
</section>
<section id="conclusion" class="level4">
<h4 class="anchored" data-anchor-id="conclusion">Conclusion</h4>
<p>This chapter has explored various techniques for addressing the overfitting problem in deep learning models. From traditional methods such as L1/L2 regularization, dropout, and batch normalization, to advanced techniques like Bayesian optimization, Gaussian processes, and Deep Kernel Learning, deep learning research continues to evolve to improve the generalization performance of models.</p>
<p>Overfitting occurs when a deep learning model becomes too specialized to the training data, resulting in poor predictive performance on new data. This can happen when the model’s complexity is too high, when there is insufficient training data, or when the data contains a lot of noise. Preventing overfitting is a crucial task when applying deep learning models to real-world problems.</p>
<p>The various techniques introduced in this chapter approach the overfitting problem in different ways:</p>
<ul>
<li><strong>Regularization:</strong> Penalizes model complexity by adding a penalty term to the loss function to prevent weights from becoming too large (L1, L2, Elastic Net).</li>
<li><strong>Dropout:</strong> Randomly removes neurons during training to prevent the model from relying too heavily on specific neurons or combinations of neurons.</li>
<li><strong>Batch Normalization:</strong> Normalizes the input to each layer, stabilizing and accelerating training.</li>
<li><strong>Hyperparameter Optimization:</strong> Uses methods like Bayesian optimization to find the optimal combination of hyperparameters that maximizes model performance.</li>
<li><strong>Gaussian Processes and Deep Kernel Learning:</strong> Explicitly models uncertainty, allowing for more reliable predictions. Properly combining these techniques and tuning them to suit the characteristics of the problem is one of the key competencies of deep learning engineers. There is no single solution that is “perfect for all cases,” and the optimal method must be found through experimentation and analysis. Deep learning research is rapidly evolving, and new techniques to address overfitting will continue to emerge.</li>
</ul>
</section>
</section>
</section>
<section id="practice-problems" class="level2">
<h2 class="anchored" data-anchor-id="practice-problems">Practice Problems</h2>
<section id="basic-problems" class="level3">
<h3 class="anchored" data-anchor-id="basic-problems">Basic Problems</h3>
<ol type="1">
<li>Explain the concepts of overfitting and underfitting, and describe how each phenomenon affects a model’s performance.</li>
<li>Describe the differences between L1 regularization and L2 regularization, and explain how each regularization affects a model’s weights.</li>
<li>Explain the working principle of dropout and describe how dropout helps prevent overfitting.</li>
<li>Explain the concept of batch normalization and describe the benefits it provides to deep learning model training.</li>
<li>Describe the changes in the graph when changing the lambda values in the custom_loss function.</li>
<li>Define L1 and L2 norms.</li>
<li>Explain how the mean and variance are calculated in the batch normalization formula, and how they are used in the normalization process.</li>
</ol>
</section>
<section id="application-problems" class="level3">
<h3 class="anchored" data-anchor-id="application-problems">Application Problems</h3>
<ol type="1">
<li>Train a polynomial regression model on a given dataset and observe the overfitting and underfitting phenomena by changing the degree of the polynomial. (Write Python code)</li>
<li>Create a simple neural network model and apply L1 or L2 regularization to observe the changes in the model’s weights and compare the performance changes based on the regularization strength. (Write Python code)</li>
<li>Train a neural network model with different dropout rates and compare the training/validation loss and accuracy for each rate. (Write Python code)</li>
<li>Compare the learning speed and convergence stability of a neural network model by adding or removing batch normalization layers. (Write Python code)</li>
<li>Use the Lagrange multiplier method to derive the optimal conditions for the L1 and L2 regularized loss function.</li>
<li>Derive the backpropagation process for batch normalization and explain how it helps alleviate the vanishing gradient problem.</li>
</ol>
</section>
<section id="advanced-problems" class="level3">
<h3 class="anchored">Advanced Problems</h3>
<ol type="1">
<li>Visualize the effect of L1 and L2 regularization on the loss surface and explain the geometric meaning of each regularization. (Write Python code)</li>
<li>Interpret dropout from the perspective of ensemble learning and explain how dropout can be used to estimate a model’s uncertainty.</li>
<li>Compare various hyperparameter optimization techniques (grid search, random search, Bayesian optimization, etc.) and explain their advantages and disadvantages.</li>
<li>Implement the core idea of Bayesian optimization without using BoTorch to find the optimal value of a simple function. (Write Python code, library use allowed)</li>
<li>Explain the basic principles of Gaussian processes and describe how they perform predictions with uncertainty.</li>
<li>Explain the conditions that a Gaussian process kernel function must satisfy and prove that the RBF kernel satisfies these conditions.</li>
<li>Explain the role of acquisition functions in Bayesian optimization and derive the Expected Improvement (EI) acquisition function formula, explaining its meaning.</li>
</ol>
<div class="callout callout-style-default callout-note callout-titled" title="Click to view contents (Exercise answers)">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Click to view contents (Exercise answers)
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<section id="exercise-answers" class="level2 callout-body-container callout-body">
<h2 class="anchored" data-anchor-id="exercise-answers">Exercise Answers</h2>
<section id="basic-problems-1" class="level3">
<h3 class="anchored" data-anchor-id="basic-problems-1">Basic Problems</h3>
<ol type="1">
<li><strong>Overfitting/Underfitting:</strong>
<ul>
<li><strong>Overfitting:</strong> The phenomenon where a model is too well-fit to the training data, resulting in poor performance on new data (validation/test data). It performs well on the training data but poorly on the validation/test data.</li>
<li><strong>Underfitting:</strong> The phenomenon where a model is too simple and fails to learn the patterns in the training data. It shows low performance on both the training and validation/test data.</li>
</ul></li>
<li><strong>L1/L2 Regularization:</strong>
<ul>
<li><strong>L1 Regularization (Lasso):</strong> Adds the absolute value sum of weights to the loss function. <span class="math inline">\(\lambda \sum_{i} |w_i|\)</span> Makes some weights exactly 0, resulting in feature selection.</li>
<li><strong>L2 Regularization (Ridge):</strong> Adds the square sum of weights to the loss function. <span class="math inline">\(\lambda \sum_{i} (w_i)^2\)</span> Makes weights close to 0 but not exactly 0.</li>
</ul></li>
<li><strong>Dropout:</strong>
<ul>
<li><strong>Mechanism:</strong> Randomly deactivates some neurons during training by setting their output to 0.</li>
<li><strong>Preventing Overfitting:</strong> Trains with different neuron combinations each time, preventing dependence on specific neurons and achieving ensemble learning effects.</li>
</ul></li>
<li><strong>Batch Normalization:</strong>
<ul>
<li><strong>Concept:</strong> Normalizes the input of each layer to have a mean of 0 and a variance of 1.</li>
<li><strong>Benefits:</strong> Improves training speed, alleviates vanishing/exploding gradient problems, allows for higher learning rates, and has some regularization effect.</li>
</ul></li>
<li><strong><code>custom_loss</code> Lambda Change:</strong>
<ul>
<li><strong>Increasing <code>lambda</code>:</strong> Increases the influence of the regularization term. Weights become smaller, and the model becomes simpler, increasing the risk of underfitting.</li>
<li><strong>Decreasing <code>lambda</code>:</strong> Decreases the influence of the regularization term. Weights become larger, and the model becomes more complex, increasing the risk of overfitting.</li>
</ul></li>
<li><strong>L1/L2 Norm:</strong>
<ul>
<li><strong>L1 Norm:</strong> The sum of the absolute values of vector elements. <span class="math inline">\(\| \mathbf{x} \|_1 = |x_1| + |x_2| + \dots + |x_n|\)</span></li>
<li><strong>L2 Norm:</strong> The square root of the sum of the squares of vector elements (Euclidean distance). <span class="math inline">\(\| \mathbf{x} \|_2 = \sqrt{|x_1|^2 + |x_2|^2 + \dots + |x_n|^2}\)</span></li>
</ul></li>
<li><strong>Batch Normalization Formula:</strong>
<ul>
<li><strong>Mean (μ):</strong> The mean of samples in a mini-batch. <span class="math inline">\(\mu = \frac{1}{m} \sum_{i=1}^{m} x_i\)</span></li>
<li><strong>Variance (σ²):</strong> The variance of samples in a mini-batch. <span class="math inline">\(\sigma^2 = \frac{1}{m} \sum_{i=1}^{m} (x_i - \mu)^2\)</span></li>
<li><strong>Normalization:</strong> <span class="math inline">\(x_{i\_\text{norm}} = \frac{x_i - \mu}{\sqrt{\sigma^2 + \epsilon}}\)</span> (ε is a small constant to prevent division by zero)</li>
</ul></li>
</ol>
</section>
<section id="applied-problems" class="level3">
<h3 class="anchored" data-anchor-id="applied-problems">Applied Problems</h3>
<ol type="1">
<li><p><strong>Polynomial Regression:</strong> (Code omitted) If the degree is too high, it overfits; if it’s too low, it underfits.</p></li>
<li><p><strong>L1/L2 Regularization:</strong> (Code omitted) The stronger the regularization strength (<code>lambda</code>), the smaller the weights become, and performance changes are observed.</p></li>
<li><p><strong>Dropout Ratio:</strong> (Code omitted) An appropriate dropout ratio can prevent overfitting and improve performance. Too high a ratio can cause underfitting.</p></li>
<li><p><strong>Batch Normalization:</strong> (Code omitted) Adding batch normalization tends to speed up learning and stabilize convergence.</p></li>
<li><p><strong>Lagrange Multiplier Method:</strong></p>
<ul>
<li><strong>L2 Regularization:</strong> <span class="math inline">\(L(\mathbf{w}, \lambda) = \text{Loss}(\mathbf{w}) + \lambda (\|\mathbf{w}\|_2^2 - c)  \rightarrow  \nabla_\mathbf{w}L = \nabla_\mathbf{w}\text{Loss}(\mathbf{w}) + 2\lambda\mathbf{w} = 0\)</span></li>
<li><strong>L1 Regularization:</strong> <span class="math inline">\(L(\mathbf{w}, \lambda) = \text{Loss}(\mathbf{w}) + \lambda (\|\mathbf{w}\|_1 - c)  \rightarrow  \nabla_\mathbf{w}L = \nabla_\mathbf{w}\text{Loss}(\mathbf{w}) + \lambda \cdot \text{sign}(\mathbf{w}) = 0\)</span> (sign(w) is the sign of w)</li>
</ul></li>
<li><p><strong>Batch Normalization Backpropagation:</strong> (derivation omitted) Batch normalization normalizes the input to each layer, mitigating the vanishing/exploding gradient problem and stabilizing learning.</p></li>
</ol>
</section>
<section id="advanced-problems-1" class="level3">
<h3 class="anchored" data-anchor-id="advanced-problems-1">Advanced Problems</h3>
<ol type="1">
<li><p><strong>Loss Plane Visualization:</strong> (code omitted) L1 regularization creates a diamond-shaped constraint, while L2 regularization creates a circular constraint, resulting in different optimal solutions.</p></li>
<li><p><strong>Dropout Ensemble:</strong> Dropout has the effect of training a different network structure each time, similar to ensemble learning. During prediction, all neurons are used (without dropout) to perform an average prediction. Monte Carlo dropout can be used to estimate the uncertainty of predictions.</p></li>
<li><p><strong>Hyperparameter Optimization Techniques:</strong></p>
<ul>
<li><strong>Grid Search:</strong> Try all combinations. Computational cost is very high.</li>
<li><strong>Random Search:</strong> Try random combinations. Can be more efficient than grid search.</li>
<li><strong>Bayesian Optimization:</strong> Use a probabilistic model based on previous search results to determine the next search point. Efficient.</li>
</ul></li>
<li><p><strong>Bayesian Optimization Implementation:</strong> (code omitted) Use a surrogate model (e.g., Gaussian process) and an acquisition function (e.g., Expected Improvement) to implement.</p></li>
<li><p><strong>Gaussian Process:</strong> A probability distribution over functions. Define the covariance between function values using a kernel function. Calculate the posterior distribution based on given data to provide the mean and variance (uncertainty) of predictions.</p></li>
<li><p><strong>Kernel Function Conditions:</strong> Must satisfy positive semi-definiteness. The kernel matrix (Gram matrix) generated for arbitrary input points must be a positive semi-definite matrix. The RBF kernel satisfies this condition. (proof omitted)</p></li>
<li><p><strong>Acquisition Function:</strong> Used to determine the next search point in Bayesian optimization. Expected Improvement (EI) considers the possibility of obtaining better results than the current optimal value and the degree of improvement, selecting the next search point accordingly. (equation derivation omitted)</p></li>
</ol>
</section>
</section>
</div>
</div>
<ol type="1">
<li><strong>Dropout: A Simple Way to Prevent Neural Networks from Overfitting</strong> (Srivastava et al., 2014): The original paper explaining the concept and effect of dropout. (<a href="https://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf">https://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf</a>)</li>
<li><strong>Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</strong> (Ioffe &amp; Szegedy, 2015): The original paper explaining the concept and effect of batch normalization. (<a href="https://arxiv.org/abs/1502.03167">https://arxiv.org/abs/1502.03167</a>)</li>
<li><strong>Deep Learning</strong> (Goodfellow et al., 2016): A deep learning textbook that discusses overfitting and regularization techniques in detail in Chapter 7 “Regularization for Deep Learning”. (<a href="http://www.deeplearningbook.org/">http://www.deeplearningbook.org/</a>)</li>
<li><strong>Understanding the difficulty of training deep feedforward neural networks</strong> (Glorot &amp; Bengio, 2010): Explains the difficulties in training early deep learning models and the importance of weight initialization methods. (<a href="http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf">http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf</a>)</li>
<li><strong>Regularization techniques for deep learning: A survey</strong> (Kukacka et al., 2017): A paper that comprehensively compares and analyzes various regularization techniques.</li>
<li><strong>A Tutorial on Bayesian Optimization</strong> (Frazier, 2018): A tutorial explaining the basic concepts and applications of Bayesian optimization. (<a href="https://arxiv.org/abs/1807.02811">https://arxiv.org/abs/1807.02811</a>)</li>
<li><strong>Bayesian Optimization</strong> (Garnett, 2023): A comprehensive textbook on Bayesian optimization (<a href="https://www.bayesoptbook.com/">https://www.bayesoptbook.com/</a>)</li>
<li><strong>Gaussian Processes for Machine Learning</strong> (Rasmussen &amp; Williams, 2006): A textbook that covers the basic principles of Gaussian processes and their applications in machine learning. (<a href="http://www.gaussianprocess.org/gpml/">http://www.gaussianprocess.org/gpml/</a>)</li>
<li><strong>Deep Kernel Learning</strong> (Wilson et al., 2016): A paper explaining the concept and method of deep kernel learning. (<a href="https://arxiv.org/abs/1511.02222">https://arxiv.org/abs/1511.02222</a>)</li>
<li><strong>Hands-On Machine Learning with Scikit-Learn, Keras &amp; TensorFlow</strong> (Aurélien Géron, 2019): A hands-on machine learning and deep learning textbook that explains overfitting and regularization techniques with actual code examples.</li>
<li><strong>Adam: A Method for Stochastic Optimization</strong> (Kingma &amp; Ba, 2014) (<a href="https://arxiv.org/abs/1412.6980">https://arxiv.org/abs/1412.6980</a>): A paper on the Adam optimizer</li>
<li><strong>Decoupled Weight Decay Regularization</strong> (Loshchilov &amp; Hutter, 2017) (<a href="https://arxiv.org/abs/1711.05101">https://arxiv.org/abs/1711.05101</a>): A paper on AdamW</li>
<li><strong>Dropout: A Simple Way to Prevent Neural Networks from Overfitting</strong> (Srivastava et al., 2014): This original paper explains the concept and effect of dropout. (<a href="https://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf">https://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf</a>)</li>
<li><strong>Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</strong> (Ioffe &amp; Szegedy, 2015): This original paper explains the concept and effect of batch normalization. (<a href="https://arxiv.org/abs/1502.03167">https://arxiv.org/abs/1502.03167</a>)</li>
<li><strong>Deep Learning</strong> (Goodfellow et al., 2016): A deep learning textbook that covers overfitting and regularization techniques in detail in Chapter 7 “Regularization for Deep Learning”. (<a href="http://www.deeplearningbook.org/">http://www.deeplearningbook.org/</a>)</li>
<li><strong>Understanding the difficulty of training deep feedforward neural networks</strong> (Glorot &amp; Bengio, 2010): Explains the difficulty of learning early deep learning models and the importance of weight initialization methods. (<a href="http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf">http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf</a>)</li>
<li><strong>Regularization techniques for deep learning: A survey</strong> (Kukacka et al., 2017): A paper that compares and analyzes various regularization techniques.</li>
<li><strong>A Tutorial on Bayesian Optimization</strong> (Frazier, 2018): A tutorial that explains the basic concepts and applications of Bayesian optimization. (<a href="https://arxiv.org/abs/1807.02811">https://arxiv.org/abs/1807.02811</a>)</li>
<li><strong>Bayesian Optimization</strong> (Garnett, 2023): A comprehensive textbook on Bayesian optimization. (<a href="https://www.bayesoptbook.com/">https://www.bayesoptbook.com/</a>)</li>
<li><strong>Gaussian Processes for Machine Learning</strong> (Rasmussen &amp; Williams, 2006): A textbook that covers the basic principles of Gaussian processes and their applications in machine learning. (<a href="http://www.gaussianprocess.org/gpml/">http://www.gaussianprocess.org/gpml/</a>)</li>
<li><strong>Deep Kernel Learning</strong> (Wilson et al., 2016): A paper that explains the concept and method of deep kernel learning. (<a href="https://arxiv.org/abs/1511.02222">https://arxiv.org/abs/1511.02222</a>)</li>
<li><strong>Hands-On Machine Learning with Scikit-Learn, Keras &amp; TensorFlow</strong> (Aurélien Géron, 2019): A hands-on machine learning and deep learning textbook that explains overfitting and regularization techniques with actual code examples.</li>
<li><strong>Adam: A Method for Stochastic Optimization</strong> (Kingma &amp; Ba, 2014) (<a href="https://arxiv.org/abs/1412.6980">https://arxiv.org/abs/1412.6980</a>): A paper on the Adam optimizer. Decoupled Weight Decay Regularization (Loshchilov &amp; Hutter, 2017) (<a href="https://arxiv.org/abs/1711.05101">https://arxiv.org/abs/1711.05101</a>): A paper on AdamW</li>
</ol>
<p>There is no original text to translate.</p>


</section>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>