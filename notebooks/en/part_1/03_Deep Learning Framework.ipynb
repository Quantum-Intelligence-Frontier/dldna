{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/Quantum-Intelligence-Frontier/dldna/blob/main/notebooks/en/part_1/03_DeepLearningFramework.ipynb\" target=\"_parent\">\n",
    "<img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Deep Learning Frameworks\n",
    "\n",
    "> \"A tool is only as good as the person using it.\" - *Anonymous, often attributed to John von Neumann*\n",
    "\n",
    "The development of frameworks in the history of deep learning has been crucial. After AlexNet's success in 2012, various frameworks emerged. Through Caffe, Theano, Torch7, and now PyTorch and TensorFlow have become mainstream.\n",
    "\n",
    "In the early 2010s, deep learning began to show remarkable results in areas such as image recognition and speech recognition, surpassing existing technologies. However, training and deploying deep learning models was still a difficult task. This was because one had to implement neural network configurations, gradient calculations, GPU acceleration, and more directly. Such complexity raised the barrier to entry for deep learning research and slowed down the pace of research. To solve these problems, deep learning frameworks emerged. Deep learning frameworks provided high-level APIs and tools for building, training, and deploying neural network models, simplifying and accelerating the development process. Initially, frameworks like Theano, Caffe, and Torch appeared and were widely used in academia and industry.\n",
    "\n",
    "In 2015, Google released TensorFlow as an open source, bringing significant changes to the deep learning framework ecosystem. TensorFlow quickly gained popularity due to its flexible architecture, powerful visualization tools, and support for large-scale distributed learning. In 2017, Facebook released PyTorch, setting another important milestone. PyTorch provided dynamic computation graphs, intuitive interfaces, and excellent debugging capabilities, rapidly spreading among researchers.\n",
    "\n",
    "Currently, deep learning frameworks have established themselves as core infrastructure for deep learning research and development, beyond simple tools. They provide key features such as automatic differentiation, GPU acceleration, model parallelization, and distributed learning, accelerating the development of new models and algorithms. Additionally, competition and cooperation between frameworks are further advancing the deep learning ecosystem.\n",
    "\n",
    "## 3.1 PyTorch\n",
    "\n",
    "PyTorch is an open-source machine learning framework based on the Torch library, used for applications such as computer vision and natural language processing. It was developed in 2016 by Facebook's AI Research lab (FAIR) as a reimplementation of Torch7 in Python. Thanks to its dynamic computation graphs and intuitive debugging features, PyTorch quickly gained popularity among researchers. Besides PyTorch, other frameworks like TensorFlow, JAX, and Caffe exist, but PyTorch has become the de facto standard in research. Many new models are often released with PyTorch implementations.\n",
    "\n",
    "After becoming proficient in one framework, leveraging the strengths of another can be a good strategy. For example, you can use TensorFlow's data preprocessing pipelines or JAX's functional transformation capabilities alongside PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install dldna[colab] # in Colab\n",
    "# !pip install dldna[all] # in your local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.6.0+cu124\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7352f02b33f0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Print PyTorch version\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "torch.manual_seed(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When generating random numbers, setting the initial seed value allows you to get the same random number every time. This is commonly used in research to ensure consistent results in repetitive training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 Tensor Objects\n",
    "\n",
    "> **Challenge**: How can large-scale matrix operations be performed efficiently using a GPU?\n",
    ">\n",
    "> **Researcher's Concern**: As deep learning models grew in size, it took too much time to train and infer using only the CPU. While GPUs were suitable for deep learning due to their specialization in parallel computing, GPU programming was complex and difficult. A tool was needed to abstract and automate GPU operations so that deep learning researchers could easily utilize GPUs.\n",
    "\n",
    "Tensors are the basic data structure in PyTorch. Since the introduction of CUDA in 2006, GPU operations have become central to deep learning, and tensors were designed to perform these operations efficiently. Tensors are multi-dimensional arrays that generalize scalars, vectors, and matrices. In deep learning, the dimensionality of data (tensor rank) is highly varied. For example, images are represented as 4D tensors (batch, channel, height, width), while natural language is represented as 3D tensors (batch, sequence length, embedding dimension). As seen in Chapter 2, it is crucial to freely transform and process these dimensions.\n",
    "\n",
    "Tensors can be declared as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.1210e-44,  0.0000e+00,  0.0000e+00,  4.1369e-41],\n",
      "         [ 1.8796e-17,  0.0000e+00,  2.8026e-45,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,         nan,         nan],\n",
      "         [ 6.3058e-44,  4.7424e+30,  1.4013e-45,  1.3563e-19]],\n",
      "\n",
      "        [[ 1.0089e-43,  0.0000e+00,  1.1210e-44,  0.0000e+00],\n",
      "         [-8.8105e+09,  4.1369e-41,  1.8796e-17,  0.0000e+00]]])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Create a 3x2x4 tensor with random values\n",
    "a = torch.Tensor(3, 2, 4)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors can also be initialized from existing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of d: <class 'list'>\n",
      "Tensor a:\n",
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n",
      "Type of a: <class 'torch.Tensor'>\n",
      "Type of d_np: <class 'numpy.ndarray'>\n",
      "Tensor b (from_numpy):\n",
      "tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "Tensor c (from np array using torch.Tensor):\n",
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n",
      "Modified d_np:\n",
      "[[100   2]\n",
      " [  3   4]]\n",
      "Tensor b (from_numpy) after modifying d_np:\n",
      "tensor([[100,   2],\n",
      "        [  3,   4]])\n",
      "Tensor c (copy) after modifying d_np:\n",
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n"
     ]
    }
   ],
   "source": [
    "# From a Python list\n",
    "d = [[1, 2], [3, 4]]\n",
    "print(f\"Type of d: {type(d)}\")\n",
    "\n",
    "a = torch.Tensor(d)  # Creates a *copy*\n",
    "print(f\"Tensor a:\\n{a}\")\n",
    "print(f\"Type of a: {type(a)}\")\n",
    "\n",
    "# From a NumPy array\n",
    "d_np = np.array(d)\n",
    "print(f\"Type of d_np: {type(d_np)}\")\n",
    "\n",
    "b = torch.from_numpy(d_np) # Shares memory with d_np (zero-copy)\n",
    "print(f\"Tensor b (from_numpy):\\n{b}\")\n",
    "\n",
    "\n",
    "c = torch.Tensor(d_np)  # Creates a *copy*\n",
    "print(f\"Tensor c (from np array using torch.Tensor):\\n{c}\")\n",
    "\n",
    "# Example of memory sharing with torch.from_numpy\n",
    "d_np[0, 0] = 100\n",
    "print(f\"Modified d_np:\\n{d_np}\")\n",
    "print(f\"Tensor b (from_numpy) after modifying d_np:\\n{b}\")\n",
    "print(f\"Tensor c (copy) after modifying d_np:\\n{c}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just because they look the same when outputted does not mean they are the same object. `d` is a Python list object, and tensors can be created from various data structures. In particular, interactions with NumPy arrays are very efficient. However, since list objects and NumPy arrays do not support GPUs, conversion to tensors is essential for large operations. The *important* point is to understand the difference between `torch.Tensor(data)` and `torch.from_numpy(data)`. The former always creates a copy, while the latter creates a *view* that shares memory with the original NumPy array (if possible - zero-copy). If you modify the NumPy array, the tensor created by `from_numpy` will also change, and vice versa.\n",
    "\n",
    "There are many ways to initialize tensors. Since Hinton's 2006 paper, the importance of initialization methods has been highlighted, and various initialization strategies have been developed. The most basic initialization functions are as follows.\n",
    "\n",
    "| Function | Description |\n",
    "| --- | --- |\n",
    "| `torch.zeros` | Initializes with 0. |\n",
    "| `torch.ones` | Initializes with 1. |\n",
    "| `torch.rand` | Initializes with random numbers from a uniform distribution between 0 and 1. |\n",
    "| `torch.randn` | Initializes with random numbers from a standard normal distribution (mean 0, variance 1). |\n",
    "| `torch.arange` | Initializes sequentially, such as n, n+1, n+2, ... ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random tensor (uniform):\n",
      "tensor([[0.5349, 0.1988, 0.6592],\n",
      "        [0.6569, 0.2328, 0.4251]])\n",
      "Random tensor (normal):\n",
      "tensor([[-1.2514, -1.8841,  0.4457],\n",
      "        [-0.7068, -1.5750, -0.6318]])\n",
      "Ones tensor:\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "Zeros tensor:\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "shape = (2, 3)\n",
    "\n",
    "rand_t = torch.rand(shape)     # Uniform distribution [0, 1)\n",
    "randn_t = torch.randn(shape)   # Standard normal distribution\n",
    "ones_t = torch.ones(shape)\n",
    "zeros_t = torch.zeros(shape)\n",
    "\n",
    "print(f\"Random tensor (uniform):\\n{rand_t}\")\n",
    "print(f\"Random tensor (normal):\\n{randn_t}\")\n",
    "print(f\"Ones tensor:\\n{ones_t}\")\n",
    "print(f\"Zeros tensor:\\n{zeros_t}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch supports over 100 tensor operations, all of which can be run on the GPU. Tensors are created on the CPU by default, so to use the GPU, you must explicitly move them using the `to()` function. Moving large tensors across CPU and GPU is costly, so careful memory management is essential. In real deep learning training, the memory bandwidth of the GPU has a decisive impact on performance. For example, when training transformer models, the larger the GPU memory, the larger the batch size that can be used, which increases training efficiency. However, high-bandwidth memory is very expensive to produce and accounts for a significant portion of the cost of GPUs. The performance difference between CPU and GPU tensor operations is particularly noticeable in operations that can be parallelized, such as matrix multiplication. For this reason, dedicated accelerators like GPUs, TPUs, and NPUs are essential in modern deep learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU computation time = 2.34 seconds\n",
      "GPU computation time = 0.14 seconds\n",
      "GPU is 16.2 times faster.\n"
     ]
    }
   ],
   "source": [
    "# Device setting\n",
    "if torch.cuda.is_available():\n",
    "    tensor = zeros_t.to(\"cuda\")\n",
    "    device = \"cuda:0\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    print('GPU not available')\n",
    "\n",
    "# CPU/GPU performance comparison\n",
    "import time\n",
    "\n",
    "# CPU operation\n",
    "x = torch.rand(10000, 10000)\n",
    "start = time.time()\n",
    "torch.matmul(x, x)\n",
    "cpu_time = time.time() - start\n",
    "print(f\"CPU computation time = {cpu_time:3.2f} seconds\")\n",
    "\n",
    "# GPU operation\n",
    "if device != \"cpu\":\n",
    "    x = x.to(device)\n",
    "    start = torch.cuda.Event(enable_timing=True)\n",
    "    end = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "    start.record()\n",
    "    torch.matmul(x, x)\n",
    "    end.record()\n",
    "    torch.cuda.synchronize()  # Wait for all operations to complete\n",
    "    gpu_time = start.elapsed_time(end) / 1000  # Convert milliseconds to seconds\n",
    "    print(f\"GPU computation time = {gpu_time:3.2f} seconds\")\n",
    "    print(f\"GPU is {cpu_time / gpu_time:3.1f} times faster.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The conversion between NumPy and tensors is implemented very efficiently, especially as seen above, using `torch.from_numpy()`, memory is shared without memory copy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy array: [[1 1]\n",
      " [2 3]]\n",
      "Tensor: tensor([[1, 1],\n",
      "        [2, 3]])\n",
      "NumPy array from Tensor: [[1 1]\n",
      " [2 3]]\n"
     ]
    }
   ],
   "source": [
    "np_a = np.array([[1, 1], [2, 3]])\n",
    "tensor_a = torch.from_numpy(np_a)\n",
    "np_b = tensor_a.numpy() # Shares memory.  If tensor_a is on CPU.\n",
    "\n",
    "print(f\"NumPy array: {np_a}\")\n",
    "print(f\"Tensor: {tensor_a}\")\n",
    "print(f\"NumPy array from Tensor: {np_b}\") #if tensor_a is on CPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When converting a tensor to NumPy, the tensor must be on the CPU. Tensors on the GPU must first be moved to the CPU using `.cpu()`. The basic properties of a tensor are `shape`, `dtype`, `device`, which can be used to check the shape and storage location of the tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape = torch.Size([2, 3])\n",
      "Data type = torch.float32\n",
      "Device = cpu\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(2, 3)\n",
    "print(f\"Shape = {a.shape}\")\n",
    "print(f\"Data type = {a.dtype}\")\n",
    "print(f\"Device = {a.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indexing and slicing use the same syntax as NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor a:\n",
      "tensor([[0.2069, 0.8296, 0.4973],\n",
      "        [0.9265, 0.8386, 0.6611],\n",
      "        [0.5329, 0.7822, 0.0975]])\n",
      "First row: tensor([0.2069, 0.8296, 0.4973])\n",
      "First column: tensor([0.2069, 0.9265, 0.5329])\n",
      "Last column: tensor([0.4973, 0.6611, 0.0975])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(3, 3)\n",
    "print(f\"Tensor a:\\n{a}\")\n",
    "print(f\"First row: {a[0]}\")\n",
    "print(f\"First column: {a[:, 0]}\")\n",
    "print(f\"Last column: {a[..., -1]}\")  # Equivalent to a[:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 Operation\n",
    "\n",
    "PyTorch supports almost all operations of NumPy. The tradition of multidimensional array operations that started from the APL language in 1964 has been passed down to PyTorch through NumPy. You can check the list of all supported operations on the official PyTorch documentation ([PyTorch documentation](https://pytorch.org/docs/stable/tensors.html)).\n",
    "\n",
    "Changing the shape of a tensor is one of the most frequently used operations in neural networks. The `view()` function can change the dimension of a tensor, and at this time, the total number of elements must be maintained. The `permute()` function rearranges the order of dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\n",
      "x: tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n",
      "y: tensor([[ 0,  4,  8],\n",
      "        [ 1,  5,  9],\n",
      "        [ 2,  6, 10],\n",
      "        [ 3,  7, 11]])\n",
      "b shape: torch.Size([2, 3, 5])\n",
      "z shape: torch.Size([5, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(12)\n",
    "print(f\"a: {a}\")\n",
    "\n",
    "x = a.view(3, 4)  # Reshape to 3x4\n",
    "print(f\"x: {x}\")\n",
    "\n",
    "y = x.permute(1, 0)  # Swap dimensions 0 and 1\n",
    "print(f\"y: {y}\")\n",
    "\n",
    "b = torch.randn(2, 3, 5)\n",
    "print(f\"b shape: {b.shape}\")\n",
    "\n",
    "z = b.permute(2, 0, 1)  # Change dimension order to (2, 0, 1)\n",
    "print(f\"z shape: {z.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrix operations are the core of deep learning, and PyTorch provides various matrix operation functions.\n",
    "\n",
    "1.  `torch.matmul`: Performs general matrix operations, which behave as follows depending on the dimensions:\n",
    "    *   1D × 1D: dot product\n",
    "    *   2D × 2D: matrix product\n",
    "    *   1D × 2D: matrix product after adding a dimension of size one to the first tensor\n",
    "    *   N-D × M-D: matrix product after broadcasting\n",
    "2.  `torch.mm`: Pure matrix multiplication operation (no broadcasting support)\n",
    "3.  `torch.bmm`: Batch matrix multiplication ((b, i, k) × (b, k, j) → (b, i, j))\n",
    "4.  `torch.einsum`: Tensor operations using Einstein summation notation, allowing for concise expression of complex tensor operations. (See \"Theory Deep Dive\" for details)\n",
    "    -   `torch.einsum('ij,jk->ik', a, b)`: Product of matrices a and b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n",
      "Y: tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n",
      "X @ Y = tensor([[20, 23, 26, 29],\n",
      "        [56, 68, 80, 92]])\n",
      "X @ Y (using einsum) = tensor([[20, 23, 26, 29],\n",
      "        [56, 68, 80, 92]])\n",
      "a: tensor([0, 1])\n",
      "b: tensor([0, 1])\n",
      "a @ b = 1\n",
      "a: tensor([0, 1])\n",
      "B: tensor([[0, 1],\n",
      "        [2, 3]])\n",
      "a @ B = tensor([2, 3])\n",
      "X @ b shape = torch.Size([3])\n",
      "X: tensor([[[ 0,  1,  2],\n",
      "         [ 3,  4,  5]],\n",
      "\n",
      "        [[ 6,  7,  8],\n",
      "         [ 9, 10, 11]],\n",
      "\n",
      "        [[12, 13, 14],\n",
      "         [15, 16, 17]]])\n",
      "Y: tensor([[[ 0,  1],\n",
      "         [ 2,  3],\n",
      "         [ 4,  5]],\n",
      "\n",
      "        [[ 6,  7],\n",
      "         [ 8,  9],\n",
      "         [10, 11]],\n",
      "\n",
      "        [[12, 13],\n",
      "         [14, 15],\n",
      "         [16, 17]]])\n",
      "X @ Y shape: torch.Size([3, 2, 2])\n",
      "X @ Y: tensor([[[ 10,  13],\n",
      "         [ 28,  40]],\n",
      "\n",
      "        [[172, 193],\n",
      "         [244, 274]],\n",
      "\n",
      "        [[550, 589],\n",
      "         [676, 724]]])\n",
      "X: tensor([[[ 0,  1,  2],\n",
      "         [ 3,  4,  5]],\n",
      "\n",
      "        [[ 6,  7,  8],\n",
      "         [ 9, 10, 11]],\n",
      "\n",
      "        [[12, 13, 14],\n",
      "         [15, 16, 17]]])\n",
      "Y: tensor([[0, 1],\n",
      "        [2, 3],\n",
      "        [4, 5]])\n",
      "X @ Y shape: torch.Size([3, 2, 2])\n",
      "X @ Y: tensor([[[ 10,  13],\n",
      "         [ 28,  40]],\n",
      "\n",
      "        [[ 46,  67],\n",
      "         [ 64,  94]],\n",
      "\n",
      "        [[ 82, 121],\n",
      "         [100, 148]]])\n",
      "X @ Y (using einsum) = tensor([[20, 23, 26, 29],\n",
      "        [56, 68, 80, 92]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(6)\n",
    "b = torch.arange(12)\n",
    "\n",
    "X = a.view(2, 3)\n",
    "Y = b.view(3, 4)\n",
    "print(f\"X: {X}\")\n",
    "print(f\"Y: {Y}\")\n",
    "\n",
    "# matmul (2,3) X (3,4) -> (2, 4)\n",
    "print(f\"X @ Y = {torch.matmul(X, Y)}\")\n",
    "\n",
    "# Using torch.einsum for matrix multiplication\n",
    "einsum_result = torch.einsum('ij,jk->ik', X, Y)\n",
    "print(f\"X @ Y (using einsum) = {einsum_result}\")\n",
    "\n",
    "\n",
    "a = torch.arange(2)\n",
    "b = torch.arange(2)\n",
    "print(f\"a: {a}\")\n",
    "print(f\"b: {b}\")\n",
    "\n",
    "# Vector x Vector operation\n",
    "print(f\"a @ b = {torch.matmul(a, b)}\")\n",
    "\n",
    "# 1D tensor (vector), 2D tensor (matrix) operation\n",
    "# (2) x (2,2) is treated as (1,2) x (2,2) for matrix multiplication.\n",
    "# Result: (1,2) x (2,2) -> (1,2)\n",
    "b = torch.arange(4)\n",
    "B = b.view(2, 2)\n",
    "print(f\"a: {a}\")\n",
    "print(f\"B: {B}\")\n",
    "print(f\"a @ B = {torch.matmul(a, B)}\")\n",
    "\n",
    "# Matrix x Vector operation\n",
    "X = torch.randn(3, 4)\n",
    "b = torch.randn(4)\n",
    "print(f\"X @ b shape = {torch.matmul(X, b).size()}\")\n",
    "\n",
    "# Batched matrix x Batched matrix\n",
    "# The leading batch dimension is maintained.\n",
    "# The 2nd and 3rd dimensions are treated as matrices for multiplication.\n",
    "X = torch.arange(18).view(3, 2, 3)\n",
    "Y = torch.arange(18).view(3, 3, 2)\n",
    "print(f\"X: {X}\")\n",
    "print(f\"Y: {Y}\")\n",
    "# Batch dimension remains the same, and (2,3)x(3,2) -> (2,2)\n",
    "print(f\"X @ Y shape: {torch.matmul(X, Y).size()}\")\n",
    "print(f\"X @ Y: {torch.matmul(X, Y)}\")\n",
    "\n",
    "# Batched matrix x Broadcasted matrix\n",
    "X = torch.arange(18).view(3, 2, 3)\n",
    "Y = torch.arange(6).view(3, 2)\n",
    "print(f\"X: {X}\")\n",
    "print(f\"Y: {Y}\")\n",
    "# The second matrix lacks a batch dimension.\n",
    "# It's broadcasted to match the batch dimension of the first matrix (repeated 3 times).\n",
    "print(f\"X @ Y shape: {torch.matmul(X, Y).size()}\")\n",
    "print(f\"X @ Y: {torch.matmul(X, Y)}\")\n",
    "\n",
    "\n",
    "# Using torch.einsum for matrix multiplication\n",
    "X = torch.arange(6).view(2, 3)\n",
    "Y = torch.arange(12).view(3, 4)\n",
    "einsum_result = torch.einsum('ij,jk->ik', X, Y)  # Equivalent to torch.matmul(X, Y)\n",
    "print(f\"X @ Y (using einsum) = {einsum_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.einsum uses Einstein notation to express tensor operations. `'ij,jk->ik'` means multiply the `(i, j)` dimensions of `X` tensor and `(j, k)` dimensions of `Y` tensor to produce `(i, k)` dimensional result. This is equivalent to matrix multiplication `torch.matmul(X, Y)`. einsum also supports various operations such as transpose, sum, inner product, outer product, batch matrix multiplication, etc. For more details, please refer to the PyTorch documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other einsum examples\n",
    "\n",
    "# Transpose\n",
    "a = torch.randn(2, 3)\n",
    "b = torch.einsum('ij->ji', a)  # Swap dimensions\n",
    "\n",
    "# Sum of all elements\n",
    "a = torch.randn(2, 3)\n",
    "b = torch.einsum('ij->', a)  # Sum all elements\n",
    "\n",
    "# Batch matrix multiplication\n",
    "a = torch.randn(3, 2, 5)\n",
    "b = torch.randn(3, 5, 3)\n",
    "c = torch.einsum('bij,bjk->bik', a, b) # Batch matrix multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.callout-note collapse=\"true\" title=\"Click to view contents (Deep Dive: Einstein Notation and torch.einsum)\"}\n",
    "## Einstein Notation and torch.einsum\n",
    "\n",
    "### Einstein Notation\n",
    "\n",
    "Einstein notation, or Einstein summation convention, is a notation introduced by Albert Einstein in 1916 to describe the theory of general relativity. Originally designed to concisely express physical equations, especially those related to relativity, its convenience and expressive power have made it widely used in various fields that deal with tensor operations.\n",
    "\n",
    "**Key Ideas:**\n",
    "\n",
    "* **Repeating indices implies summation:** If an index appears twice in a single term, it implies summation over all possible values of that index. This allows for the omission of explicit summation symbols ($\\sum$), making notation more concise.\n",
    "* **Free and dummy indices:**\n",
    "    * **Free index:** An index that appears in the resulting tensor, appearing only once in each term.\n",
    "    * **Dummy index (or bound index):** An index that is summed over, appearing twice in a single term.\n",
    "\n",
    "**Basic Rules**\n",
    "\n",
    "1.  **If an index appears twice in a term, it is summed over.**\n",
    "2.  **Free indices determine the dimension of the resulting tensor.**\n",
    "3.  **Dummy indices are used only for internal calculations and do not appear in the result.**\n",
    "4.  **Index letters can be chosen arbitrarily but should be consistent to avoid confusion.** (Conventions often use $i, j, k, l, m, n$, etc.)\n",
    "5.  **The left side of an arrow ($\\rightarrow$)** represents input tensors, and **the right side** represents output tensors.\n",
    "\n",
    "**Examples**\n",
    "\n",
    "*   **Dot product:** $a_i b_i$ (equivalent to $\\sum_i a_i b_i$)\n",
    "*   **Matrix multiplication:** $A_{ij} B_{jk} = C_{ik}$ (equivalent to $\\sum_j A_{ij}B_{jk}$)\n",
    "*   **Transpose:** $A_{ij} = B_{ji}$ ($B$ is the transpose of $A$)\n",
    "*   **Trace:** $A_{ii}$ (equivalent to $\\sum_i A_{ii}$)\n",
    "*   **Outer product:** $a_i b_j = C_{ij}$\n",
    "*   **Element-wise multiplication (Hadamard product):** $A_{ij}B_{ij} = C_{ij}$\n",
    "\n",
    "**Examples of Use in Deep Learning**\n",
    "* **Batched Matrix Multiplication:** $A\\_{bij} B\\_{bjk} = C\\_{bik}$ ($b$: batch dimension)\n",
    "* **Attention Mechanism:** $e\\_{ij} = Q\\_{ik} K\\_{jk}$, $a\\_{ij} = \\text{softmax}(e\\_{ij})$, $v\\_{i} = a\\_{ij} V\\_{j}$ ($Q$: query, $K$: key, $V$: value)\n",
    "* **Bilinear Transformation:** $x\\_i W\\_{ijk} y\\_j = z\\_k$\n",
    "* **Multi-Dimensional Convolution:** $I\\_{b,c,i,j} \\* F\\_{o,c,k,l} = O\\_{b,o,i',j'}$ ($b$: batch, $c$: input channel, $o$: output channel, $i, j$: input spatial dimension, $k, l$: filter spatial dimension)\n",
    "* **Batch Normalization:** $\\gamma\\_c \\* \\frac{x\\_{b,c,h,w} - \\mu\\_c}{\\sigma\\_c} + \\beta\\_c$ ($c$: channel dimension, $b$: batch, $h$: height, $w$: width)\n",
    "* **RNN Hidden State Update:** $h\\_t = \\tanh(W\\_{ih}x\\_t + b\\_{ih} + W\\_{hh}h\\_{t-1} + b\\_{hh})$ ($h$: hidden, $x$: input, $W$: weight, $b$: bias)\n",
    "* **LSTM Cell State Update:** $c\\_t = f\\_t \\* c\\_{t-1} + i\\_t \\* \\tilde{c}\\_t$ ($c$: cell state, $f$: forget gate, $i$: input gate, $\\tilde{c}\\_t$: candidate cell state)\n",
    "\n",
    "### torch.einsum\n",
    "\n",
    "`torch.einsum` is a PyTorch function that performs tensor operations using Einstein notation. `einsum` is short for \"Einstein summation\".\n",
    "\n",
    "**Usage:**\n",
    "\n",
    "```python\n",
    "torch.einsum(equation, *operands)\n",
    "```\n",
    "\n",
    "* `equation`: Einstein notation string, e.g., `'ij,jk->ik'`.\n",
    "* `*operands`: Tensors participating in the operation (variable number of arguments).\n",
    "\n",
    "**Advantages**\n",
    "\n",
    "* **Conciseness:** Complex tensor operations can be expressed in a single line of code.\n",
    "* **Readability:** Einstein notation clearly conveys the meaning of tensor operations.\n",
    "* **Flexibility:** Various tensor operations can be combined to define new operations easily.\n",
    "* **Optimization:** PyTorch automatically optimizes `einsum` operations for efficient computation. (In some cases) It can be faster than manual implementation by utilizing optimized routines from libraries like BLAS, cuBLAS, or optimizing operation order.\n",
    "* **Autograd Support:** Operations defined with `einsum` are fully compatible with PyTorch's autograd system.\n",
    "\n",
    "**torch.einsum Examples:**\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "# Matrix Multiplication\n",
    "A = torch.randn(3, 4)\n",
    "B = torch.randn(4, 5)\n",
    "C = torch.einsum('ij,jk->ik', A, B)  # C = A @ B\n",
    "\n",
    "# Transpose\n",
    "A = torch.randn(3, 4)\n",
    "B = torch.einsum('ij->ji', A)  # B = A.T\n",
    "\n",
    "# Trace\n",
    "A = torch.randn(3, 3)\n",
    "trace = torch.einsum('ii->', A)  # trace = torch.trace(A)\n",
    "```\n",
    "# Batch Matrix Multiplication\n",
    "A = torch.randn(2, 3, 4)\n",
    "B = torch.randn(2, 4, 5)\n",
    "C = torch.einsum('bij,bjk->bik', A, B) # C = torch.bmm(A, B)\n",
    "\n",
    "# Outer Product\n",
    "a = torch.randn(3)\n",
    "b = torch.randn(4)\n",
    "C = torch.einsum('i,j->ij', a, b) # C = torch.outer(a, b)\n",
    "\n",
    "# Element-wise Multiplication\n",
    "A = torch.randn(2,3)\n",
    "B = torch.randn(2,3)\n",
    "C = torch.einsum('ij,ij->ij', A, B) # C = A * B\n",
    "\n",
    "# Biliner Transformation\n",
    "x = torch.randn(3)\n",
    "W = torch.randn(5, 3, 4)\n",
    "y = torch.randn(4)\n",
    "z = torch.einsum('i,ijk,j->k', x, W, y) # z_k = sum_i sum_j x_i * W_{ijk} * y_j\n",
    "\n",
    "# Multidimensional Tensor Contraction\n",
    "tensor = torch.randn(3, 4, 5, 6)\n",
    "result = torch.einsum('...ij->...i', tensor)  # sum over the last two dimensions\n",
    "\n",
    "**`torch.einsum` vs. Other Operations:**\n",
    "\n",
    "| Operation                    | `torch.einsum`           | Other Methods                                   |\n",
    "| :---------------------- | :----------------------- | :------------------------------------------ |\n",
    "| Matrix Multiplication                | `'ij,jk->ik'`           | `torch.matmul(A, B)` or `A @ B`          |\n",
    "| Transpose                    | `'ij->ji'`           | `torch.transpose(A, 0, 1)` or `A.T`        |\n",
    "| Trace                        | `'ii->'`              | `torch.trace(A)`                            |\n",
    "| Batch Matrix Multiplication            | `'bij,bjk->bik'`        | `torch.bmm(A, B)`                           |\n",
    "| Dot Product                    | `'i,i->'`              | `torch.dot(a, b)`                            |\n",
    "| Outer Product                    | `'i,j->ij'`           | `torch.outer(a, b)`                          |\n",
    "| Element-wise Multiplication                | `'ij,ij->ij'`          | `A * B`                                      |\n",
    "| Tensor Contraction (sum, mean, etc.) | `'ijk->i'` (example)      | `torch.sum(A, dim=(1, 2))`                   |\n",
    "\n",
    "**Limitations of `torch.einsum`**\n",
    "\n",
    "  * **Initial Learning Curve:** May be difficult for users unfamiliar with Einstein notation at first.\n",
    "  * **Readability of Complex Operations:** For very complex operations, the `einsum` string can become long and decrease readability. In such cases, breaking down the operation into multiple steps or using comments is recommended.\n",
    "  * **Inability to Express All Operations:** Since `einsum` is based on linear algebra operations, non-linear operations (e.g., `max`, `min`, `sort`) or conditional operations cannot be directly expressed. In such cases, other PyTorch functions must be used in conjunction.\n",
    "\n",
    "**Optimizing `einsum` with `torch.compile`**\n",
    "`torch.compile` (PyTorch 2.0 and above) can further optimize `einsum` operations. `compile` performs various optimizations, such as analyzing code through JIT (Just-In-Time) compilation, merging tensor operations, or optimizing memory access patterns.\n",
    "\n",
    "```python\n",
    "import torch\n",
    "# Available in PyTorch 2.0 and above\n",
    "\n",
    "@torch.compile\n",
    "def my_einsum_function(a, b):\n",
    "    return torch.einsum('ij,jk->ik', a, b)\n",
    "\n",
    "# Compile on the first call, execute optimized code on subsequent calls\n",
    "result = my_einsum_function(torch.randn(10, 20), torch.randn(20, 30))\n",
    "```\n",
    "\n",
    "**Conclusion:**\n",
    "\n",
    "Einstein notation and `torch.einsum` are powerful tools for expressing and calculating complex tensor operations in deep learning in a concise and efficient manner. Although they may seem unfamiliar at first, they can greatly improve code readability and efficiency once you get used to them. Especially when dealing with deep learning models that involve complex tensor operations, such as transformer models, they show their value. Using `torch.compile` together can further enhance performance.\n",
    "\n",
    "**References:**\n",
    "\n",
    "1.  **Einstein Notation:** https://en.wikipedia.org/wiki/Einstein_notation\n",
    "2.  **`torch.einsum` documentation:** https://pytorch.org/docs/stable/generated/torch.einsum.html\n",
    "3.  **A basic introduction to NumPy's einsum:** https://ajcr.net/Basic-guide-to-einsum/\n",
    "4.  **Einsum is All You Need - Einstein Summation in Deep Learning:** https://rockt.github.io/2018/04/30/einsum\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3 Computational Graph for Gradient Operations\n",
    "\n",
    "Automatic Differentiation has been studied since the 1970s, but it has gained significant attention along with the development of deep learning since 2015. PyTorch implements automatic differentiation through a dynamic computation graph, which is an actual implementation of the chain rule discussed in Chapter 2.\n",
    "\n",
    "PyTorch's automatic differentiation can track and store gradients at each operation step. To do this, gradient tracking must be explicitly declared for tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.requires_grad (default): False\n",
      "a.requires_grad (after setting to True): True\n",
      "x.requires_grad (declared at creation): True\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn((2,))\n",
    "print(f\"a.requires_grad (default): {a.requires_grad}\")  # False (default)\n",
    "\n",
    "a.requires_grad_(True)  # In-place modification\n",
    "print(f\"a.requires_grad (after setting to True): {a.requires_grad}\")  # True\n",
    "\n",
    "# Declare during creation\n",
    "x = torch.arange(2, dtype=torch.float32, requires_grad=True)\n",
    "print(f\"x.requires_grad (declared at creation): {x.requires_grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, let's consider a simple loss function as follows (refer to Figure 3-1, previous version).\n",
    "\n",
    "$$y = \\frac {1}{N}\\displaystyle\\sum_{i}^{N} \\{(x_i - 1)^2 + 4) \\}$$\n",
    "\n",
    "The operation for $x_i$ can be expressed sequentially as $a_i = x_i - 1$, $b_i = a_i^2$, $c_i = b_i + 4$, $y = \\frac{1}{N}\\sum_{i=1}^{N} c_i$. \n",
    "\n",
    "Let's perform forward and backward operations on this equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = 4.5\n",
      "x.grad = tensor([-1.,  0.])\n"
     ]
    }
   ],
   "source": [
    "a = x - 1\n",
    "b = a**2\n",
    "c = b + 4\n",
    "y = c.mean()\n",
    "\n",
    "print(f\"y = {y}\")\n",
    "\n",
    "# Perform backward operation\n",
    "y.backward()\n",
    "\n",
    "# Print the gradient of x (x.grad)\n",
    "print(f\"x.grad = {x.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient of each step can be calculated as follows:\n",
    "\n",
    "$\\frac{\\partial a_i}{\\partial x_i} = 1, \\frac{\\partial b_i}{\\partial a_i} = 2 \\cdot a_i, \\frac{\\partial c_i}{\\partial b_i} = 1,  \\frac{\\partial y}{\\partial c_i} = \\frac{1}{N}$\n",
    "\n",
    "Therefore, by the chain rule,\n",
    "\n",
    "$\\frac{\\partial y}{\\partial x_i} = \\frac{\\partial y}{\\partial  c_i}\\frac{\\partial c_i}{\\partial b_i}\\frac{\\partial b_i}{\\partial a_i}\\frac{\\partial a_i}{\\partial x_i} =  \\frac{1}{N} \\cdot 1 \\cdot 2 \\cdot a_i \\cdot 1 = \\frac{2}{N}a_i = \\frac{2}{N}(x_i - 1)$\n",
    "\n",
    "Since $x_i$ is in [0, 1] and N=2 (the number of elements in x), $\\frac{\\partial y}{\\partial x_i}$ becomes $[-0.5, 0.5]$. This matches the result of PyTorch's automatic differentiation.\n",
    "\n",
    "PyTorch has implemented the concept of automatic differentiation, which has been studied since the 1970s, in a modern way. In particular, the dynamic creation of computation graphs and gradient tracking functions are very useful. However, sometimes it is necessary to disable these automatic differentiation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z.requires_grad: True\n",
      "z.requires_grad (inside no_grad): False\n",
      "z_det.requires_grad: False\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, 4)\n",
    "w = torch.randn(4, 2)\n",
    "b = torch.randn(2)\n",
    "\n",
    "# If gradient tracking is needed\n",
    "z = torch.matmul(x, w) + b\n",
    "z.requires_grad_(True)  # Can also be set using requires_grad_()\n",
    "print(f\"z.requires_grad: {z.requires_grad}\")\n",
    "\n",
    "# Disable gradient tracking method 1: Using 'with' statement\n",
    "with torch.no_grad():\n",
    "    z = torch.matmul(x, w) + b\n",
    "    print(f\"z.requires_grad (inside no_grad): {z.requires_grad}\")\n",
    "\n",
    "# Disable gradient tracking method 2: Using detach()\n",
    "z_det = z.detach()\n",
    "print(f\"z_det.requires_grad: {z_det.requires_grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disabling gradient tracking is particularly useful in the following cases:\n",
    "\n",
    "1.  **During inference**: When only the forward pass is needed, it saves memory and computation cost.\n",
    "2.  **Fine-tuning**: When updating specific parameters and keeping the rest fixed.\n",
    "3.  **Performance optimization**: Backpropagation incurs additional memory and computation costs, so disabling it when not necessary.\n",
    "\n",
    "Especially in fine-tuning large language models, where most parameters are typically frozen and only a few are updated, selective activation of gradient tracking is a crucial feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.4 Data Loading\n",
    "\n",
    "Data loading is a core element of deep learning. Until the early 2000s, each research team used its own data processing method, but with the emergence of large datasets like ImageNet in 2009, the need for standardized data loading systems became prominent.\n",
    "\n",
    "PyTorch provides two key classes to separate data processing and training logic:\n",
    "\n",
    "1.  `torch.utils.data.Dataset`: Provides a consistent access interface for data and labels. You must implement the `__len__` and `__getitem__` methods.\n",
    "2.  `torch.utils.data.DataLoader`: Offers an efficient data loading mechanism in batch units. It wraps around `Dataset` to automate mini-batch creation, shuffling, and parallel data loading.\n",
    "\n",
    "The following is an example of generating random data using the Dirichlet distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data (a):\n",
      "[[0.46073711 0.01119455 0.28991657 0.11259078 0.12556099]\n",
      " [0.07331166 0.43554042 0.1243009  0.13339224 0.23345478]]\n",
      "Labels (b):\n",
      "[[1 0 0 0 0]\n",
      " [0 1 0 0 0]]\n",
      "Number of data samples = 100\n",
      "Data at index 0 = (tensor([1.4867e-01, 1.6088e-01, 1.2207e-02, 3.6049e-02, 1.1054e-04, 8.1160e-02,\n",
      "        2.9811e-02, 1.9398e-01, 4.9448e-02, 2.8769e-01]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1]))\n",
      "Data type = <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "\n",
    "# Initialize with Dirichlet distribution\n",
    "a = np.random.dirichlet(np.ones(5), size=2)\n",
    "b = np.zeros_like(a)\n",
    "# Generate label values\n",
    "b = (a == a.max(axis=1)[:, None]).astype(int)\n",
    "\n",
    "print(f\"Data (a):\\n{a}\")\n",
    "print(f\"Labels (b):\\n{b}\")\n",
    "\n",
    "\n",
    "# Create a custom Dataset class by inheriting from PyTorch's Dataset.\n",
    "class RandomData(data.Dataset):\n",
    "    def __init__(self, feature, length):\n",
    "        super().__init__()\n",
    "        self.feature = feature\n",
    "        self.length = length\n",
    "        self.generate_data()\n",
    "\n",
    "    def generate_data(self):\n",
    "        x = np.random.dirichlet(np.ones(self.feature), size=self.length)\n",
    "        y = (x == x.max(axis=1)[:, None]).astype(int)  # One-hot encoding\n",
    "\n",
    "        self.data = x  # numpy object\n",
    "        self.label = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Return data and label as torch tensors\n",
    "        return torch.tensor(self.data[index], dtype=torch.float32), torch.tensor(self.label[index], dtype=torch.int64)\n",
    "\n",
    "\n",
    "dataset = RandomData(feature=10, length=100)\n",
    "print(f\"Number of data samples = {len(dataset)}\")\n",
    "print(f\"Data at index 0 = {dataset[0]}\")\n",
    "print(f\"Data type = {type(dataset[0][0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`DataLoader` provides various features for batch processing. The main parameters are as follows.\n",
    "\n",
    "*   `batch_size`: number of samples per batch\n",
    "*   `shuffle`: randomize data order (generally set to `True` during training)\n",
    "*   `num_workers`: number of processes for parallelizing data loading\n",
    "*   `drop_last`: whether to drop the last incomplete batch (if `True`, it is discarded)\n",
    "\n",
    "It reads data from a `Dataset` using `__getitem__` and converts the result into tensor objects. In particular, setting `num_workers` is important when handling large image or video datasets. However, for small datasets, a single process may be more efficient. Setting `num_workers` too high can lead to overhead, so it's crucial to find an appropriate value (typically trying the number of cores or twice the number of cores)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st batch training data = tensor([[3.3120e-02, 1.4274e-01, 9.7984e-02, 1.9628e-03, 6.8926e-02, 3.4525e-01,\n",
      "         4.6966e-02, 6.0947e-02, 4.2738e-02, 1.5937e-01],\n",
      "        [8.0707e-02, 4.9181e-02, 3.1863e-02, 1.4238e-02, 1.6089e-02, 1.7980e-01,\n",
      "         1.7544e-01, 1.3465e-01, 1.6361e-01, 1.5442e-01],\n",
      "        [4.2364e-02, 3.3635e-02, 2.0840e-01, 1.6919e-02, 4.5977e-02, 6.5791e-02,\n",
      "         1.8726e-01, 1.0325e-01, 2.2029e-01, 7.6117e-02],\n",
      "        [1.4867e-01, 1.6088e-01, 1.2207e-02, 3.6049e-02, 1.1054e-04, 8.1160e-02,\n",
      "         2.9811e-02, 1.9398e-01, 4.9448e-02, 2.8769e-01]]), \n",
      " Data shape = torch.Size([4, 10])\n",
      "1st batch label data = tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]]), \n",
      " Data shape = torch.Size([4, 10])\n",
      "1st batch label data type = <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "data_loader = data.DataLoader(dataset, batch_size=4, shuffle=True, num_workers=0)\n",
    "\n",
    "# Read one batch.\n",
    "train_x, train_y = next(iter(data_loader))\n",
    "\n",
    "print(f\"1st batch training data = {train_x}, \\n Data shape = {train_x.shape}\")\n",
    "print(f\"1st batch label data = {train_y}, \\n Data shape = {train_y.shape}\")\n",
    "print(f\"1st batch label data type = {type(train_y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch provides specialized packages for domain-specific data processing. Since 2016, as deep learning has expanded to various fields, the need for specialized data processing for each domain has emerged.\n",
    "\n",
    "*   `torchvision`: computer vision\n",
    "*   `torchaudio`: audio processing\n",
    "*   `torchtext`: natural language processing\n",
    "\n",
    "Fashion-MNIST is a dataset released by Zalando Research in 2017, designed to replace MNIST. The composition of the dataset is as follows.\n",
    "\n",
    "| Category | Description |\n",
    "| --- | --- |\n",
    "| Training data | 60,000 |\n",
    "| Test data | 10,000 |\n",
    "| Image size | 28x28 grayscale |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data mean: tensor([0.2860]), std: tensor([0.3530])\n",
      "Label: 5\n",
      "Label map: Sandal\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Normalize, Compose\n",
    "import seaborn_image as isns\n",
    "import matplotlib.pyplot as plt # Added for visualization\n",
    "\n",
    "\n",
    "# Function to calculate mean and std of the dataset\n",
    "def calculate_mean_std(dataset):\n",
    "    dataloader = DataLoader(dataset, batch_size=len(dataset), shuffle=False)\n",
    "    data, _ = next(iter(dataloader))\n",
    "    mean = data.mean(axis=(0, 2, 3))  # Calculate mean across channel dimension\n",
    "    std = data.std(axis=(0, 2, 3))    # Calculate std across channel dimension\n",
    "    return mean, std\n",
    "\n",
    "\n",
    "# Datasets.  Note:  We *don't* apply Normalize here yet.\n",
    "train_dataset = datasets.FashionMNIST(\n",
    "    root=\"data\", train=True, download=True, transform=ToTensor()\n",
    ")\n",
    "test_dataset = datasets.FashionMNIST(\n",
    "    root=\"data\", train=False, download=True, transform=ToTensor()\n",
    ")\n",
    "\n",
    "# Calculate mean and std for normalization\n",
    "train_mean, train_std = calculate_mean_std(train_dataset)\n",
    "print(f\"Train data mean: {train_mean}, std: {train_std}\")\n",
    "\n",
    "# Now define transforms *with* normalization\n",
    "transform = Compose([\n",
    "    ToTensor(),\n",
    "    Normalize(train_mean, train_std)  # Use calculated mean and std\n",
    "])\n",
    "\n",
    "# Re-create datasets with the normalization transform\n",
    "train_dataset = datasets.FashionMNIST(\n",
    "    root=\"data\", train=True, download=True, transform=transform\n",
    ")\n",
    "test_dataset = datasets.FashionMNIST(\n",
    "    root=\"data\", train=False, download=True, transform=transform\n",
    ")\n",
    "\n",
    "\n",
    "# Check one training data sample.\n",
    "sample_idx = torch.randint(len(train_dataset), size=(1,)).item()\n",
    "img, label = train_dataset[sample_idx]  # Use a random index\n",
    "\n",
    "print(f\"Label: {label}\")\n",
    "\n",
    "# Manually create a label map\n",
    "labels_map = {\n",
    "    0: \"T-shirt\",\n",
    "    1: \"Trouser\",\n",
    "    2: \"Pullover\",\n",
    "    3: \"Dress\",\n",
    "    4: \"Coat\",\n",
    "    5: \"Sandal\",\n",
    "    6: \"Shirt\",\n",
    "    7: \"Sneaker\",\n",
    "    8: \"Bag\",\n",
    "    9: \"Ankle Boot\",\n",
    "}\n",
    "\n",
    "print(f\"Label map: {labels_map[label]}\")\n",
    "\n",
    "# Plot using seaborn-image.\n",
    "isns.imgplot(img.squeeze())  # Squeeze to remove channel dimension for grayscale\n",
    "plt.title(f\"Label: {labels_map[label]}\") # Add title to plot\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Define data loaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False) # No need to shuffle test data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.5 Data Transform\n",
    "\n",
    "Data Transform is a very important preprocessing step in deep learning. After the success of AlexNet in 2012, Data Augmentation became a key factor in improving model performance. PyTorch provides various tools for such transformations. Using `transforms.Compose`, multiple transformations can be applied sequentially. Additionally, custom transformations can be easily implemented through the `Lambda` function.\n",
    "\n",
    "Data transformation is very important for improving the generalization performance of models. Especially in the field of computer vision, data augmentation through various transformations has become a standard practice. The `Normalize` transformation is an essential step to standardize the data for stable model learning.\n",
    "\n",
    "To apply the `Normalize` transformation, you need to know the mean and standard deviation of the dataset. The code to calculate this is as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data mean: tensor([0.2860]), std: tensor([0.3530])\n",
      "Transformed image shape: torch.Size([1, 18, 18])\n",
      "Transformed image min/max: -0.8102576732635498, 2.022408962249756\n"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "import PIL\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "\n",
    "# Calculate mean and std of the dataset\n",
    "def calculate_mean_std(dataset):\n",
    "    dataloader = DataLoader(dataset, batch_size=len(dataset), shuffle=False) # Load all data at once\n",
    "    data, _ = next(iter(dataloader))\n",
    "    # For grayscale images, calculate mean and std over height, width dimensions (0, 2, 3)\n",
    "    # For RGB images, the calculation would be over (0, 1, 2)\n",
    "    mean = data.mean(dim=(0, 2, 3))  # Calculate mean across batch and spatial dimensions\n",
    "    std = data.std(dim=(0, 2, 3))    # Calculate std across batch and spatial dimensions\n",
    "    return mean, std\n",
    "\n",
    "# --- Example usage with FashionMNIST ---\n",
    "# 1.  Create dataset *without* normalization first:\n",
    "train_dataset_for_calc = datasets.FashionMNIST(\n",
    "    root=\"data\", train=True, download=True, transform=transforms.ToTensor()  # Only ToTensor\n",
    ")\n",
    "\n",
    "# 2. Calculate mean and std:\n",
    "train_mean, train_std = calculate_mean_std(train_dataset_for_calc)\n",
    "print(f\"Train data mean: {train_mean}, std: {train_std}\")\n",
    "\n",
    "\n",
    "# 3.  *Now* create the dataset with normalization:\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(train_mean, train_std)  # Use calculated mean and std\n",
    "])\n",
    "\n",
    "# Example of defining a custom transform using Lambda\n",
    "def crop_image(image: PIL.Image.Image) -> PIL.Image.Image:\n",
    "    # Original image is assumed to be 28x28.\n",
    "    left, top, width, height = 5, 5, 18, 18 # Example crop parameters\n",
    "    return transforms.functional.crop(image, top=top, left=left, width=width, height=height)\n",
    "\n",
    "# Compose transforms, including the custom one and normalization.\n",
    "transform_with_crop = transforms.Compose([\n",
    "    transforms.Lambda(crop_image), # Custom cropping\n",
    "    transforms.ColorJitter(),\n",
    "    transforms.RandomInvert(),\n",
    "    transforms.ToTensor(), # Must be *before* Normalize\n",
    "    transforms.Normalize(train_mean, train_std) # Use calculated mean and std\n",
    "])\n",
    "\n",
    "train_dataset_transformed = datasets.FashionMNIST(root=\"data\", train=True, download=True, transform=transform_with_crop)\n",
    "# Get one sample to check the transformation.\n",
    "sample_img, sample_label = train_dataset_transformed[0]\n",
    "print(f\"Transformed image shape: {sample_img.shape}\")\n",
    "print(f\"Transformed image min/max: {sample_img.min()}, {sample_img.max()}\") # Check normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code, we first create a dataset with only `ToTensor()` transformation applied and calculate the mean and standard deviation. Then, we define the final transformation including `Normalize` transformation using the calculated values. The example also includes adding a custom `crop_image` function to the transformation pipeline using `Lambda` function. `ToTensor()` should come before `Normalize`. `ToTensor()` converts images in the range of [0, 255] to tensors in the range of [0, 1], and `Normalize` normalizes this [0, 1] range data to have a mean of 0 and a standard deviation of 1. It is common to apply data augmentation only to the training data and not to the validation/test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.6 Model\n",
    "\n",
    "The implementation method of neural network models has developed in various ways since the 1980s. PyTorch adopted an object-oriented model implementation method from its release in 2016, which is implemented through `nn.Module`. This method greatly improved the reusability and extensibility of the model.\n",
    "\n",
    "The model class is implemented by inheriting `nn.Module` and generally includes the following methods:\n",
    "\n",
    "*   `__init__()`: Defines and initializes the components of the neural network (layers, activation functions, etc.).\n",
    "*   `forward()`: Receives input data, performs the model's forward operation, and returns the output (logit or prediction value).\n",
    "*   (Optional) `training_step()`, `validation_step()`, `test_step()`: When used with libraries like PyTorch Lightning, defines the actions for each training/validation/test step.\n",
    "*   (Optional) Other user-defined methods: Additional methods can be added to perform specific functions of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (network_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "class SimpleNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()  # Or super(SimpleNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.network_stack = nn.Sequential(\n",
    "            nn.Linear(28 * 28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)  # Flatten the image data into a 1D array\n",
    "        logits = self.network_stack(x)\n",
    "        return logits\n",
    "\n",
    "# Move model to the appropriate device (CPU or GPU)\n",
    "model = SimpleNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logit has several meanings.\n",
    "\n",
    "*   Mathematical meaning: a function that converts a probability in the range [0, 1] to a real number in the range [−∞, ∞].\n",
    "*   Meaning in deep learning: the raw output value of an unnormalized neural network.\n",
    "\n",
    "In multi-class classification problems, the softmax function is often applied at the end to convert it into a probability value that can be compared with the label. In this case, the logit becomes the input value of the softmax function.\n",
    "\n",
    "The model is generated from the class and transmitted to the device. If a GPU exists, the model is loaded into GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits: tensor([[ 0.0464, -0.0368,  0.0447, -0.0640, -0.0253,  0.0242,  0.0378, -0.1139,\n",
      "          0.0005,  0.0299]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Prediction probabilities: tensor([[0.1052, 0.0968, 0.1050, 0.0942, 0.0979, 0.1029, 0.1043, 0.0896, 0.1005,\n",
      "         0.1035]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Predicted class: tensor([0], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(1, 28, 28, device=device)\n",
    "logits = model(x)  # Don't call forward() directly!  Call the *model* object.\n",
    "prediction = nn.Softmax(dim=1)(logits)  # Convert logits to probabilities\n",
    "y_label = prediction.argmax(1) # Get the predicted class\n",
    "\n",
    "print(f\"Logits: {logits}\")\n",
    "print(f\"Prediction probabilities: {prediction}\")\n",
    "print(f\"Predicted class: {y_label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing to note is that you should not call the model's `forward()` method directly. Instead, when you call the model object like a function (`model(x)`), it will automatically execute `forward()` and integrate with PyTorch's autograd system. The model object's `__call__` method calls `forward()` and performs additional necessary work such as hooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.7 Training \n",
    "\n",
    "> **Challenge**: How can we efficiently train large datasets and complex models?\n",
    ">\n",
    "> **Researcher's Concerns**: The performance of deep learning models is greatly affected by the quantity and quality of data, as well as the complexity of the model. However, training large datasets and complex models required a lot of time and computing resources. Stabilizing the training process, preventing overfitting, and finding optimal hyperparameters were also difficult problems. To solve these problems, efficient training algorithms, optimization techniques, and automated training loops were needed.\n",
    "\n",
    "After preparing the data and model for training, we perform the actual training. To make the neural network model a good approximator, we need to update the parameters repeatedly. We define a loss function that calculates the difference between labels and predictions and select an optimizer to continuously update the parameters, reducing the error. \n",
    "\n",
    "The order of training is as follows:\n",
    "\n",
    "1. Initialize dataset and data loader\n",
    "2. Load batch unit data\n",
    "3. Calculate prediction values through forward propagation\n",
    "4. Calculate errors through loss function\n",
    "5. Calculate gradients through backpropagation\n",
    "6. Update parameters through optimizer\n",
    "\n",
    "One iteration of the entire dataset is called an epoch, and repeating this process multiple times is called a training loop.\n",
    "\n",
    "##### Hyperparameters\n",
    "Training requires three key hyperparameters.\n",
    "\n",
    "- Number of epochs: determines how many times to repeat the epoch. It's best to repeat until just before overfitting.\n",
    "- Batch size: the number of training data to pass through the model at once. Passing through all the data is often unrealistic due to GPU memory limits and exponential increases in matrix operation time. We update the model parameters incrementally with some data to approach the optimal value. If the batch size is too small, the change may be too large, making it difficult to approach the minimum value.\n",
    "- Learning rate: adjusts the scale of the updated value. It can be likened to a step size that gradually finds the optimum. Typically, it has a small value. The next chapter explores the relationship between learning rate and optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3가지 초매개변수\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "learning_rate = 1e-3 # 최적화기를 위해 앞서 지정했음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training Loop\n",
    "\n",
    "The training loop proceeds in two stages for each epoch.\n",
    "1. Training stage: parameter optimization\n",
    "2. Validation stage: performance evaluation\n",
    "\n",
    "With the advent of batch normalization in 2015, distinguishing between train() and eval() modes became important. In eval() mode, training-only operations such as batch normalization or dropout are disabled to improve inference speed.\n",
    "\n",
    "##### Loss Function\n",
    "\n",
    "The loss function is a key element in neural network learning. Since the McCulloch-Pitts neuron model in 1943, various loss functions have been proposed. In particular, the introduction of cross-entropy from information theory in 1989 was an important turning point in the development of deep learning.\n",
    "\n",
    "##### Binary Cross-Entropy (BCE)\n",
    "\n",
    "BCE, which is mainly used for binary classification, is defined as follows.\n",
    "\n",
    "$$\\mathcal{L} = - \\sum_{i} [y_i \\log{x_i} + (1-y_i)\\log{(1-x_i)}] $$\n",
    "\n",
    "Here, $y$ is the actual label and $x$ is the model's prediction value, both of which are in the range [0, 1].\n",
    "\n",
    "PyTorch provides various loss functions.\n",
    "*   `nn.MSELoss`: for regression problems (Mean Squared Error)\n",
    "*   `nn.NLLLoss`: negative log likelihood\n",
    "*   `nn.CrossEntropyLoss`: a combination of `LogSoftmax` and `NLLLoss`\n",
    "*   `nn.BCEWithLogitsLoss`: integrating sigmoid layer and BCE for numerical stability\n",
    "\n",
    "Notably, `nn.BCEWithLogitsLoss` integrates the sigmoid layer and BCE for numerical stability. Using the log function has the following advantages (described in more detail in Chapter 2):\n",
    "1.  Mitigating abrupt numerical changes\n",
    "2.  Converting multiplication to addition to improve calculation efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the loss function\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Optimizer\n",
    "\n",
    "The optimization algorithm started with the basic gradient descent in the 1950s and made great progress with the emergence of Adam in 2014. `torch.optim` provides various optimizers, and currently, Adam and AdamW are the mainstream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare the optimizer.\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Learning rate scheduler (optional, but often beneficial)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code, we added a learning rate scheduler using `torch.optim.lr_scheduler.StepLR`. The learning rate is decreased by multiplying `gamma` every `step_size` epoch. Learning rate scheduling can have a big impact on training speed and stability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training Loop\n",
    "\n",
    "Let's construct a training loop that is repeatedly performed for the dataset. One epoch typically consists of two parts: training and validation.\n",
    "\n",
    "1.  **Training Loop**: Optimizes parameters using the training dataset.\n",
    "2.  **Validation Loop**: Checks how the model's performance changes using the test (validation) dataset.\n",
    "\n",
    "During training, the model's mode can be set to `train` and `eval`. This can be thought of as a kind of switch. With the emergence of batch normalization in 2015, the distinction between `train()` and `eval()` modes became important. In `eval()` mode, training-only operations such as batch normalization or dropout are disabled to improve inference speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# TensorBoard writer setup\n",
    "writer = SummaryWriter('runs/fashion_mnist_experiment_1')\n",
    "\n",
    "\n",
    "def train_loop(model, data_loader, loss_fn, optimizer, epoch):  # Added epoch for logging\n",
    "\n",
    "    model.train()  # Set the model to training mode\n",
    "\n",
    "    size = len(data_loader.dataset)  # Total number of data samples\n",
    "    num_batches = len(data_loader)\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch_count, (input_data, label_data) in enumerate(data_loader):\n",
    "        # Move data to the GPU (if available).\n",
    "        input_data = input_data.to(device)\n",
    "        label_data = label_data.to(device)\n",
    "\n",
    "        # Compute predictions\n",
    "        preds = model(input_data)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn(preds, label_data)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()  # Perform backpropagation\n",
    "\n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()  # Zero the gradients before next iteration\n",
    "\n",
    "        if batch_count % 100 == 0:\n",
    "            loss, current = loss.item(), batch_count * batch_size + len(input_data)\n",
    "            # print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "    avg_train_loss = total_loss / num_batches\n",
    "    return avg_train_loss\n",
    "\n",
    "\n",
    "def eval_loop(model, data_loader, loss_fn):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    correct, test_loss = 0.0, 0.0\n",
    "\n",
    "    size = len(data_loader.dataset)  # Total data size\n",
    "    num_batches = len(data_loader)  # Number of batches\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation within this block\n",
    "        for input_data, label_data in data_loader:  # No need for enumerate as count is not used\n",
    "            # Move data to GPU (if available).\n",
    "            input_data = input_data.to(device)\n",
    "            label_data = label_data.to(device)\n",
    "\n",
    "            # Compute predictions\n",
    "            preds = model(input_data)\n",
    "\n",
    "            test_loss += loss_fn(preds, label_data).item()\n",
    "            correct += (preds.argmax(1) == label_data).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "\n",
    "    # print(f\"\\n Test Result \\n Accuracy: {(100 * correct):>0.1f}%, Average loss: {test_loss:>8f} \\n\")\n",
    "    return test_loss, correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Entire Training Process\n",
    "\n",
    "The entire training process repeats training and validation for each epoch. It uses `tqdm` to visually display the progress and TensorBoard to record the change in learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84baac2d3bc14a3b960d258d62b7996a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Epoch: 0, Train Loss: 1.5232, Test Loss: 0.9543, Test Accuracy: 0.71%, LR: 0.001000\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Epoch: 1, Train Loss: 0.7920, Test Loss: 0.7059, Test Accuracy: 0.76%, LR: 0.001000\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Epoch: 2, Train Loss: 0.6442, Test Loss: 0.6208, Test Accuracy: 0.78%, LR: 0.001000\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Epoch: 3, Train Loss: 0.5790, Test Loss: 0.5757, Test Accuracy: 0.79%, LR: 0.001000\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Epoch: 4, Train Loss: 0.5383, Test Loss: 0.5440, Test Accuracy: 0.80%, LR: 0.001000\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Progress bar utility\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "epochs = 5  # Reduced for demonstration\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
    "    train_loss = train_loop(model, train_dataloader, loss_fn, optimizer, epoch)\n",
    "    test_loss, correct = eval_loop(model, test_dataloader, loss_fn)\n",
    "\n",
    "    # Log training and validation metrics to TensorBoard\n",
    "    writer.add_scalar('Loss/train', train_loss, epoch)\n",
    "    writer.add_scalar('Loss/test', test_loss, epoch)\n",
    "    writer.add_scalar('Accuracy/test', correct, epoch)\n",
    "    writer.add_scalar('Learning Rate', optimizer.param_groups[0]['lr'], epoch) # Log learning rate\n",
    "\n",
    "    print(f'Epoch: {epoch}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}, Test Accuracy: {correct:.2f}%, LR: {optimizer.param_groups[0][\"lr\"]:.6f}')\n",
    "    scheduler.step()  # Update learning rate.  Place *after* logging.\n",
    "\n",
    "print(\"Done!\")\n",
    "writer.close() # Close TensorBoard Writer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This training-validation cycle has been a standard way of training deep learning models since the 1990s, particularly as the validation phase plays an important role in monitoring overfitting and determining early stopping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.8 Model Saving and Loading \n",
    "\n",
    "Model saving is a very important part in deep learning practice. The trained model can be saved and reloaded later for reuse or deployed to other environments (e.g., server, mobile device). PyTorch provides two main ways of saving.\n",
    "\n",
    "##### Saving Only Weights\n",
    "\n",
    "The learned parameters (weights and biases) of the model are stored in a Python dictionary called `state_dict`. The `state_dict` is a structure that maps each layer to its parameter tensor. This method has the advantage that it can load weights even if the model structure changes, so it is generally recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_112013/3522135054.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_saved_weights.load_state_dict(torch.load('model_weights.pth'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5459668265935331, 0.8036)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save model weights\n",
    "torch.save(model.state_dict(), 'model_weights.pth')\n",
    "\n",
    "# Load weights\n",
    "model_saved_weights = SimpleNetwork()  # Create an empty model with the same architecture\n",
    "model_saved_weights.load_state_dict(torch.load('model_weights.pth'))\n",
    "model_saved_weights.to(device) # Don't forget to move to the correct device!\n",
    "model_saved_weights.eval() # Set to evaluation mode\n",
    "\n",
    "# Check performance (assuming eval_loop is defined)\n",
    "eval_loop(model_saved_weights, test_dataloader, loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Saving the Entire Model\n",
    "\n",
    "Since 2018, as model architectures have become more complex, a method of saving both the model structure and weights together is also used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_112013/3185686172.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_saved = torch.load('model_trained.pth')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5459668265935331, 0.8036)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save(model, 'model_trained.pth')\n",
    "\n",
    "# Load the entire model\n",
    "model_saved = torch.load('model_trained.pth')\n",
    "model_saved.to(device)  # Move the loaded model to the correct device.\n",
    "model_saved.eval() #  Set the loaded model to evaluation mode\n",
    "\n",
    "# Check performance\n",
    "eval_loop(model_saved, test_dataloader, loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Storing the entire model can be convenient, but it may cause compatibility issues when the model class definition changes. Especially in production environments, where the model architecture rarely changes, storing only the weights can be more stable. Additionally, storing the entire model uses Python's `pickle` module, which has a vulnerability that can execute arbitrary code, making it a security risk.\n",
    "\n",
    "##### Safetensors: A Safer Alternative\n",
    "\n",
    "Recently, new storage formats like `safetensors` have emerged to improve security and loading speed instead of `pickle`. `Safetensors` is a format for safely and efficiently storing tensor data.\n",
    "\n",
    "*   **Security:** `Safetensors` does not allow the execution of arbitrary code, making it much safer than `pickle`.\n",
    "*   **Zero-copy:** It loads data directly into memory without copying, resulting in fast loading speeds.\n",
    "*   **Lazy loading:** It can load only the necessary parts, reducing memory usage.\n",
    "*   **Support for various frameworks**: PyTorch, TensorFlow, JAX, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5459668265935331, 0.8036)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Install safetensors: pip install safetensors\n",
    "\n",
    "from safetensors.torch import save_file, load_file\n",
    "\n",
    "# Save using safetensors\n",
    "state_dict = model.state_dict()\n",
    "save_file(state_dict, \"model_weights.safetensors\")\n",
    "\n",
    "# Load using safetensors\n",
    "loaded_state_dict = load_file(\"model_weights.safetensors\", device=device) # Load directly to the device.\n",
    "model_new = SimpleNetwork().to(device) # Create an instance of your model class\n",
    "model_new.load_state_dict(loaded_state_dict)\n",
    "model_new.eval()\n",
    "\n",
    "# Check performance\n",
    "eval_loop(model_new, test_dataloader, loss_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 TensorBoard \n",
    "\n",
    "TensorBoard is a tool that records, tracks, and efficiently visualizes various logs generated during deep learning training. It is a type of log data recording/visualization tool, often referred to as a dashboard. Initially developed for TensorFlow, it is now integrated with PyTorch. There are other visualization tools similar to TensorBoard, including:\n",
    "\n",
    "- Weights & Biases (WandB): A cloud-based MLOps integrated platform that provides extensive features such as experiment tracking, dataset version management, and model management. It is particularly renowned for its team collaboration features, making it widely used in corporate environments.\n",
    "- Vertex AI: A fully managed ML tool from Google Cloud, offering native integration with BigQuery, Dataproc, and Spark. It enables rapid model building, deployment, and scaling, making it suitable for large-scale ML workflows.\n",
    "- MLflow: An open-source tool that provides experiment tracking, model packaging, and a central registry. It simplifies ML model tracking and deployment, making it widely used in data science and ML fields.\n",
    "\n",
    "There are many other tools besides these three. Here, we will primarily use TensorBoard.\n",
    "\n",
    "### 3.2.1 Basic Usage of TensorBoard\n",
    "\n",
    "TensorBoard emerged in 2015 along with TensorFlow. At that time, the complexity of deep learning models increased rapidly, and the need to effectively monitor the training process arose.\n",
    "\n",
    "The core features of TensorBoard are as follows:\n",
    "1. Scalar metric tracking: recording numerical values such as loss and accuracy\n",
    "2. Model structure visualization: diagramming the computation graph\n",
    "3. Distribution tracking: observing changes in weight and gradient distributions\n",
    "4. Embedding projection: 2D/3D visualization of high-dimensional vectors\n",
    "5. Hyperparameter optimization: comparing experiment results with different settings\n",
    "\n",
    "TensorBoard is a powerful tool for visualizing and analyzing deep learning training processes. The basic usage of TensorBoard consists of three main steps: installation, log directory setting, and callback setting.\n",
    "\n",
    "##### Installation Method\n",
    "\n",
    "TensorBoard can be installed using pip or conda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!pip install tensorboard\n",
    "# 또는\n",
    "!conda install -c conda-forge tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Log Directory Setting\n",
    "\n",
    "TensorBoard reads event files stored in the log directory and visualizes them. In Jupyter Notebook or Colab, it is set as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# 로그 디렉토리 설정\n",
    "log_dir = 'logs/experiment_1'\n",
    "writer = SummaryWriter(log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Running TensorBoard\n",
    "\n",
    "TensorBoard can be run in two ways.\n",
    "\n",
    "1. Running from the command line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "tensorboard --logdir=logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Run in Jupyter Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running, you can access the TensorBoard dashboard at http://localhost:6006 in your web browser.\n",
    "\n",
    "##### Running on a Remote Server\n",
    "\n",
    "When running TensorBoard on a remote server, use SSH tunneling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "ssh -L 6006:127.0.0.1:6006 username@server_ip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Main Parameters (SummaryWriter)**\n",
    "\n",
    "`SummaryWriter` is a core class that generates data to be recorded in TensorBoard. The main parameters are as follows:\n",
    "\n",
    "*   `log_dir`: the directory path where log files will be saved.\n",
    "*   `comment`: a string to be appended to `log_dir`.\n",
    "*   `flush_secs`: the frequency (in seconds) at which logs are written to disk.\n",
    "*   `max_queue`: sets the maximum number of pending events/steps to be stored.\n",
    "\n",
    "**Main Methods (SummaryWriter)**\n",
    "\n",
    "*   `add_scalar(tag, scalar_value, global_step=None)`: records a scalar value (e.g., loss, accuracy).\n",
    "*   `add_histogram(tag, values, global_step=None, bins='tensorflow')`: records a histogram (value distribution).\n",
    "*   `add_image(tag, img_tensor, global_step=None, dataformats='CHW')`: records an image.\n",
    "*   `add_figure(tag, figure, global_step=None, close=True)`: records a Matplotlib figure.\n",
    "*   `add_video(tag, vid_tensor, global_step=None, fps=4, dataformats='NCHW')`: records a video.\n",
    "*   `add_audio(tag, snd_tensor, global_step=None, sample_rate=44100)`: records audio.\n",
    "*   `add_text(tag, text_string, global_step=None)`: records text.\n",
    "*   `add_graph(model, input_to_model=None, verbose=False)`: records a model graph.\n",
    "*   `add_embedding(mat, metadata=None, label_img=None, global_step=None, tag='default', metadata_header=None)`: records an embedding projector.\n",
    "*   `add_hparams(hparam_dict, metric_dict, hparam_domain_discrete=None, run_name=None)`: records hyperparameters and their metrics.\n",
    "*   `flush()`: writes all pending events to disk.\n",
    "*   `close()`: ends logging and releases resources.\n",
    "\n",
    "**Main Callback Parameters (TensorFlow/Keras)**\n",
    "\n",
    "When using TensorBoard with TensorFlow/Keras, the `tf.keras.callbacks.TensorBoard` callback is used. The main parameters are as follows:\n",
    "\n",
    "*   `log_dir`: the location where logs will be saved.\n",
    "*   `histogram_freq`: the frequency (in epochs) at which histograms are computed (0 means no computation). Used to visualize weight, bias, and activation value distributions.\n",
    "*   `write_graph`: whether to visualize the model graph.\n",
    "*   `write_images`: whether to visualize model weights as images.\n",
    "*   `update_freq`: the frequency at which losses and metrics are recorded ('batch', 'epoch', or an integer).\n",
    "*   `profile_batch`: specifies the batch range to profile (e.g., `profile_batch='5, 8'`). Profiling is useful for finding performance bottlenecks.\n",
    "*   `embeddings_freq`: the frequency at which embedding layers are visualized.\n",
    "*   `embeddings_metadata`: the path to an embedding metadata file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 TensorBoard's Major Visualization Features\n",
    "\n",
    "TensorBoard can visualize various indicators that occur during the model learning process. The main visualization dashboards include scalars, histograms, distributions, graphs, and embeddings.\n",
    "\n",
    "##### Scalar Indicator Visualization\n",
    "The scalar dashboard visualizes changes in numerical indicators such as loss values and accuracy. It can track various statistical values in the model training process, such as learning rates, gradient norms, and average/standard deviation of layer weights. It can also monitor quality evaluation indicators such as FID (Fréchet Inception Distance) scores or QICE (Quantile Interval Coverage Error) in the latest generative models. Through these indicators, it is possible to monitor the model's learning progress in real-time and detect problems such as overfitting or learning instability early on. Scalar values can be recorded as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.add_scalar('Loss/train', train_loss, step)\n",
    "writer.add_scalar('Accuracy/train', train_acc, step)\n",
    "writer.add_scalar('Learning/learning_rate', current_lr, step)\n",
    "writer.add_scalar('Gradients/norm', grad_norm, step)\n",
    "writer.add_scalar('Quality/fid_score', fid_score, step)\n",
    "writer.add_scalar('Metrics/qice', qice_value, step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Histograms and Distribution Visualizations\n",
    "You can observe the distribution changes of weights and biases. Histograms visually show the distribution of weights, biases, gradients, and activation values for each layer, helping to understand the internal state of the model. In particular, it is very useful for model debugging as it can detect problems such as weights becoming saturated at specific values or gradients vanishing/exploding early in the learning process. Histograms can be logged as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    writer.add_histogram(f'Parameters/{name}', param.data, global_step)\n",
    "    if param.grad is not None:\n",
    "        writer.add_histogram(f'Gradients/{name}', param.grad, global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model Structure Visualization\n",
    "The model's structure can be visually confirmed. In particular, the hierarchical structure and connections of complex neural networks can be intuitively grasped. TensorBoard expresses the flow of data, the input/output shape of each layer, the order of operations, etc. in a graphical form through a calculation graph, and detailed information can be reviewed by expanding each node. Recently, it has been especially useful for visualizing complex attention mechanisms, cross-attention layers, and conditional branching structures of Transformers or Diffusion models. This is very useful for model debugging and optimization, and is especially helpful for understanding complex architectures with skip connections or parallel structures. The model graph can be recorded as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.add_graph(model, input_to_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Embedding Visualization\n",
    "TensorBoard's Projector can project high-dimensional embeddings into 2D or 3D space for visualization, which is useful for analyzing the relationship between word embeddings or image feature vectors. It visualizes complex high-dimensional data while preserving cluster structures and relative distances through dimension reduction techniques such as PCA or UMAP. In particular, UMAP preserves both local and global structure well while allowing for fast visualization. This allows you to check how data points with similar characteristics are clustered, whether class distinctions are made well, and track how the feature space changes during training. Embeddings can be recorded as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.add_embedding(\n",
    "    features,\n",
    "    metadata=labels,\n",
    "    label_img=images,\n",
    "    global_step=step\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hyperparameter Visualization\n",
    "The results of hyperparameter tuning can be visualized. Not only learning rate, batch size, and dropout ratio, but also structural parameters such as the number of attention heads in Transformer models, prompt length, and token embedding dimensions can be analyzed. Inference parameters such as noise scheduling, sampling step count, and CFG (Classifier-Free Guidance) weights, which are important in the latest LLM or Diffusion models, can also be visualized together. The performance of the model for various hyperparameter combinations is expressed in parallel coordinate graphs or scatter plots to help find the optimal configuration. In particular, it is easy to analyze the effect of interactions between hyperparameters on model performance because multiple experiment results can be compared at a glance. Hyperparameters and related metrics can be recorded as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.add_hparams(\n",
    "    {\n",
    "        'lr': learning_rate, \n",
    "        'batch_size': batch_size, \n",
    "        'num_heads': n_heads,\n",
    "        'cfg_scale': guidance_scale,\n",
    "        'sampling_steps': num_steps,\n",
    "        'prompt_length': max_length\n",
    "    },\n",
    "    {\n",
    "        'accuracy': accuracy, \n",
    "        'loss': final_loss,\n",
    "        'fid_score': fid_score\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Image Visualization\n",
    "You can visualize images or intermediate feature maps generated during the learning process. By visualizing the filters and activation maps of convolutional layers, you can intuitively understand what features the model is learning and check which parts of the input image each layer is paying attention to. Especially in latest generation models like Stable Diffusion or DALL-E, it is very useful to visually track changes in the quality of generated images. With the emergence of hybrid models, more sophisticated and realistic image generation has become possible. Images can be recorded as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 이미지나 생성된 이미지 시각화\n",
    "writer.add_images('Images/generated', generated_images, global_step)\n",
    "\n",
    "# 디퓨전 모델의 중간 생성 과정 시각화\n",
    "writer.add_images('Diffusion/steps', diffusion_steps, global_step)\n",
    "\n",
    "# 어텐션 맵 시각화\n",
    "writer.add_image('Attention/maps', attention_visualization, global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through TensorBoard's visualization function, you can intuitively understand the model's learning process and quickly identify problems. In particular, it is useful for early termination of the learning process or hyperparameter adjustment because it can monitor the progress of learning in real time. Embedding visualization is especially useful for understanding the relationship between high-dimensional data, and helps analyze the structure of the feature space learned by the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3 TensorBoard Example\n",
    "\n",
    "This section provides a concrete example of applying the various features of TensorBoard, which were previously discussed, to actual deep learning model training. Using the MNIST handwritten digit dataset, we train a simple CNN (Convolutional Neural Network) model and explain step-by-step how to visualize key indicators and data that occur during training through TensorBoard.\n",
    "\n",
    "**Key Visualization Elements:**\n",
    "\n",
    "| Visualization Type          | Visualization Content                                                                                           | TensorBoard Tab |\n",
    "| :------------------- | :--------------------------------------------------------------------------------------------------- | :---------- |\n",
    "| **Scalar Metrics**     | Training/test loss, training/test accuracy, learning rate, gradient norm                                     | SCALARS     |\n",
    "| **Histograms/Distributions**   | Weight distributions of all layers, gradient distributions of all layers                          | DISTRIBUTIONS, HISTOGRAMS |\n",
    "| **Model Structure**       | Computational graph of the MNIST CNN model                                                       | GRAPHS      |\n",
    "| **Feature Maps**         | Feature maps of Conv1 layer, feature maps of Conv2 layer, input image grid, visualization of Conv1 filters                        | IMAGES      |\n",
    "| **Embeddings**          | 32-dimensional feature vectors of FC1 layer, 2D visualization using t-SNE, MNIST image labels                            | PROJECTOR   |\n",
    "| **Hyperparameters**   | Batch size, learning rate, dropout ratio, optimizer type, weight decay, momentum, scheduler step/gamma       | HPARAMS     |\n",
    "\n",
    "**Visualization Frequency:**\n",
    "\n",
    "*   Scalar/histogram: every 50 batches\n",
    "*   Feature maps/images: every 50 batches\n",
    "*   Embeddings: at the end of each epoch\n",
    "*   Hyperparameters: at the start and end of training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example uses the `dld` package. It imports the necessary modules and starts training. The `train()` function trains a CNN model on the MNIST dataset with default hyperparameters and logs the training process to TensorBoard. To experiment with different hyperparameters, you can pass an `hparams_dict` argument to the `train()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In a notebook cell:\n",
    "from dldna.chapter_03.train import train\n",
    "\n",
    "# Run with default hyperparameters\n",
    "train()\n",
    "\n",
    "# Run with custom hyperparameters\n",
    "my_hparams = {\n",
    "    'batch_size': 128,\n",
    "    'learning_rate': 0.01,\n",
    "    'epochs': 8,\n",
    "}\n",
    "train(hparams_dict=my_hparams, log_dir='runs/my_custom_run')\n",
    "\n",
    "# Start TensorBoard (in a separate cell, or from the command line)\n",
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Running TensorBoard:**\n",
    "\n",
    "After training is complete, run TensorBoard from the shell using the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tensorboard --logdir runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can view the TensorBoard dashboard by accessing `http://localhost:6006` in your web browser. \n",
    "\n",
    "You can see that several cards have been created for each item as follows.\n",
    "![TensorBoard](../../../assets/images/03_01.png)\n",
    "\n",
    "In each item, you can check individual values and images.\n",
    "![TensorBoard](../../../assets/images/03_02.png)\n",
    "\n",
    "\n",
    "**Utilizing the TensorBoard Dashboard**\n",
    "\n",
    "*   **SCALARS tab:** Tracks changes in training/testing loss, accuracy, learning rate, etc. over time. This helps determine if the model is learning well and whether overfitting is occurring.\n",
    "*   **GRAPHS tab:** Visualizes the model's computation graph, showing data flow and operations at a glance. This helps understand complex model structures.\n",
    "*   **DISTRIBUTIONS/HISTOGRAMS tab:** Visualizes weight and gradient distributions. This helps diagnose if weight initialization is proper and whether vanishing or exploding gradients are occurring.\n",
    "*   **IMAGES tab:** Visualizes input images, feature maps, filters, etc. in image form. This intuitively shows which parts of the image the model is looking at and whether feature extraction is working well.\n",
    "*   **PROJECTOR tab:** Projects high-dimensional embeddings into 2D/3D for visualization. This helps identify data clustering and outliers.\n",
    "*   **HPARAMS tab:** Compares experimental results with various hyperparameter combinations, aiding in finding optimal settings.\n",
    "\n",
    "In this example, we looked at how to use TensorBoard to visualize deep learning model training processes. TensorBoard is an essential tool not only for simple visualization but also for understanding model behavior, diagnosing issues, and improving performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Hugging Face Transformers\n",
    "\n",
    "Hugging Face started in 2016 as a French company that created a chatbot app for teenagers. Initially, it aimed to provide an AI friend for emotional support and entertainment, but it took a significant turn when it open-sourced its chatbot's NLP model. This coincided with the emergence of high-performance language models like BERT and GPT, which were difficult to utilize at the time, causing a significant stir. The release of the Transformers library in 2019 brought innovation to the field of natural language processing. While PyTorch provides the basic operations and learning framework for deep learning, Hugging Face focused on the implementation and application of actual language models based on it. In particular, it made it easy to share and reuse pre-trained models, making large language models that were once exclusive to a few major companies available to anyone.\n",
    "\n",
    "Hugging Face has built an open ecosystem, earning it the nickname \"AI's GitHub.\" Currently, over 1 million models and tens of thousands of datasets are shared, evolving into a platform for ethical and responsible AI development beyond a simple code repository. The model card system is introduced to specify each model's limitations and biases, and a community-based feedback system continuously verifies the quality and ethics of models. These efforts have presented a new paradigm for responsible technological advancement beyond democratizing AI development. Hugging Face's approach balances technical innovation with ethical considerations, making it an exemplary case in modern AI development.\n",
    "\n",
    "### 3.3.1 Introduction to the Transformers Library\n",
    "\n",
    "Transformers provide a unified interface for easily downloading and using pre-trained models. It works on top of frameworks like PyTorch or TensorFlow, ensuring compatibility with existing deep learning ecosystems. Support for new frameworks like JAX has also broadened researchers' options. The core components of Transformers are largely divided into two parts.\n",
    "\n",
    "##### Model Hub and Pipeline\n",
    "\n",
    "The model hub acts as a central repository for pre-trained models. Models specialized in various natural language processing tasks such as text generation, classification, translation, summarization, and question-answering are available. Each model is provided with detailed metadata including performance metrics, license information, and learning data sources. The Model Card system is particularly notable for specifying the limitations and biases of models, encouraging responsible AI development.\n",
    "\n",
    "The pipeline abstracts complex pre-processing and post-processing steps into a simple interface. This is especially useful in production environments, significantly reducing model integration costs. Internally, pipelines automatically configure tokenizers and models and perform optimizations like batch processing or GPU acceleration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6703892f09b4ade869f16b776740536",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5210ba6dfe24216ab409ef41d197c2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc25443102364b8e96035a7c0218e23b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e19048c8971437b82f666267ec92f21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "result = classifier(\"I love this book!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokenizer and Model Class\n",
    "\n",
    "The tokenizer converts input text into a numerical sequence that the model can process. Each model has its own dedicated tokenizer, which reflects the characteristics of the training data. The tokenizer consistently handles complex preprocessing beyond simple word separation, including subword tokenization, special token addition, padding, and truncation. In particular, it supports various tokenization algorithms such as WordPiece, BPE, and SentencePiece in an integrated manner, allowing for the selection of the optimal tokenization method suitable for each language and domain.\n",
    "\n",
    "The model class implements the neural network that performs actual operations. It supports various architectures such as BERT, GPT, and T5, and allows automatic selection of the model architecture through the AutoModel series of classes. Each model is provided with pre-trained weights and can be fine-tuned for specific tasks as needed. Additionally, optimization techniques such as model parallelization, quantization, and pruning can be applied immediately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 Key Use Cases\n",
    "\n",
    "The Transformers library is used for various natural language processing tasks. Since 2020, the advancement of GPT-series models has significantly improved text generation capabilities, and with the emergence of high-performance open-source models like Llama 3 in 2024, the scope of applications has expanded further. In particular, Llama 3's 405B parameter model shows performance comparable to GPT-4, achieving significant advancements in multilingual processing, coding, and inference capabilities. These developments have enabled various applications in real-world business environments, including customer support, content generation, data analysis, and automated task processing. The improved code generation and debugging capabilities have also contributed to enhanced developer productivity.\n",
    "\n",
    "**Utilizing the Hugging Face Hub:**\n",
    "\n",
    "The Hugging Face Hub ([https://huggingface.co/models](https://huggingface.co/models)) is a platform where you can search, filter, and download numerous models and datasets.\n",
    "\n",
    "*   **Model Search:** You can search for models by name (e.g., \"bert\", \"gpt2\", \"t5\") or task (e.g., \"text-classification\", \"question-answering\") in the top-left search bar.\n",
    "*   **Filtering:** The left panel allows filtering by task, library, language, dataset, and other criteria.\n",
    "*   **Model Page:** Each model page provides useful information, including model descriptions, usage examples, performance metrics, and model cards.\n",
    "\n",
    "**Text Generation and Classification**\n",
    "\n",
    "Text generation involves creating natural text based on a given prompt. The latest models offer advanced features such as:\n",
    "- Multimodal generation: Creating content that combines text and images\n",
    "- Automated code generation: Writing optimized code in various programming languages\n",
    "- Conversational agents: Implementing intelligent chatbots that understand context\n",
    "- Domain-specific text: Generating documents for specialized domains like medicine or law"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Design a webpage that is compatible with your browser with our FREE SEO Service.\n",
      "\n",
      "You read that right. By utilizing a web browser's default settings, your webpage should be free from advertisements and other types of spam. The best way to avoid this\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Text generation pipeline (using gpt2 model)\n",
    "generator = pipeline('text-generation', model='gpt2')  # Smaller model\n",
    "result = generator(\"Design a webpage that\", max_length=50, num_return_sequences=1)\n",
    "print(result[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text classification will be further refined in 2025, providing the following features:\n",
    "\n",
    "- Zero-shot/few-shot learning: Immediate adaptation to new categories is possible through Hugging Face's Transformer library. In particular, natural language inference-based pre-trained models can achieve over 90% accuracy with fewer than 8 examples and can be applied to various domains.\n",
    "- Multilingual classification: Hugging Face's ModernBERT and other state-of-the-art multilingual models support more than 16 major languages. In particular, the 150M parameter base model also achieves an F1 score of over 80%, showing excellent performance even in low-resource languages.\n",
    "- Hierarchical classification: Hugging Face's HiGen framework provides specialized functionality for hierarchical label classification. By capturing the semantic relationship between text and labels through level-based loss functions, it shows high performance, especially in classes with insufficient data.\n",
    "- Real-time classification: Real-time processing of streaming data is possible through Hugging Face pipelines. With optimization techniques like Flash Attention integrated by default, long sequences can be processed efficiently, providing high throughput in real-time applications.\n",
    "\n",
    "##### Fine-tuning and Model Sharing\n",
    "\n",
    "Hugging Face provides the latest fine-tuning technologies to support efficient learning of large language models. These technologies can greatly reduce learning costs and time while maintaining model performance.\n",
    "\n",
    "- QLoRA (Quantized Low-Rank Adaptation): Provided through Hugging Face's PEFT library, it combines 4-bit quantization and low-rank adaptation to reduce memory usage by over 90%. In particular, fine-tuning of 65B parameter models is possible on a single 48GB GPU.\n",
    "- Spectrum: A selective layer optimization technique integrated with Hugging Face's TRL library. By analyzing the signal-to-noise ratio of each layer and selectively learning only the important layers, it improves computational efficiency.\n",
    "- Flash Attention: Supported by default from Hugging Face Transformer version 2.2, it can be easily activated with the attn_implementation=\"flash_attention_2\" parameter. In particular, memory efficiency is greatly improved in long sequence processing.\n",
    "- DeepSpeed: Perfectly integrated through Hugging Face's Accelerate library, it efficiently supports large-scale distributed learning through the ZeRO optimizer. In particular, it can also be used during inference, allowing large models to be distributed and loaded across multiple GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad7aa580feaf4d5fa3abcd96b1bc43e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24731acb95cb4dbea5d194f768b17df3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee9456452ccb4910849e2973a1765462",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "688ecbdc102a4cc6b562c911f79851c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62d43521fee04e07a6ce10a5488d2b38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2522a9d78974c45beeb8575e3c29e85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sean/anaconda3/envs/DL/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 00:00, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.667500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments, Trainer, DataCollatorWithPadding\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. Load a pre-trained model and tokenizer ---\n",
    "model_name = \"distilbert-base-uncased\"  # Use a small, fast model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)  # Binary classification\n",
    "\n",
    "# --- 2. Create a simple dataset (for demonstration) ---\n",
    "raw_data = {\n",
    "    \"text\": [\n",
    "        \"This is a positive example!\",\n",
    "        \"This is a negative example.\",\n",
    "        \"Another positive one.\",\n",
    "        \"And a negative one.\"\n",
    "    ],\n",
    "    \"label\": [1, 0, 1, 0],  # 1 for positive, 0 for negative\n",
    "}\n",
    "dataset = Dataset.from_dict(raw_data)\n",
    "\n",
    "# --- 3. Tokenize the dataset ---\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True) #padding is handled by data collator\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_dataset = tokenized_dataset.remove_columns([\"text\"]) # remove text, keep label\n",
    "\n",
    "# --- 4. Data Collator (for dynamic padding) ---\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# --- 5. Training Arguments ---\n",
    "fp16_enabled = False\n",
    "if torch.cuda.is_available():\n",
    "    try:\n",
    "        if torch.cuda.get_device_capability()[0] >= 7:\n",
    "            fp16_enabled = True\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=1,          # Keep it short\n",
    "    per_device_train_batch_size=2,  # Small batch size\n",
    "    logging_steps=1,           # Log every step\n",
    "    save_strategy=\"no\",         # No saving\n",
    "    report_to=\"none\",          # No reporting\n",
    "    fp16=fp16_enabled,  # Use fp16 if avail.\n",
    "    # --- Optimization techniques (demonstration) ---\n",
    "    # gradient_checkpointing=True,  # Enable gradient checkpointing (if needed for large models)\n",
    "    # gradient_accumulation_steps=2, # Increase effective batch size\n",
    ")\n",
    "\n",
    "\n",
    "# --- 6. Trainer ---\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    # eval_dataset=...,  # Add an eval dataset if you have one\n",
    "    data_collator=data_collator,  # Use the data collator\n",
    "    # optimizers=(optimizer, scheduler) # you could also customize optimizer\n",
    ")\n",
    "\n",
    "# --- 7. Train ---\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "print(\"Training finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model sharing ecosystem currently supports the following latest features as of 2025.\n",
    "- Model card auto-generation: Hugging Face's automated model card system automatically analyzes and documents performance metrics and bias. In particular, it can clearly describe the characteristics and limitations of the model in a standardized format through the Model Card Toolkit.\n",
    "- Version management: The Git-based version management system on Hugging Face Hub tracks the change history and performance changes of models. It can automatically record and compare performance metrics and parameter changes for each version.\n",
    "- Collaboration tools: It provides an integrated collaboration environment with Hugging Face Spaces. Team members can share and feedback on model development, testing, and deployment processes in real-time, and also support integration with CI/CD pipelines.\n",
    "- Ethical AI: Through Hugging Face's ethical AI framework, it automatically verifies and evaluates the bias of models. In particular, it can analyze performance differences for various demographic groups and identify potential risks in advance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice Problems\n",
    "\n",
    "**1. Basic Problems**\n",
    "\n",
    "* Explain the differences between PyTorch tensors and NumPy arrays, and how to convert between them.\n",
    "* Describe the role of the `torch.nn.Linear` layer and how to initialize its weights.\n",
    "* Explain how automatic differentiation works in PyTorch and the role of the `requires_grad` attribute.\n",
    "\n",
    "**2. Applied Problems**\n",
    "\n",
    "* Write code to split a given dataset into training, validation, and test sets using `torch.utils.data.Dataset` and `torch.utils.data.DataLoader`, and load data in batches.\n",
    "* Implement a simple CNN model (e.g., LeNet-5) by inheriting from `nn.Module`, and use `torchsummary` to check the model's structure and number of parameters.\n",
    "* Train a model using the MNIST or Fashion-MNIST dataset, and visualize the training process (loss, accuracy, etc.) using TensorBoard.\n",
    "\n",
    "**3. Advanced Problems**\n",
    "\n",
    "* Implement matrix multiplication, transposition, batch matrix multiplication, and bilinear transformation using `torch.einsum`. (Provide Einstein notation for each operation and implement it in PyTorch code.)\n",
    "* Create a custom dataset and apply data augmentation using `torchvision.transforms`. (e.g., image rotation, cropping, color conversion)\n",
    "* Explain how to calculate higher-order derivatives using `torch.autograd.grad` and provide a simple example code. (e.g., calculating the Hessian matrix)\n",
    "* Explain why the `forward()` method of `torch.nn.Module` is not called directly, but instead, the model object is called as a function. (Hint: relationship between the `__call__` method and the automatic differentiation system)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.callout-note collapse=\"true\" title=\"Click to view contents (answer)\"}\n",
    "## Practice Problem Solutions\n",
    "\n",
    "### 1. Basic Problem Solutions\n",
    "\n",
    "1.  **Tensor vs. NumPy Array:**\n",
    "    *   **Difference:** Tensors support GPU acceleration and automatic differentiation. NumPy arrays are CPU-based general-purpose array operations.\n",
    "    *   **Conversion:** `torch.from_numpy()`, `.numpy()` (note: for GPU tensors, use `.cpu()` first).\n",
    "\n",
    "    ```python\n",
    "    # Example\n",
    "    import torch\n",
    "    import numpy as np\n",
    "    numpy_array = np.array([1, 2, 3])\n",
    "    torch_tensor = torch.from_numpy(numpy_array)  # or torch.tensor()\n",
    "    numpy_back = torch_tensor.cpu().numpy()\n",
    "    ```\n",
    "2.  **`nn.Linear`:**\n",
    "    *   **Role:** `y = xW^T + b` (linear transformation). It multiplies input `x` by weight `W` and adds bias `b`.\n",
    "    *   **Initialization:** Default is Kaiming He initialization (uniform distribution). Can be changed using the `torch.nn.init` module.\n",
    "\n",
    "    ```python\n",
    "    # Example\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.init as init\n",
    "    linear_layer = nn.Linear(in_features=10, out_features=5)\n",
    "    init.xavier_uniform_(linear_layer.weight) # Xavier initialization\n",
    "    ```\n",
    "\n",
    "3.  **Autograd (Automatic Differentiation):**\n",
    "    *   **Operation:** When `requires_grad=True`, tensor operations create a computation graph, and `.backward()` calculates gradients using the chain rule.\n",
    "    *   **`requires_grad`:** Sets whether to calculate and track gradients.\n",
    "\n",
    "    ```python\n",
    "    # Example\n",
    "    import torch\n",
    "    x = torch.tensor([2.0], requires_grad=True)\n",
    "    y = x**2 + 3*x + 1\n",
    "    y.backward()\n",
    "    print(x.grad)  # Output: tensor([7.])\n",
    "    ```\n",
    "\n",
    "### 2. Applied Problem Solutions\n",
    "\n",
    "4.  **`Dataset`, `DataLoader`:**\n",
    "\n",
    "    ```python\n",
    "    from torch.utils.data import Dataset, DataLoader, random_split\n",
    "    import torchvision.transforms as transforms\n",
    "    from torchvision import datasets\n",
    "\n",
    "    # Custom Dataset (example)\n",
    "    class CustomDataset(Dataset):\n",
    "        def __init__(self, data, targets, transform=None):\n",
    "            self.data = data\n",
    "            self.targets = targets\n",
    "            self.transform = transform\n",
    "        def __len__(self):\n",
    "            return len(self.data)\n",
    "        def __getitem__(self, idx):\n",
    "            sample, label = self.data[idx], self.targets[idx]\n",
    "            if self.transform:\n",
    "                sample = self.transform(sample)\n",
    "            return sample, label\n",
    "    ```\n",
    "# MNIST DataLoader Example (using torchvision)\n",
    "transform = transforms.ToTensor() # convert image data to tensor\n",
    "mnist_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "train_size = int(0.8 * len(mnist_dataset))\n",
    "val_size = len(mnist_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(mnist_dataset, [train_size, val_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "\n",
    "```\n",
    "\n",
    "5.  **LeNet-5, `torchsummary`, TensorBoard:** (full code is in the previous answer, here's only the key part)\n",
    "\n",
    "```python\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# LeNet-5 model\n",
    "class LeNet5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet5, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5, stride=1, padding=2)\n",
    "        self.pool1 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5, stride=1)\n",
    "        self.pool2 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = LeNet5()\n",
    "summary(model, input_size=(1, 28, 28)) # model structure summary\n",
    "\n",
    "# ... (training code, see previous answer) ...\n",
    "\n",
    "writer = SummaryWriter() # TensorBoard\n",
    "# ... (log with writer.add_scalar() during training) ...\n",
    "writer.close()\n",
    "```\n",
    "\n",
    "### 3. Advanced Problem Solutions\n",
    "\n",
    "6.  **`torch.einsum`:**\n",
    "\n",
    "```python\n",
    "import torch\n",
    "```\n",
    "A = torch.randn(3, 4)\n",
    "B = torch.randn(4, 5)\n",
    "C = torch.einsum(\"ij,jk->ik\", A, B)   # matrix multiplication\n",
    "D = torch.einsum(\"ij->ji\", A)        # transpose\n",
    "E = torch.einsum(\"bi,bj,ijk->bk\", A, B, torch.randn(2,3,4))  # bilinear transformation\n",
    "```\n",
    "\n",
    "7.  **Custom dataset, data augmentation:**\n",
    "\n",
    "    ```python\n",
    "    from torch.utils.data import Dataset\n",
    "    from torchvision import transforms\n",
    "    from PIL import Image\n",
    "    import os\n",
    "\n",
    "    class CustomImageDataset(Dataset): # inherit from Dataset\n",
    "        def __init__(self, root_dir, transform=None):\n",
    "            # ... (constructor implementation) ...\n",
    "            pass\n",
    "        def __len__(self):\n",
    "            # ... (return the number of data) ...\n",
    "            pass\n",
    "        def __getitem__(self, idx):\n",
    "            # ... (return the sample corresponding to idx) ...\n",
    "            pass\n",
    "\n",
    "    # data augmentation\n",
    "    transform = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),  # random size and ratio cropping\n",
    "        transforms.RandomHorizontalFlip(),     # horizontal flip\n",
    "        transforms.ToTensor(),              # convert to tensor\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # normalization\n",
    "    ])\n",
    "\n",
    "    # dataset = CustomImageDataset(root_dir='path/to/images', transform=transform)\n",
    "    ```\n",
    "\n",
    "8.  **Higher-order functions:**\n",
    "\n",
    "    ```python\n",
    "    import torch\n",
    "\n",
    "    x = torch.tensor(2.0, requires_grad=True)\n",
    "    y = x**3\n",
    "\n",
    "    # first derivative\n",
    "    first_derivative = torch.autograd.grad(y, x, create_graph=True)[0]  # create_graph=True\n",
    "    print(first_derivative)\n",
    "\n",
    "    # second derivative (hessian)\n",
    "    second_derivative = torch.autograd.grad(first_derivative, x)[0]\n",
    "    print(second_derivative)\n",
    "    ```\n",
    "\n",
    "9. **`__call__` method:**\n",
    "\n",
    "The `__call__` method of `nn.Module` performs additional tasks (such as hook registration and automatic differentiation settings) before and after calling `forward()`. Simply calling `forward()` directly may omit these features, resulting in incorrect gradient calculations or malfunctioning of other model features (e.g., setting the `training` attribute of `nn.Module`). Therefore, you *must* call the model object like a function (`model(input)`).\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reference Materials**\n",
    "\n",
    "1.  **PyTorch Official Tutorial:** [https://pytorch.org/tutorials/](https://www.google.com/url?sa=E&source=gmail&q=https://pytorch.org/tutorials/)\n",
    "2.  **Deep Learning with PyTorch (Stevens, Antiga, Viehmann, 2020):** [https://pytorch.org/deep-learning-with-pytorch](https://www.google.com/search?q=https://pytorch.org/deep-learning-with-pytorch)\n",
    "3.  **Programming PyTorch for Deep Learning (Delugach, 2023):** [https://www.oreilly.com/library/view/programming-pytorch-for/9781098142481/](https://www.google.com/search?q=https://www.oreilly.com/library/view/programming-pytorch-for/9781098142481/)\n",
    "4.  **PyTorch Recipes (Kalyan, 2019):** [https://pytorch.org/tutorials/recipes/recipes_index.html](https://www.google.com/url?sa=E&source=gmail&q=https://pytorch.org/tutorials/recipes/recipes_index.html)\n",
    "5.  **Understanding the difficulty of training deep feedforward neural networks (Glorot & Bengio, 2010):** [http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)\n",
    "6.  **Fastai library:** [https://docs.fast.ai/](https://www.google.com/url?sa=E&source=gmail&q=https://docs.fast.ai/)\n",
    "7.  **PyTorch Lightning:** [https://www.pytorchlightning.ai/](https://www.google.com/url?sa=E&source=gmail&q=https://www.pytorchlightning.ai/)\n",
    "8.  **Hugging Face Transformers documentation:** [https://huggingface.co/docs/transformers/index](https://www.google.com/url?sa=E&source=gmail&q=https://huggingface.co/docs/transformers/index)\n",
    "9.  **TensorBoard documentation:** [https://www.tensorflow.org/tensorboard](https://www.google.com/url?sa=E&source=gmail&q=https://www.tensorflow.org/tensorboard)\n",
    "10. **Weights & Biases documentation:** [https://docs.wandb.ai/](https://www.google.com/url?sa=E&source=gmail&q=https://docs.wandb.ai/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
