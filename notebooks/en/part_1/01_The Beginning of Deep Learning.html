<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>the-beginning-of-deep-learning – Deep Learning DNA: Surviving Architectures and Essential Principles</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap-f507c7d0488cb7630e20aad62ad8c2aa.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script>window.MathJax = {loader: {load: ['[tex]/boldsymbol']},tex: {packages: {'[+]': ['boldsymbol']}}};</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../../notebooks/en/part_1/01_The Beginning of Deep Learning.html">part_1</a></li><li class="breadcrumb-item"><a href="../../../notebooks/en/part_1/01_The Beginning of Deep Learning.html">1. The Beginning of Deep Learning</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../../../">English</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Language</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index_de.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Deutsch</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index_en.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">English</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index_es.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Español</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">한국어</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index_zh.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">中文</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/en/part_1/00_Introduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">part_1</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/en/part_1/01_The Beginning of Deep Learning.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">1. The Beginning of Deep Learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/en/part_1/02_Mathematics of Deep Learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">2. Mathematics of Deep Learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/en/part_1/03_Deep Learning Framework.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">3. Deep Learning Framework</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/en/part_1/04_Activation Function.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">4. Activation Function</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/en/part_1/05_Optimization and Visualization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">5. Optimization and Visualization</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/en/part_1/06_Overfitting and Development of Solution Techniques.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">6. Overfitting and Development of Solution Techniques</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/en/part_1/07_Evolution of Convolutional Neural Networks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">7. Evolution of Convolutional Neural Networks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/en/part_1/08_The Birth of Transformer.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">8. The Birth of Transformer</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/en/part_1/09_The Evolution of Transformers.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">9. The Evolution of Transformers</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/en/part_1/10_Multimodal Deep Learning: The Beginning of Multisensory Convergence.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">10. Multimodal Deep Learning: The Beginning of Multisensory Convergence</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/en/part_1/11_Multimodal Deep Learning: Intelligence Beyond Limits.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">11. Multimodal Deep Learning: Intelligence Beyond Limits</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Deep Learning Frontier</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/en/Deep Learning Frontier/01_SLM: Small but Powerful Language Model.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">1. SLM: Small but Powerful Language Model</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/en/Deep Learning Frontier/02_Autonomous Driving.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">2. Autonomous Driving</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#the-beginning-of-deep-learning-fundamental-principles-and-the-context-of-technological-evolution" id="toc-the-beginning-of-deep-learning-fundamental-principles-and-the-context-of-technological-evolution" class="nav-link active" data-scroll-target="#the-beginning-of-deep-learning-fundamental-principles-and-the-context-of-technological-evolution">1. The Beginning of Deep Learning: Fundamental Principles and the Context of Technological Evolution</a>
  <ul class="collapse">
  <li><a href="#purpose-of-this-book" id="toc-purpose-of-this-book" class="nav-link" data-scroll-target="#purpose-of-this-book">1.1 Purpose of This Book</a></li>
  <li><a href="#history-of-deep-learning" id="toc-history-of-deep-learning" class="nav-link" data-scroll-target="#history-of-deep-learning">1.2 History of Deep Learning</a></li>
  <li><a href="#hebbian-learning" id="toc-hebbian-learning" class="nav-link" data-scroll-target="#hebbian-learning">1.3 Hebbian Learning</a>
  <ul class="collapse">
  <li><a href="#hebbian-learning-rule" id="toc-hebbian-learning-rule" class="nav-link" data-scroll-target="#hebbian-learning-rule">1.3.1 Hebbian Learning Rule</a></li>
  <li><a href="#association-with-brain-plasticity" id="toc-association-with-brain-plasticity" class="nav-link" data-scroll-target="#association-with-brain-plasticity">1.3.2 Association with Brain Plasticity</a></li>
  </ul></li>
  <li><a href="#neural-network-nn" id="toc-neural-network-nn" class="nav-link" data-scroll-target="#neural-network-nn">1.4 Neural Network (NN)</a>
  <ul class="collapse">
  <li><a href="#basic-structure-of-a-neural-network" id="toc-basic-structure-of-a-neural-network" class="nav-link" data-scroll-target="#basic-structure-of-a-neural-network">1.4.1 Basic Structure of a Neural Network</a></li>
  <li><a href="#linear-approximator-for-house-price-prediction" id="toc-linear-approximator-for-house-price-prediction" class="nav-link" data-scroll-target="#linear-approximator-for-house-price-prediction">1.4.2 Linear Approximator for House Price Prediction</a></li>
  <li><a href="#the-road-to-neural-networks-matrix-operations" id="toc-the-road-to-neural-networks-matrix-operations" class="nav-link" data-scroll-target="#the-road-to-neural-networks-matrix-operations">1.4.3 The Road to Neural Networks: Matrix Operations</a></li>
  <li><a href="#implementation-with-numpy" id="toc-implementation-with-numpy" class="nav-link" data-scroll-target="#implementation-with-numpy">1.3.4 Implementation with NumPy</a></li>
  </ul></li>
  <li><a href="#deep-neural-networks" id="toc-deep-neural-networks" class="nav-link" data-scroll-target="#deep-neural-networks">1.5 Deep Neural Networks</a>
  <ul class="collapse">
  <li><a href="#structure-of-deep-neural-networks" id="toc-structure-of-deep-neural-networks" class="nav-link" data-scroll-target="#structure-of-deep-neural-networks">1.5.1 Structure of Deep Neural Networks</a></li>
  </ul></li>
  <li><a href="#implementation-of-neural-networks" id="toc-implementation-of-neural-networks" class="nav-link" data-scroll-target="#implementation-of-neural-networks">1.5.2 Implementation of Neural Networks</a>
  <ul class="collapse">
  <li><a href="#neural-network-training" id="toc-neural-network-training" class="nav-link" data-scroll-target="#neural-network-training">1.5.3 Neural Network Training</a></li>
  </ul></li>
  <li><a href="#practice-problems" id="toc-practice-problems" class="nav-link" data-scroll-target="#practice-problems">Practice Problems</a>
  <ul class="collapse">
  <li><a href="#basic-problems" id="toc-basic-problems" class="nav-link" data-scroll-target="#basic-problems">1. Basic Problems</a></li>
  <li><a href="#application-problems" id="toc-application-problems" class="nav-link" data-scroll-target="#application-problems">2. Application Problems</a></li>
  <li><a href="#advanced-problems" id="toc-advanced-problems" class="nav-link" data-scroll-target="#advanced-problems">3. Advanced Problems</a></li>
  </ul></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../../notebooks/en/part_1/01_The Beginning of Deep Learning.html">part_1</a></li><li class="breadcrumb-item"><a href="../../../notebooks/en/part_1/01_The Beginning of Deep Learning.html">1. The Beginning of Deep Learning</a></li></ol></nav></header>




<p><a href="https://colab.research.google.com/github/Quantum-Intelligence-Frontier/dldna/blob/main/notebooks/en/part_1/01_Getting_Started_with_Deep_Learning.ipynb" target="_parent"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"> </a></p>
<section id="the-beginning-of-deep-learning-fundamental-principles-and-the-context-of-technological-evolution" class="level1">
<h1>1. The Beginning of Deep Learning: Fundamental Principles and the Context of Technological Evolution</h1>
<p><strong>The Start of Exploring Deep Learning DNA</strong></p>
<blockquote class="blockquote">
<p>“True innovation in technology comes from past failures” - Geoffrey Hinton, 2018 Turing Award Lecture</p>
</blockquote>
<section id="purpose-of-this-book" class="level2">
<h2 class="anchored" data-anchor-id="purpose-of-this-book">1.1 Purpose of This Book</h2>
<p>Deep learning, a field of machine learning, has been rapidly advancing with remarkable achievements. Large language models like GPT-4 and Gemini have emerged, and there are both expectations and concerns about general artificial intelligence (AGI). As research papers and technologies are developing quickly, even experts are having a hard time keeping up.</p>
<p>This situation is similar to the late 1980s when PCs and programming languages became popular. At that time, many technologies emerged, but only a few core technologies eventually became the foundation of modern computing. Similarly, among various deep learning architectures such as neural networks, CNN, RNN, transformer, diffusion, and multimodal, only a few that <strong>share essential DNA</strong> will remain as the basis of AI and continue to evolve.</p>
<p>This book starts from this perspective. Instead of simply explaining API usage, basic theory, or examples, it <strong>dissects the DNA of technological development</strong>. From the 1943 McCulloch-Pitts neuron model to the latest multimodal architecture in 2025, it focuses on the <strong>background</strong> of how each technology emerged, the <strong>fundamental problems</strong> they tried to solve, and the <strong>connection</strong> with previous technologies. In other words, it draws a map of the deep learning technology family tree. Section 1.2 provides a brief summary of the contents.</p>
<p>To achieve this, this book has the following characteristics:</p>
<ul>
<li><strong>Explanation from a DNA perspective:</strong> Instead of simply listing technologies, it explains why each technology emerged, what problems it solved, and how it is related to previous technologies, i.e., the <strong>phylogeny</strong> of technology.</li>
<li><strong>Concise yet in-depth explanation:</strong> It helps readers understand core concepts and principles clearly while omitting unnecessary details.</li>
<li><strong>Reflection of the latest technological trends:</strong> It includes the latest technologies up to 2025 (e.g., Retentive Networks, Mixture of Experts, Multimodal Models) and covers the forefront of deep learning development.</li>
<li><strong>A bridge between practice and research:</strong> It provides a balance of practical code examples and mathematical intuition to connect theory and reality.</li>
<li><strong>Advanced examples</strong>: It offers not only working code but also fully developed examples that can be directly applied to actual research or development.</li>
</ul>
<p>Through this, we hope to help practitioners and researchers enhance their expertise. We also aim to address the ethical and social implications of AI technology and technological democratization.</p>
</section>
<section id="history-of-deep-learning" class="level2">
<h2 class="anchored" data-anchor-id="history-of-deep-learning">1.2 History of Deep Learning</h2>
<blockquote class="blockquote">
<p><strong>Challenge:</strong> How can machines be made to think and learn like humans?</p>
<p><strong>Researcher’s Dilemma:</strong> Mimicking the complex workings of the human brain was an extremely difficult task. Early researchers relied on simple rule-based systems or limited knowledge databases, but these had limitations in handling the diversity and complexity of the real world. To create truly intelligent systems, the ability to learn from data, recognize complex patterns, and understand abstract concepts was necessary. The question was how to implement this.</p>
</blockquote>
<p>The history of deep learning began in 1943 when Warren McCulloch and Walter Pitts described the neural action with the <strong>McCulloch-Pitts neuron</strong>, a mathematical model that defined the basic building block of neural networks. In 1949, Donald Hebb introduced the <strong>Hebbian Learning</strong> rule, explaining the principle of learning by adjusting synaptic weights. In 1958, Frank Rosenblatt’s Perceptron was the first practical neural network but faced limitations in nonlinear classification, such as the XOR problem.</p>
<p>The 1980s marked a significant breakthrough. In 1980, Kunihiko Fukushima proposed the <strong>Neocognitron (based on the Convolution Principle)</strong>, which later became the core idea of CNNs. The most important advancement was the development of the <strong>backpropagation algorithm</strong> by Geoffrey Hinton’s team in 1986, enabling efficient learning in multilayer neural networks and becoming a cornerstone of neural network learning. In 2006, Hinton proposed the term “deep learning,” marking a new era.</p>
<p>Subsequently, with the advancement of large-scale data and computing power, deep learning experienced rapid growth. In 2012, AlexNet won the ImageNet competition with overwhelming performance, proving the practicality of deep learning. This was followed by innovative architectures using <strong>Recurrent Networks</strong>, such as <strong>LSTM (1997)</strong> and <strong>Attention Mechanism (2014)</strong>. Notably, in 2017, Google’s <strong>Transformer</strong> revolutionized the paradigm of natural language processing by directly connecting different parts of input sequences through self-attention, solving long-distance dependency problems.</p>
<p>Based on the Transformer, BERT and GPT series emerged, leading to a significant leap in language model performance. <strong>Word2Vec in 2013</strong> opened new horizons for word embeddings. In the field of <strong>Generative Models</strong>, after the emergence of <strong>GAN in 2014</strong>, <strong>Diffusion Models in 2020</strong> enabled high-quality image generation. In 2021, the <strong>Vision Transformer (ViT)</strong> was introduced, successfully applying transformers to image processing and accelerating the development of <strong>Multimodal Learning</strong>.</p>
<p>Recently, large language models like GPT-4 and Gemini have heightened expectations for achieving AGI. These models are becoming more sophisticated by utilizing advanced architectures such as <strong>Retentive Networks in 2023</strong>, efficiency technologies like <strong>FlashAttention after 2023</strong>, and techniques such as <strong>Mixture of Experts (MoE) in 2024</strong>. Moreover, they are evolving into <strong>Multimodal</strong> models (e.g., Gemini Ultra 2.0 in 2024 and Gemini 2.0 in 2025) that integrate various forms of data like text, images, and audio, demonstrating high-dimensional cognitive abilities such as inference, creation, and problem-solving beyond simple question-answering.</p>
<p>The advancement of deep learning is based on the following key elements: 1. Increased availability of large-scale data 2. Advancements in high-performance computing resources such as GPUs 3. Development of efficient learning algorithms and <strong>Core Architecture</strong>, <strong>Generative Models</strong> such as <strong>Backpropagation, Attention, Transformer</strong></p>
<p>These advancements continue, but there are still challenges to be addressed. Model interpretability, data efficiency, energy consumption, and the development of <strong>Efficiency &amp; Advanced Concepts</strong> are important tasks.</p>
<p>The following is a visualization of the technical DNA lineage.</p>
<div id="cell-3" class="cell" data-execution_count="5">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 2025 Deep Learning Technology DNA Tree (Multimodal Updated)</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> anytree <span class="im">import</span> Node, RenderTree</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co"># ==== Core Mathematical Foundations &amp; Algorithms ====</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>root <span class="op">=</span> Node(<span class="st">"1943: McCulloch-Pitts Neuron"</span>)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>math_lineage <span class="op">=</span> Node(<span class="st">"Mathematical Foundations &amp; Algorithms"</span>, parent<span class="op">=</span>root)</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>hebbian <span class="op">=</span> Node(<span class="st">"1949: Hebbian Learning"</span>, parent<span class="op">=</span>math_lineage)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>backprop <span class="op">=</span> Node(<span class="st">"1986: Backpropagation (Rumelhart, Hinton, Williams)"</span>, parent<span class="op">=</span>math_lineage)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>neuroplasticity <span class="op">=</span> Node(<span class="st">"1958: Cortical Plasticity Theory (Mountcastle)"</span>, parent<span class="op">=</span>math_lineage)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>sparsity <span class="op">=</span> Node(<span class="st">"2023: Sparse Symbolic Representations (DeepMind)"</span>, parent<span class="op">=</span>backprop)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>liquid_clocks <span class="op">=</span> Node(<span class="st">"2024: Liquid Time-constant Networks"</span>, parent<span class="op">=</span>sparsity)</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>dynamic_manifolds <span class="op">=</span> Node(<span class="st">"2025: Dynamic Neural Manifolds"</span>, parent<span class="op">=</span>liquid_clocks)</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="co"># ==== Core Architecture ====</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>core_arch <span class="op">=</span> Node(<span class="st">"Core Architecture"</span>, parent<span class="op">=</span>root)</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>conv_principle <span class="op">=</span> Node(<span class="st">"1980: Convolution Principle (Neocognitron - Fukushima)"</span>, parent<span class="op">=</span>core_arch)</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>attention <span class="op">=</span> Node(<span class="st">"2014: Attention Mechanism (Bahdanau)"</span>, parent<span class="op">=</span>core_arch)</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>transformer <span class="op">=</span> Node(<span class="st">"2017: Transformer (Vaswani)"</span>, parent<span class="op">=</span>attention)</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>retentive_net <span class="op">=</span> Node(<span class="st">"2023: Retentive Networks (Microsoft)"</span>, parent<span class="op">=</span>transformer)</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>hybrid_ssm <span class="op">=</span> Node(<span class="st">"2024: Hybrid State-Space Models"</span>, parent<span class="op">=</span>retentive_net)</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a><span class="co"># ==== Computer Vision ====</span></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>cv_lineage <span class="op">=</span> Node(<span class="st">"Computer Vision"</span>, parent<span class="op">=</span>root)</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>lenet <span class="op">=</span> Node(<span class="st">"1998: LeNet-5 (LeCun)"</span>, parent<span class="op">=</span>cv_lineage)</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>alexnet <span class="op">=</span> Node(<span class="st">"2012: AlexNet (Krizhevsky)"</span>, parent<span class="op">=</span>lenet)</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>resnet <span class="op">=</span> Node(<span class="st">"2015: ResNet (He)"</span>, parent<span class="op">=</span>alexnet)</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>vision_transformer <span class="op">=</span> Node(<span class="st">"2021: ViT (Vision Transformer) (Dosovitskiy)"</span>, parent<span class="op">=</span>resnet)</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>vit22b <span class="op">=</span> Node(<span class="st">"2023: ViT-22B (Google)"</span>, parent<span class="op">=</span>vision_transformer)</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>masked_autoenc <span class="op">=</span> Node(<span class="st">"2024: MAE v3 (Meta)"</span>, parent<span class="op">=</span>vit22b)</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>vit40b <span class="op">=</span> Node(<span class="st">"2025: ViT-40B (Google/Sydney)"</span>, parent<span class="op">=</span>masked_autoenc)</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>efficient_vit <span class="op">=</span> Node(<span class="st">"2025: EfficientViT-XXL"</span>, parent<span class="op">=</span>vit40b)</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a><span class="co"># ==== NLP ====</span></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>nlp_lineage <span class="op">=</span> Node(<span class="st">"NLP"</span>, parent<span class="op">=</span>root)</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>word2vec <span class="op">=</span> Node(<span class="st">"2013: Word2Vec (Mikolov)"</span>, parent<span class="op">=</span>nlp_lineage)</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>bert <span class="op">=</span> Node(<span class="st">"2018: BERT (Devlin)"</span>, parent<span class="op">=</span>word2vec)</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>gpt3 <span class="op">=</span> Node(<span class="st">"2020: GPT-3 (OpenAI)"</span>, parent<span class="op">=</span>bert)</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>gpt5 <span class="op">=</span> Node(<span class="st">"2023: GPT-5 (OpenAI)"</span>, parent<span class="op">=</span>gpt3)</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>gpt55_turbo <span class="op">=</span> Node(<span class="st">"2024: GPT-5.5 Turbo"</span>, parent<span class="op">=</span>gpt5)</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>gpt6 <span class="op">=</span> Node(<span class="st">"2025: GPT-6 (Multimodal Agent)"</span>, parent<span class="op">=</span>gpt55_turbo)</span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a><span class="co"># ==== Multimodal Learning ====</span></span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a>mm_lineage <span class="op">=</span> Node(<span class="st">"Multimodal Learning"</span>, parent<span class="op">=</span>root)</span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>clip <span class="op">=</span> Node(<span class="st">"2021: CLIP (OpenAI)"</span>, parent<span class="op">=</span>mm_lineage)</span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>flamingo <span class="op">=</span> Node(<span class="st">"2022: Flamingo (DeepMind)"</span>, parent<span class="op">=</span>clip)</span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a>kosmos <span class="op">=</span> Node(<span class="st">"2023: Kosmos-2.5 (Microsoft)"</span>, parent<span class="op">=</span>flamingo)</span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a>gemini <span class="op">=</span> Node(<span class="st">"2024: Gemini Ultra 2.0 (Google)"</span>, parent<span class="op">=</span>kosmos)</span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a>gemini_multiverse <span class="op">=</span> Node(<span class="st">"2025: Gemini Multiverse (Google)"</span>, parent<span class="op">=</span>gemini)</span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>project_starline <span class="op">=</span> Node(<span class="st">"2025: Project Starline 2.0 (3D Multimodal)"</span>, parent<span class="op">=</span>gemini_multiverse)</span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a><span class="co"># ==== Generative Models ====</span></span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a>gen_lineage <span class="op">=</span> Node(<span class="st">"Generative Models"</span>, parent<span class="op">=</span>root)</span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a>vae <span class="op">=</span> Node(<span class="st">"2013: VAE (Kingma)"</span>, parent<span class="op">=</span>gen_lineage)</span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a>gan <span class="op">=</span> Node(<span class="st">"2014: GAN (Goodfellow)"</span>, parent<span class="op">=</span>vae)</span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a>stylegan <span class="op">=</span> Node(<span class="st">"2018: StyleGAN (Karras)"</span>, parent<span class="op">=</span>gan)</span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a>diffusion <span class="op">=</span> Node(<span class="st">"2020: Diffusion Models (Ho)"</span>, parent<span class="op">=</span>stylegan)</span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a>sdxl_turbo <span class="op">=</span> Node(<span class="st">"2023: SDXL-Turbo (Stability AI)"</span>, parent<span class="op">=</span>diffusion)</span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a>meta3d_diff <span class="op">=</span> Node(<span class="st">"2024: Meta 3D Diffusion"</span>, parent<span class="op">=</span>sdxl_turbo)</span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a>holo_gen <span class="op">=</span> Node(<span class="st">"2025: HoloGen (Neural Holography)"</span>, parent<span class="op">=</span>meta3d_diff)</span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a><span class="co"># ==== Reinforcement Learning ====</span></span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a>rl_lineage <span class="op">=</span> Node(<span class="st">"Reinforcement Learning"</span>, parent<span class="op">=</span>root)</span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a>tdlearn <span class="op">=</span> Node(<span class="st">"1988: TD Learning (Sutton)"</span>, parent<span class="op">=</span>rl_lineage)</span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a>dqn <span class="op">=</span> Node(<span class="st">"2013: DQN (DeepMind)"</span>, parent<span class="op">=</span>tdlearn)</span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a>alphago <span class="op">=</span> Node(<span class="st">"2016: AlphaGo (Silver)"</span>, parent<span class="op">=</span>dqn)</span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a>muzero <span class="op">=</span> Node(<span class="st">"2019: MuZero (DeepMind)"</span>, parent<span class="op">=</span>alphago)</span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a>robot_transformer <span class="op">=</span> Node(<span class="st">"2023: RT-2 (Google)"</span>, parent<span class="op">=</span>muzero)</span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a>agentic_cortex <span class="op">=</span> Node(<span class="st">"2024: Agentic Cortex (DeepMind)"</span>, parent<span class="op">=</span>robot_transformer)</span>
<span id="cb1-70"><a href="#cb1-70" aria-hidden="true" tabindex="-1"></a>autogpt5 <span class="op">=</span> Node(<span class="st">"2025: AutoGPT-5 (Fully Autonomous Agent)"</span>, parent<span class="op">=</span>agentic_cortex)</span>
<span id="cb1-71"><a href="#cb1-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-72"><a href="#cb1-72" aria-hidden="true" tabindex="-1"></a><span class="co"># ==== Efficiency &amp; Advanced Concepts ====</span></span>
<span id="cb1-73"><a href="#cb1-73" aria-hidden="true" tabindex="-1"></a>eff_lineage <span class="op">=</span> Node(<span class="st">"Efficiency &amp; Advanced Concepts"</span>, parent<span class="op">=</span>root)</span>
<span id="cb1-74"><a href="#cb1-74" aria-hidden="true" tabindex="-1"></a>flash_attn3 <span class="op">=</span> Node(<span class="st">"2023: FlashAttention-v3"</span>, parent<span class="op">=</span>eff_lineage)</span>
<span id="cb1-75"><a href="#cb1-75" aria-hidden="true" tabindex="-1"></a>moa <span class="op">=</span> Node(<span class="st">"2024: MoA (Mixture of Agents)"</span>, parent<span class="op">=</span>flash_attn3)</span>
<span id="cb1-76"><a href="#cb1-76" aria-hidden="true" tabindex="-1"></a>nas3 <span class="op">=</span> Node(<span class="st">"2025: Neural Architecture Search 3.0"</span>, parent<span class="op">=</span>moa)</span>
<span id="cb1-77"><a href="#cb1-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-78"><a href="#cb1-78" aria-hidden="true" tabindex="-1"></a><span class="co"># ==== Print Tree Structure ====</span></span>
<span id="cb1-79"><a href="#cb1-79" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"2025 Neural Network Evolution Tree:</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb1-80"><a href="#cb1-80" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> pre, _, node <span class="kw">in</span> RenderTree(root):</span>
<span id="cb1-81"><a href="#cb1-81" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>pre<span class="sc">}{</span>node<span class="sc">.</span>name<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>2025 Neural Network Evolution Tree:

1943: McCulloch-Pitts Neuron
├── Mathematical Foundations &amp; Algorithms
│   ├── 1949: Hebbian Learning
│   ├── 1986: Backpropagation (Rumelhart, Hinton, Williams)
│   │   └── 2023: Sparse Symbolic Representations (DeepMind)
│   │       └── 2024: Liquid Time-constant Networks
│   │           └── 2025: Dynamic Neural Manifolds
│   └── 1958: Cortical Plasticity Theory (Mountcastle)
├── Core Architecture
│   ├── 1980: Convolution Principle (Neocognitron - Fukushima)
│   └── 2014: Attention Mechanism (Bahdanau)
│       └── 2017: Transformer (Vaswani)
│           └── 2023: Retentive Networks (Microsoft)
│               └── 2024: Hybrid State-Space Models
├── Computer Vision
│   └── 1998: LeNet-5 (LeCun)
│       └── 2012: AlexNet (Krizhevsky)
│           └── 2015: ResNet (He)
│               └── 2021: ViT (Vision Transformer) (Dosovitskiy)
│                   └── 2023: ViT-22B (Google)
│                       └── 2024: MAE v3 (Meta)
│                           └── 2025: ViT-40B (Google/Sydney)
│                               └── 2025: EfficientViT-XXL
├── NLP
│   └── 2013: Word2Vec (Mikolov)
│       └── 2018: BERT (Devlin)
│           └── 2020: GPT-3 (OpenAI)
│               └── 2023: GPT-5 (OpenAI)
│                   └── 2024: GPT-5.5 Turbo
│                       └── 2025: GPT-6 (Multimodal Agent)
├── Multimodal Learning
│   └── 2021: CLIP (OpenAI)
│       └── 2022: Flamingo (DeepMind)
│           └── 2023: Kosmos-2.5 (Microsoft)
│               └── 2024: Gemini Ultra 2.0 (Google)
│                   └── 2025: Gemini Multiverse (Google)
│                       └── 2025: Project Starline 2.0 (3D Multimodal)
├── Generative Models
│   └── 2013: VAE (Kingma)
│       └── 2014: GAN (Goodfellow)
│           └── 2018: StyleGAN (Karras)
│               └── 2020: Diffusion Models (Ho)
│                   └── 2023: SDXL-Turbo (Stability AI)
│                       └── 2024: Meta 3D Diffusion
│                           └── 2025: HoloGen (Neural Holography)
├── Reinforcement Learning
│   └── 1988: TD Learning (Sutton)
│       └── 2013: DQN (DeepMind)
│           └── 2016: AlphaGo (Silver)
│               └── 2019: MuZero (DeepMind)
│                   └── 2023: RT-2 (Google)
│                       └── 2024: Agentic Cortex (DeepMind)
│                           └── 2025: AutoGPT-5 (Fully Autonomous Agent)
└── Efficiency &amp; Advanced Concepts
    └── 2023: FlashAttention-v3
        └── 2024: MoA (Mixture of Agents)
            └── 2025: Neural Architecture Search 3.0</code></pre>
</div>
</div>
</section>
<section id="hebbian-learning" class="level2">
<h2 class="anchored" data-anchor-id="hebbian-learning">1.3 Hebbian Learning</h2>
<p>After the artificial neural network model (McCulloch-Pitts Neuron) proposed by Warren McCulloch and Walter Pitts in 1943, Donald O. Hebb, a Canadian psychologist, presented the basic principle of neural network learning in his book “The Organization of Behavior” in 1949. This principle is called <strong>Hebb’s Rule</strong> or <strong>Hebbian Learning</strong>, which has had a significant impact on artificial neural network research, including deep learning.</p>
<section id="hebbian-learning-rule" class="level3">
<h3 class="anchored" data-anchor-id="hebbian-learning-rule">1.3.1 Hebbian Learning Rule</h3>
<p>The core idea of Hebbian learning is very simple. When two neurons are activated simultaneously or repeatedly, the connection strength between them increases. On the other hand, if two neurons are activated at different times or only one neuron is activated and the other is not, the connection strength weakens or disappears.</p>
<p>This can be expressed mathematically as follows:</p>
<p><span class="math display">\[
\Delta w_{ij} = \eta \cdot x_i \cdot y_j
\]</span></p>
<p>Here,</p>
<ul>
<li><span class="math inline">\(\Delta w_{ij}\)</span> is the change in the connection strength (weight) between neuron <span class="math inline">\(i\)</span> and neuron <span class="math inline">\(j\)</span>.</li>
<li><span class="math inline">\(\eta\)</span> is the learning rate, a constant that controls the size of the connection strength change.</li>
<li><span class="math inline">\(x_i\)</span> is the activation value (input) of neuron <span class="math inline">\(i\)</span>.</li>
<li><span class="math inline">\(y_j\)</span> is the activation value (output) of neuron <span class="math inline">\(j\)</span>.</li>
</ul>
<p>This formula shows that when neurons <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> are both activated (<span class="math inline">\(x_i\)</span> and <span class="math inline">\(y_j\)</span> are both positive), the connection strength increases (<span class="math inline">\(\Delta w_{ij}\)</span> is positive). On the other hand, if only one of them is activated or both are deactivated, the connection strength decreases or remains unchanged. Hebbian learning is one of the early forms of <strong>unsupervised learning</strong>, where the neural network adjusts its connection strengths through input data patterns without given answers (labels).</p>
</section>
<section id="association-with-brain-plasticity" class="level3">
<h3 class="anchored" data-anchor-id="association-with-brain-plasticity">1.3.2 Association with Brain Plasticity</h3>
<p>Hebbian learning provides important insights into the actual operation of the brain, beyond a simple mathematical rule. The brain changes constantly through experience and learning, and this change is called <strong>brain plasticity</strong> or <strong>neural plasticity</strong>. Hebbian learning plays a key role in explaining <strong>synaptic plasticity</strong>, a form of neural plasticity. Synapses are the connection sites between neurons and determine the efficiency of information transmission. Hebbian learning clearly shows the basic principle of synaptic plasticity, namely, <strong>“neurons that are activated together are connected together”</strong>. Long-Term Potentiation (LTP) and Long-Term Depression (LTD) are representative examples of synaptic plasticity. LTP is a phenomenon in which synaptic connections are strengthened according to Hebb’s learning rule, while LTD is the opposite phenomenon. LTP and LTD play important roles in learning, memory, and brain development processes.</p>
</section>
</section>
<section id="neural-network-nn" class="level2">
<h2 class="anchored" data-anchor-id="neural-network-nn">1.4 Neural Network (NN)</h2>
<p>A neural network is a function approximator that generates values as close as possible to the desired output from the input. This can be expressed mathematically as <span class="math inline">\(f_\theta\)</span>, where <span class="math inline">\(f\)</span> represents the function and <span class="math inline">\(\theta\)</span> represents the parameters consisting of weights and biases. The core of a neural network is that it can automatically learn these parameters through data.</p>
<p>The neural network, first proposed by Warren McCullough and Walter Pitts in 1944, was inspired by biological neurons but modern neural networks are purely mathematical models. In fact, neural networks are powerful mathematical tools that can approximate continuous functions, which has been proven by the Universal Approximation Theorem.</p>
<section id="basic-structure-of-a-neural-network" class="level3">
<h3 class="anchored">1.4.1 Basic Structure of a Neural Network</h3>
<p>A neural network consists of a hierarchical structure composed of an input layer, hidden layers, and an output layer. Each layer is made up of nodes (neurons) that are connected to each other and transmit information. Basically, a neural network is a combination of linear transformations and nonlinear activation functions.</p>
<p>Mathematically, each layer of the neural network performs the following linear transformation:</p>
<p><span class="math display">\[ y = Wx + b \]</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(x\)</span> is the input vector</li>
<li><span class="math inline">\(W\)</span> is the weight matrix</li>
<li><span class="math inline">\(b\)</span> is the bias vector</li>
<li><span class="math inline">\(y\)</span> is the output vector</li>
</ul>
<p>This structure may seem simple, but a neural network with sufficient neurons and layers can approximate any continuous function to any desired degree of accuracy. This is why neural networks can learn complex patterns and solve various problems.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Click to view contents (Deep Dive: Universal Approximation Theorem)">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Click to view contents (Deep Dive: Universal Approximation Theorem)
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<section id="universal-approximation-theorem" class="level2 callout-body-container callout-body">
<h2 class="anchored" data-anchor-id="universal-approximation-theorem">Universal Approximation Theorem</h2>
<blockquote class="blockquote">
<p><strong>Challenge:</strong> How can we prove that a neural network can approximate any complex function?</p>
<p><strong>Researcher’s Concern:</strong> Even with many layers and neurons, it was not obvious whether a neural network could truly express <em>all</em> continuous functions. There was concern that combinations of simple linear transformations alone might not be able to capture complex nonlinearities. Without theoretical guarantees, relying solely on empirical results was a significant hindrance to the development of neural networks.</p>
</blockquote>
<p><strong>Universal Approximation Theorem</strong></p>
<p>The Universal Approximation Theorem is a core theory that supports the powerful expressive capabilities of neural networks. This theorem proves that a <em>single-layer neural network with a sufficiently wide hidden layer</em> can approximate <strong>any continuous function</strong> to any desired level of accuracy.</p>
<p><strong>Key Ideas:</strong></p>
<ul>
<li><strong>Nonlinear Activation Functions:</strong> Nonlinear activation functions like ReLU, sigmoid, and tanh are the key elements that enable neural networks to express nonlinearities. Without these activation functions, no matter how many layers are stacked, the result would only be a combination of linear transformations.</li>
<li><strong>Sufficiently Wide Hidden Layer:</strong> If the number of neurons in the hidden layer is sufficiently large, the neural network gains the “flexibility” to express any complex function. It’s similar to being able to create any mosaic picture with enough pieces.</li>
</ul>
<p><strong>Mathematical Expression:</strong></p>
<p><strong>Theorem (Universal Approximation Theorem):</strong></p>
<p>Let <span class="math inline">\(f : K \rightarrow \mathbb{R}\)</span> be any continuous function defined on a compact set <span class="math inline">\(K \subset \mathbb{R}^d\)</span>. For any error bound <span class="math inline">\(\epsilon &gt; 0\)</span>, there <em>exists</em> a <em>single-layer neural network</em> <span class="math inline">\(F(x)\)</span> that satisfies the following condition:</p>
<p><span class="math inline">\(|f(x) - F(x)| &lt; \epsilon\)</span>, for all <span class="math inline">\(x \in K\)</span>.</p>
<p>Here, <span class="math inline">\(F(x)\)</span> takes the form of:</p>
<p><span class="math inline">\(F(x) = \sum_{i=1}^{N} w_i \cdot \sigma(v_i^T x + b_i)\)</span></p>
<p><strong>Detailed Explanation:</strong></p>
<ul>
<li><p><strong><span class="math inline">\(f : K \rightarrow \mathbb{R}\)</span>:</strong></p>
<ul>
<li><span class="math inline">\(f\)</span> is the target function to be approximated.</li>
<li><span class="math inline">\(K\)</span> is the domain of the function, a <em>compact set</em> in <span class="math inline">\(\mathbb{R}^d\)</span> (d-dimensional real space). A compact set intuitively means a “bounded and closed” set. For example, in one dimension, a closed interval [a, b] is a compact set. This condition does not impose significant restrictions in practical scenarios because most real input data have limited ranges.</li>
<li><span class="math inline">\(\mathbb{R}\)</span> is the set of real numbers. Thus, function <span class="math inline">\(f\)</span> maps each point (<span class="math inline">\(x\)</span>) in <span class="math inline">\(K\)</span> to a real value (<span class="math inline">\(f(x)\)</span>). (For multivariable functions or multiple outputs, additional explanations are provided below.)</li>
</ul></li>
<li><p><strong><span class="math inline">\(\epsilon &gt; 0\)</span>:</strong> An arbitrary positive number representing the <em>accuracy</em> of the approximation. The smaller <span class="math inline">\(\epsilon\)</span> is, the more accurate the approximation.</p></li>
<li><p><strong><span class="math inline">\(|f(x) - F(x)| &lt; \epsilon\)</span>:</strong> For all <span class="math inline">\(x \in K\)</span>, the difference between the actual function value <span class="math inline">\(f(x)\)</span> and the neural network’s output <span class="math inline">\(F(x)\)</span> is less than <span class="math inline">\(\epsilon\)</span>. This means the neural network can approximate function <span class="math inline">\(f\)</span> within an error margin of <span class="math inline">\(\epsilon\)</span>.</p></li>
<li><p><strong><span class="math inline">\(F(x) = \sum_{i=1}^{N} w_i \cdot \sigma(v_i^T x + b_i)\)</span>:</strong> Represents the structure of a single-layer neural network.</p></li>
<li><p><strong><span class="math inline">\(N\)</span>:</strong> The number of neurons (units) in the hidden layer. The Universal Approximation Theorem guarantees that there exists a <em>sufficiently large</em> <span class="math inline">\(N\)</span>, but does not specify how large it needs to be.</p></li>
<li><p><strong><span class="math inline">\(w_i \in \mathbb{R}\)</span>:</strong> The <em>output weight</em> between the <span class="math inline">\(i\)</span>th hidden layer neuron and the output layer neuron. It is a scalar value.</p></li>
<li><p><strong><span class="math inline">\(\sigma\)</span>:</strong> A <em>nonlinear activation function</em>. Various functions such as ReLU, sigmoid, tanh, and leaky ReLU can be used. For the Universal Approximation Theorem to hold, <span class="math inline">\(\sigma\)</span> must be <strong>non-polynomial</strong> and either <strong>bounded</strong> or <strong>piecewise continuous</strong>.</p></li>
<li><p><strong><span class="math inline">\(v_i \in \mathbb{R}^d\)</span>:</strong> The <em>input weight vector</em> of the <span class="math inline">\(i\)</span>th hidden layer neuron. It has the same dimension as the input <span class="math inline">\(x\)</span>.</p></li>
<li><p><strong><span class="math inline">\(v_i^T x\)</span>:</strong> The inner product (dot product) of vector <span class="math inline">\(v_i\)</span> and input vector <span class="math inline">\(x\)</span>.</p></li>
<li><p><strong><span class="math inline">\(b_i \in \mathbb{R}\)</span>:</strong> The <em>bias</em> of the <span class="math inline">\(i\)</span>th hidden layer neuron. It is a scalar value.</p></li>
</ul>
<p><strong>Additional Explanation (Multivariate Functions, Multiple Outputs):</strong></p>
<ul>
<li><strong>Multivariate functions:</strong> The Universal Approximation Theorem also holds when the input <span class="math inline">\(x\)</span> is a vector (<span class="math inline">\(x \in \mathbb{R}^d\)</span>, <span class="math inline">\(d &gt; 1\)</span>). The operation <span class="math inline">\(v_i^T x\)</span> (inner product) naturally handles multivariate inputs.</li>
<li><strong>Multiple outputs:</strong> If the function <span class="math inline">\(f\)</span> has multiple output values (<span class="math inline">\(f : K \rightarrow \mathbb{R}^m\)</span>, <span class="math inline">\(m &gt; 1\)</span>), separate output layer neurons and weights can be used for each output. Thus, <span class="math inline">\(F(x)\)</span> will have a vector-shaped output, and the approximation error for each output can be made less than <span class="math inline">\(\epsilon\)</span>.</li>
</ul>
<p><strong>Error Convergence Rate (Barron’s Theorem):</strong></p>
<p>According to Barron’s theorem, under certain conditions (regarding the activation function and the Fourier transform of the function being approximated), the error <span class="math inline">\(\epsilon\)</span> has the following relationship with the number of neurons <span class="math inline">\(N\)</span>:</p>
<p><span class="math inline">\(\epsilon(N) = O(N^{-1/2})\)</span></p>
<p>This means that as the number of neurons increases, the error decreases at a rate of <span class="math inline">\(N^{-1/2}\)</span>. In other words, doubling the number of neurons will roughly halve the error. This is the <em>general</em> convergence rate, and specific functions or activation functions may exhibit faster or slower convergence rates.</p>
<p><strong>Counterexamples and Limitations:</strong></p>
<ul>
<li><strong>Boundary approximation:</strong> Functions like <span class="math inline">\(e^{-1/x^2}\)</span>, which are infinitely differentiable at <span class="math inline">\(x=0\)</span> but change rapidly, can be difficult to approximate with neural networks near <span class="math inline">\(x=0\)</span>. This problem occurs because the Taylor series of such functions is zero, but the function itself is not.</li>
<li><strong>Exponential complexity of discrete functions:</strong> The number of neurons required to approximate an <span class="math inline">\(n\)</span>-variable Boolean function can be proportional to <span class="math inline">\(2^n / n\)</span> in the worst case. This means that as the number of input variables increases, the required number of neurons can increase <em>exponentially</em>. This shows that neural networks cannot efficiently approximate all functions.</li>
</ul>
<p><strong>Key Summary:</strong> The universal approximation theorem states that a feedforward neural network with a sufficiently large hidden layer can approximate any continuous function to an arbitrary accuracy over a bounded closed set, provided that the activation function is nonpolynomial. This means that the neural network has a very powerful representational power and provides a theoretical foundation for deep learning. Barron’s theorem provides insight into the rate of convergence of the error.</p>
<p><strong>Important points</strong></p>
<ul>
<li><strong>Existence proof:</strong> The universal approximation theorem proves <em>existence</em>, but does not provide a <em>learning algorithm</em>. It guarantees that such a neural network <em>exists</em>, but finding it in practice is another problem (solved by backpropagation and gradient descent).</li>
<li><strong>Single-layer vs.&nbsp;multilayer:</strong> In practice, <em>multilayer</em> neural networks are often more efficient and have better generalization performance than <em>single-layer</em> ones. The universal approximation theorem provides a theoretical foundation for deep learning, but the success of deep learning is due to a combination of factors such as multilayer structure, special architectures, and efficient learning algorithms. Single-layer neural networks can theoretically express everything, but are much harder to train in practice.</li>
<li><strong>Limitations:</strong> The universal approximation theorem is a powerful result, but it does not guarantee that every function can be <em>efficiently</em> approximated. As seen in counterexamples, certain functions may require an enormous number of neurons to approximate.</li>
</ul>
<p><strong>References:</strong></p>
<ol type="1">
<li><strong>Cybenko, G. (1989).</strong> Approximation by superpositions of a sigmoidal function. <em>Mathematics of Control, Signals, and Systems</em>, 2(4), 303-314. (Early universal approximation theorem for sigmoid activation function)</li>
<li><strong>Hornik, K., Stinchcombe, M., &amp; White, H. (1989).</strong> Multilayer feedforward networks are universal approximators. <em>Neural Networks</em>, 2(5), 359-366. (Universal approximation theorem for more general activation functions)</li>
<li><strong>Barron, A. R. (1993).</strong> Universal approximation bounds for superpositions of a sigmoidal function. <em>IEEE Transactions on Information Theory</em>, 39(3), 930-945. (Barron’s theorem on the rate of convergence of the error)</li>
<li><strong>Pinkus, A. (1999).</strong> Approximation theory of the MLP model in neural networks. <em>Acta Numerica</em>, 8, 143-195. (More in-depth review of the universal approximation theorem)</li>
<li><strong>Goodfellow, I., Bengio, Y., &amp; Courville, A. (2016).</strong> <em>Deep Learning</em>. MIT Press. (Chapter 6.4: Deep learning textbook with related content)</li>
</ol>
</section>
</div>
</div>
</section>
<section id="linear-approximator-for-house-price-prediction" class="level3">
<h3 class="anchored" data-anchor-id="linear-approximator-for-house-price-prediction">1.4.2 Linear Approximator for House Price Prediction</h3>
<p>To understand the basic concept of neural networks, let’s look at a simple linear regression problem. Here, we use the California housing price dataset from the <code>scikit-learn</code> library. This dataset includes several features of houses, and we can create a model that predicts house prices using these features. For simplicity, let’s assume that house prices are determined by one feature, the median income (<code>MedInc</code>), and implement a linear approximator.</p>
<div id="cell-8" class="cell" data-execution_count="1">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> fetch_california_housing</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LinearRegression</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the California housing dataset</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>housing <span class="op">=</span> fetch_california_housing(as_frame<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> housing.frame</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Use only Median Income (MedInc) and Median House Value (MedHouseVal)</span></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> data[[<span class="st">"MedInc"</span>, <span class="st">"MedHouseVal"</span>]]</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the first 5 rows of the data</span></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(data.head())</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Split the data into training and testing sets</span></span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>    data[[<span class="st">"MedInc"</span>]], data[<span class="st">"MedHouseVal"</span>], test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span></span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Create and train a linear regression model</span></span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LinearRegression()</span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>model.fit(X_train, y_train)</span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Make predictions on the test data</span></span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> model.predict(X_test)</span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a><span class="co"># Prepare data for visualization</span></span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a>plot_data <span class="op">=</span> pd.DataFrame({<span class="st">'MedInc'</span>: X_test[<span class="st">'MedInc'</span>], <span class="st">'MedHouseVal'</span>: y_test, <span class="st">'Predicted'</span>: y_pred})</span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a><span class="co"># Sort for better line plot visualization.  Crucially, sort *after* prediction.</span></span>
<span id="cb3-34"><a href="#cb3-34" aria-hidden="true" tabindex="-1"></a>plot_data <span class="op">=</span> plot_data.sort_values(by<span class="op">=</span><span class="st">'MedInc'</span>)</span>
<span id="cb3-35"><a href="#cb3-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-36"><a href="#cb3-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-37"><a href="#cb3-37" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize using Seaborn</span></span>
<span id="cb3-38"><a href="#cb3-38" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb3-39"><a href="#cb3-39" aria-hidden="true" tabindex="-1"></a>sns.scatterplot(x<span class="op">=</span><span class="st">'MedInc'</span>, y<span class="op">=</span><span class="st">'MedHouseVal'</span>, data<span class="op">=</span>plot_data, label<span class="op">=</span><span class="st">'Actual'</span>, alpha<span class="op">=</span><span class="fl">0.6</span>)</span>
<span id="cb3-40"><a href="#cb3-40" aria-hidden="true" tabindex="-1"></a>sns.lineplot(x<span class="op">=</span><span class="st">'MedInc'</span>, y<span class="op">=</span><span class="st">'Predicted'</span>, data<span class="op">=</span>plot_data, color<span class="op">=</span><span class="st">'red'</span>, label<span class="op">=</span><span class="st">'Predicted'</span>, linewidth<span class="op">=</span><span class="fl">2.5</span>)</span>
<span id="cb3-41"><a href="#cb3-41" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'California Housing Prices Prediction (Linear Regression)'</span>)</span>
<span id="cb3-42"><a href="#cb3-42" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Median Income (MedInc)'</span>)</span>
<span id="cb3-43"><a href="#cb3-43" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Median House Value (MedHouseVal)'</span>)</span>
<span id="cb3-44"><a href="#cb3-44" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb3-45"><a href="#cb3-45" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb3-46"><a href="#cb3-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-47"><a href="#cb3-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-48"><a href="#cb3-48" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the trained weight (coefficient) and bias (intercept)</span></span>
<span id="cb3-49"><a href="#cb3-49" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Weight (Coefficient):"</span>, model.coef_[<span class="dv">0</span>])</span>
<span id="cb3-50"><a href="#cb3-50" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Bias (Intercept):"</span>, model.intercept_)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>   MedInc  MedHouseVal
0  8.3252        4.526
1  8.3014        3.585
2  7.2574        3.521
3  5.6431        3.413
4  3.8462        3.422</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="01_The Beginning of Deep Learning_files/figure-html/cell-3-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Weight (Coefficient): 0.4193384939381271
Bias (Intercept): 0.4445972916907879</code></pre>
</div>
</div>
<p>The code first loads the California housing price dataset using the <code>fetch_california_housing</code> function. It retrieves the data in Pandas DataFrame format with <code>as_frame=True</code>, then selects only the features for house prices (<code>MedHouseVal</code>) and median income (<code>MedInc</code>). The data is split into training and test sets using the <code>train_test_split</code> function, and a linear regression model is created using the <code>LinearRegression</code> class. The model is trained on the training data using the <code>fit</code> method, and it makes predictions on the test data using the <code>predict</code> method. The actual values and predicted values are visualized using Seaborn. Finally, the weights and biases of the trained model are printed.</p>
<p>In this way, even a simple linear transformation can make some predictions possible. Neural networks add non-linear activation functions to this and stack multiple layers to approximate much more complex functions.</p>
</section>
<section id="the-road-to-neural-networks-matrix-operations" class="level3">
<h3 class="anchored" data-anchor-id="the-road-to-neural-networks-matrix-operations">1.4.3 The Road to Neural Networks: Matrix Operations</h3>
<p>The precursor to neural networks is linear approximators. Here, we will take a closer look at how the previous example arrives at its actual values. For an actual value <span class="math inline">\(\boldsymbol y\)</span>, the simplest linear equation is <span class="math inline">\(\boldsymbol y = \boldsymbol x \boldsymbol W + \boldsymbol b\)</span>.</p>
<p>Here, <span class="math inline">\(\boldsymbol W\)</span> represents the weight parameter and <span class="math inline">\(\boldsymbol b\)</span> represents the bias. Optimizing these two parameters through data is at the core of neural network learning. As we will see in section 1.4, neural networks introduce non-linearity by adding an activation function to linear transformations and optimize parameters through backpropagation. Here, we will look at the simple calculation process using only linear transformation and backpropagation.</p>
<p>Initially, the parameters are set to arbitrary values.</p>
<p><span class="math display">\[ \boldsymbol W =  \begin{bmatrix}
   0.1  \\
   0.1   \\
   \end{bmatrix} \]</span></p>
<p><span class="math display">\[ \boldsymbol b =  \begin{bmatrix}
   0  \\
   0   \\
   0   \\
   \end{bmatrix} \]</span></p>
<p>Using these values, the prediction is performed as follows:</p>
<p><span class="math display">\[ \hat{\boldsymbol y} =  \begin{bmatrix}
   1.5 &amp; 1  \\
   2.4 &amp; 2  \\
   3.5 &amp; 3   \\
   \end{bmatrix}
   \begin{bmatrix}
   0.1  \\
   0.1   \\
   \end{bmatrix} +
   \begin{bmatrix}
   0  \\
   0   \\
   0   \\
   \end{bmatrix} =
    \begin{bmatrix}
   0.25  \\
   0.44   \\
   0.65   \\
   \end{bmatrix}\]</span></p>
<p>Here, <span class="math inline">\(\hat{\boldsymbol y}\)</span> represents the predicted value. The difference (loss) between the actual and predicted values is as follows:</p>
<p><span class="math display">\[ L = \boldsymbol y - \hat {\boldsymbol y}  = \begin{bmatrix}
   2.1  \\
   4.2   \\
   5.9   \\
   \end{bmatrix} -
   \begin{bmatrix}
   0.25  \\
   0.44   \\
   0.65   \\
   \end{bmatrix} =
  \begin{bmatrix}
   1.85  \\
   3.76  \\
   5.25  \\
   \end{bmatrix} \]</span></p>
<p>Parameter optimization uses gradients. <strong>The gradient points in the direction of increasing error</strong>, so subtracting it from the current parameters reduces the error. Introducing a learning rate (<span class="math inline">\(\eta\)</span>) yields:</p>
<p><span class="math display">\[ \text{new parameters} = \text{current parameters} - \eta \times \text{gradients} \]</span></p>
<p>For example, when <span class="math inline">\(\eta=0.01\)</span>, the weight update is as follows:</p>
<p><span class="math display">\[ \begin{bmatrix}
   0.30116    \\
   0.26746667    \\
   \end{bmatrix} =
   \begin{bmatrix}
    0.1    \\
    0.1    \\
   \end{bmatrix} - 0.01 \times
    \begin{bmatrix}
    -20.116   \\
    -16.74666667   \\
   \end{bmatrix}\]</span></p>
<p>The bias is also updated in the same manner. Repeating these forward and backward calculations to optimize parameters is the learning process of neural networks.</p>
</section>
<section id="implementation-with-numpy" class="level3">
<h3 class="anchored">1.3.4 Implementation with NumPy</h3>
<p>Let’s look at implementing linear approximators using NumPy. First, prepare the input data and target values.</p>
<div id="cell-11" class="cell" data-execution_count="4">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Set input values and target values</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.array([[<span class="fl">1.5</span>, <span class="dv">1</span>], [<span class="fl">2.4</span>, <span class="dv">2</span>], [<span class="fl">3.5</span>, <span class="dv">3</span>]])</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.array([<span class="fl">2.1</span>, <span class="fl">4.2</span>, <span class="fl">5.9</span>])</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>learning_rate <span class="op">=</span> <span class="fl">0.01</span>  <span class="co">#  Adding the learning_rate variable here, even though it's unused, for consistency.</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"X ="</span>, X)</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"y ="</span>, y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>X = [[1.5 1. ]
 [2.4 2. ]
 [3.5 3. ]]
y = [2.1 4.2 5.9]</code></pre>
</div>
</div>
<p>The learning rate is set to 0.01. The learning rate is a hyperparameter that affects the model’s learning speed and stability. Weights and biases are initialized.</p>
<div id="cell-13" class="cell" data-execution_count="5">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>m, n <span class="op">=</span> X.shape</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize weights and bias</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>weights <span class="op">=</span> np.array([<span class="fl">0.1</span>, <span class="fl">0.1</span>])</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>bias <span class="op">=</span> <span class="fl">0.0</span>  <span class="co"># Corrected: Bias should be a single scalar value.</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"X.shape ="</span>, X.shape)</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Initial weights ="</span>, weights)</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Initial bias ="</span>, bias)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>X.shape = (3, 2)
Initial weights = [0.1 0.1]
Initial bias = 0.0</code></pre>
</div>
</div>
<p>Forward calculation performs a linear transformation, which can be expressed as follows. <span class="math display">\[ \boldsymbol y = \boldsymbol X \boldsymbol W + \boldsymbol b \]</span></p>
<div id="cell-15" class="cell" data-execution_count="6">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>y_predicted <span class="op">=</span> np.dot(X, weights) <span class="op">+</span> bias</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Predicted values ="</span>, y_predicted)</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>error <span class="op">=</span> y <span class="op">-</span> y_predicted</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Error ="</span>, error)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Predicted values = [0.25 0.44 0.65]
Error = [1.85 3.76 5.25]</code></pre>
</div>
</div>
<p>I calculated the loss. The next step is to calculate the gradient from the loss. How do I do it? The gradients of the weight and bias are as follows.</p>
<p><span class="math inline">\(\nabla_w = -\frac{2}{m}\mathbf{X}^T\mathbf{e}\)</span></p>
<p><span class="math inline">\(\nabla_b = -\frac{2}{m}\mathbf{e}\)</span></p>
<p>Here, <span class="math inline">\(\mathbf{e}\)</span> is the error vector. Once I have the gradient, I subtract the gradient value from the existing parameter to get the updated new value of the parameter.</p>
<div id="cell-17" class="cell" data-execution_count="8">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>weights_gradient <span class="op">=</span> <span class="op">-</span><span class="dv">2</span><span class="op">/</span>m <span class="op">*</span> np.dot(X.T, error)</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>bias_gradient <span class="op">=</span> <span class="op">-</span><span class="dv">2</span><span class="op">/</span>m <span class="op">*</span> error.<span class="bu">sum</span>()  <span class="co"># Corrected: Sum the errors for bias gradient</span></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>weights <span class="op">-=</span> learning_rate <span class="op">*</span> weights_gradient</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>bias <span class="op">-=</span> learning_rate <span class="op">*</span> bias_gradient</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Updated weights ="</span>, weights)</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Updated bias ="</span>, bias)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Updated weights = [0.50232    0.43493333]
Updated bias = 0.14479999999999998</code></pre>
</div>
</div>
<p>The above steps are backward calculations. It is also called backpropagation because the gradient is sequentially passed backwards. Now, let’s implement the entire training process as a function.</p>
<div id="cell-19" class="cell" data-execution_count="12">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train(X: np.ndarray, y: np.ndarray, lr: <span class="bu">float</span>, iters: <span class="bu">int</span> <span class="op">=</span> <span class="dv">100</span>, verbose: <span class="bu">bool</span> <span class="op">=</span> <span class="va">False</span>) <span class="op">-&gt;</span> <span class="bu">tuple</span>:</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Linear regression training function.</span></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a><span class="co">        X: Input data, shape (m, n)</span></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a><span class="co">        y: Target values, shape (m,)</span></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a><span class="co">        lr: Learning rate</span></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a><span class="co">        iters: Number of iterations</span></span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a><span class="co">        verbose: Whether to print intermediate steps</span></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a><span class="co">        Tuple: Trained weights and bias</span></span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>    m, n <span class="op">=</span> X.shape</span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>    weights <span class="op">=</span> np.array([<span class="fl">0.1</span>, <span class="fl">0.1</span>])</span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>    bias <span class="op">=</span> <span class="fl">0.0</span>  <span class="co"># Corrected: Bias should be a scalar</span></span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(iters):</span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Forward pass</span></span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a>        y_predicted <span class="op">=</span> np.dot(X, weights) <span class="op">+</span> bias</span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a>        error <span class="op">=</span> y <span class="op">-</span> y_predicted</span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Backward pass</span></span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a>        weights_gradient <span class="op">=</span> <span class="op">-</span><span class="dv">2</span><span class="op">/</span>m <span class="op">*</span> np.dot(X.T, error)</span>
<span id="cb14-25"><a href="#cb14-25" aria-hidden="true" tabindex="-1"></a>        bias_gradient <span class="op">=</span> <span class="op">-</span><span class="dv">2</span><span class="op">/</span>m <span class="op">*</span> error </span>
<span id="cb14-26"><a href="#cb14-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-27"><a href="#cb14-27" aria-hidden="true" tabindex="-1"></a>        weights <span class="op">-=</span> lr <span class="op">*</span> weights_gradient</span>
<span id="cb14-28"><a href="#cb14-28" aria-hidden="true" tabindex="-1"></a>        bias <span class="op">-=</span> lr <span class="op">*</span> bias_gradient</span>
<span id="cb14-29"><a href="#cb14-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-30"><a href="#cb14-30" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> verbose:</span>
<span id="cb14-31"><a href="#cb14-31" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"Iteration </span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">:"</span>)</span>
<span id="cb14-32"><a href="#cb14-32" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">"Weights gradient ="</span>, weights_gradient)</span>
<span id="cb14-33"><a href="#cb14-33" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">"Bias gradient ="</span>, bias_gradient)</span>
<span id="cb14-34"><a href="#cb14-34" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">"Updated weights ="</span>, weights)</span>
<span id="cb14-35"><a href="#cb14-35" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">"Updated bias ="</span>, bias)</span>
<span id="cb14-36"><a href="#cb14-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-37"><a href="#cb14-37" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> weights, bias</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>We test the trained model.</p>
<div id="cell-21" class="cell" data-execution_count="13">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Train the model</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>weights, bias <span class="op">=</span> train(X, y, learning_rate, iters<span class="op">=</span><span class="dv">2000</span>)</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Trained weights:"</span>, weights)</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Trained bias:"</span>, bias)</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Test predictions</span></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>test_X <span class="op">=</span> np.array([[<span class="fl">1.5</span>, <span class="dv">1</span>], [<span class="fl">2.4</span>, <span class="dv">2</span>], [<span class="fl">3.5</span>, <span class="dv">3</span>]])</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>test_y <span class="op">=</span> np.dot(test_X, weights) <span class="op">+</span> bias</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Predictions:"</span>, test_y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Trained weights: [0.93453357 0.83998906]
Trained bias: [-0.14178921  0.27714103  0.10916541]
Predictions: [2.10000021 4.19999973 5.9000001 ]</code></pre>
</div>
</div>
<p>It can be seen that it is almost close to the actual value. What if the number of iterations is small?</p>
<div id="cell-23" class="cell" data-execution_count="14">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Train the model</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>weights, bias <span class="op">=</span> train(X, y, learning_rate, iters<span class="op">=</span><span class="dv">50</span>)</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Trained weights:"</span>, weights)</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Trained bias:"</span>, bias)</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Test predictions</span></span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>test_X <span class="op">=</span> np.array([[<span class="fl">1.5</span>, <span class="dv">1</span>], [<span class="fl">2.4</span>, <span class="dv">2</span>], [<span class="fl">3.5</span>, <span class="dv">3</span>]])</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>test_y <span class="op">=</span> np.dot(test_X, weights) <span class="op">+</span> bias</span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Predictions:"</span>, test_y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Trained weights: [0.95069505 0.82053576]
Trained bias: [-1.23073214e-04  1.51665327e-01  1.39109392e-01]
Predictions: [2.24645526 4.07440496 5.92814933]</code></pre>
</div>
</div>
<p>In the case of 50 iterations, it can be seen that there is quite an error between the predicted value and the actual value. One more thing to look at is the learning rate. Why is a very small value multiplied by the gradient? Let’s repeat it once and output the calculated parameter value.</p>
<div id="cell-25" class="cell" data-execution_count="15">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>num_iters <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>weights, bias <span class="op">=</span> train(X, y, learning_rate, iters<span class="op">=</span>num_iters, verbose<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Iteration 1:
Weights gradient = [-20.116      -16.74666667]
Bias gradient = [-1.23333333 -2.50666667 -3.5       ]
Updated weights = [0.30116    0.26746667]
Updated bias = [0.01233333 0.02506667 0.035     ]</code></pre>
</div>
</div>
<p>By comparing the trained weight and bias values obtained through 1,000 iterations of training, we can see that the gradient value is very large. If the learning rate does not greatly reduce the gradient value, the parameters will not be able to reduce the error and will continue to oscillate. Please try testing with a large learning rate value.</p>
<p>What’s different about this ‘linear approximator’ compared to a neural network approximator? The difference is one thing: after linear calculation, it passes through an activation function. This can be expressed in the following formula.</p>
<p><span class="math display">\[ \boldsymbol y = f_{active} ( \boldsymbol x \boldsymbol W + \boldsymbol b ) \]</span></p>
<p>The code is also simple. There are several types of activation functions, and if you use the tanh function, it becomes as follows.</p>
<div id="cell-27" class="cell" data-execution_count="16">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>y_predicted <span class="op">=</span> np.tanh(np.dot(X, weights) <span class="op">+</span> bias)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Neural networks typically represent each step of applying linear transformations and activation functions as the concept of a layer. Therefore, implementing it in two steps as follows is more suitable for layer representation and is preferred.</p>
<div id="cell-29" class="cell" data-execution_count="17">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>out_1 <span class="op">=</span> np.dot(X, weights) <span class="op">+</span> bias  <span class="co"># First layer</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>y_predicted <span class="op">=</span> np.tanh(out_1)       <span class="co"># Second layer (activation)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="callout callout-style-default callout-note callout-titled" title="Click to view contents (Deep Dive: Cerebral Cortex Plasticity Theory)">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Click to view contents (Deep Dive: Cerebral Cortex Plasticity Theory)
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<section id="cortical-plasticity-theory" class="level2 callout-body-container callout-body">
<h2 class="anchored" data-anchor-id="cortical-plasticity-theory">Cortical Plasticity Theory</h2>
<section id="mountcastles-cortical-plasticity-theory" class="level3">
<h3 class="anchored" data-anchor-id="mountcastles-cortical-plasticity-theory">Mountcastle’s Cortical Plasticity Theory</h3>
<p>Vernon Mountcastle was a scientist who made significant contributions to the field of neuroscience in the late 20th century, particularly in the study of the functional organization of the cerebral cortex. One of Mountcastle’s major achievements was the discovery of <strong>columnar organization</strong>. He found that the cerebral cortex is organized in vertical columns, and neurons within the same column respond to similar stimuli.</p>
<p>Mountcastle’s theory provides an important foundation for understanding the plasticity of the cerebral cortex. According to his theory:</p>
<ul>
<li><strong>Columns as functional units:</strong> The cerebral cortex is composed of columns as basic functional units. Each column contains a group of neurons that respond to specific sensory modalities or movement patterns.</li>
<li><strong>Plasticity of columns:</strong> The structure and function of columns can change based on experience. Repeated exposure to specific stimuli can increase the size of the column processing those stimuli or enhance its responsiveness. Conversely, lack of stimulation can decrease the size of the column or weaken its responsiveness.</li>
<li><strong>Competitive interactions:</strong> Adjacent columns interact competitively. An increase in activity in one column can suppress activity in another column, which serves as a basis for experience-dependent cortical reorganization. For example, frequent use of a specific finger can expand the cortical area responsible for that finger, while relatively shrinking the areas responsible for other fingers.</li>
</ul>
<p>Mountcastle’s columnar organization and plasticity theory have the following clinical implications:</p>
<ul>
<li><strong>Recovery after brain damage:</strong> Functional recovery after stroke or traumatic brain injury can occur through reorganization of the surrounding cortex.</li>
<li><strong>Sensory loss and rehabilitation:</strong> After visual or auditory loss, the cortical areas responsible for those senses can be used for other senses (cross-modal plasticity).</li>
<li><strong>Learning and skill acquisition:</strong> Learning new skills or improving specific functions through repetitive training can induce changes in the corresponding cortical columns.</li>
</ul>
</section>
<section id="connection-to-deep-learning" class="level3">
<h3 class="anchored" data-anchor-id="connection-to-deep-learning">Connection to Deep Learning</h3>
<p>Mountcastle’s cortical plasticity theory has inspired deep learning, particularly the structure and learning principles of artificial neural networks (ANNs).</p>
<ul>
<li><strong>Hierarchical structure:</strong> The columnar organization of the cerebral cortex is similar to the hierarchical structure of deep learning models. Deep learning models consist of multiple layers, each extracting increasingly abstract features from input data, similar to how cortical columns process sensory information in a step-by-step manner to perform complex cognitive functions.</li>
<li><strong>Weight adjustment:</strong> Deep learning models adjust connection strengths (weights) during the learning process to learn the relationship between input data and output. This is similar to the mechanism of changing neuronal connection strengths within columns proposed by Mountcastle. As experience strengthens or weakens neuronal responsiveness to specific stimuli, deep learning models also adjust weights based on training data to improve performance.</li>
<li><strong>Competitive learning:</strong> Some deep learning models, such as self-organizing maps (SOMs), use principles similar to the competitive interactions between columns proposed by Mountcastle. SOMs learn based on competition among neurons for input data features, where only the winning neuron is activated and updates the weights of its surrounding neurons. This is similar to how adjacent columns in the cortex compete and divide functions. Mountcastle’s theory of cortical plasticity in the brain not only broadened our understanding of the brain’s functional organization and learning mechanisms but also provided important insights into the development of deep learning models. Deep learning models that mimic the brain’s operation have greatly contributed to advances in the field of artificial intelligence, and it is expected that the interaction between neuroscience and artificial intelligence will become even more active in the future.</li>
</ul>
</section>
</section>
</div>
</div>
</section>
</section>
<section id="deep-neural-networks" class="level2">
<h2 class="anchored" data-anchor-id="deep-neural-networks">1.5 Deep Neural Networks</h2>
<p>Deep learning is a method of learning by stacking multiple layers of neural networks. The term ‘deep’ was used because the layers are deep. The basic component, the linear transformation layer, is also called the fully connected layer or dense layer. These layers are connected in the following structure:</p>
<p>Fully Connected Layer 1 - Activation Layer 1 - Fully Connected Layer 2 - Activation Layer 2 - …</p>
<p>The activation layer plays a crucial role in the neural network. If only linear layers are stacked continuously, it becomes mathematically equivalent to a single linear transformation. For example, connecting two linear layers can be expressed as:</p>
<p><span class="math display">\[ \boldsymbol y = (\boldsymbol X \boldsymbol W_1 + \boldsymbol b_1)\boldsymbol W_2 + \boldsymbol b_2 = \boldsymbol X(\boldsymbol W_1\boldsymbol W_2) + (\boldsymbol b_1\boldsymbol W_2 + \boldsymbol b_2) = \boldsymbol X\boldsymbol W + \boldsymbol b \]</span></p>
<p>This ultimately becomes another linear transformation, and the advantage of stacking multiple layers disappears. The activation layer breaks this linearity, allowing each layer to learn independently. Deep learning is powerful because it can learn more complex patterns as the number of layers increases.</p>
<section id="structure-of-deep-neural-networks" class="level3">
<h3 class="anchored" data-anchor-id="structure-of-deep-neural-networks">1.5.1 Structure of Deep Neural Networks</h3>
<p><img src="../../../assets/images/01_dnn.png" alt="image info" style="width: 800px;"></p>
<p>The output of each layer becomes the input to the next layer, and calculations are performed sequentially. Forward propagation is a sequence of relatively simple operations.</p>
<p>In backpropagation, two types of gradients are calculated for each layer:</p>
<ol type="1">
<li><p>Gradient with respect to weights: <span class="math inline">\(\frac{\partial E}{\partial \boldsymbol W}\)</span> - used for parameter updates</p></li>
<li><p>Gradient with respect to input: <span class="math inline">\(\frac{\partial E}{\partial \boldsymbol x}\)</span> - propagated to the previous layer</p></li>
</ol>
<p>These two gradients must be stored and managed independently. The weight gradient is used by the optimizer to update parameters, and the input gradient is used in the backpropagation process for learning in the previous layer.</p>
</section>
</section>
<section id="implementation-of-neural-networks" class="level2">
<h2 class="anchored" data-anchor-id="implementation-of-neural-networks">1.5.2 Implementation of Neural Networks</h2>
<p>To implement the basic structure of neural networks, a layer-based design is applied. First, a base class that all layers will inherit from is defined.</p>
<div id="cell-32" class="cell" data-execution_count="18">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> BaseLayer():</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># __init__ can be omitted as it implicitly inherits from 'object' in Python 3</span></span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">raise</span> <span class="pp">NotImplementedError</span>  <span class="co"># Should be implemented in derived classes</span></span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> backward(<span class="va">self</span>, output_error, lr):</span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">raise</span> <span class="pp">NotImplementedError</span>  <span class="co"># Should be implemented in derived classes</span></span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> print_params(<span class="va">self</span>):</span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Default implementation (optional).  Child classes should override.</span></span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"Layer parameters (Not implemented in BaseLayer)"</span>)</span>
<span id="cb23-15"><a href="#cb23-15" aria-hidden="true" tabindex="-1"></a>        <span class="co"># raise NotImplementedError # Or keep NotImplementedError</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>BaseLayer defines the interface for forward and backward operations. Each layer implements this interface to perform its unique operation. The following is an implementation of a fully connected layer.</p>
<div id="cell-34" class="cell" data-execution_count="19">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> FCLayer(BaseLayer):</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, in_size, out_size):</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>        <span class="co"># super().__init__()  # No need to call super() for object inheritance</span></span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.in_size <span class="op">=</span> in_size</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.out_size <span class="op">=</span> out_size</span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a>        <span class="co"># He initialization (weights)</span></span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.weights <span class="op">=</span> np.random.randn(in_size, out_size) <span class="op">*</span> np.sqrt(<span class="fl">2.0</span> <span class="op">/</span> in_size)</span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Bias initialization (zeros)</span></span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bias <span class="op">=</span> np.zeros(out_size)  <span class="co"># or np.zeros((out_size,))</span></span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-13"><a href="#cb24-13" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb24-14"><a href="#cb24-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.in_x <span class="op">=</span> x  <span class="co"># Store input for use in backward pass</span></span>
<span id="cb24-15"><a href="#cb24-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> np.dot(x, <span class="va">self</span>.weights) <span class="op">+</span> <span class="va">self</span>.bias</span>
<span id="cb24-16"><a href="#cb24-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-17"><a href="#cb24-17" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> backward(<span class="va">self</span>, out_error, lr):</span>
<span id="cb24-18"><a href="#cb24-18" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Matrix multiplication order: out_error (batch_size, out_size), self.weights (in_size, out_size)</span></span>
<span id="cb24-19"><a href="#cb24-19" aria-hidden="true" tabindex="-1"></a>        in_x_gradient <span class="op">=</span> np.dot(out_error, <span class="va">self</span>.weights.T)</span>
<span id="cb24-20"><a href="#cb24-20" aria-hidden="true" tabindex="-1"></a>        weight_gradient <span class="op">=</span> np.dot(<span class="va">self</span>.in_x.T, out_error)</span>
<span id="cb24-21"><a href="#cb24-21" aria-hidden="true" tabindex="-1"></a>        bias_gradient <span class="op">=</span> np.<span class="bu">sum</span>(out_error, axis<span class="op">=</span><span class="dv">0</span>)  <span class="co"># Sum over all samples (rows)</span></span>
<span id="cb24-22"><a href="#cb24-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-23"><a href="#cb24-23" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.weights <span class="op">-=</span> lr <span class="op">*</span> weight_gradient</span>
<span id="cb24-24"><a href="#cb24-24" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bias <span class="op">-=</span> lr <span class="op">*</span> bias_gradient</span>
<span id="cb24-25"><a href="#cb24-25" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> in_x_gradient</span>
<span id="cb24-26"><a href="#cb24-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-27"><a href="#cb24-27" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> print_params(<span class="va">self</span>):</span>
<span id="cb24-28"><a href="#cb24-28" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"Weights:</span><span class="ch">\n</span><span class="st">"</span>, <span class="va">self</span>.weights)</span>
<span id="cb24-29"><a href="#cb24-29" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"Bias:</span><span class="ch">\n</span><span class="st">"</span>, <span class="va">self</span>.bias)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>The fully connected layer transforms the input using weights and biases. The weight initialization used the He initialization method. This is a method proposed by He et al.&nbsp;in 2015, which is particularly effective when used with the ReLU activation function.</p>
<div id="cell-36" class="cell" data-execution_count="20">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> relu(x):</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.maximum(x, <span class="dv">0</span>)</span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> relu_deriv(x):</span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.array(x <span class="op">&gt;</span> <span class="dv">0</span>, dtype<span class="op">=</span>np.float32)  <span class="co"># or dtype=int</span></span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> leaky_relu(x):</span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.maximum(<span class="fl">0.01</span> <span class="op">*</span> x, x)</span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> leaky_relu_deriv(x):</span>
<span id="cb25-13"><a href="#cb25-13" aria-hidden="true" tabindex="-1"></a>    dx <span class="op">=</span> np.ones_like(x)</span>
<span id="cb25-14"><a href="#cb25-14" aria-hidden="true" tabindex="-1"></a>    dx[x <span class="op">&lt;</span> <span class="dv">0</span>] <span class="op">=</span> <span class="fl">0.01</span></span>
<span id="cb25-15"><a href="#cb25-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> dx</span>
<span id="cb25-16"><a href="#cb25-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-17"><a href="#cb25-17" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> tanh(x):</span>
<span id="cb25-18"><a href="#cb25-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.tanh(x)</span>
<span id="cb25-19"><a href="#cb25-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-20"><a href="#cb25-20" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> tanh_deriv(x):</span>
<span id="cb25-21"><a href="#cb25-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="dv">1</span> <span class="op">-</span> np.tanh(x)<span class="op">**</span><span class="dv">2</span></span>
<span id="cb25-22"><a href="#cb25-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-23"><a href="#cb25-23" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sigmoid(x):</span>
<span id="cb25-24"><a href="#cb25-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="dv">1</span> <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> np.exp(<span class="op">-</span>x))</span>
<span id="cb25-25"><a href="#cb25-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-26"><a href="#cb25-26" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sigmoid_deriv(x):  <span class="co"># Numerically stable version</span></span>
<span id="cb25-27"><a href="#cb25-27" aria-hidden="true" tabindex="-1"></a>    s <span class="op">=</span> sigmoid(x)</span>
<span id="cb25-28"><a href="#cb25-28" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> s <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> s)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>ReLU has become the standard activation function in deep learning since it was first proposed in 2011. It has the advantage of effectively solving the gradient disappearance problem while being simple to calculate. To calculate backwards, we declare the derivative function of the activation function relu_deriv(). ReLU is a function that returns 0 if the input value is less than 0 and itself if it is greater than 0. Therefore, the derivative function returns 0 for values less than or equal to 0 and 1 for values greater than 0. Here, we use Tanh as the activation function. The following is the activation layer.</p>
<div id="cell-38" class="cell" data-execution_count="21">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ActivationLayer(BaseLayer):</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, activation, activation_deriv):</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.activation <span class="op">=</span> activation</span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.activation_deriv <span class="op">=</span> activation_deriv</span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, input_data):</span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.<span class="bu">input</span> <span class="op">=</span> input_data</span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.activation(input_data)</span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> backward(<span class="va">self</span>, output_error, lr):</span>
<span id="cb26-12"><a href="#cb26-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.activation_deriv(<span class="va">self</span>.<span class="bu">input</span>) <span class="op">*</span> output_error</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Activation layers add non-linearity so that the neural network can approximate complex functions. In the backpropagation process, it multiplies the derivative and output error according to the chain rule.</p>
<div id="cell-40" class="cell" data-execution_count="22">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mse(y_label, y_pred):</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (np.mean(np.power(y_label <span class="op">-</span> y_pred,<span class="dv">2</span>)))</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mse_deriv(y_label, y_pred):</span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (<span class="dv">2</span><span class="op">/</span>y_label.size) <span class="op">*</span> (y_pred <span class="op">-</span> y_label) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Mean Squared Error (MSE) is a widely used loss function in regression problems. It calculates the average of the squared differences between predicted and actual values. These components can be integrated to implement the entire neural network.</p>
<div id="cell-42" class="cell" data-execution_count="23">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Network:</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.layers <span class="op">=</span> []</span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.loss <span class="op">=</span> <span class="va">None</span></span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.loss_deriv <span class="op">=</span> <span class="va">None</span></span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> add_layer(<span class="va">self</span>, layer):</span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.layers.append(layer)</span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-10"><a href="#cb28-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> set_loss(<span class="va">self</span>, loss, loss_deriv):</span>
<span id="cb28-11"><a href="#cb28-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.loss <span class="op">=</span> loss</span>
<span id="cb28-12"><a href="#cb28-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.loss_deriv <span class="op">=</span> loss_deriv</span>
<span id="cb28-13"><a href="#cb28-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-14"><a href="#cb28-14" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _forward_pass(<span class="va">self</span>, x):</span>
<span id="cb28-15"><a href="#cb28-15" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> x</span>
<span id="cb28-16"><a href="#cb28-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> layer <span class="kw">in</span> <span class="va">self</span>.layers:</span>
<span id="cb28-17"><a href="#cb28-17" aria-hidden="true" tabindex="-1"></a>            output <span class="op">=</span> layer.forward(output)</span>
<span id="cb28-18"><a href="#cb28-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> output</span>
<span id="cb28-19"><a href="#cb28-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-20"><a href="#cb28-20" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict(<span class="va">self</span>, inputs):</span>
<span id="cb28-21"><a href="#cb28-21" aria-hidden="true" tabindex="-1"></a>        predictions <span class="op">=</span> []</span>
<span id="cb28-22"><a href="#cb28-22" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> x <span class="kw">in</span> inputs:</span>
<span id="cb28-23"><a href="#cb28-23" aria-hidden="true" tabindex="-1"></a>            output <span class="op">=</span> <span class="va">self</span>._forward_pass(x)</span>
<span id="cb28-24"><a href="#cb28-24" aria-hidden="true" tabindex="-1"></a>            predictions.append(output)</span>
<span id="cb28-25"><a href="#cb28-25" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> predictions</span>
<span id="cb28-26"><a href="#cb28-26" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb28-27"><a href="#cb28-27" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> train(<span class="va">self</span>, x_train, y_train, epochs, lr):</span>
<span id="cb28-28"><a href="#cb28-28" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb28-29"><a href="#cb28-29" aria-hidden="true" tabindex="-1"></a>            epoch_loss <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb28-30"><a href="#cb28-30" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb28-31"><a href="#cb28-31" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> x, y <span class="kw">in</span> <span class="bu">zip</span>(x_train, y_train):</span>
<span id="cb28-32"><a href="#cb28-32" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Forward pass</span></span>
<span id="cb28-33"><a href="#cb28-33" aria-hidden="true" tabindex="-1"></a>                output <span class="op">=</span> <span class="va">self</span>._forward_pass(x)</span>
<span id="cb28-34"><a href="#cb28-34" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb28-35"><a href="#cb28-35" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Calculate loss</span></span>
<span id="cb28-36"><a href="#cb28-36" aria-hidden="true" tabindex="-1"></a>                epoch_loss <span class="op">+=</span> <span class="va">self</span>.loss(y, output)</span>
<span id="cb28-37"><a href="#cb28-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-38"><a href="#cb28-38" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Backward pass</span></span>
<span id="cb28-39"><a href="#cb28-39" aria-hidden="true" tabindex="-1"></a>                error <span class="op">=</span> <span class="va">self</span>.loss_deriv(y, output)</span>
<span id="cb28-40"><a href="#cb28-40" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> layer <span class="kw">in</span> <span class="bu">reversed</span>(<span class="va">self</span>.layers):</span>
<span id="cb28-41"><a href="#cb28-41" aria-hidden="true" tabindex="-1"></a>                    error <span class="op">=</span> layer.backward(error, lr)</span>
<span id="cb28-42"><a href="#cb28-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-43"><a href="#cb28-43" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Calculate average loss for the epoch</span></span>
<span id="cb28-44"><a href="#cb28-44" aria-hidden="true" tabindex="-1"></a>            avg_loss <span class="op">=</span> epoch_loss <span class="op">/</span> <span class="bu">len</span>(x_train)</span>
<span id="cb28-45"><a href="#cb28-45" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> epoch <span class="op">==</span> epochs <span class="op">-</span><span class="dv">1</span>:</span>
<span id="cb28-46"><a href="#cb28-46" aria-hidden="true" tabindex="-1"></a>                <span class="bu">print</span>(<span class="ss">f'epoch </span><span class="sc">{</span>epoch<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">/</span><span class="sc">{</span>epochs<span class="sc">}</span><span class="ss">   error=</span><span class="sc">{</span>avg_loss<span class="sc">:.6f}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<section id="neural-network-training" class="level3">
<h3 class="anchored" data-anchor-id="neural-network-training">1.5.3 Neural Network Training</h3>
<p>Neural network training is a process of optimizing weights by repeating forward and backward propagation. First, let’s look at the learning process of the neural network through the XOR problem.</p>
<div id="cell-44" class="cell" data-execution_count="24">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="co"># XOR training data</span></span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>x_train <span class="op">=</span> np.array([[[<span class="dv">0</span>,<span class="dv">0</span>]], [[<span class="dv">0</span>,<span class="dv">1</span>]], [[<span class="dv">1</span>,<span class="dv">0</span>]], [[<span class="dv">1</span>,<span class="dv">1</span>]]])</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>y_train <span class="op">=</span> np.array([[[<span class="dv">0</span>]], [[<span class="dv">1</span>]], [[<span class="dv">1</span>]], [[<span class="dv">0</span>]]])</span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Network architecture</span></span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a>net <span class="op">=</span> Network()</span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a>net.add_layer(FCLayer(<span class="dv">2</span>, <span class="dv">30</span>))                     <span class="co"># Input layer -&gt; Hidden layer</span></span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a>net.add_layer(ActivationLayer(tanh, tanh_deriv))  <span class="co"># tanh activation</span></span>
<span id="cb29-9"><a href="#cb29-9" aria-hidden="true" tabindex="-1"></a>net.add_layer(FCLayer(<span class="dv">30</span>, <span class="dv">1</span>))                     <span class="co"># Hidden layer -&gt; Output layer</span></span>
<span id="cb29-10"><a href="#cb29-10" aria-hidden="true" tabindex="-1"></a>net.add_layer(ActivationLayer(tanh, tanh_deriv))  <span class="co"># tanh activation</span></span>
<span id="cb29-11"><a href="#cb29-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-12"><a href="#cb29-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-13"><a href="#cb29-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Training settings and execution</span></span>
<span id="cb29-14"><a href="#cb29-14" aria-hidden="true" tabindex="-1"></a>net.set_loss(mse, mse_deriv)</span>
<span id="cb29-15"><a href="#cb29-15" aria-hidden="true" tabindex="-1"></a>net.train(x_train, y_train, epochs<span class="op">=</span><span class="dv">2000</span>, lr<span class="op">=</span><span class="fl">5e-3</span>)</span>
<span id="cb29-16"><a href="#cb29-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-17"><a href="#cb29-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Prediction test</span></span>
<span id="cb29-18"><a href="#cb29-18" aria-hidden="true" tabindex="-1"></a>out <span class="op">=</span> net.predict(x_train)</span>
<span id="cb29-19"><a href="#cb29-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"predict=</span><span class="sc">{</span>out<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>epoch 2000/2000   error=0.002251
predict=[array([[0.00471695]]), array([[0.93254742]]), array([[0.93421712]]), array([[0.0080288]])]</code></pre>
</div>
</div>
<p>The activation function used tanh() for training, and it can be seen that the neural network was trained to produce similar values for the XOR output logic. Now, let’s look at the learning of the neural network on a real dataset through the MNIST handwritten digit classification problem.</p>
<p>The following is an example of MNIST handwriting. First, load the libraries needed to use PyTorch.</p>
<div id="cell-47" class="cell" data-execution_count="25">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torchvision</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision.datasets <span class="im">import</span> MNIST</span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torchvision.transforms <span class="im">as</span> transforms</span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> random_split</span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> MNIST(root <span class="op">=</span> <span class="st">'data/'</span>, download <span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb31-10"><a href="#cb31-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">len</span>(dataset))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>60000</code></pre>
</div>
</div>
<div id="cell-48" class="cell" data-execution_count="26">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>image, label <span class="op">=</span> dataset[<span class="dv">10</span>]</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>plt.imshow(image, cmap <span class="op">=</span> <span class="st">'gray'</span>)</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Label:'</span>, label)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Label: 3</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="01_The Beginning of Deep Learning_files/figure-html/cell-22-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Handwritten labels are not in categorical form as integers. We will create and use a function similar to to_category used in Keras.</p>
<div id="cell-50" class="cell" data-execution_count="27">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> to_categorical(y, num_classes):</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">""" 1-hot encodes a tensor """</span></span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.eye(num_classes, dtype<span class="op">=</span><span class="st">'uint8'</span>)[y]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="cell-51" class="cell" data-execution_count="28">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a><span class="co">## MNIST dataset(images and labels)</span></span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a>mnist_dataset <span class="op">=</span> MNIST(root <span class="op">=</span> <span class="st">'data/'</span>, train <span class="op">=</span> <span class="va">True</span>, transform <span class="op">=</span> transforms.ToTensor())</span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a>train_data, test_data <span class="op">=</span> random_split(mnist_dataset , [<span class="dv">50000</span>, <span class="dv">10000</span>])</span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a>train_loader <span class="op">=</span> torch.utils.data.DataLoader(train_data, batch_size<span class="op">=</span><span class="dv">2000</span>, shuffle<span class="op">=</span><span class="va">True</span>, num_workers<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb36-8"><a href="#cb36-8" aria-hidden="true" tabindex="-1"></a>test_loader <span class="op">=</span> torch.utils.data.DataLoader(train_data, batch_size<span class="op">=</span><span class="dv">10</span>, shuffle<span class="op">=</span><span class="va">True</span>, num_workers<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb36-9"><a href="#cb36-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-10"><a href="#cb36-10" aria-hidden="true" tabindex="-1"></a>train_images, train_labels <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(train_loader)) <span class="co"># 한번의 배치만 가져온다.</span></span>
<span id="cb36-11"><a href="#cb36-11" aria-hidden="true" tabindex="-1"></a>x_train <span class="op">=</span> train_images.reshape(train_images.shape[<span class="dv">0</span>], <span class="dv">1</span>, <span class="dv">28</span><span class="op">*</span><span class="dv">28</span>)</span>
<span id="cb36-12"><a href="#cb36-12" aria-hidden="true" tabindex="-1"></a>y_train <span class="op">=</span> to_categorical(train_labels, <span class="dv">10</span>)</span>
<span id="cb36-13"><a href="#cb36-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-14"><a href="#cb36-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-15"><a href="#cb36-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(x_train.shape)</span>
<span id="cb36-16"><a href="#cb36-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(y_train.shape)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>torch.Size([2000, 1, 784])
(2000, 10)</code></pre>
</div>
</div>
<p>After loading the data, I separated it into two parts for training and testing. I used PyTorch’s DataLoader to load the data. Here, I used a batch size of 2000 to use only 2000 pieces of training data. I brought in only one batch at a time with next(iter(train_loader)) and changed the data shape from (1, 28, 28) to (1, 784). This is called flattening. After processing the image and label data separately, I checked the dimensions.</p>
<div id="cell-53" class="cell" data-execution_count="29">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a><span class="co"># # Network</span></span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a>net <span class="op">=</span> Network()</span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a>net.add_layer(FCLayer(<span class="dv">28</span><span class="op">*</span><span class="dv">28</span>, <span class="dv">100</span>))                <span class="co"># input_shape=(1, 28*28)    ;   output_shape=(1, 100)</span></span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a>net.add_layer(ActivationLayer(tanh, tanh_deriv))</span>
<span id="cb38-6"><a href="#cb38-6" aria-hidden="true" tabindex="-1"></a>net.add_layer(FCLayer(<span class="dv">100</span>, <span class="dv">50</span>))                   <span class="co"># input_shape=(1, 100)      ;   output_shape=(1, 50)</span></span>
<span id="cb38-7"><a href="#cb38-7" aria-hidden="true" tabindex="-1"></a>net.add_layer(ActivationLayer(tanh, tanh_deriv))</span>
<span id="cb38-8"><a href="#cb38-8" aria-hidden="true" tabindex="-1"></a>net.add_layer(FCLayer(<span class="dv">50</span>, <span class="dv">10</span>))                    <span class="co"># input_shape=(1, 50)       ;   output_shape=(1, 10)</span></span>
<span id="cb38-9"><a href="#cb38-9" aria-hidden="true" tabindex="-1"></a>net.add_layer(ActivationLayer(tanh, tanh_deriv))</span>
<span id="cb38-10"><a href="#cb38-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-11"><a href="#cb38-11" aria-hidden="true" tabindex="-1"></a>net.set_loss(mse, mse_deriv)</span>
<span id="cb38-12"><a href="#cb38-12" aria-hidden="true" tabindex="-1"></a>net.train(x_train[<span class="dv">0</span>:<span class="dv">1000</span>], y_train[<span class="dv">0</span>:<span class="dv">1000</span>], epochs<span class="op">=</span><span class="dv">35</span>, lr<span class="op">=</span><span class="fl">0.1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_936812/3322560381.py:14: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword
  return np.dot(x, self.weights) + self.bias
/tmp/ipykernel_936812/3322560381.py:19: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword
  weight_gradient = np.dot(self.in_x.T, out_error)</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>epoch 35/35   error=0.002069</code></pre>
</div>
</div>
<div id="cell-54" class="cell" data-execution_count="30">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Make predictions with the trained model.</span></span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>test_images, test_labels <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(test_loader))</span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a>x_test <span class="op">=</span> test_images.reshape(test_images.shape[<span class="dv">0</span>], <span class="dv">1</span>, <span class="dv">28</span><span class="op">*</span><span class="dv">28</span>)</span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a>y_test <span class="op">=</span> to_categorical(test_labels, <span class="dv">10</span>)</span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">len</span>(x_test))</span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-8"><a href="#cb41-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Use only the first 2 samples for prediction.</span></span>
<span id="cb41-9"><a href="#cb41-9" aria-hidden="true" tabindex="-1"></a>out <span class="op">=</span> net.predict(x_test[:<span class="dv">2</span>])  <span class="co"># Corrected slicing: use [:2] for the first two samples</span></span>
<span id="cb41-10"><a href="#cb41-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb41-11"><a href="#cb41-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Predicted values : "</span>)</span>
<span id="cb41-12"><a href="#cb41-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(out, end<span class="op">=</span><span class="st">"</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb41-13"><a href="#cb41-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"True values : "</span>)</span>
<span id="cb41-14"><a href="#cb41-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(y_test[:<span class="dv">2</span>])  <span class="co"># Corrected slicing: use [:2] to match the prediction</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>10


Predicted values : 
[array([[-0.02857555,  0.04630796,  0.01640415,  0.34762487,  0.01307466,
        -0.14719773,  0.01654099,  0.12845884,  0.74751837,  0.05102324]]), array([[ 0.01248236,  0.00248117,  0.70203826,  0.12074454,  0.088309  ,
        -0.24138211, -0.04961493,  0.20394738,  0.28894724,  0.07850696]])]
True values : 
[[0 0 0 1 0 0 0 0 0 0]
 [0 0 0 1 0 0 0 0 0 0]]</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_936812/3322560381.py:14: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword
  return np.dot(x, self.weights) + self.bias</code></pre>
</div>
</div>
<p>So far, we have directly implemented a “function approximator” that performs predictions by stacking linear transformations and nonlinear activation functions. From simple XOR problems to MNIST handwritten digit classification, we have looked at the core principles of how neural networks learn from data and recognize complex patterns. Deep learning frameworks like PyTorch and TensorFlow make this process much more efficient and convenient, but their underlying operation is not significantly different from the code we implemented directly.</p>
<p>This book will not stop here, but will track the DNA of deep learning technology development from the 1943 McCulloch-Pitts neuron to the latest multimodal architectures in 2025. We will deeply explore why each technology emerged, what fundamental problems it tried to solve, and what connections it has with previous technologies, just like exploring the evolution of living organisms.</p>
<p>In Chapter 2, we cover the mathematical foundations essential for understanding deep learning. We concisely organize key concepts from linear algebra, calculus, probability, and statistics from the perspective of deep learning to help readers understand the subsequent content. If you lack background knowledge in mathematics or are more interested in practical implementation than theory, you can skip to Chapter 3. From Chapter 3 onwards, we use PyTorch and Hugging Face libraries to implement and experiment with the latest deep learning models, allowing you to gain practical experience. However, for a deeper understanding of deep learning and long-term development, it is crucial to build a mathematical foundation.</p>
<p>At the end of each chapter, practice problems will be provided to check readers’ understanding and serve as a springboard for further exploration. We expect that through the problem-solving process, readers will not only find answers but also gain a deeper understanding of deep learning principles and expand their creative thinking.</p>
</section>
</section>
<section id="practice-problems" class="level2">
<h2 class="anchored" data-anchor-id="practice-problems">Practice Problems</h2>
<section id="basic-problems" class="level3">
<h3 class="anchored" data-anchor-id="basic-problems">1. Basic Problems</h3>
<ol type="1">
<li>Explain mathematically why the perceptron cannot solve the XOR problem.<br>
</li>
<li>In the above XOR example, describe the results when using different activation functions such as relu, relu_deriv, etc.<br>
</li>
<li>Explain how the chain rule is applied in the backpropagation algorithm with an example.</li>
</ol>
</section>
<section id="application-problems" class="level3">
<h3 class="anchored" data-anchor-id="application-problems">2. Application Problems</h3>
<ol start="4" type="1">
<li>Analyze the advantages and disadvantages of using the Swish activation function instead of ReLU in house price prediction models.<br>
</li>
<li>Explain why a 3-layer neural network has better representation power than a 2-layer neural network from a functional space perspective.</li>
</ol>
</section>
<section id="advanced-problems" class="level3">
<h3 class="anchored">3. Advanced Problems</h3>
<ol start="6" type="1">
<li>Prove mathematically the mechanism by which ResNet’s skip connection solves the gradient vanishing problem.<br>
</li>
<li>Analyze why the attention mechanism in the Transformer architecture is suitable for sequence modeling.</li>
</ol>
<div class="callout callout-style-default callout-note callout-titled" title="Click to view contents (answer)">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Click to view contents (answer)
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<section id="solution" class="level2 callout-body-container callout-body">
<h2 class="anchored" data-anchor-id="solution">Solution</h2>
<section id="basic-problem-solutions" class="level3">
<h3 class="anchored" data-anchor-id="basic-problem-solutions">1. Basic Problem Solutions</h3>
<ol type="1">
<li><p><strong>XOR Problem</strong>: Limitation of linear classifiers → Need for non-linear decision boundaries</p>
<div class="sourceCode" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a>XOR_input <span class="op">=</span> np.array([[<span class="dv">0</span>,<span class="dv">0</span>],[<span class="dv">0</span>,<span class="dv">1</span>],[<span class="dv">1</span>,<span class="dv">0</span>],[<span class="dv">1</span>,<span class="dv">1</span>]])</span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Cannot distinguish between 0 and 1 with linear combination</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li><p><strong>ReLU Training Problem</strong>: ReLU: Sensitive to learning rate and prone to “Dead ReLU” problem (neurons become inactive and cannot learn). Other activation functions (Leaky ReLU, ELU, Swish, etc.) can mitigate the Dead ReLU problem, making them more stable than ReLU for solving XOR problems. Sigmoid may be difficult to learn due to vanishing gradient problem. Tanh is more stable than ReLU but may suffer from vanishing gradient problem in deep networks.</p></li>
<li><p><strong>Backpropagation Chain Rule</strong>: <span class="math inline">\(\frac{\partial L}{\partial W} = \frac{\partial L}{\partial y}\cdot\frac{\partial y}{\partial W}\)</span></p></li>
</ol>
</section>
<section id="applied-problem-solutions" class="level3">
<h3 class="anchored" data-anchor-id="applied-problem-solutions">2. Applied Problem Solutions</h3>
<ol start="4" type="1">
<li><strong>Swish Function Advantages</strong>:
<ul>
<li>Mitigates ReLU’s dying neuron problem</li>
<li>Improves learning stability with a differentiable curve</li>
</ul></li>
<li><strong>Three-Layer Neural Network Excellence</strong>:
<ul>
<li>Hilbert’s 13th Problem: Cannot express continuous functions of three variables with two-layer networks</li>
<li>Kolmogorov–Arnold Theorem: Can approximate any continuous function with three layers</li>
</ul></li>
</ol>
</section>
<section id="advanced-problem-solutions" class="level3">
<h3 class="anchored" data-anchor-id="advanced-problem-solutions">3. Advanced Problem Solutions</h3>
<ol start="6" type="1">
<li><p><strong>ResNet Skip Connection</strong>: <span class="math inline">\(H(x) = F(x) + x\)</span> → <span class="math inline">\(\frac{\partial L}{\partial x} = \frac{\partial L}{\partial H} \cdot (F'(x) + 1)\)</span></p></li>
<li><p><strong>Transformer Advantages</strong>:</p>
<ul>
<li>Enables parallel processing (overcomes sequential processing limitation of RNNs)</li>
<li>Captures long-range dependencies (learns importance with attention weights)</li>
</ul></li>
</ol>
</section>
</section>
</div>
</div>
<section id="essential-reference-materials" class="level4">
<h4 class="anchored" data-anchor-id="essential-reference-materials">Essential Reference Materials</h4>
<ol type="1">
<li><p><strong><a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://www.deeplearningbook.org/">Deep Learning (Goodfellow, Bengio, Courville, 2016)</a></strong></p></li>
<li><p><strong><a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://link.springer.com/article/10.1007/BF02551274">Approximation by superpositions of a sigmoidal function (Cybenko, 1989)</a></strong></p></li>
<li><p><strong><a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://www.sciencedirect.com/science/article/abs/pii/0893608089900208">Multilayer feedforward networks are universal approximators (Hornik, Stinchcombe, &amp; White, 1989)</a></strong></p></li>
<li><p><strong><a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf">Understanding the difficulty of training deep feedforward neural networks (Glorot &amp; Bengio, 2010)</a></strong></p></li>
<li><p><strong><a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://arxiv.org/abs/1502.01852">Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification (He et al., 2015)</a></strong></p></li>
<li><p><strong><a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=http://neuralnetworksanddeeplearning.com/">Neural Networks and Deep Learning (Michael Nielsen, online book)</a></strong></p></li>
<li><p><strong><a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://explained.ai/matrix-calculus/">The Matrix Calculus You Need For Deep Learning (Parr &amp; Howard, 2018)</a></strong></p></li>
</ol>


</section>
</section>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>