<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>the-birth-of-transformer – Deep Learning DNA: Surviving Architectures and Essential Principles</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap-f507c7d0488cb7630e20aad62ad8c2aa.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script>window.MathJax = {loader: {load: ['[tex]/boldsymbol']},tex: {packages: {'[+]': ['boldsymbol']}}};</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../../notebooks/en/part_1/01_The Beginning of Deep Learning.html">part_1</a></li><li class="breadcrumb-item"><a href="../../../notebooks/en/part_1/08_The Birth of Transformer.html">8. The Birth of Transformer</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../../../">English</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Language</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index_de.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Deutsch</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index_en.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">English</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index_es.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Español</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">한국어</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index_zh.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">中文</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/en/part_1/00_Introduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">part_1</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/en/part_1/01_The Beginning of Deep Learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">1. The Beginning of Deep Learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/en/part_1/02_Mathematics of Deep Learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">2. Mathematics of Deep Learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/en/part_1/03_Deep Learning Framework.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">3. Deep Learning Framework</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/en/part_1/04_Activation Function.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">4. Activation Function</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/en/part_1/05_Optimization and Visualization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">5. Optimization and Visualization</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/en/part_1/06_Overfitting and Development of Solution Techniques.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">6. Overfitting and Development of Solution Techniques</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/en/part_1/07_Evolution of Convolutional Neural Networks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">7. Evolution of Convolutional Neural Networks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/en/part_1/08_The Birth of Transformer.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">8. The Birth of Transformer</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/en/part_1/09_The Evolution of Transformers.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">9. The Evolution of Transformers</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/en/part_1/10_Multimodal Deep Learning: The Beginning of Multisensory Convergence.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">10. Multimodal Deep Learning: The Beginning of Multisensory Convergence</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/en/part_1/11_Multimodal Deep Learning: Intelligence Beyond Limits.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">11. Multimodal Deep Learning: Intelligence Beyond Limits</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Deep Learning Frontier</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/en/Deep Learning Frontier/01_SLM: Small but Powerful Language Model.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">1. SLM: Small but Powerful Language Model</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/en/Deep Learning Frontier/02_Autonomous Driving.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">2. Autonomous Driving</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#chapter-the-birth-of-transformers" id="toc-chapter-the-birth-of-transformers" class="nav-link active" data-scroll-target="#chapter-the-birth-of-transformers">8 Chapter The Birth of Transformers</a>
  <ul class="collapse">
  <li><a href="#transformer---revolution-in-sequence-processing" id="toc-transformer---revolution-in-sequence-processing" class="nav-link" data-scroll-target="#transformer---revolution-in-sequence-processing">8.1 Transformer - Revolution in Sequence Processing</a></li>
  <li><a href="#the-evolution-of-transformers" id="toc-the-evolution-of-transformers" class="nav-link" data-scroll-target="#the-evolution-of-transformers">8.2 The Evolution of Transformers</a>
  <ul class="collapse">
  <li><a href="#the-limitations-of-rnns-and-the-birth-of-attention" id="toc-the-limitations-of-rnns-and-the-birth-of-attention" class="nav-link" data-scroll-target="#the-limitations-of-rnns-and-the-birth-of-attention">8.2.1 The Limitations of RNNs and the Birth of Attention</a></li>
  <li><a href="#basic-concept-of-attention" id="toc-basic-concept-of-attention" class="nav-link" data-scroll-target="#basic-concept-of-attention">8.2.2 Basic Concept of Attention</a></li>
  <li><a href="#evolution-to-self-attention" id="toc-evolution-to-self-attention" class="nav-link" data-scroll-target="#evolution-to-self-attention">8.2.3 Evolution to Self-Attention</a></li>
  <li><a href="#multi-head-attention-and-parallel-processing" id="toc-multi-head-attention-and-parallel-processing" class="nav-link" data-scroll-target="#multi-head-attention-and-parallel-processing">8.2.4 Multi-Head Attention and Parallel Processing</a></li>
  <li><a href="#detailed-analysis-of-multi-head-attention" id="toc-detailed-analysis-of-multi-head-attention" class="nav-link" data-scroll-target="#detailed-analysis-of-multi-head-attention">Detailed Analysis of Multi-Head Attention</a></li>
  <li><a href="#masking-strategies-for-parallel-learning" id="toc-masking-strategies-for-parallel-learning" class="nav-link" data-scroll-target="#masking-strategies-for-parallel-learning">8.2.5 Masking Strategies for Parallel Learning</a></li>
  <li><a href="#evolution-of-head-meaning-from-head-to-brain" id="toc-evolution-of-head-meaning-from-head-to-brain" class="nav-link" data-scroll-target="#evolution-of-head-meaning-from-head-to-brain">8.2.6 Evolution of Head Meaning: From “Head” to “Brain”</a></li>
  </ul></li>
  <li><a href="#processing-location-information" id="toc-processing-location-information" class="nav-link" data-scroll-target="#processing-location-information">8.3 Processing Location Information</a>
  <ul class="collapse">
  <li><a href="#importance-of-sequential-information" id="toc-importance-of-sequential-information" class="nav-link" data-scroll-target="#importance-of-sequential-information">8.3.1 Importance of Sequential Information</a></li>
  <li><a href="#design-of-positional-encoding" id="toc-design-of-positional-encoding" class="nav-link" data-scroll-target="#design-of-positional-encoding">8.3.2 Design of Positional Encoding</a></li>
  </ul></li>
  <li><a href="#transformers-overall-architecture" id="toc-transformers-overall-architecture" class="nav-link" data-scroll-target="#transformers-overall-architecture">8.4 Transformer’s Overall Architecture</a>
  <ul class="collapse">
  <li><a href="#integration-of-basic-components" id="toc-integration-of-basic-components" class="nav-link" data-scroll-target="#integration-of-basic-components">8.4.1 Integration of Basic Components</a></li>
  <li><a href="#encoder-composition" id="toc-encoder-composition" class="nav-link" data-scroll-target="#encoder-composition">8.4.2 Encoder Composition</a></li>
  <li><a href="#configuration-of-the-decoder" id="toc-configuration-of-the-decoder" class="nav-link" data-scroll-target="#configuration-of-the-decoder">8.4.3 Configuration of the Decoder</a></li>
  <li><a href="#description-of-the-overall-structure" id="toc-description-of-the-overall-structure" class="nav-link" data-scroll-target="#description-of-the-overall-structure">8.4.4 Description of the Overall Structure</a></li>
  </ul></li>
  <li><a href="#transformer-examples" id="toc-transformer-examples" class="nav-link" data-scroll-target="#transformer-examples">8.5 Transformer Examples</a>
  <ul class="collapse">
  <li><a href="#simple-copy-task" id="toc-simple-copy-task" class="nav-link" data-scroll-target="#simple-copy-task">8.5.1 Simple Copy Task</a></li>
  <li><a href="#digit-addition-task" id="toc-digit-addition-task" class="nav-link" data-scroll-target="#digit-addition-task">8.5.2 Digit Addition Task</a></li>
  <li><a href="#parser-task" id="toc-parser-task" class="nav-link" data-scroll-target="#parser-task">8.5.3 Parser Task</a></li>
  </ul></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a></li>
  <li><a href="#practice-problems" id="toc-practice-problems" class="nav-link" data-scroll-target="#practice-problems">Practice Problems</a>
  <ul class="collapse">
  <li><a href="#basic-problems" id="toc-basic-problems" class="nav-link" data-scroll-target="#basic-problems">Basic Problems</a></li>
  <li><a href="#application-problems" id="toc-application-problems" class="nav-link" data-scroll-target="#application-problems">Application Problems</a></li>
  <li><a href="#advanced-problems" id="toc-advanced-problems" class="nav-link" data-scroll-target="#advanced-problems">Advanced Problems</a></li>
  </ul></li>
  <li><a href="#references-1" id="toc-references-1" class="nav-link" data-scroll-target="#references-1">References</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../../notebooks/en/part_1/01_The Beginning of Deep Learning.html">part_1</a></li><li class="breadcrumb-item"><a href="../../../notebooks/en/part_1/08_The Birth of Transformer.html">8. The Birth of Transformer</a></li></ol></nav></header>




<p><a href="https://colab.research.google.com/github/Quantum-Intelligence-Frontier/dldna/blob/main/notebooks/en/part_1/08_Birth_of_Transformer.ipynb" target="_parent"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"> </a></p>
<section id="chapter-the-birth-of-transformers" class="level1">
<h1>8 Chapter The Birth of Transformers</h1>
<blockquote class="blockquote">
<p>“Attention is all you need.” - Ashish Vaswani et al., NeurIPS 2017.</p>
</blockquote>
<p>The year 2017 is special in the history of natural language processing. Google announced the Transformer in the paper “Attention is All You Need”. This can be compared to the revolution that AlexNet brought to computer vision in 2012. With the advent of the Transformer, natural language processing (NLP) has entered a new era. Since then, powerful language models like BERT and GPT based on Transformers have emerged, opening up a new chapter in the history of artificial intelligence.</p>
<p><strong>Note</strong></p>
<p>Chapter 8 reconstructs the process of Google’s research team developing the Transformer <em>dramatically</em>. Based on various materials such as original papers, research blogs, and academic presentation materials, we aimed to vividly depict the <strong>concerns and problem-solving processes</strong> that researchers might have faced. In this process, some contents were reconstructed based on <strong>reasonable inference and imagination</strong>.</p>
<section id="transformer---revolution-in-sequence-processing" class="level2">
<h2 class="anchored" data-anchor-id="transformer---revolution-in-sequence-processing">8.1 Transformer - Revolution in Sequence Processing</h2>
<blockquote class="blockquote">
<p><strong>Challenge:</strong> How to overcome the fundamental limitations of existing RNN-based models?</p>
<p><strong>Researcher’s Concerns:</strong> At that time, the natural language processing field was dominated by RNN-based models such as RNN, LSTM, and GRU. However, these models had to process input sequences sequentially, making parallelization impossible, and long-range dependency problems occurred when handling long sentences. Researchers had to develop a new architecture that could overcome these fundamental limitations, be faster and more efficient, and understand long contexts well.</p>
</blockquote>
<p>Natural language processing had long been limited by sequential processing. Sequential processing refers to processing sentences one word or token at a time in order. Like RNN and LSTM, humans read text one word at a time. This sequential processing had two serious problems: 1. it could not efficiently utilize parallel processing hardware like GPUs, and 2. there was a “long-range dependency problem” where information from the front of the sentence (words) was not properly transmitted to the back, making it difficult to process relationships between elements (words, etc.) that were far apart in the sentence.</p>
<p>The attention mechanism that emerged in 2014 partially solved these problems. Existing RNNs only referenced the last hidden state of the encoder when the decoder generated output. Attention allowed the decoder to directly reference all intermediate hidden states of the encoder. However, there were still fundamental limitations. The RNN structure itself was based on sequential processing, so it could only process inputs one word at a time. Therefore, GPU-based parallel processing was impossible, and processing long sequences took a long time.</p>
<p>In 2017, Google’s research team developed the Transformer to dramatically improve machine translation performance. The Transformer fundamentally solved these limitations by removing RNNs entirely and introducing a method that processes sequences using only self-attention.</p>
<p>The Transformer has the following three core advantages: 1. Parallel processing: can process all positions in the sequence simultaneously, maximizing GPU utilization. 2. Global dependency: all tokens can directly define their relationship strengths with other tokens. 3. Flexible processing of position information: effectively expresses order information through positional encoding while flexibly responding to sequences of various lengths. Transformers soon became the basis for powerful language models like BERT and GPT, and expanded to other areas such as Vision Transformers. The transformer is not just a new architecture, but has brought fundamental reconsideration to the way deep learning processes information. In particular, in the field of computer vision, it has led to the success of ViT (Vision Transformer), becoming a strong competitor that threatens CNNs.</p>
</section>
<section id="the-evolution-of-transformers" class="level2">
<h2 class="anchored" data-anchor-id="the-evolution-of-transformers">8.2 The Evolution of Transformers</h2>
<p>In early 2017, a Google research team encountered difficulties in the field of machine translation. At that time, the dominant RNN-based sequence-to-sequence (seq-to-seq) model had a chronic problem: its performance deteriorated significantly when dealing with long sentences. The research team tried to improve the RNN structure in various ways, but it was only a temporary solution and not a fundamental one. Meanwhile, one of the researchers noticed the attention mechanism proposed by Bahdanau et al.&nbsp;in 2014. “If attention can alleviate long-range dependency problems, can we process sequences using only attention, without RNNs?”</p>
<p>Many people are confused about the Q, K, V concept when they first encounter the attention mechanism. In fact, the initial form of attention was the concept of “alignment score” that appeared in Bahdanau’s 2014 paper. This was a score that indicated which part of the encoder the decoder should focus on when generating output words, and it essentially represented the <strong>relevance between two vectors</strong>.</p>
<p>Perhaps the research team started with a practical question: “How can we quantify the relationship between words?” They began with a relatively simple idea of calculating the similarity between vectors and using it as a weight to combine contextual information. In fact, Google’s initial design document (“Transformers: Iterative Self-Attention and Processing for Various Tasks”) used a method similar to “alignment score” to represent the relationship between words, instead of using the terms Q, K, and V.</p>
<p>From now on, let’s follow the process of how Google researchers solved the problem to understand the attention mechanism. Starting from the basic idea of calculating vector similarity, we will explain how they eventually completed the Transformer architecture step by step.</p>
<section id="the-limitations-of-rnns-and-the-birth-of-attention" class="level3">
<h3 class="anchored" data-anchor-id="the-limitations-of-rnns-and-the-birth-of-attention">8.2.1 The Limitations of RNNs and the Birth of Attention</h3>
<p>The research team first tried to clearly identify the limitations of RNNs. Through experiments, they confirmed that as sentence length increased, especially beyond 50 words, BLEU scores decreased significantly. A bigger problem was that even with GPU acceleration, the sequential processing of RNNs made it difficult to fundamentally improve speed. To overcome these limitations, the research team conducted an in-depth analysis of the attention mechanism proposed by Bahdanau et al.&nbsp;(2014). Attention had the effect of alleviating long-range dependency problems by allowing the decoder to refer to all states of the encoder. The following is a basic implementation of the attention mechanism.</p>
<div id="cell-3" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip install dldna[colab] <span class="co"># in Colab</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="co"># !pip install dldna[all] # in your local</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>load_ext autoreload</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>autoreload <span class="dv">2</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="cell-4" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Example word vectors (3-dimensional)</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>word_vectors <span class="op">=</span> {</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">'time'</span>: np.array([<span class="fl">0.2</span>, <span class="fl">0.8</span>, <span class="fl">0.3</span>]),   <span class="co"># In reality, these would be hundreds of dimensions</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">'flies'</span>: np.array([<span class="fl">0.7</span>, <span class="fl">0.2</span>, <span class="fl">0.9</span>]),</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>    <span class="st">'like'</span>: np.array([<span class="fl">0.3</span>, <span class="fl">0.5</span>, <span class="fl">0.2</span>]),</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>    <span class="st">'an'</span>: np.array([<span class="fl">0.1</span>, <span class="fl">0.3</span>, <span class="fl">0.4</span>]),</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>    <span class="st">'arrow'</span>: np.array([<span class="fl">0.8</span>, <span class="fl">0.1</span>, <span class="fl">0.6</span>])</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> calculate_similarity_matrix(word_vectors):</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Calculates the similarity matrix between word vectors."""</span></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> np.vstack(<span class="bu">list</span>(word_vectors.values()))</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.dot(X, X.T)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>The autoreload extension is already loaded. To reload it, use:
  %reload_ext autoreload</code></pre>
</div>
</div>
</section>
<section id="basic-concept-of-attention" class="level3">
<h3 class="anchored" data-anchor-id="basic-concept-of-attention">8.2.2 Basic Concept of Attention</h3>
<p>The content described in this section is a concept introduced in the initial design document called “Transformers: Iterative Self-Attention and Processing for Various Tasks”. Let’s take a step-by-step look at the code below to explain the basic attention concept. First, let’s just look at the similarity matrix (steps 1 and 2 of the source code). Words typically have several hundred dimensions. Here, they are represented as 3-dimensional vectors for example purposes. If we make these into matrices, we simply get a matrix composed of column vectors where each column is a word vector. If we transpose this matrix, we get a matrix where the word vectors are row vectors. When we operate on these two matrices, each element (i, j) becomes the dot product value between the i-th word and the j-th word, and thus the distance (similarity) between the two words.</p>
<div id="cell-6" class="cell" data-execution_count="8">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> visualize_similarity_matrix(words, similarity_matrix):</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Visualizes the similarity matrix in ASCII art format."""</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>    max_word_len <span class="op">=</span> <span class="bu">max</span>(<span class="bu">len</span>(word) <span class="cf">for</span> word <span class="kw">in</span> words)</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>    col_width <span class="op">=</span> max_word_len <span class="op">+</span> <span class="dv">4</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>    header <span class="op">=</span> <span class="st">" "</span> <span class="op">*</span> (col_width) <span class="op">+</span> <span class="st">""</span>.join(<span class="ss">f"</span><span class="sc">{</span>word<span class="sc">:</span><span class="op">&gt;</span>{col_width}<span class="sc">}</span><span class="ss">"</span> <span class="cf">for</span> word <span class="kw">in</span> words)</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(header)</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, word <span class="kw">in</span> <span class="bu">enumerate</span>(words):</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>        row_str <span class="op">=</span> <span class="ss">f"</span><span class="sc">{</span>word<span class="sc">:</span><span class="op">&lt;</span>{col_width}<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>        row_values <span class="op">=</span> [<span class="ss">f"</span><span class="sc">{</span>similarity_matrix[i, j]<span class="sc">:.2f}</span><span class="ss">"</span> <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(words))]</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>        row_str <span class="op">+=</span> <span class="st">""</span>.join(<span class="ss">f"[</span><span class="sc">{</span>value<span class="sc">:</span><span class="op">&gt;</span>{col_width<span class="op">-</span><span class="dv">2</span>}<span class="sc">}</span><span class="ss">]"</span> <span class="cf">for</span> value <span class="kw">in</span> row_values)</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(row_str)</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Example word vectors (in practice, these would have hundreds of dimensions)</span></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>word_vectors <span class="op">=</span> {</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>    <span class="st">'time'</span>: np.array([<span class="fl">0.2</span>, <span class="fl">0.8</span>, <span class="fl">0.3</span>]),</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>    <span class="st">'flies'</span>: np.array([<span class="fl">0.7</span>, <span class="fl">0.2</span>, <span class="fl">0.9</span>]),</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>    <span class="st">'like'</span>: np.array([<span class="fl">0.3</span>, <span class="fl">0.5</span>, <span class="fl">0.2</span>]),</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>    <span class="st">'an'</span>: np.array([<span class="fl">0.1</span>, <span class="fl">0.3</span>, <span class="fl">0.4</span>]),</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>    <span class="st">'arrow'</span>: np.array([<span class="fl">0.8</span>, <span class="fl">0.1</span>, <span class="fl">0.6</span>])</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>words <span class="op">=</span> <span class="bu">list</span>(word_vectors.keys()) <span class="co"># Preserve order</span></span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Convert word vectors into a matrix</span></span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.vstack([word_vectors[word] <span class="cf">for</span> word <span class="kw">in</span> words])</span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Calculate the similarity matrix (dot product)</span></span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>similarity_matrix <span class="op">=</span> calculate_similarity_matrix(word_vectors)</span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a><span class="co"># Print results</span></span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Input matrix shape:"</span>, X.shape)</span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Input matrix:</span><span class="ch">\n</span><span class="st">"</span>, X)</span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Input matrix transpose:</span><span class="ch">\n</span><span class="st">"</span>, X.T)</span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Similarity matrix shape:"</span>, similarity_matrix.shape)</span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Similarity matrix:"</span>) <span class="co"># Output from visualize_similarity_matrix</span></span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a>visualize_similarity_matrix(words, similarity_matrix)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Input matrix shape: (5, 3)
Input matrix:
 [[0.2 0.8 0.3]
 [0.7 0.2 0.9]
 [0.3 0.5 0.2]
 [0.1 0.3 0.4]
 [0.8 0.1 0.6]]

Input matrix transpose:
 [[0.2 0.7 0.3 0.1 0.8]
 [0.8 0.2 0.5 0.3 0.1]
 [0.3 0.9 0.2 0.4 0.6]]

Similarity matrix shape: (5, 5)
Similarity matrix:
              time    flies     like       an    arrow
time     [   0.77][   0.57][   0.52][   0.38][   0.42]
flies    [   0.57][   1.34][   0.49][   0.49][   1.12]
like     [   0.52][   0.49][   0.38][   0.26][   0.41]
an       [   0.38][   0.49][   0.26][   0.26][   0.35]
arrow    [   0.42][   1.12][   0.41][   0.35][   1.01]</code></pre>
</div>
</div>
<p>For example, the value of 0.57 in the (1,2) element of the similarity matrix becomes the distance (similarity) between the vector of times on the row axis and the vector of flies on the column axis. This can be expressed mathematically as follows.</p>
<ul>
<li>Matrix X of word vectors of sentences</li>
</ul>
<p><span class="math inline">\(\mathbf{X} = \begin{bmatrix}
\mathbf{x_1} \\
\mathbf{x_2} \\
\vdots \\
\mathbf{x_n}
\end{bmatrix}\)</span></p>
<ul>
<li>Transpose of X</li>
</ul>
<p><span class="math inline">\(\mathbf{X}^T = \begin{bmatrix}
\mathbf{x_1}^T &amp; \mathbf{x_2}^T &amp; \cdots &amp; \mathbf{x_n}^T
\end{bmatrix}\)</span></p>
<ul>
<li><span class="math inline">\(\mathbf{X}\mathbf{X}^T\)</span> operation</li>
</ul>
<p><span class="math inline">\(\mathbf{X}\mathbf{X}^T = \begin{bmatrix}
\mathbf{x_1} \cdot \mathbf{x_1} &amp; \mathbf{x_1} \cdot \mathbf{x_2} &amp; \cdots &amp; \mathbf{x_1} \cdot \mathbf{x_n} \\
\mathbf{x_2} \cdot \mathbf{x_1} &amp; \mathbf{x_2} \cdot \mathbf{x_2} &amp; \cdots &amp; \mathbf{x_2} \cdot \mathbf{x_n} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\mathbf{x_n} \cdot \mathbf{x_1} &amp; \mathbf{x_n} \cdot \mathbf{x_2} &amp; \cdots &amp; \mathbf{x_n} \cdot \mathbf{x_n}
\end{bmatrix}\)</span></p>
<ul>
<li>Each element (i,j)</li>
</ul>
<p><span class="math inline">\((\mathbf{X}\mathbf{X}^T)_{ij} = \mathbf{x_i} \cdot \mathbf{x_j} = \sum_{k=1}^d x_{ik}x_{jk}\)</span></p>
<p>Each element of this n×n matrix is the dot product between two word vectors, and thus becomes the distance (similarity) between the two words. This is the “attention score”.</p>
<p>The following is a 3-step process of converting a similarity matrix into a weight matrix using softmax.</p>
<div id="cell-9" class="cell" data-execution_count="9">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. Convert similarities to weights (probability distribution) (softmax)</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> softmax(x):</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>    exp_x <span class="op">=</span> np.exp(x <span class="op">-</span> np.<span class="bu">max</span>(x, axis<span class="op">=-</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>))  <span class="co"># trick for stability</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> exp_x <span class="op">/</span> exp_x.<span class="bu">sum</span>(axis<span class="op">=-</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>attention_weights <span class="op">=</span> softmax(similarity_matrix)</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Attention weights shape:"</span>, attention_weights.shape)</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Attention weights:</span><span class="ch">\n</span><span class="st">"</span>, attention_weights)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Attention weights shape: (5, 5)
Attention weights:
 [[0.25130196 0.20574865 0.19571417 0.17014572 0.1770895 ]
 [0.14838442 0.32047566 0.13697608 0.13697608 0.25718775]
 [0.22189237 0.21533446 0.19290396 0.17109046 0.19877876]
 [0.20573742 0.22966017 0.18247272 0.18247272 0.19965696]
 [0.14836389 0.29876818 0.14688764 0.13833357 0.26764673]]</code></pre>
</div>
</div>
<p>Attention weights apply the softmax function. It performs two key transformations:</p>
<ol type="1">
<li>Convert similarity scores to values between 0 and 1</li>
<li>Convert each row into a probability distribution by making the sum of the rows equal to 1</li>
</ol>
<p>Converting the similarity matrix into weights allows the relationship between words and other words to be expressed probabilistically. Since both the row and column axes are word orders in sentences, weight row 1 is the ‘time’ word row, and columns are all sentence words. Therefore:</p>
<ol type="1">
<li>The relationships between all other words (‘time’, ‘flies’, ‘like’, ‘an’, ‘arrow’) are expressed as probability values</li>
<li>The sum of these probability values is 1</li>
<li>High probability values mean stronger relevance</li>
</ol>
<p>These converted weights are used in the next step as a ratio multiplied by the sentence. By applying this ratio, each word in the sentence indicates how much information it reflects. This is similar to determining how much attention each word should pay when “referencing” the information of other words.</p>
<div id="cell-11" class="cell" data-execution_count="17">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 4. Generate contextualized representations using the weights</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>contextualized_vectors <span class="op">=</span> np.dot(attention_weights, X)</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Contextualized vectors shape:"</span>, contextualized_vectors.shape)</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Contextualized vectors:</span><span class="ch">\n</span><span class="st">"</span>, contextualized_vectors)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
Contextualized vectors shape: (5, 3)
Contextualized vectors:
 [[0.41168487 0.40880105 0.47401919]
 [0.51455048 0.31810231 0.56944172]
 [0.42911583 0.38823778 0.48665295]
 [0.43462426 0.37646585 0.49769319]
 [0.51082753 0.32015331 0.55869952]]</code></pre>
</div>
</div>
<p>The dot product of the weight matrix and the word matrix (composed of word vectors) requires interpretation. Assuming the first row of attention_weights is [0.5, 0.2, 0.1, 0.1, 0.1], each value represents the probability of the relevance of ‘time’ to other words. If we express the first weight row as <span class="math inline">\(\begin{bmatrix} \alpha_{11} &amp; \alpha_{12} &amp; \alpha_{13} &amp; \alpha_{14} &amp; \alpha_{15} \end{bmatrix}\)</span>, then the word matrix operation for this first weight row can be expressed as follows.</p>
<p><span class="math inline">\(\begin{bmatrix} \alpha_{11} &amp; \alpha_{12} &amp; \alpha_{13} &amp; \alpha_{14} &amp; \alpha_{15} \end{bmatrix} \begin{bmatrix} \vec{v}_{\text{time}} \\ \vec{v}_{\text{flies}} \\ \vec{v}_{\text{like}} \\ \vec{v}_{\text{an}} \\ \vec{v}_{\text{arrow}} \end{bmatrix}\)</span></p>
<p>This can be represented in Python code as follows.</p>
<div id="cell-13" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>time_contextualized <span class="op">=</span> <span class="fl">0.5</span><span class="op">*</span>time_vector <span class="op">+</span> <span class="fl">0.2</span><span class="op">*</span>flies_vector <span class="op">+</span> <span class="fl">0.1</span><span class="op">*</span>like_vector <span class="op">+</span> <span class="fl">0.1</span><span class="op">*</span>an_vector <span class="op">+</span> <span class="fl">0.1</span><span class="op">*</span>arrow_vector</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="co"># 0.5는 time과 time의 관련도 확률값</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="co"># 0.2는 time과 files의 관련도 확률값</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>The operation is to multiply these probabilities (the probability that each word is related to time) by the original vector of each word and add them all up. As a result, the new vector of <strong>‘time’ becomes a weighted average of the meanings of other words, reflected in their degree of relevance</strong>. The key point is that we are getting a weighted average. Therefore, it was necessary to have a preceding step to obtain the weight matrix for the weighted average.</p>
<p>The final contextualized vector has a shape of (5, 3), which is because the result of multiplying the attention weight matrix of size (5, 5) and the word vector matrix X of size (5, 3) becomes (5, 5) @ (5, 3) = (5, 3).</p>
<p>There is no original text to translate.</p>
</section>
<section id="evolution-to-self-attention" class="level3">
<h3 class="anchored">8.2.3 Evolution to Self-Attention</h3>
<p>The Google research team analyzed the basic attention mechanism (Section 8.2.2) and found several <strong>limitations</strong>. The biggest problem was that it was inefficient for word vectors to perform multiple roles such as <strong>similarity calculation</strong> and <strong>information transmission</strong> simultaneously. For example, the word “bank” has <em>different meanings</em> such as “bank” or “riverbank” depending on the context, and accordingly, its <em>relationship with other words</em> should also change. However, it was difficult to express these various meanings and relationships with <strong>one vector</strong>.</p>
<p>The research team sought a way to <strong>independently optimize</strong> each role. This was like evolving the role of filters in CNNs that extract image features into a <em>learnable form</em>, designing attention to <em>learn</em> specialized representations for each role. This idea started with transforming word vectors into different spaces for different roles.</p>
<p><strong>Limitations of Basic Concepts (Code Example)</strong></p>
<div id="cell-17" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> basic_self_attention(word_vectors):</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>    similarity_matrix <span class="op">=</span> np.dot(word_vectors, word_vectors.T)</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>    attention_weights <span class="op">=</span> softmax(similarity_matrix)</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>    contextualized_vectors <span class="op">=</span> np.dot(attention_weights, word_vectors)</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> contextualized_vectors</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>In the above code, <code>word_vectors</code> plays three roles at the same time.</p>
<ol type="1">
<li><strong>Subject of similarity calculation:</strong> It is used when calculating similarity with other words.</li>
<li><strong>Object of similarity calculation:</strong> It has its similarity calculated from other words.</li>
<li><strong>Information transmission:</strong> It is used for weighted average when creating the final context vector.</li>
</ol>
<p><strong>First improvement: separation of information transmission role</strong></p>
<p>The research team first separated the <strong>information transmission role</strong>. The simplest way to separate the role of a vector in linear algebra is to use a <em>separate learnable matrix</em> to <em>linearly transform</em> the vector into a new space.</p>
<div id="cell-19" class="cell" data-execution_count="10">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> improved_self_attention(word_vectors, W_similarity, W_content):</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>    similarity_vectors <span class="op">=</span> np.dot(word_vectors, W_similarity)</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>    content_vectors <span class="op">=</span> np.dot(word_vectors, W_content)</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate similarity by taking the dot product between similarity_vectors</span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>    attention_scores <span class="op">=</span> np.dot(similarity_vectors, similarity_vectors.T)</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Convert to probability distribution using softmax</span></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>    attention_weights <span class="op">=</span> softmax(attention_scores)</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Generate the final contextualized representation by multiplying weights and content_vectors</span></span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>    contextualized_vectors <span class="op">=</span> np.dot(attention_weights, content_vectors)</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> contextualized_vectors</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<ul>
<li><code>W_similarity</code>: a <em>learnable</em> matrix that projects word vectors into an optimized space for similarity calculation.</li>
<li><code>W_content</code>: a <em>learnable</em> matrix that projects word vectors into an optimized space for information transmission.</li>
</ul>
<p>This improvement allowed <code>similarity_vectors</code> to specialize in similarity calculations and <code>content_vectors</code> to specialize in information transmission. This became the precursor to the concept of information aggregation through Value.</p>
<p><strong>The Second Improvement: Complete Separation of Similarity Roles (Birth of Q, K)</strong></p>
<p>The next step was to separate the similarity calculation process into two roles. Instead of having <code>similarity_vectors</code> play both the “questioning role” (Query) and the “answering role” (Key), it evolved to <em>completely separate</em> these two roles.</p>
<div id="cell-21" class="cell" data-execution_count="11">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SelfAttention(nn.Module):</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, embed_dim):</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 각각의 역할을 위한 독립적인 선형 변환</span></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.q <span class="op">=</span> nn.Linear(embed_dim, embed_dim)  <span class="co"># 질문(Query)을 위한 변환</span></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.k <span class="op">=</span> nn.Linear(embed_dim, embed_dim)  <span class="co"># 답변(Key)을 위한 변환</span></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.v <span class="op">=</span> nn.Linear(embed_dim, embed_dim)  <span class="co"># 정보 전달(Value)을 위한 변환</span></span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>        Q <span class="op">=</span> <span class="va">self</span>.q(x)  <span class="co"># 질문자로서의 표현</span></span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>        K <span class="op">=</span> <span class="va">self</span>.k(x)  <span class="co"># 응답자로서의 표현</span></span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>        V <span class="op">=</span> <span class="va">self</span>.v(x)  <span class="co"># 전달할 정보의 표현</span></span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 질문과 답변 간의 관련성(유사도) 계산</span></span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a>        scores <span class="op">=</span> torch.matmul(Q, K.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>))</span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a>        weights <span class="op">=</span> F.softmax(scores, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 관련성에 따른 정보 집계 (가중 평균)</span></span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.matmul(weights, V)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><strong>Meaning of Q, K, V Space Separation</strong></p>
<p>Even if the order of Q and K is changed (instead of <span class="math inline">\(QK^T\)</span>, <span class="math inline">\(KQ^T\)</span>), mathematically, the same similarity matrix can be obtained. Looking only at the mathematics, why are these two, which are essentially the same, named “Query” and “Key”? The key point is that it is optimized for <em>better similarity calculation</em> in separate spaces. This naming seems to be because the attention mechanism of the transformer model was inspired by information retrieval systems. In search systems, a “query” refers to the information the user wants, and a “key” plays a similar role to the index terms of each document. Attention calculates the similarity between queries and keys to find relevant information.</p>
<p>For example,</p>
<ul>
<li>“I need to deposit money in the bank” (bank)</li>
<li>“The river bank is covered with flowers” (riverbank)</li>
</ul>
<p>In these two sentences, “bank” has different meanings depending on the context. Through Q, K space separation,</p>
<ul>
<li>“bank” and other words are placed in <em>different ways</em> in Q, K spaces to optimize similarity calculations.</li>
<li>In financial contexts, vectors are arranged to increase similarity with ‘money’, ‘deposit’, etc.</li>
<li>In geographical contexts, they are arranged to increase similarity with ‘river’, ‘covered’, etc.</li>
</ul>
<p>In other words, the Q-K pair means calculating similarity by performing an inner product in <em>two optimized spaces</em>. The important point is that Q, K spaces are <em>optimized through learning</em>. It is likely that Google’s research team discovered that Q and K matrices are actually optimized to work like queries and keys during the learning process.</p>
<p><strong>Importance of Q, K Space Separation</strong></p>
<p>Another advantage of separating Q and K is <em>securing flexibility</em>. If Q and K are placed in the same space, the method of similarity calculation may be limited (e.g., symmetric similarity). However, by separating Q and K, more complex and asymmetric relationships (e.g., “A is the cause of B”) can also be learned. Additionally, through different transformations (<span class="math inline">\(W^Q\)</span>, <span class="math inline">\(W^K\)</span>), Q and K can express the role of each word in more detail, increasing the model’s expressive power. Finally, by separating Q and K spaces, the optimization goals of each space become clearer, allowing for a natural division of roles where Q space learns expressions suitable for questions and K space learns expressions suitable for answers.</p>
<p><strong>Role of Value</strong></p>
<p>If Q, K are spaces for similarity calculation, V is a space that <em>contains the information to be actually transmitted</em>. The transformation into V space is optimized in the direction that best expresses the semantic information of the word. While Q, K determine “which words’ information to reflect and how much,” V is responsible for “what information to actually transmit.” In the above “bank” example,</p>
<ul>
<li>Q, K calculate the similarity with financial-related words according to the context,</li>
<li>V expresses the meaning information of ‘bank’ as an actual financial institution.</li>
</ul>
<p>This separation of the three spaces optimizes “how to find information (Q, K)” and “the content of the information to be transmitted (V)” independently, similar to how CNN separates “which patterns to find (filter learning)” and “how to express found patterns (channel learning)”.</p>
<p><strong>Mathematical Expression of Attention</strong></p>
<p>The final attention mechanism is expressed by the following formula.</p>
<p><span class="math display">\[\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V\]</span> * <span class="math inline">\(Q \in \mathbb{R}^{n \times d_k}\)</span>: Query matrix * <span class="math inline">\(K \in \mathbb{R}^{n \times d_k}\)</span>: Key matrix * <span class="math inline">\(V \in \mathbb{R}^{n \times d_v}\)</span>: Value matrix (<span class="math inline">\(d_v\)</span> is generally equal to <span class="math inline">\(d_k\)</span>) * <span class="math inline">\(n\)</span>: Sequence length * <span class="math inline">\(d_k\)</span>: Dimension of Query and Key vectors * <span class="math inline">\(d_v\)</span>: Dimension of Value vector * <span class="math inline">\(\frac{QK^T}{\sqrt{d_k}}\)</span>: Scaled Dot-Product Attention. As the dimension increases, the dot product value increases, preventing the gradient from disappearing when passing through the softmax function.</p>
<p>This advanced structure became a key element of the transformer and later became the foundation for modern language models such as BERT and GPT.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Click to view contents (Theory Deep Dive: Integrated Understanding and Latest Theories of Self-Attention Mechanism)">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Click to view contents (Theory Deep Dive: Integrated Understanding and Latest Theories of Self-Attention Mechanism)
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<section id="integrated-understanding-of-self-attention-mechanism-and-latest-theories" class="level2 callout-body-container callout-body">
<h2 class="anchored" data-anchor-id="integrated-understanding-of-self-attention-mechanism-and-latest-theories">Integrated Understanding of Self-Attention Mechanism and Latest Theories</h2>
<section id="mathematical-principles-and-computational-complexity" class="level3">
<h3 class="anchored" data-anchor-id="mathematical-principles-and-computational-complexity">1. Mathematical Principles and Computational Complexity</h3>
<p>Self-attention generates a new representation that reflects the context by calculating the relationship between each word in the input sequence and all other words, including itself. This process is largely divided into three stages.</p>
<ol type="1">
<li><p><strong>Query, Key, Value Generation:</strong></p>
<p>For each word embedding vector (<span class="math inline">\(x_i\)</span>) in the input sequence, three linear transformations are applied to generate Query (<span class="math inline">\(q_i\)</span>), Key (<span class="math inline">\(k_i\)</span>), and Value (<span class="math inline">\(v_i\)</span>) vectors. These transformations are performed using learnable weight matrices (<span class="math inline">\(W^Q\)</span>, <span class="math inline">\(W^K\)</span>, <span class="math inline">\(W^V\)</span>).</p>
<p><span class="math inline">\(q_i = x_i W^Q\)</span></p>
<p><span class="math inline">\(k_i = x_i W^K\)</span></p>
<p><span class="math inline">\(v_i = x_i W^V\)</span></p>
<p><span class="math inline">\(W^Q, W^K, W^V \in \mathbb{R}^{d_{model} \times d_k}\)</span>: learnable weight matrices. (<span class="math inline">\(d_{model}\)</span>: embedding dimension, <span class="math inline">\(d_k\)</span>: dimension of query, key, value vectors)</p></li>
<li><p><strong>Attention Score Calculation and Normalization</strong></p>
<p>For each word pair, the dot product of Query and Key vectors is calculated to obtain the attention score.</p>
<p><span class="math display">\[\text{score}(q_i, k_j) = q_i \cdot k_j^T\]</span></p>
<p>This score represents how related the two words are. After the dot product operation, scaling is performed to prevent the inner product value from becoming too large, which alleviates the gradient vanishing problem. Scaling is done by dividing by the square root of the Key vector dimension (<span class="math inline">\(d_k\)</span>).</p>
<p><span class="math display">\[\text{scaled score}(q_i, k_j) = \frac{q_i \cdot k_j^T}{\sqrt{d_k}}\]</span></p>
<p>Finally, the softmax function is applied to normalize the attention scores and obtain the attention weights for each word.</p>
<p><span class="math display">\[\alpha_{ij} = \text{softmax}(\text{scaled score}(q_i, k_j)) = \frac{\exp(\text{scaled score}(q_i, k_j))}{\sum_{l=1}^{n} \exp(\text{scaled score}(q_i, k_l))}\]</span></p>
<p>Here, <span class="math inline">\(\alpha_{ij}\)</span> is the attention weight that the <span class="math inline">\(i\)</span>-th word gives to the <span class="math inline">\(j\)</span>-th word, and <span class="math inline">\(n\)</span> is the sequence length.</p></li>
<li><p><strong>Weighted Average Calculation</strong></p>
<p>Using the attention weights (<span class="math inline">\(\alpha_{ij}\)</span>), the weighted average of the Value vectors (<span class="math inline">\(v_j\)</span>) is calculated. This weighted average becomes the context vector (<span class="math inline">\(c_i\)</span>) that integrates all the word information in the input sequence.</p></li>
</ol>
<p><span class="math display">\[c_i = \sum_{j=1}^{n} \alpha_{ij} v_j\]</span></p>
<p><strong>Entire Process Expressed in Matrix Form</strong></p>
<p>When the input embedding matrix is <span class="math inline">\(X \in \mathbb{R}^{n \times d_{model}}\)</span>, the entire self-attention process can be expressed as follows:</p>
<p><span class="math display">\[\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V\]</span></p>
<p>where <span class="math inline">\(Q = XW^Q\)</span>, <span class="math inline">\(K = XW^K\)</span>, and <span class="math inline">\(V = XW^V\)</span>.</p>
<p><strong>Computational Complexity</strong></p>
<p>The computational complexity of self-attention is <span class="math inline">\(O(n^2)\)</span> for the input sequence length (<span class="math inline">\(n\)</span>), as each word must calculate its relationship with all other words. * <strong><span class="math inline">\(QK^T\)</span> calculation:</strong> Performing inner product operations between <span class="math inline">\(n\)</span> query vectors and <span class="math inline">\(n\)</span> key vectors requires <span class="math inline">\(O(n^2d_k)\)</span> computations. * <strong>Softmax operation:</strong> To calculate attention weights for each query, softmax operations are performed over <span class="math inline">\(n\)</span> keys, resulting in a computational complexity of <span class="math inline">\(O(n^2)\)</span>. * <strong>Weighted average with <span class="math inline">\(V\)</span>:</strong> Multiplying <span class="math inline">\(n\)</span> value vectors and <span class="math inline">\(n\)</span> attention weights requires <span class="math inline">\(O(n^2d_k)\)</span> computations.</p>
</section>
<section id="extension-from-the-kernel-machine-perspective" class="level3">
<h3 class="anchored" data-anchor-id="extension-from-the-kernel-machine-perspective">2. Extension from the Kernel Machine Perspective</h3>
<section id="asymmetric-kernel-function" class="level4">
<h4 class="anchored" data-anchor-id="asymmetric-kernel-function">2.1 Asymmetric Kernel Function</h4>
<p>Interpreting attention as an asymmetric kernel function: <span class="math inline">\(K(Q_i, K_j) = \exp\left(\frac{Q_i \cdot K_j}{\sqrt{d_k}}\right)\)</span></p>
<p>This kernel learns a feature mapping that reconstructs the input space.</p>
</section>
<section id="singular-value-decomposition-svd-analysis" class="level4">
<h4 class="anchored" data-anchor-id="singular-value-decomposition-svd-analysis">2.2 Singular Value Decomposition (SVD) Analysis</h4>
<p>Asymmetric KSVD of the attention matrix: <span class="math inline">\(A = U\Sigma V^T \quad \text{where } \Sigma = \text{diag}(\sigma_1, \sigma_2, ...)\)</span></p>
<p>-<span class="math inline">\(U\)</span>: Principal directions in query space (context request patterns) -<span class="math inline">\(V\)</span>: Principal directions in key space (information provision patterns) -<span class="math inline">\(\sigma_i\)</span>: Interaction intensity (≥0.9 explanatory power concentration phenomenon observed)</p>
</section>
</section>
<section id="energy-based-models-and-dynamics" class="level3">
<h3 class="anchored" data-anchor-id="energy-based-models-and-dynamics">3. Energy-Based Models and Dynamics</h3>
<section id="energy-function-formulation" class="level4">
<h4 class="anchored" data-anchor-id="energy-function-formulation">3.1 Energy Function Formulation</h4>
<p><span class="math inline">\(E(Q,K,V) = -\sum_{i,j} \frac{Q_i \cdot K_j}{\sqrt{d_k}}V_j + \text{log-partition function}\)</span></p>
<p>Output is interpreted as an energy minimization process: <span class="math inline">\(\text{Output} = \arg\min_V E(Q,K,V)\)</span></p>
</section>
<section id="equivalence-to-hopfield-networks" class="level4">
<h4 class="anchored" data-anchor-id="equivalence-to-hopfield-networks">3.2 Equivalence to Hopfield Networks</h4>
<p>Continuous Hopfield network equations: <span class="math inline">\(\tau\frac{dX}{dt} = -X + \text{softmax}(XWX^T)XW\)</span></p>
<p>where <span class="math inline">\(\tau\)</span> is the time constant, and <span class="math inline">\(W\)</span> is the learned connection strength matrix</p>
</section>
</section>
<section id="low-dimensional-structure-and-optimization" class="level3">
<h3 class="anchored" data-anchor-id="low-dimensional-structure-and-optimization">4. Low-Dimensional Structure and Optimization</h3>
<section id="rank-collapse-phenomenon" class="level4">
<h4 class="anchored" data-anchor-id="rank-collapse-phenomenon">4.1 Rank Collapse Phenomenon</h4>
<p>In deep layers: <span class="math inline">\(\text{rank}(A) \leq \lfloor0.1n\rfloor\)</span>(empirical observation)</p>
<p>This implies efficient information compression.</p>
</section>
<section id="comparison-of-efficient-attention-techniques" class="level4">
<h4 class="anchored" data-anchor-id="comparison-of-efficient-attention-techniques">4.2 Comparison of Efficient Attention Techniques</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Technique</th>
<th>Principle</th>
<th>Complexity</th>
<th>Application</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Linformer</td>
<td>Low-rank projection</td>
<td><span class="math inline">\(O(n)\)</span></td>
<td>Long text processing</td>
</tr>
<tr class="even">
<td>Performer</td>
<td>Random Fourier features</td>
<td><span class="math inline">\(O(n\log n)\)</span></td>
<td>Genome analysis</td>
</tr>
<tr class="odd">
<td>Reformer</td>
<td>LSH bucketing</td>
<td><span class="math inline">\(O(n\log n)\)</span></td>
<td>Real-time translation</td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="dynamical-system-analysis" class="level3">
<h3 class="anchored" data-anchor-id="dynamical-system-analysis">5. Dynamical System Analysis</h3>
<section id="lyapunov-stability" class="level4">
<h4 class="anchored" data-anchor-id="lyapunov-stability">5.1 Lyapunov Stability</h4>
<p><span class="math inline">\(V(X) = \|X - X^*\|^2\)</span> Lyapunov function</p>
<p>Attention updates guarantee asymptotic stability.</p>
</section>
<section id="frequency-domain-interpretation" class="level4">
<h4 class="anchored" data-anchor-id="frequency-domain-interpretation">5.2 Frequency Domain Interpretation</h4>
<p>Fourier transform of the attention spectrum: <span class="math inline">\(\mathcal{F}(A)_{kl} = \sum_{m,n} A_{mn}e^{-i2\pi(mk/M+nl/N)}\)</span></p>
<p>Low-frequency components capture over 80% of the information</p>
</section>
</section>
<section id="information-theoretic-interpretation" class="level3">
<h3 class="anchored" data-anchor-id="information-theoretic-interpretation">6. Information-Theoretic Interpretation</h3>
<section id="mutual-information-maximization" class="level4">
<h4 class="anchored" data-anchor-id="mutual-information-maximization">6.1 Mutual Information Maximization</h4>
<p><span class="math inline">\(\max I(X;Y) = H(Y) - H(Y|X) \quad \text{s.t. } Y = \text{Attention}(X)\)</span></p>
<p>Softmax generates the optimal distribution that maximizes entropy <span class="math inline">\(H(Y)\)</span></p>
</section>
<section id="signal-to-noise-ratio-snr-analysis" class="level4">
<h4 class="anchored" data-anchor-id="signal-to-noise-ratio-snr-analysis">6.2 Signal-to-Noise Ratio (SNR) Analysis</h4>
<p>SNR decay with layer depth <span class="math inline">\(l\)</span>: <span class="math inline">\(\text{SNR}^{(l)} \propto e^{-0.2l} \quad \text{(ResNet-50 based)}\)</span></p>
</section>
</section>
<section id="neuroscientific-inspiration" class="level3">
<h3 class="anchored" data-anchor-id="neuroscientific-inspiration">7. Neuroscientific Inspiration</h3>
<section id="visual-cortex-v4-region" class="level4">
<h4 class="anchored" data-anchor-id="visual-cortex-v4-region">7.1 Visual Cortex V4 Region</h4>
<ul>
<li>Direction-selective neurons ≈ attention heads responding to specific patterns</li>
<li>Receptive field hierarchy ≈ multi-scale attention</li>
</ul>
</section>
<section id="prefrontal-working-memory" class="level4">
<h4 class="anchored" data-anchor-id="prefrontal-working-memory">7.2 Prefrontal Working Memory</h4>
<ul>
<li>Persistent neuronal activity ≈ attention’s long-term dependency processing</li>
<li>Context maintenance mechanism ≈ decoder’s masking technique</li>
</ul>
</section>
</section>
<section id="advanced-mathematical-modeling" class="level3">
<h3 class="anchored" data-anchor-id="advanced-mathematical-modeling">8. Advanced Mathematical Modeling</h3>
<section id="tensor-network-extension" class="level4">
<h4 class="anchored" data-anchor-id="tensor-network-extension">8.1 Tensor Network Extension</h4>
<p>MPO (Matrix Product Operator) Representation</p>
<p><span class="math inline">\(A_{ij} = \sum_{\alpha=1}^r Q_{i\alpha}K_{j\alpha}\)</span> where <span class="math inline">\(r\)</span> is the tensor network bond dimension</p>
</section>
<section id="differential-geometric-interpretation" class="level4">
<h4 class="anchored" data-anchor-id="differential-geometric-interpretation">8.2 Differential Geometric Interpretation</h4>
<p>Riemannian curvature of the attention manifold <span class="math inline">\(R_{ijkl} = \partial_i\Gamma_{jk}^m - \partial_j\Gamma_{ik}^m + \Gamma_{il}^m\Gamma_{jk}^l - \Gamma_{jl}^m\Gamma_{ik}^l\)</span></p>
<p>Curvature analysis enables estimation of the model’s expressive power limits</p>
</section>
</section>
<section id="recent-research-trends-2025" class="level3">
<h3 class="anchored" data-anchor-id="recent-research-trends-2025">9. Recent Research Trends (2025)</h3>
<ol type="1">
<li><p><strong>Quantum Attention</strong></p>
<ul>
<li>Represent query/key as quantum superposition states: <span class="math inline">\(|\psi_Q\rangle = \sum c_i|i\rangle\)</span></li>
<li>Accelerate quantum inner product operations</li>
</ul></li>
<li><p><strong>Bio-inspired Optimization</strong></p>
<ul>
<li>Apply Spike-Timing-Dependent Plasticity (STDP)</li>
</ul>
<p><span class="math inline">\(\Delta W_{ij} \propto x_i x_j - \beta W_{ij}\)</span></p></li>
<li><p><strong>Dynamic Energy Adjustment</strong></p>
<ul>
<li>Meta-learning-based real-time energy function tuning<br>
</li>
<li>Integrated simulation with physics engines</li>
</ul></li>
</ol>
<hr>
</section>
<section id="references" class="level3">
<h3 class="anchored" data-anchor-id="references">References</h3>
<ol type="1">
<li>Vaswani et al., “Attention Is All You Need”, NeurIPS 2017<br>
</li>
<li>Choromanski et al., “Rethinking Attention with Performers”, ICLR 2021<br>
</li>
<li>Ramsauer et al., “Hopfield Networks is All You Need”, ICLR 2021<br>
</li>
<li>Wang et al., “Linformer: Self-Attention with Linear Complexity”, arXiv 2020<br>
</li>
<li>Chen et al., “Theoretical Analysis of Self-Attention via Signal Propagation”, NeurIPS 2023</li>
</ol>
</section>
</section>
</div>
</div>
</section>
<section id="multi-head-attention-and-parallel-processing" class="level3">
<h3 class="anchored" data-anchor-id="multi-head-attention-and-parallel-processing">8.2.4 Multi-Head Attention and Parallel Processing</h3>
<p>The Google research team came up with the idea of “capturing different types of relationships in <em>multiple small attention spaces</em> instead of one large attention space” to further improve the performance of self-attention. They thought that if they could consider various aspects of the input sequence simultaneously, like multiple experts analyzing a problem from their own perspectives, they could obtain richer contextual information.</p>
<p>Based on this idea, the research team devised <strong>Multi-Head Attention</strong>, which divides Q, K, V vectors into multiple small spaces and calculates attention in parallel. In the original paper (“Attention is All You Need”), 512-dimensional embeddings were divided into 8 heads of 64 dimensions for processing. Subsequent models like BERT further expanded this structure (e.g., BERT-base splits 768 dimensions into 12 heads of 64 dimensions).</p>
<p><strong>How Multi-Head Attention Works</strong></p>
<div id="cell-25" class="cell" data-execution_count="12">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> math</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MultiHeadAttention(nn.Module):</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config):</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span> config.hidden_size <span class="op">%</span> config.num_attention_heads <span class="op">==</span> <span class="dv">0</span></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.d_k <span class="op">=</span> config.hidden_size <span class="op">//</span> config.num_attention_heads  <span class="co"># Dimension of each head</span></span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.h <span class="op">=</span> config.num_attention_heads  <span class="co"># Number of heads</span></span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Linear transformation layers for Q, K, V, and output</span></span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.linear_layers <span class="op">=</span> nn.ModuleList([</span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>            nn.Linear(config.hidden_size, config.hidden_size)</span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">4</span>)  <span class="co"># For Q, K, V, and output</span></span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a>        ])</span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(config.attention_probs_dropout_prob) <span class="co"># added</span></span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.attention_weights <span class="op">=</span> <span class="va">None</span> <span class="co"># added</span></span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> attention(<span class="va">self</span>, query, key, value, mask<span class="op">=</span><span class="va">None</span>): <span class="co"># separate function</span></span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a>        scores <span class="op">=</span> torch.matmul(query, key.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>)) <span class="op">/</span> math.sqrt(<span class="va">self</span>.d_k) <span class="co"># scaled dot product</span></span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-25"><a href="#cb14-25" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> mask <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb14-26"><a href="#cb14-26" aria-hidden="true" tabindex="-1"></a>            scores <span class="op">=</span> scores.masked_fill(mask <span class="op">==</span> <span class="dv">0</span>, <span class="op">-</span><span class="fl">1e9</span>)</span>
<span id="cb14-27"><a href="#cb14-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-28"><a href="#cb14-28" aria-hidden="true" tabindex="-1"></a>        p_attn <span class="op">=</span> scores.softmax(dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb14-29"><a href="#cb14-29" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.attention_weights <span class="op">=</span> p_attn.detach()  <span class="co"># Store attention weights</span></span>
<span id="cb14-30"><a href="#cb14-30" aria-hidden="true" tabindex="-1"></a>        p_attn <span class="op">=</span> <span class="va">self</span>.dropout(p_attn)</span>
<span id="cb14-31"><a href="#cb14-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-32"><a href="#cb14-32" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.matmul(p_attn, value), p_attn</span>
<span id="cb14-33"><a href="#cb14-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-34"><a href="#cb14-34" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, query, key, value, mask<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb14-35"><a href="#cb14-35" aria-hidden="true" tabindex="-1"></a>        batch_size <span class="op">=</span> query.size(<span class="dv">0</span>)</span>
<span id="cb14-36"><a href="#cb14-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-37"><a href="#cb14-37" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 1) Linear projections in batch from d_model =&gt; h x d_k</span></span>
<span id="cb14-38"><a href="#cb14-38" aria-hidden="true" tabindex="-1"></a>        query, key, value <span class="op">=</span> [l(x).view(batch_size, <span class="op">-</span><span class="dv">1</span>, <span class="va">self</span>.h, <span class="va">self</span>.d_k).transpose(<span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb14-39"><a href="#cb14-39" aria-hidden="true" tabindex="-1"></a>                             <span class="cf">for</span> l, x <span class="kw">in</span> <span class="bu">zip</span>(<span class="va">self</span>.linear_layers, (query, key, value))]</span>
<span id="cb14-40"><a href="#cb14-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-41"><a href="#cb14-41" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 2) Apply attention on all the projected vectors in batch.</span></span>
<span id="cb14-42"><a href="#cb14-42" aria-hidden="true" tabindex="-1"></a>        x, attn <span class="op">=</span> <span class="va">self</span>.attention(query, key, value, mask<span class="op">=</span>mask)</span>
<span id="cb14-43"><a href="#cb14-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-44"><a href="#cb14-44" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 3) "Concat" using a view and apply a final linear.</span></span>
<span id="cb14-45"><a href="#cb14-45" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.transpose(<span class="dv">1</span>, <span class="dv">2</span>).contiguous().view(batch_size, <span class="op">-</span><span class="dv">1</span>, <span class="va">self</span>.h <span class="op">*</span> <span class="va">self</span>.d_k)</span>
<span id="cb14-46"><a href="#cb14-46" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.linear_layers[<span class="op">-</span><span class="dv">1</span>](x)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="detailed-analysis-of-multi-head-attention" class="level3">
<h3 class="anchored" data-anchor-id="detailed-analysis-of-multi-head-attention">Detailed Analysis of Multi-Head Attention</h3>
<p><strong>Code Structure (<code>__init__</code> and <code>forward</code>)</strong></p>
<p>The code for multi-head attention is largely composed of initialization (<code>__init__</code>) and forward pass (<code>forward</code>) methods. Let’s take a closer look at the role of each method and their detailed operations.</p>
<ul>
<li><strong><code>__init__</code> Method</strong>:
<ul>
<li><code>d_k</code>: Represents the dimension of each attention head. This value is determined by dividing the model’s hidden size by the number of heads (num_attention_heads), which decides the amount of information each head processes.</li>
<li><code>h</code>: Sets the number of attention heads. This value is a hyperparameter that determines how many different perspectives the model views the input from.</li>
<li><code>linear_layers</code>: Creates four linear transformation layers for query (Q), key (K), value (V), and final output. These layers transform the input to fit each head and integrate the results of the heads at the end.</li>
</ul></li>
<li><strong><code>forward</code> Method</strong>:
<ol type="1">
<li><strong>Linear Transformation and Splitting</strong>:
<ul>
<li>Applies linear transformation to the input <code>query</code>, <code>key</code>, and <code>value</code> using <code>self.linear_layers</code>. This process transforms the inputs into a form suitable for each head.</li>
<li>Uses the <code>view</code> function to change the tensor shape from (batch_size, sequence_length, hidden_size) to (batch_size, sequence_length, h, d_k). This step divides the entire input into h heads.</li>
<li>Applies the <code>transpose</code> function to change the tensor dimension from (batch_size, sequence_length, h, d_k) to (batch_size, h, sequence_length, d_k). Now each head is ready to perform attention calculations independently.</li>
</ul></li>
<li><strong>Applying Attention</strong>:
<ul>
<li>Calls the <code>attention</code> function, namely scaled dot-product attention, for each head to calculate attention weights and the result of each head.</li>
</ul></li>
<li><strong>Combining and Final Linear Transformation</strong>:
<ul>
<li>Uses <code>transpose</code> and <code>contiguous</code> to revert each head’s result (<code>x</code>) back to the shape (batch_size, sequence_length, h, d_k).</li>
<li>Applies the <code>view</code> function to integrate into the shape (batch_size, sequence_length, h * d_k), which is equivalent to (batch_size, sequence_length, hidden_size).</li>
<li>Finally, applies <code>self.linear_layers[-1]</code> to generate the final output. This linear transformation combines the results of the heads and produces the desired output format for the model.</li>
</ul></li>
</ol></li>
<li><strong><code>attention</code> Method (Scaled Dot-Product Attention)</strong>:
<ul>
<li>This function is where the actual attention mechanism is performed for each head, returning the result of each head and attention weights.</li>
<li><strong>Key Point:</strong> When calculating <code>scores</code>, dividing by the square root of the dimension of the <code>key</code> vector (<span class="math inline">\(\sqrt{d_k}\)</span>) is crucial for scaling.
<ul>
<li><strong>Purpose:</strong> Prevents the input values to the softmax function from becoming excessively large as the dot product values (<span class="math inline">\(QK^T\)</span>) increase. This mitigates the vanishing gradient problem, making learning more stable and contributing to improved model performance.</li>
</ul></li>
</ul></li>
</ul>
<hr>
<p><strong>Role of Each Head and Advantages of Multi-Head Attention</strong> Multi-head attention can be thought of as using multiple “small lenses” to observe the target from various angles. Each head independently transforms the query (Q), key (K), and value (V) and performs attention calculations. This allows for focusing on different subspaces within the entire input sequence to extract information.</p>
<ul>
<li><strong>Capturing various relationships</strong>: Each head can specialize in learning different types of linguistic relationships. For example, one head may focus on subject-verb relationships, another on adjective-noun relationships, and another on the relationship between pronouns and their antecedents.</li>
<li><strong>Computational efficiency</strong>: Each head calculates attention in a relatively small dimension (d_k), which is more efficient in terms of computational cost than calculating attention in a single large dimension.</li>
<li><strong>Parallel processing</strong>: The calculations for each head are independent of each other. Therefore, parallel processing using GPUs is possible, which significantly accelerates the computation speed.</li>
</ul>
<p><strong>Real Analysis Cases</strong></p>
<p>Research results show that each head of multi-head attention actually captures different linguistic features. For example, the paper <a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://arxiv.org/abs/1906.04341">“What does BERT Look At? An Analysis of BERT’s Attention”</a> analyzed the multi-head attention of the BERT model and found that some heads play a more important role in understanding the syntactic structure of sentences, while others are more important for understanding semantic similarities between words.</p>
<hr>
<p><strong>Mathematical Expressions</strong></p>
<ul>
<li><strong>Overall</strong>: <span class="math inline">\(\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \ldots, \text{head}_h)W^O\)</span></li>
<li><strong>Each head</strong>: <span class="math inline">\(\text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)\)</span></li>
<li><strong>Attention function</strong>: <span class="math inline">\(\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V\)</span></li>
</ul>
<p><strong>Notation Explanation</strong>:</p>
<ul>
<li><span class="math inline">\(h\)</span>: number of heads</li>
<li><span class="math inline">\(W_i^Q \in \mathbb{R}^{d_{\text{model}} \times d_k}\)</span>: query transformation matrix for the i-th head</li>
<li><span class="math inline">\(W_i^K \in \mathbb{R}^{d_{\text{model}} \times d_k}\)</span>: key transformation matrix for the i-th head</li>
<li><span class="math inline">\(W_i^V \in \mathbb{R}^{d_{\text{model}} \times d_v}\)</span>: value transformation matrix for the i-th head</li>
<li><span class="math inline">\(W^O \in \mathbb{R}^{hd_v \times d_{\text{model}}}\)</span>: linear transformation matrix for the final output</li>
</ul>
<p><strong>Importance of Final Linear Transformation (<span class="math inline">\(W^O\)</span>)</strong>: The additional linear transformation (<span class="math inline">\(W^O\)</span>) that projects the concatenated outputs of each head back to the original embedding dimension (<span class="math inline">\(d_{model}\)</span>) plays a crucial role.</p>
<ul>
<li><strong>Information integration</strong>: It integrates the diverse perspectives of information extracted from different heads in a balanced and stable manner, enriching the overall contextual information.</li>
<li><strong>Optimal combination</strong>: Through the learning process, it learns how to combine the information from each head most effectively. This is similar to combining individual model predictions in an ensemble model, not by simple averaging, but by using learned weights.</li>
</ul>
<hr>
<p><strong>Conclusion</strong></p>
<p>Multi-head attention is a key mechanism that enables transformer models to efficiently capture contextual information from input sequences and accelerate computation speed through parallel processing using GPUs. This allows transformers to show outstanding performance in various natural language processing tasks.</p>
</section>
<section id="masking-strategies-for-parallel-learning" class="level3">
<h3 class="anchored" data-anchor-id="masking-strategies-for-parallel-learning">8.2.5 Masking Strategies for Parallel Learning</h3>
<p>After implementing multi-head attention, the research team faced an important problem in the actual learning process. It was the phenomenon of <strong>“information leakage”</strong> where the model referenced future words to predict current words. For example, when predicting the blank in the sentence “The cat ___ on the mat”, the model could easily predict “sits” by looking ahead at the word “mat”.</p>
<p><strong>The Need for Masking: Preventing Information Leakage</strong></p>
<p>This information leakage resulted in the model not developing actual inference capabilities, but rather simply “peeking” at the answers. The model performed well on training data but failed to make accurate predictions on new, unseen data.</p>
<p>To address this issue, the research team introduced a sophisticated <strong>masking</strong> strategy. Two types of masks are used in Transformers:</p>
<ol type="1">
<li><strong>Causal Mask (Look-Ahead Mask):</strong> Blocks the model from referencing future information in autoregressive models.</li>
<li><strong>Padding Mask:</strong> Removes the influence of meaningless padding tokens when processing variable-length sequences.</li>
</ol>
<p><strong>1. Causal Mask</strong></p>
<p>The causal mask plays a role in hiding future information. Running the following code allows for visual confirmation of how the attention score matrix is masked to remove future information.</p>
<div id="cell-28" class="cell" data-execution_count="4">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_08.visualize_masking <span class="im">import</span> visualize_causal_mask</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>visualize_causal_mask()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>1. Original attention score matrix:
                       I        love        deep    learning
I           [      0.90][      0.70][      0.30][      0.20]
love        [      0.60][      0.80][      0.90][      0.40]
deep        [      0.20][      0.50][      0.70][      0.90]
learning    [      0.40][      0.30][      0.80][      0.60]

Each row represents the attention scores from the current position to all positions
--------------------------------------------------

2. Lower triangular mask (1: allowed, 0: blocked):
                       I        love        deep    learning
I           [      1.00][      0.00][      0.00][      0.00]
love        [      1.00][      1.00][      0.00][      0.00]
deep        [      1.00][      1.00][      1.00][      0.00]
learning    [      1.00][      1.00][      1.00][      1.00]

Only the diagonal and below are 1, the rest are 0
--------------------------------------------------

3. Mask converted to -inf:
                       I        love        deep    learning
I           [   1.0e+00][      -inf][      -inf][      -inf]
love        [   1.0e+00][   1.0e+00][      -inf][      -inf]
deep        [   1.0e+00][   1.0e+00][   1.0e+00][      -inf]
learning    [   1.0e+00][   1.0e+00][   1.0e+00][   1.0e+00]

Converting 0 to -inf so that it becomes 0 after softmax
--------------------------------------------------

4. Attention scores with mask applied:
                       I        love        deep    learning
I           [       1.9][      -inf][      -inf][      -inf]
love        [       1.6][       1.8][      -inf][      -inf]
deep        [       1.2][       1.5][       1.7][      -inf]
learning    [       1.4][       1.3][       1.8][       1.6]

Future information (upper triangle) is masked with -inf
--------------------------------------------------

5. Final attention weights (after softmax):
                       I        love        deep    learning
I           [      1.00][      0.00][      0.00][      0.00]
love        [      0.45][      0.55][      0.00][      0.00]
deep        [      0.25][      0.34][      0.41][      0.00]
learning    [      0.22][      0.20][      0.32][      0.26]

The sum of each row becomes 1, and future information is masked to 0</code></pre>
</div>
</div>
<p>Sequence Processing Structure and Matrix</p>
<p>Let’s explain why future information becomes an upper triangular matrix form using the sentence “I love deep learning” as an example. The word order is [I(0), love(1), deep(2), learning(3)]. In the attention score matrix (<span class="math inline">\(QK^T\)</span>), both rows and columns follow this word order.</p>
<div id="cell-30" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>attention_scores <span class="op">=</span> [</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>    [<span class="fl">0.9</span>, <span class="fl">0.7</span>, <span class="fl">0.3</span>, <span class="fl">0.2</span>],  <span class="co"># I -&gt; I, love, deep, learning</span></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>    [<span class="fl">0.6</span>, <span class="fl">0.8</span>, <span class="fl">0.9</span>, <span class="fl">0.4</span>],  <span class="co"># love -&gt; I, love, deep, learning</span></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>    [<span class="fl">0.2</span>, <span class="fl">0.5</span>, <span class="fl">0.7</span>, <span class="fl">0.9</span>],  <span class="co"># deep -&gt; I, love, deep, learning</span></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>    [<span class="fl">0.4</span>, <span class="fl">0.3</span>, <span class="fl">0.8</span>, <span class="fl">0.6</span>]   <span class="co"># learning -&gt; I, love, deep, learning</span></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<ul>
<li>Each row of Q is the query vector of the word being processed.</li>
<li>Each column of K (since K is transposed) is the key vector of the reference word.</li>
</ul>
<p>Interpreting the matrix above:</p>
<ol type="1">
<li>1st row (I): Relation between [I] and [I, love, deep, learning]</li>
<li>2nd row (love): Relation between [love] and [I, love, deep, learning]</li>
<li>3rd row (deep): Relation between [deep] and [I, love, deep, learning]</li>
<li>4th row (learning): Relation between [learning] and [I, love, deep, learning]</li>
</ol>
<p>When processing the word “deep” (3rd row):</p>
<ul>
<li>Reference available: [I, love, deep] (words that have appeared so far)</li>
<li>Reference not available: [learning] (future word that has not appeared yet)</li>
</ul>
<p>Therefore, based on rows, future words (future information) of the corresponding column words become the <strong>upper triangular</strong> part. Conversely, referenceable words become the <strong>lower triangular</strong>.</p>
<p>The causal relationship mask makes the lower triangular part 1 and the upper triangular part 0, then changes the 0 of the upper triangular to <span class="math inline">\(-\infty\)</span>. The <span class="math inline">\(-\infty\)</span> becomes 0 when it passes through the softmax function. The mask matrix is simply added to the attention score matrix. As a result, in the attention score matrix to which softmax is applied, future information is changed to 0 and blocked.</p>
<p><strong>2. Padding Mask</strong></p>
<p>In natural language processing, sentences have different lengths. To process them in batches, all sentences must be made the same length, and the empty space of shorter sentences is filled with padding tokens (PAD). However, these padding tokens are meaningless, so they should not be included in attention calculations.</p>
<div id="cell-32" class="cell" data-execution_count="14">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_08.visualize_masking <span class="im">import</span> visualize_padding_mask</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>visualize_padding_mask()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
2. Create padding mask (1: valid token, 0: padding token):
tensor([[[1., 1., 1., 1.]],

        [[1., 1., 1., 0.]],

        [[1., 1., 1., 1.]],

        [[1., 1., 1., 1.]]])

Positions that are not padding (0) are 1, padding positions are 0
--------------------------------------------------

3. Original attention scores (first sentence):
                       I        love        deep    learning
I           [      0.90][      0.70][      0.30][      0.20]
love        [      0.60][      0.80][      0.90][      0.40]
deep        [      0.20][      0.50][      0.70][      0.90]
learning    [      0.40][      0.30][      0.80][      0.60]

Attention scores at each position
--------------------------------------------------

4. Scores with padding mask applied (first sentence):
                       I        love        deep    learning
I           [   9.0e-01][   7.0e-01][   3.0e-01][   2.0e-01]
love        [   6.0e-01][   8.0e-01][   9.0e-01][   4.0e-01]
deep        [   2.0e-01][   5.0e-01][   7.0e-01][   9.0e-01]
learning    [   4.0e-01][   3.0e-01][   8.0e-01][   6.0e-01]

The scores at padding positions are masked with -inf
--------------------------------------------------

5. Final attention weights (first sentence):
                       I        love        deep    learning
I           [      0.35][      0.29][      0.19][      0.17]
love        [      0.23][      0.28][      0.31][      0.19]
deep        [      0.17][      0.22][      0.27][      0.33]
learning    [      0.22][      0.20][      0.32][      0.26]

The weights at padding positions become 0, and the sum of the weights at the remaining positions is 1</code></pre>
</div>
</div>
<p>Let’s take the following sentences as examples.</p>
<ul>
<li>“I love ML” → [I, love, ML, PAD]</li>
<li>“Deep learning is fun” → [Deep, learning, is, fun]</li>
</ul>
<p>In the first sentence, since there are only three words, the end was filled with PAD. The padding mask removes the effect of these PAD tokens. A mask is created by marking actual words as 1 and padding tokens as 0, and 2. the attention score at the padding position is made <span class="math inline">\(-\infty\)</span> so that it becomes 0 after passing through the softmax.</p>
<p>As a result, the following effects are obtained.</p>
<ol type="1">
<li>Actual words can give and receive attention to each other freely.</li>
<li>Padding tokens are completely excluded from attention calculation.</li>
<li>The context is formed only with the meaningful parts of each sentence.</li>
</ol>
<div id="cell-34" class="cell" data-execution_count="2">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_attention_mask(size):</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create a lower triangular matrix (including the diagonal)</span></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>    mask <span class="op">=</span> torch.tril(torch.ones(size, size))</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Mask with -inf (becomes 0 after softmax)</span></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>    mask <span class="op">=</span> mask.masked_fill(mask <span class="op">==</span> <span class="dv">0</span>, <span class="bu">float</span>(<span class="st">'-inf'</span>))</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> mask</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> masked_attention(Q, K, V, mask):</span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate attention scores</span></span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>    scores <span class="op">=</span> torch.matmul(Q, K.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>))</span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Apply mask</span></span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>    scores <span class="op">=</span> scores <span class="op">+</span> mask</span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Apply softmax</span></span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a>    weights <span class="op">=</span> F.softmax(scores, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate final attention output</span></span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> torch.matmul(weights, V)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><strong>Innovation and Impact of Masking Strategies</strong></p>
<p>The two masking strategies (padding mask, causal mask) developed by the research team made the learning process of the transformer more robust and later became the foundation for self-regressive models such as GPT. In particular, the causal mask induced the language model to grasp the context sequentially, similar to the actual human language understanding process.</p>
<p><strong>Efficiency of Implementation</strong></p>
<p>Masking is performed immediately after calculating the attention scores, <em>before</em> applying the softmax function. The positions masked with <span class="math inline">\(-\infty\)</span> values become 0 when passing through the softmax function, completely blocking the information at those positions. This is an optimized approach in terms of both computational efficiency and memory usage.</p>
<p>The introduction of these masking strategies enabled the transformer to perform true parallel learning, which had a significant impact on the development of modern language models.</p>
</section>
<section id="evolution-of-head-meaning-from-head-to-brain" class="level3">
<h3 class="anchored" data-anchor-id="evolution-of-head-meaning-from-head-to-brain">8.2.6 Evolution of Head Meaning: From “Head” to “Brain”</h3>
<p>In deep learning, the term “head” has undergone gradual and fundamental changes in meaning with the development of neural network architectures. Initially, it was used with a relatively simple meaning of “part close to the output layer,” but recently, it has been extended to a more abstract and complex meaning of “independent module responsible for specific functions of the model.”</p>
<ol type="1">
<li><strong>Early Days: “Near Output Layer”</strong></li>
</ol>
<p>In early deep learning models (e.g., simple multilayer perceptrons (MLPs)), the “head” generally referred to the last part of the network that took the feature vector extracted through a feature extractor (backbone) as input and performed the final prediction (classification, regression, etc.). In this case, the head mainly consisted of fully connected layers and activation functions.</p>
<div id="cell-37" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SimpleModel(nn.Module):</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, num_classes):</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.backbone <span class="op">=</span> nn.Sequential( <span class="co"># Feature extractor</span></span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">784</span>, <span class="dv">128</span>),</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">128</span>, <span class="dv">64</span>),</span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>            nn.ReLU()</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.head <span class="op">=</span> nn.Linear(<span class="dv">64</span>, num_classes)  <span class="co"># Head (output layer)</span></span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a>        features <span class="op">=</span> <span class="va">self</span>.backbone(x)</span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> <span class="va">self</span>.head(features)</span>
<span id="cb21-15"><a href="#cb21-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> output</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<ol start="2" type="1">
<li><strong>Multi-Task Learning: “Task-Specific Branching”</strong></li>
</ol>
<p>As deep learning models using large datasets like ImageNet have advanced, multi-task learning has emerged, where multiple heads branch out from a single feature extractor to perform different tasks. For example, in object detection models, one head classifies the type of object from an image, while another head predicts the bounding box that indicates the location of the object through regression, both being used simultaneously.</p>
<div id="cell-39" class="cell" data-execution_count="3">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MultiTaskModel(nn.Module):</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, num_classes):</span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.backbone <span class="op">=</span> ResNet50()  <span class="co"># Feature extractor (ResNet)</span></span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.classification_head <span class="op">=</span> nn.Linear(<span class="dv">2048</span>, num_classes)  <span class="co"># Classification head</span></span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bbox_head <span class="op">=</span> nn.Linear(<span class="dv">2048</span>, <span class="dv">4</span>)  <span class="co"># Bounding box regression head</span></span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a>        features <span class="op">=</span> <span class="va">self</span>.backbone(x)</span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a>        class_output <span class="op">=</span> <span class="va">self</span>.classification_head(features)</span>
<span id="cb22-15"><a href="#cb22-15" aria-hidden="true" tabindex="-1"></a>        bbox_output <span class="op">=</span> <span class="va">self</span>.bbox_head(features)</span>
<span id="cb22-16"><a href="#cb22-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> class_output, bbox_output</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<ol start="3" type="1">
<li><strong>Attention is All You Need paper (Transformer) concept of “head”:</strong></li>
</ol>
<p>The Transformer’s multi-head attention has taken a step further. In the Transformer, it no longer follows the fixed notion that “head = part closer to output”.</p>
<div id="cell-41" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MultiHeadAttention(nn.Module):</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, num_heads):</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.heads <span class="op">=</span> nn.ModuleList([</span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>            AttentionHead() <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_heads)  <span class="co"># num_heads개의 독립적인 어텐션 헤드</span></span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>        ])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<ul>
<li><strong>Independent Modules:</strong> Here, each “head” is a separate module that receives input and performs the attention mechanism independently. Each head has different weights and pays attention to different aspects of the input sequence.</li>
<li><strong>Parallel Processing:</strong> Multiple heads work in parallel, processing various types of information simultaneously.</li>
<li><strong>Intermediate Processing Steps:</strong> Heads are no longer limited to just the output layer. The transformer’s encoder and decoder consist of multiple layers of multi-head attention, where each layer’s head learns different representations of the input sequence.</li>
</ul>
<ol start="4" type="1">
<li><p><strong>Recent Trends: “Functional Modules”</strong></p>
<p>In recent deep learning models, the term “head” is used more flexibly. Even if it’s not necessarily near the output layer, an independent module that performs a specific function is often referred to as a “head”.</p>
<ul>
<li><strong>Language Models:</strong> Large language models like BERT and GPT use various types of heads, such as “language modeling head”, “masked language modeling head”, and “next sentence prediction head”.</li>
<li><strong>Vision Transformers:</strong> In ViT, an image is divided into patches, and a “patch embedding head” is used to process each patch like a token.</li>
</ul></li>
</ol>
<p><strong>Conclusion</strong></p>
<p>The meaning of “head” in deep learning has evolved from “simply the part close to the output” to “independent modules that perform specific functions (including parallel and intermediate processing)”. This change reflects the trend of deep learning architectures becoming more complex and sophisticated, with each part of the model becoming more subdivided and specialized. The transformer’s multi-head attention is a prime example of this shift in meaning, showing that the term “head” no longer refers to just one “brain” but rather multiple “brains” working together.</p>
</section>
</section>
<section id="processing-location-information" class="level2">
<h2 class="anchored" data-anchor-id="processing-location-information">8.3 Processing Location Information</h2>
<p><strong>Challenge:</strong> How can we effectively express the order of words without using RNN?</p>
<p><strong>Researcher’s Dilemma:</strong> Since the Transformer does not process data sequentially like RNN, it was necessary to explicitly inform the word location information. Researchers tried various methods (location index, learnable embedding, etc.), but they could not get satisfactory results. They had to find a new way to effectively express location information, just like deciphering a cryptogram.</p>
<p>The Transformer, unlike RNN, does not use recurrent structure or convolutional operation, so it was necessary to provide sequence order information separately. This is because the meaning of “dog bites man” and “man bites dog” changes completely depending on the order, even though the words are the same. The attention operation (<span class="math inline">\(QK^T\)</span>) itself only calculates the similarity between word vectors and does not consider word location information, so the research team had to think about how to inject location information into the model. This was a <strong>challenge</strong> of how to effectively express the order of words without using RNN.</p>
<section id="importance-of-sequential-information" class="level3">
<h3 class="anchored" data-anchor-id="importance-of-sequential-information">8.3.1 Importance of Sequential Information</h3>
<p>The research team considered various positional encoding methods.</p>
<ol type="1">
<li><strong>Using Location Index Directly:</strong> The simplest approach is to add the location index (0, 1, 2, …) of each word to the embedding vector.</li>
</ol>
<div id="cell-44" class="cell" data-execution_count="15">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_08.visualize_positional_embedding <span class="im">import</span> visualize_position_embedding</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>visualize_position_embedding()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>1. Original embedding matrix:
                dim1      dim2      dim3      dim4
I         [    0.20][    0.30][    0.10][    0.40]
love      [    0.50][    0.20][    0.80][    0.10]
deep      [    0.30][    0.70][    0.20][    0.50]
learning  [    0.60][    0.40][    0.30][    0.20]

Each row is the embedding vector of a word
--------------------------------------------------

2. Position indices:
[0 1 2 3]

Indices representing the position of each word (starting from 0)
--------------------------------------------------

3. Embeddings with position information added:
                dim1      dim2      dim3      dim4
I         [    0.20][    0.30][    0.10][    0.40]
love      [    1.50][    1.20][    1.80][    1.10]
deep      [    2.30][    2.70][    2.20][    2.50]
learning  [    3.60][    3.40][    3.30][    3.20]

Result of adding position indices to each embedding vector (broadcasting)
--------------------------------------------------

4. Changes due to adding position information:

I (0):
  Original:     [0.2 0.3 0.1 0.4]
  Pos. Added: [0.2 0.3 0.1 0.4]
  Difference:     [0. 0. 0. 0.]

love (1):
  Original:     [0.5 0.2 0.8 0.1]
  Pos. Added: [1.5 1.2 1.8 1.1]
  Difference:     [1. 1. 1. 1.]

deep (2):
  Original:     [0.3 0.7 0.2 0.5]
  Pos. Added: [2.3 2.7 2.2 2.5]
  Difference:     [2. 2. 2. 2.]

learning (3):
  Original:     [0.6 0.4 0.3 0.2]
  Pos. Added: [3.6 3.4 3.3 3.2]
  Difference:     [3. 3. 3. 3.]</code></pre>
</div>
</div>
<p>However, this approach had two problems.</p>
<ul>
<li><strong>Unable to handle sequences longer than the training data:</strong> If an unseen position (e.g.&nbsp;100th) is entered as input, it cannot find the appropriate expression.</li>
<li><strong>Difficulty in expressing relative distance information:</strong> It is difficult to express that the distance between positions 2 and 4 is the same as the distance between positions 102 and 104.</li>
</ul>
<ol start="2" type="1">
<li><strong>Learnable position embedding:</strong> A method using a learnable embedding vector for each position was also considered.</li>
</ol>
<div id="cell-46" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Conceptual code</span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>    positional_embeddings <span class="op">=</span> nn.Embedding(max_seq_length, embedding_dim)</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>    positions <span class="op">=</span> torch.arange(seq_length)</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>    positional_encoding <span class="op">=</span> positional_embeddings(positions)</span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>    final_embedding <span class="op">=</span> word_embedding <span class="op">+</span> positional_encoding</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>This approach can learn unique expressions for each position, but the fundamental limitation that it cannot handle sequences longer than the training data still remained.</p>
<p><strong>Core Conditions for Positional Encoding</strong></p>
<p>Through trial and error, the research team realized that positional encoding must satisfy the following three core conditions:</p>
<ol type="1">
<li><strong>No Sequence Length Limitation:</strong> It should be able to properly express positions that were not seen during training (e.g., the 1000th position).</li>
<li><strong>Expression of Relative Distance Relationship:</strong> The distance between positions 2 and 4 should be expressed in the same way as the distance between positions 102 and 104. In other words, the relative distance between positions should be preserved.</li>
<li><strong>Compatibility with Attention Operation:</strong> Positional information should not interfere with attention weight calculation while effectively conveying order information.</li>
</ol>
</section>
<section id="design-of-positional-encoding" class="level3">
<h3 class="anchored" data-anchor-id="design-of-positional-encoding">8.3.2 Design of Positional Encoding</h3>
<p>After much consideration, the research team discovered an innovative solution called <strong>Positional Encoding</strong>, which utilizes the periodic characteristics of sine and cosine functions.</p>
<p><strong>Principle of Sine-Cosine Function-Based Positional Encoding</strong></p>
<p>By encoding each position using sine and cosine functions with different frequencies, the relative distance between positions can be naturally expressed.</p>
<div id="cell-48" class="cell" data-execution_count="17">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_08.positional_encoding_utils <span class="im">import</span> visualize_sinusoidal_features</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>visualize_sinusoidal_features()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="08_The Birth of Transformer_files/figure-html/cell-21-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Figure 3 is a visualization of the movement of positions, showing how the sine function expresses positional relationships. It satisfies the second condition, “relative distance relationship expression”. All shifted curves maintain the same shape as the original curve while maintaining a constant interval. This means that if the distances between positions are the same (e.g., 2→7 and 102→107), their relationships are also expressed equally.</p>
<p>Figure 4 is a positional encoding heatmap (Positional Encoding Matrix), showing what unique pattern (horizontal axis) each position (vertical axis) has. The columns on the horizontal axis represent sine/cosine functions of different periods, with longer periods to the right. A unique pattern is created for each row (position) by the combination of red (positive) and blue (negative). By using a variety of frequencies from short to long periods, a unique pattern is created for each position. This approach satisfies the first condition, “no limit on sequence length”. By combining sine/cosine functions of different periods, it can mathematically generate unique values for infinitely many positions.</p>
<p>Using this mathematical property, the research team implemented the positional encoding algorithm as follows.</p>
<p><strong>Positional Encoding Implementation</strong></p>
<div id="cell-50" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> positional_encoding(seq_length, d_model):</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 1. 위치별 인코딩 행렬 생성</span></span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>    position <span class="op">=</span> np.arange(seq_length)[:, np.newaxis]  <span class="co"># [0, 1, 2, ..., seq_length-1]</span></span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 2. 각 차원별 주기 계산</span></span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a>    div_term <span class="op">=</span> np.exp(np.arange(<span class="dv">0</span>, d_model, <span class="dv">2</span>) <span class="op">*</span> <span class="op">-</span>(np.log(<span class="fl">10000.0</span>) <span class="op">/</span> d_model))</span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 예: d_model=512일 때</span></span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># div_term[0] ≈ 1.0        (가장 짧은 주기)</span></span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># div_term[256] ≈ 0.0001   (가장 긴 주기)</span></span>
<span id="cb28-10"><a href="#cb28-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb28-11"><a href="#cb28-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 3. 짝수/홀수 차원에 사인/코사인 적용</span></span>
<span id="cb28-12"><a href="#cb28-12" aria-hidden="true" tabindex="-1"></a>    pe <span class="op">=</span> np.zeros((seq_length, d_model))</span>
<span id="cb28-13"><a href="#cb28-13" aria-hidden="true" tabindex="-1"></a>    pe[:, <span class="dv">0</span>::<span class="dv">2</span>] <span class="op">=</span> np.sin(position <span class="op">*</span> div_term)  <span class="co"># 짝수 차원</span></span>
<span id="cb28-14"><a href="#cb28-14" aria-hidden="true" tabindex="-1"></a>    pe[:, <span class="dv">1</span>::<span class="dv">2</span>] <span class="op">=</span> np.cos(position <span class="op">*</span> div_term)  <span class="co"># 홀수 차원</span></span>
<span id="cb28-15"><a href="#cb28-15" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb28-16"><a href="#cb28-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> pe</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<ul>
<li><code>position</code>: An array of the form <code>[0, 1, 2, ..., seq_length-1]</code>. It represents the location index of each word.</li>
<li><code>div_term</code>: A value that determines the period for each dimension. As <code>d_model</code> increases, the period becomes longer.</li>
<li><code>pe[:, 0::2] = np.sin(position * div_term)</code>: Apply the sine function to even-indexed dimensions.</li>
<li><code>pe[:, 1::2] = np.cos(position * div_term)</code>: Apply the cosine function to odd-indexed dimensions.</li>
</ul>
<p><strong>Mathematical Expression</strong></p>
<p>Each dimension of the positional encoding is calculated using the following formula:</p>
<ul>
<li><span class="math inline">\(PE_{(pos, 2i)} = \sin(pos / 10000^{2i/d_{\text{model}}})\)</span></li>
<li><span class="math inline">\(PE_{(pos, 2i+1)} = \cos(pos / 10000^{2i/d_{\text{model}}})\)</span></li>
</ul>
<p>where</p>
<ul>
<li><span class="math inline">\(pos\)</span>: the position of the word (0, 1, 2, …)</li>
<li><span class="math inline">\(i\)</span>: the dimension index (0, 1, 2, …, <span class="math inline">\(d_{model}\)</span>-1)</li>
<li><span class="math inline">\(d_{model}\)</span>: the embedding dimension (and positional encoding dimension)</li>
</ul>
<p><strong>Period Change Check</strong></p>
<div id="cell-52" class="cell" data-execution_count="18">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_08.positional_encoding_utils <span class="im">import</span> show_positional_periods</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>show_positional_periods()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>1. Periods of positional encoding:
First dimension (i=0): 1.00
Middle dimension (i=128): 100.00
Last dimension (i=255): 9646.62

2. Positional encoding formula values (10000^(2i/d_model)):
i=  0: 1.0000000000
i=128: 100.0000000000
i=255: 9646.6161991120

3. Actual div_term values (first/middle/last):
First (i=0): 1.0000000000
Middle (i=128): 0.0100000000
Last (i=255): 0.0001036633</code></pre>
</div>
</div>
<p>The key point here is the 3 steps.</p>
<div id="cell-54" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 3. 짝수/홀수 차원에 사인/코사인 적용</span></span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>    pe <span class="op">=</span> np.zeros((seq_length, d_model))</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>    pe[:, <span class="dv">0</span>::<span class="dv">2</span>] <span class="op">=</span> np.sin(position <span class="op">*</span> div_term)  <span class="co"># 짝수 차원</span></span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>    pe[:, <span class="dv">1</span>::<span class="dv">2</span>] <span class="op">=</span> np.cos(position <span class="op">*</span> div_term)  <span class="co"># 홀수 차원</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>The result above shows the change in period with dimension.</p>
<p><strong>Final Embedding</strong></p>
<p>The generated positional encoding <code>pe</code> has a shape of (seq_length, d_model), and is added to the original word embedding matrix (sentence_embedding) to create the final embedding.</p>
<div id="cell-56" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>final_embedding <span class="op">=</span> sentence_embedding <span class="op">+</span> positional_encoding</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>The final embedding added in this way contains both the meaning of the word and its position information. For example, the word “bank” has different final vector values depending on its position in the sentence, which helps to distinguish between the meanings of “bank” as a financial institution and “bank” as the side of a river.</p>
<p>This allows the transformer to effectively process sequential information without using an RNN, and provides a basis for maximizing the advantages of parallel processing.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Click to view content (Deep Dive: The Evolution of Positional Encoding, Latest Techniques, and Mathematical Foundations)">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Click to view content (Deep Dive: The Evolution of Positional Encoding, Latest Techniques, and Mathematical Foundations)
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<section id="evolution-of-positional-encoding-latest-techniques-and-mathematical-foundations" class="level3 callout-body-container callout-body">
<h3 class="anchored" data-anchor-id="evolution-of-positional-encoding-latest-techniques-and-mathematical-foundations">Evolution of Positional Encoding, Latest Techniques, and Mathematical Foundations</h3>
<p>In Section 8.3.2, we examined the sine-cosine function-based positional encoding that underlies transformer models. However, since the publication of the “Attention is All You Need” paper, positional encoding has evolved in various directions. This deep dive section comprehensively covers learnable positional encoding, relative positional encoding, and the latest research trends, while providing an in-depth analysis of the mathematical expressions and pros and cons of each technique.</p>
<section id="learnable-positional-encoding" class="level4">
<h4 class="anchored" data-anchor-id="learnable-positional-encoding">1. Learnable Positional Encoding</h4>
<ul>
<li><p><strong>Concept</strong>: Instead of using a fixed function, the model learns to express position information through embeddings during training.</p></li>
<li><p><strong>1.1 Mathematical Expression</strong>: Learnable positional embeddings are represented by the following matrix:</p>
<p><span class="math inline">\(P \in \mathbb{R}^{L_{max} \times d}\)</span></p>
<p>where <span class="math inline">\(L_{max}\)</span> is the maximum sequence length and <span class="math inline">\(d\)</span> is the embedding dimension. The embedding for position <span class="math inline">\(i\)</span> is given by the <span class="math inline">\(i\)</span>-th row of the matrix <span class="math inline">\(P\)</span>, i.e., <span class="math inline">\(P[i,:]\)</span>.</p></li>
<li><p><strong>1.2 Extrapolation Problem Solution Techniques</strong>: When dealing with sequences longer than the training data, there is a problem that there is no information about positions beyond the learned embeddings. Techniques have been researched to solve this issue.</p>
<ul>
<li><p><strong>Position Interpolation (Chen et al., 2023)</strong>: New position embeddings are generated by linearly interpolating between learned embeddings.</p>
<p><span class="math inline">\(P_{ext}(i) = P[\lfloor \alpha i \rfloor] + (\alpha i - \lfloor \alpha i \rfloor)(P[\lfloor \alpha i \rfloor +1] - P[\lfloor \alpha i \rfloor])\)</span></p>
<p>where <span class="math inline">\(\alpha = \frac{\text{training sequence length}}{\text{inference sequence length}}\)</span>.</p></li>
<li><p><strong>NTK-aware Scaling (2023)</strong>: Based on Neural Tangent Kernel (NTK) theory, this method introduces a smoothing effect by gradually increasing the frequency.</p></li>
</ul></li>
<li><p><strong>1.3 Latest Application Cases</strong>:</p>
<ul>
<li><strong>BERT</strong>: Initially limited to 512 tokens, RoBERTa extended it to 1024 tokens.</li>
<li><strong>GPT-3</strong>: Has a limit of 2048 tokens and used a technique to gradually increase the sequence length during training.</li>
</ul></li>
<li><p><strong>Advantages</strong>:</p>
<ul>
<li><strong>Flexibility</strong>: Can learn position information specialized for the data.</li>
<li><strong>Potential Performance Improvement</strong>: May show better performance than fixed functions in certain tasks.</li>
</ul></li>
<li><p><strong>Disadvantages</strong>:</p>
<ul>
<li><strong>Overfitting Risk</strong>: Generalization performance may degrade for sequences of lengths not seen during training.</li>
<li><strong>Difficulty in Handling Long Sequences</strong>: Additional techniques are needed to solve the extrapolation problem.</li>
</ul></li>
</ul>
</section>
<section id="relative-positional-encoding" class="level4">
<h4 class="anchored" data-anchor-id="relative-positional-encoding">2. Relative Positional Encoding</h4>
<ul>
<li><strong>Core Idea</strong>: Focuses on the relative distance between words rather than absolute position information.</li>
<li><strong>Background</strong>: In natural language, the meaning of a word is often more greatly influenced by its relative relationship with surrounding words than by its absolute position. Moreover, absolute positional encoding has the disadvantage of being ineffective in capturing relationships between distant words.</li>
<li><strong>2.1 Mathematical Extension</strong>:
<ul>
<li><p><strong>Shaw et al.&nbsp;(2018) Formula</strong>: When calculating the relationship between Query and Key vectors in the attention mechanism, a learnable embedding (<span class="math inline">\(a_{i-j}\)</span>) for relative distance is added.</p>
<p><span class="math inline">\(e_{i,j} = a_{i-j}\)</span></p>
<p>This allows the model to consider the relative position of words when computing attention weights.</p></li>
</ul></li>
<li><strong>2.2 Latest Techniques</strong>:
<ul>
<li><strong>Disentangled Relative Positional Encoding</strong>: Proposes separating the embedding space into two parts: one for capturing local context and another for global context, improving the model’s ability to handle both short-range and long-range dependencies.</li>
<li><strong>Relative Positional Encoding with Graph Attention</strong>: Integrates relative positional encoding with graph attention mechanisms to better model complex relationships between words in a sentence. <span class="math inline">\(e_{ij} = \frac{x_iW^Q(x_jW^K + a_{i-j})^T}{\sqrt{d}}\)</span></li>
</ul></li>
</ul>
<p>Here, <span class="math inline">\(a_{i-j} \in \mathbb{R}^d\)</span> is a learnable vector for the relative position <span class="math inline">\(i-j\)</span>.</p>
<ul>
<li><p><strong>Rotary Positional Encoding (RoPE)</strong>: Encodes relative positions using rotation matrices.</p>
<p><span class="math inline">\(\text{RoPE}(x, m) = x \odot e^{im\theta}\)</span></p>
<p>where <span class="math inline">\(\theta\)</span> is a hyperparameter controlling frequency, and <span class="math inline">\(\odot\)</span> denotes complex multiplication (or the corresponding rotation matrix).</p></li>
<li><p><strong>Simplified version of T5</strong>: Uses learnable biases (<span class="math inline">\(b\)</span>) for relative positions and clips values when the relative distance exceeds a certain range.</p>
<p><span class="math inline">\(e_{ij} = \frac{x_iW^Q(x_jW^K)^T + b_{\text{clip}(i-j)}}{\sqrt{d}}\)</span></p>
<p><span class="math inline">\(b \in \mathbb{R}^{2k+1}\)</span> is a bias vector for clipped relative positions <span class="math inline">\([-k, k]\)</span>.</p></li>
<li><p><strong>Advantages</strong>:</p>
<ul>
<li><strong>Improved generalization</strong>: Generalizes better to sequences of lengths not seen during training.</li>
<li><strong>Improved capture of long-range dependencies</strong>: More effectively models relationships between distant words.</li>
</ul></li>
<li><p><strong>Disadvantages</strong>:</p>
<ul>
<li><strong>Increased computational complexity</strong>: Attention calculations can become more complex as relative distances are considered (especially when considering relative distances for all word pairs).</li>
</ul></li>
</ul>
</section>
<section id="optimization-of-cnn-based-positional-encoding" class="level4">
<h4 class="anchored" data-anchor-id="optimization-of-cnn-based-positional-encoding">3. Optimization of CNN-based Positional Encoding</h4>
<ul>
<li><p><strong>3.1 Applying Depth-wise Convolution</strong>: Performs independent convolutions for each channel, reducing parameters and increasing calculation efficiency. <span class="math inline">\(P(i) = \sum_{k=-K}^K w_k \cdot x_{i+k}\)</span></p>
<p>where <span class="math inline">\(K\)</span> is the kernel size, and <span class="math inline">\(w_k\)</span> is a learnable weight.</p></li>
<li><p><strong>3.2 Multi-scale Convolution</strong>: Similar to ResNet, utilizes parallel convolution channels to capture various ranges of position information.</p>
<p><span class="math inline">\(P(i) = \text{Concat}(\text{Conv}_{3x1}(x), \text{Conv}_{5x1}(x))\)</span></p></li>
</ul>
</section>
<section id="dynamics-of-recursive-positional-encoding" class="level4">
<h4 class="anchored" data-anchor-id="dynamics-of-recursive-positional-encoding">4. Dynamics of Recursive Positional Encoding</h4>
<ul>
<li><p><strong>4.1 LSTM-based Encoding</strong>: Uses LSTMs to encode sequential position information.</p>
<p><span class="math inline">\(h_t = \text{LSTM}(x_t, h_{t-1})\)</span> <span class="math inline">\(P(t) = W_ph_t\)</span></p></li>
<li><p><strong>4.2 Latest Variation: Neural ODE</strong>: Models continuous-time dynamics, overcoming the limitations of discrete LSTMs.</p>
<p><span class="math inline">\(\frac{dh(t)}{dt} = f_\theta(h(t), t)\)</span> <span class="math inline">\(P(t) = \int_0^t f_\theta(h(\tau), \tau)d\tau\)</span></p></li>
</ul>
</section>
<section id="quantum-mechanical-interpretation-of-complex-positional-encoding" class="level4">
<h4 class="anchored" data-anchor-id="quantum-mechanical-interpretation-of-complex-positional-encoding">5. Quantum Mechanical Interpretation of Complex Positional Encoding</h4>
<ul>
<li><p><strong>5.1 Complex Embedding Representation</strong>: Represents position information in complex form.</p>
<p><span class="math inline">\(z(i) = r(i)e^{i\phi(i)}\)</span></p>
<p>where <span class="math inline">\(r\)</span> is the size of the position, and <span class="math inline">\(\phi\)</span> is the phase angle.</p></li>
<li><p><strong>5.2 Phase Shift Theorem</strong>: Represents position shifts as rotations on the complex plane.</p>
<p><span class="math inline">\(z(i+j) = z(i) \cdot e^{i\omega j}\)</span></p>
<p>where <span class="math inline">\(\omega\)</span> is a learnable frequency parameter.</p></li>
</ul>
</section>
<section id="hybrid-approach" class="level4">
<h4 class="anchored" data-anchor-id="hybrid-approach">6. Hybrid Approach</h4>
<ul>
<li><p><strong>6.1 Composite Positional Encoding:</strong> <span class="math inline">\(P(i)=αP_{abs}(i)+βP_{rel}(i)\)</span></p>
<p><span class="math inline">\(P(i)=αP_{abs}  (i)+βP_{rel}(i)\)</span> α, β = learnable weights</p></li>
<li><p><strong>6.2 Dynamic Positional Encoding:</strong></p>
<p><span class="math inline">\(P(i) = \text{MLP}(i, \text{Context})\)</span> Learning context-dependent positional representations</p></li>
</ul>
</section>
<section id="experimental-performance-comparison-glue-benchmark" class="level4">
<h4 class="anchored" data-anchor-id="experimental-performance-comparison-glue-benchmark">7. Experimental Performance Comparison (GLUE Benchmark)</h4>
<p>The following is the result of an experimental performance comparison of various positional encoding methods on the GLUE benchmark. (Actual performance may vary depending on model structure, data, hyperparameter settings, etc.)</p>
<table class="caption-top table">
<colgroup>
<col style="width: 29%">
<col style="width: 14%">
<col style="width: 24%">
<col style="width: 31%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Method</th>
<th style="text-align: left;">Accuracy</th>
<th style="text-align: left;">Inference Time (ms)</th>
<th style="text-align: left;">Memory Usage (GB)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Absolute (Sinusoidal)</td>
<td style="text-align: left;">88.2</td>
<td style="text-align: left;">12.3</td>
<td style="text-align: left;">2.1</td>
</tr>
<tr class="even">
<td style="text-align: left;">Relative (RoPE)</td>
<td style="text-align: left;">89.7</td>
<td style="text-align: left;">14.5</td>
<td style="text-align: left;">2.4</td>
</tr>
<tr class="odd">
<td style="text-align: left;">CNN Multi-Scale</td>
<td style="text-align: left;">87.9</td>
<td style="text-align: left;">13.8</td>
<td style="text-align: left;">3.2</td>
</tr>
<tr class="even">
<td style="text-align: left;">Complex (CLEX)</td>
<td style="text-align: left;">90.1</td>
<td style="text-align: left;">15.2</td>
<td style="text-align: left;">2.8</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Dynamic PE</td>
<td style="text-align: left;">90.3</td>
<td style="text-align: left;">17.1</td>
<td style="text-align: left;">3.5</td>
</tr>
</tbody>
</table>
</section>
<section id="latest-research-trends-2024" class="level4">
<h4 class="anchored" data-anchor-id="latest-research-trends-2024">8. Latest Research Trends (2024)</h4>
<p>Recently, new positional encoding techniques inspired by quantum computing, biological systems, and other fields have been researched.</p>
<ul>
<li><strong>Quantum Positional Encoding</strong>:
<ul>
<li>Utilizing qubit rotation gates: <span class="math inline">\(R_z(\theta_i)|x\rangle\)</span></li>
<li>Location search based on Grover’s algorithm</li>
</ul></li>
<li><strong>Biologically Inspired Encoding</strong>:
<ul>
<li>Applying the STDP (Spike-Timing-Dependent Plasticity) rule of synaptic plasticity: <span class="math inline">\(\Delta w_{ij} \propto e^{-\frac{|i-j|}{\tau}}\)</span></li>
</ul></li>
<li><strong>Graph Neural Network Integration</strong>:
<ul>
<li>Representing positions as nodes and relationships as edges: <span class="math inline">\(P(i) = \sum_{j \in \mathcal{N}(i)} \alpha_{ij}Wx_j\)</span></li>
</ul></li>
</ul>
</section>
<section id="selection-guidelines" class="level4">
<h4 class="anchored" data-anchor-id="selection-guidelines">9. Selection Guidelines</h4>
<ul>
<li><strong>Fixed-Length Sequences</strong>: Learnable PE. Low risk of overfitting and easy optimization.</li>
<li><strong>Variable Length/Extrapolation Needed</strong>: RoPE. Excellent length extensibility due to rotational invariance.</li>
<li><strong>Low-Latency Real-Time Processing</strong>: CNN-based. Optimized for parallel processing and hardware acceleration.</li>
<li><strong>Physical Signal Processing</strong>: Complex PE. Preserves frequency information and is compatible with Fourier transforms.</li>
<li><strong>Multimodal Data</strong>: Dynamic PE. Adapts to cross-modal context responses.</li>
</ul>
</section>
<section id="mathematical-appendix" class="level4">
<h4 class="anchored" data-anchor-id="mathematical-appendix">Mathematical Appendix</h4>
<ul>
<li><p><strong>Group-Theoretic Properties of RoPE</strong>:</p>
<p>Representation of the SO(2) rotation group: <span class="math inline">\(R(\theta) = \begin{bmatrix} \cos\theta &amp; -\sin\theta \\ \sin\theta &amp; \cos\theta \end{bmatrix}\)</span></p>
<p>This property guarantees the preservation of relative position in attention scores.</p></li>
<li><p><strong>Efficient Calculation of Relative Position Bias</strong>:</p>
<p>Utilizing Toeplitz matrix structure: <span class="math inline">\(B = [b_{i-j}]_{i,j}\)</span></p>
<p>Implementation with <span class="math inline">\(O(n\log n)\)</span> complexity using FFT is possible</p></li>
<li><p><strong>Gradient Flow of Complex PE</strong>:</p>
<p>Applying the Wirtinger derivative rule: <span class="math inline">\(\frac{\partial L}{\partial z} = \frac{1}{2}\left(\frac{\partial L}{\partial \text{Re}(z)} - i\frac{\partial L}{\partial \text{Im}(z)}\right)\)</span></p></li>
</ul>
<hr>
<p><strong>Conclusion</strong>: Positional encoding is a key element that has a significant impact on the performance of transformer models and has evolved in various ways beyond simple sine-cosine functions. Each method has its own strengths and weaknesses, as well as mathematical basis, and it is important to choose an appropriate method according to the characteristics and requirements of the problem. Recently, new positional encoding techniques inspired by various fields such as quantum computing and biology are being studied, and continuous development is expected in the future.</p>
</section>
</section>
</div>
</div>
</section>
</section>
<section id="transformers-overall-architecture" class="level2">
<h2 class="anchored" data-anchor-id="transformers-overall-architecture">8.4 Transformer’s Overall Architecture</h2>
<p>So far, we have looked at how the core components of the transformer have evolved. Now, let’s see how these elements are integrated into a complete architecture. This is the overall architecture of the transformer.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../../assets/images/transformer/tr_architecture.png" class="img-fluid figure-img"></p>
<figcaption>Transformer Architecture</figcaption>
</figure>
</div>
<p><em>Image source: The Illustrated Transformer (Jay Alammar, 2018) CC BY 4.0 License</em></p>
<p>For educational purposes, the source code of the transformer implemented is in chapter_08/transformer. This implementation was modified with reference to Harvard NLP Group’s The Annotated Transformer. The main modifications are as follows:</p>
<ol type="1">
<li><strong>Modularization:</strong> The implementation that was in one file was divided into several modules to increase readability and reusability.</li>
<li><strong>Pre-LN structure adoption:</strong> Unlike the original paper, we used a Pre-LN structure that applies layer normalization <em>before</em> attention/feedforward operations. (Recent studies have reported that Pre-LN is more favorable for learning stability and performance.)</li>
<li><strong><code>TransformerConfig</code> class addition:</strong> A separate class was introduced for model settings to make hyperparameter management easier.</li>
<li><strong>PyTorch-style implementation:</strong> We used PyTorch’s features such as <code>nn.ModuleList</code> to make the code more concise and intuitive.</li>
<li>The Noam optimizer was implemented but not used.</li>
</ol>
<section id="integration-of-basic-components" class="level3">
<h3 class="anchored" data-anchor-id="integration-of-basic-components">8.4.1 Integration of Basic Components</h3>
<p>The transformer is largely composed of an <strong>encoder</strong> and a <strong>decoder</strong>, each consisting of the following components:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 19%">
<col style="width: 38%">
<col style="width: 41%">
</colgroup>
<thead>
<tr class="header">
<th>Component</th>
<th>Encoder</th>
<th>Decoder</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Multi-head attention</strong></td>
<td>Self-attention (Self-Attention)</td>
<td>Masked self-attention (Masked Self-Attention)</td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td>Encoder-decoder attention (Encoder-Decoder Attention)</td>
</tr>
<tr class="odd">
<td><strong>Feedforward network</strong></td>
<td>Applied independently to each position</td>
<td>Applied independently to each position</td>
</tr>
<tr class="even">
<td><strong>Residual connection</strong></td>
<td>Adds the input and output of each sub-layer (attention, feedforward)</td>
<td>Adds the input and output of each sub-layer (attention, feedforward)</td>
</tr>
<tr class="odd">
<td><strong>Layer normalization</strong></td>
<td>Applied to the input of each sub-layer (Pre-LN)</td>
<td>Applied to the input of each sub-layer (Pre-LN)</td>
</tr>
</tbody>
</table>
<p><strong>Encoder Layer - Code</strong></p>
<div id="cell-60" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> TransformerEncoderLayer(nn.Module):</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config):</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.attention <span class="op">=</span> MultiHeadAttention(config)</span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.feed_forward <span class="op">=</span> FeedForward(config)</span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a>        <span class="co"># SublayerConnection for Pre-LN structure</span></span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.sublayer <span class="op">=</span> nn.ModuleList([</span>
<span id="cb33-8"><a href="#cb33-8" aria-hidden="true" tabindex="-1"></a>            SublayerConnection(config) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>)</span>
<span id="cb33-9"><a href="#cb33-9" aria-hidden="true" tabindex="-1"></a>        ])</span>
<span id="cb33-10"><a href="#cb33-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-11"><a href="#cb33-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x, attention_mask<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb33-12"><a href="#cb33-12" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.sublayer[<span class="dv">0</span>](x, <span class="kw">lambda</span> x: <span class="va">self</span>.attention(x, x, x, attention_mask))</span>
<span id="cb33-13"><a href="#cb33-13" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.sublayer[<span class="dv">1</span>](x, <span class="va">self</span>.feed_forward)</span>
<span id="cb33-14"><a href="#cb33-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<ul>
<li><strong>Multi-Head Attention:</strong> Calculates the relationship between all pairs of positions in the input sequence in parallel. Each head analyzes the sequence from a different perspective and combines the results to capture rich contextual information. (In the example “The cat sits on the mat”, different heads learn relationships such as subject-verb, prepositional phrase, article-noun, etc.)</li>
<li><strong>Feed-Forward Network:</strong> A network composed of two linear transformations and a GELU activation function, applied <em>independently</em> to each position.</li>
</ul>
<div id="cell-62" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> FeedForward(nn.Module):</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config):</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.linear1 <span class="op">=</span> nn.Linear(config.hidden_size, config.intermediate_size)</span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.linear2 <span class="op">=</span> nn.Linear(config.intermediate_size, config.hidden_size)</span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.activation <span class="op">=</span> nn.GELU()</span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.linear1(x)</span>
<span id="cb34-10"><a href="#cb34-10" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.activation(x)</span>
<span id="cb34-11"><a href="#cb34-11" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.linear2(x)</span>
<span id="cb34-12"><a href="#cb34-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>The reason a feedforward network is needed is related to the information density of the attention output. The result of the attention operation (<span class="math inline">\(\text{Attention}(Q, K, V) = \text{softmax}(\frac{QK^T}{\sqrt{d_k}})V\)</span>) is a weighted sum of <span class="math inline">\(V\)</span> vectors, with contextual information densely packed in the <span class="math inline">\(d_{model}\)</span> dimension (512 in the paper). <strong>Applying the ReLU activation function directly may cause a significant portion of this dense information to be lost (ReLU sets negative values to 0)</strong>. Therefore, the feedforward network first expands the <span class="math inline">\(d_{model}\)</span> dimension to a larger dimension (<span class="math inline">\(4 \times d_{model}\)</span>, 2048 in the paper) to widen the representation space, applies ReLU (or GELU), and then reduces it back to the original dimension, adding non-linearity in this way.</p>
<div id="cell-64" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> W1(x)    <span class="co"># hidden_size -&gt; intermediate_size (512 -&gt; 2048)</span></span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> ReLU(x)  <span class="co"># or GELU</span></span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> W2(x)    <span class="co"># intermediate_size -&gt; hidden_size (2048 -&gt; 512)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<ul>
<li><strong>Residual Connection:</strong> This is a method of adding the input and output of each sublayer (multi-head attention or feedforward network). This alleviates the vanishing/exploding gradient problem and helps with learning deep networks. (See Chapter 7 Residual Connection).</li>
<li><strong>Layer Normalization:</strong> Applied to the <em>input</em> of each sublayer (Pre-LN).</li>
</ul>
<div id="cell-66" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LayerNorm(nn.Module):</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config):</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.gamma <span class="op">=</span> nn.Parameter(torch.ones(config.hidden_size))</span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.beta <span class="op">=</span> nn.Parameter(torch.zeros(config.hidden_size))</span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.eps <span class="op">=</span> config.layer_norm_eps</span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-8"><a href="#cb36-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb36-9"><a href="#cb36-9" aria-hidden="true" tabindex="-1"></a>        mean <span class="op">=</span> x.mean(<span class="op">-</span><span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb36-10"><a href="#cb36-10" aria-hidden="true" tabindex="-1"></a>        std <span class="op">=</span> (x <span class="op">-</span> mean).<span class="bu">pow</span>(<span class="dv">2</span>).mean(<span class="op">-</span><span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>).sqrt()</span>
<span id="cb36-11"><a href="#cb36-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.gamma <span class="op">*</span> (x <span class="op">-</span> mean) <span class="op">/</span> (std <span class="op">+</span> <span class="va">self</span>.eps) <span class="op">+</span> <span class="va">self</span>.beta</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Layer normalization is a technique proposed in the 2016 paper “Layer Normalization” by Ba, Kiros, and Hinton. While batch normalization performs normalization over the batch dimension, layer normalization computes the mean and variance over the feature dimension for each sample and normalizes it.</p>
<p><strong>Advantages of Layer Normalization</strong></p>
<ol type="1">
<li><strong>Batch size independence:</strong> It is unaffected by batch size, making it stable in small batch sizes or online learning environments.</li>
<li><strong>Sequence length agnosticism:</strong> It is suitable for models that process variable-length sequences, such as RNNs and Transformers.</li>
<li><strong>Learning stabilization and acceleration:</strong> It stabilizes the input distribution of each layer, mitigating gradient vanishing/exploding problems and accelerating learning.</li>
</ol>
<p>In transformers, the Pre-LN method is used, applying layer normalization <em>before</em> passing through each sub-layer (multi-head attention, feed-forward network).</p>
<p><strong>Layer Normalization Visualization</strong></p>
<div id="cell-68" class="cell" data-execution_count="19">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_08.visualize_layer_norm <span class="im">import</span> visualize_layer_normalization</span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>visualize_layer_normalization()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="08_The Birth of Transformer_files/figure-html/cell-30-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>========================================
Input Data Shape: (2, 5, 6)
Mean Shape: (2, 5, 1)
Standard Deviation Shape: (2, 5, 1)
Normalized Data Shape: (2, 5, 6)
Gamma (Scale) Values:
 [0.95208258 0.9814341  0.8893665  0.88037934 1.08125258 1.135624  ]
Beta (Shift) Values:
 [-0.00720101  0.10035329  0.0361636  -0.06451198  0.03613956  0.15380366]
Scaled &amp; Shifted Data Shape: (2, 5, 6)
========================================</code></pre>
</div>
</div>
<p>The above figure shows the operation of Layer Normalization step by step.</p>
<ul>
<li><strong>Original Data (Top Left):</strong> The pre-normalized data is spread out and has an inconsistent mean and standard deviation.</li>
<li><strong>After Normalization (Top Right):</strong> The data is normalized to be near a mean of 0 and a standard deviation of 1.</li>
<li><strong>Scaling and Shifting (Center):</strong> Learnable parameters γ (gamma, scaling) and β (beta, shifting) are applied to give the data distribution a slight change, adjusting the model’s expressiveness.</li>
<li><strong>Heatmap (Bottom):</strong> It shows the individual value changes before and after normalization and after applying scale/shift based on the first batch of data.</li>
<li><strong>γ/β Values (Bottom Right):</strong> The values of γ and β for each hidden dimension are shown in a bar graph.</li>
</ul>
<p>In this way, Layer Normalization improves learning stability and speed by normalizing the input to each layer.</p>
<p><strong>Key Points:</strong></p>
<ul>
<li>Normalize each layer’s input (mean 0, standard deviation 1)</li>
<li>Adjust expressiveness using learnable scale (γ) and shift (β)</li>
<li>Unlike batch normalization, maintain sample independence</li>
</ul>
<p>The combination of these components (multi-head attention, feedforward network, residual connection, and Layer Normalization) maximizes the advantages of each element. Multi-head attention captures various aspects of the input sequence, feedforward networks add non-linearity, and residual connections and Layer Normalization enable stable learning even in deep networks.</p>
</section>
<section id="encoder-composition" class="level3">
<h3 class="anchored" data-anchor-id="encoder-composition">8.4.2 Encoder Composition</h3>
<p>The Transformer has an encoder-decoder structure for machine translation. The encoder understands the source language (e.g., English) and the decoder generates the target language (e.g., French). Although the encoder and decoder share multi-head attention and feed-forward networks as basic components, they are composed differently according to their purposes.</p>
<p><strong>Encoder vs Decoder Composition Comparison</strong></p>
<table class="caption-top table">
<colgroup>
<col style="width: 17%">
<col style="width: 31%">
<col style="width: 50%">
</colgroup>
<thead>
<tr class="header">
<th>Component</th>
<th>Encoder</th>
<th>Decoder</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Number of Attention Layers</td>
<td>1 (Self-Attention)</td>
<td>2 (Masked Self-Attention, Encoder-Decoder Attention)</td>
</tr>
<tr class="even">
<td>Masking Strategy</td>
<td>Only padding mask</td>
<td>Padding mask + causal mask</td>
</tr>
<tr class="odd">
<td>Context Processing</td>
<td>Bidirectional context processing</td>
<td>Unidirectional context processing (self-recurrent)</td>
</tr>
<tr class="even">
<td>Input Reference</td>
<td>Refers to its own input only</td>
<td>Refers to its own input + encoder output reference</td>
</tr>
</tbody>
</table>
<p>Several attention terms are summarized as follows:</p>
<p><strong>Attention Concept Summary</strong></p>
<table class="caption-top table">
<colgroup>
<col style="width: 14%">
<col style="width: 25%">
<col style="width: 4%">
<col style="width: 55%">
</colgroup>
<thead>
<tr class="header">
<th>Attention Type</th>
<th>Characteristics</th>
<th>Description Location</th>
<th>Core Concepts</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Basic Attention</td>
<td>- Calculates similarity using the same word vector<br>- Creates context information using simple weighted sum<br>- Simplified version of seq2seq model application</td>
<td>8.2.2</td>
<td>- Calculates similarity between word vectors using dot product<br>- Transforms weights using softmax<br>- Applies padding mask to all attention by default</td>
</tr>
<tr class="even">
<td>Self-Attention (Self-Attention)</td>
<td>- Separates Q, K, V spaces<br>- Independently optimizes each space<br>- Input sequence refers to itself<br>- Used in the encoder</td>
<td>8.2.3</td>
<td>- Separates roles of similarity calculation and information transmission<br>- Uses learnable Q, K, V transformations<br>- Enables bidirectional context processing</td>
</tr>
<tr class="odd">
<td>Masked Self-Attention</td>
<td>- Blocks future information<br>- Uses causal mask<br>- Used in the decoder</td>
<td>8.2.5</td>
<td>- Masks future information using an upper triangular matrix<br>- Enables self-recurrent generation<br>- Enables unidirectional context processing</td>
</tr>
<tr class="even">
<td>Cross (Encoder-Decoder) Attention</td>
<td>- Query: Decoder state<br>- Key, Value: Encoder output<br>- Also called cross attention<br>- Used in the decoder</td>
<td>8.4.3</td>
<td>- Decoder refers to encoder information<br>- Calculates relationships between two sequences<br>- Reflects context during translation/generation</td>
</tr>
</tbody>
</table>
<p>In the Transformer, self, masked, and cross attention names are used. The attention mechanism is the same, but it is distinguished according to the source of Q, K, and V.</p>
<p><strong>Encoder Composition Components</strong> | Component | Description | | ————————– | ————————————————————————————- | | Embeddings | Converts input tokens into vectors and adds position information to encode the meaning and order of the input sequence. | | TransformerEncoderLayer (x N) | Stacks the same layer multiple times to hierarchically extract more abstract and complex features from the input sequence. | | LayerNorm | Normalizes the distribution of the final output’s characteristics, stabilizing it and putting it in a form that is easy for the decoder to reference. |</p>
<div id="cell-71" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> TransformerEncoder(nn.Module):</span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config):</span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.embeddings <span class="op">=</span> Embeddings(config)</span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.layers <span class="op">=</span> nn.ModuleList([</span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a>            TransformerEncoderLayer(config) </span>
<span id="cb39-7"><a href="#cb39-7" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(config.num_hidden_layers)</span>
<span id="cb39-8"><a href="#cb39-8" aria-hidden="true" tabindex="-1"></a>        ])</span>
<span id="cb39-9"><a href="#cb39-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.norm <span class="op">=</span> LayerNorm(config)</span>
<span id="cb39-10"><a href="#cb39-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb39-11"><a href="#cb39-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, input_ids, attention_mask<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb39-12"><a href="#cb39-12" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.embeddings(input_ids)</span>
<span id="cb39-13"><a href="#cb39-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-14"><a href="#cb39-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i, layer <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="va">self</span>.layers):</span>
<span id="cb39-15"><a href="#cb39-15" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> layer(x, attention_mask)</span>
<span id="cb39-16"><a href="#cb39-16" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb39-17"><a href="#cb39-17" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> <span class="va">self</span>.norm(x)</span>
<span id="cb39-18"><a href="#cb39-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> output</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>The encoder consists of an embedding layer, multiple encoder layers, and a final normalization layer.</p>
<p><strong>1. Self-Attention Mechanism (Example)</strong></p>
<p>The self-attention of the encoder calculates the relationship between all word pairs in the input sequence, enriching the contextual information for each word.</p>
<ul>
<li><strong>Example:</strong> “The patient bear can bear the pain no longer.”</li>
<li><strong>Role:</strong> When understanding the meaning of the second ‘bear’, self-attention considers the relationship with <em>all</em> words in the sentence, such as ‘patient’ (patient), ‘bear’ (bear), and ‘pain’ (pain). This allows it to accurately understand that ‘bear’ is used to mean “endure” or “tolerate” (bidirectional context processing).</li>
</ul>
<p><strong>2. Importance of Dropout Location</strong></p>
<p>Dropout plays a crucial role in preventing overfitting and improving learning stability. In the transformer encoder, dropout is applied at the following locations:</p>
<ul>
<li><strong>After embedding output:</strong> Immediately after combining token embeddings and position information.</li>
<li><strong>After each sublayer (attention, FFN) output:</strong> Follows the Pre-LN structure (normalization → sublayer → dropout → residual connection).</li>
<li><strong>Inside FFN:</strong> After the first linear transformation and ReLU activation function.</li>
</ul>
<p>This dropout arrangement controls the flow of information, preventing the model from relying too heavily on specific features and improving generalization performance.</p>
<p><strong>3. Encoder Stack Structure</strong></p>
<p>The transformer encoder has a stacked structure of identical encoder layers.</p>
<ul>
<li><strong>Original paper:</strong> Uses 6 encoder layers.</li>
<li><strong>Role division</strong>:
<ul>
<li><strong>Lower layers:</strong> Learn surface-level language patterns, such as adjacent words and punctuation.</li>
<li><strong>Middle layers:</strong> Learn grammatical structures.</li>
<li><strong>Upper layers:</strong> Learn high-dimensional semantic relationships, such as coreference.</li>
</ul></li>
</ul>
<p>As the layers are stacked deeper, more abstract and complex features can be learned. Subsequent studies have introduced models with many more layers (BERT-base: 12 layers, GPT-3: 96 layers, PaLM: 118 layers) thanks to advances in hardware and learning techniques (Pre-LayerNorm, gradient clipping, learning rate warmup, mixed precision training, gradient accumulation, etc.).</p>
<p><strong>4. Encoder’s Final Output and Decoder Utilization</strong></p>
<p>The final output of the encoder is a vector representation that richly contains contextual information for each input token. This output is used as <strong>Key</strong> and <strong>Value</strong> in the decoder’s <strong>Encoder-Decoder Attention (Cross-Attention)</strong>. The decoder references the encoder’s output to generate each token of the output sequence, performing accurate translation/generation considering the context of the original sentence.</p>
</section>
<section id="configuration-of-the-decoder" class="level3">
<h3 class="anchored" data-anchor-id="configuration-of-the-decoder">8.4.3 Configuration of the Decoder</h3>
<p>The decoder is similar to the encoder, but it differs in that it generates output autoregressively.</p>
<p><strong>Entire Code for Decoder Layer</strong></p>
<div id="cell-74" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> TransformerDecoderLayer(nn.Module):</span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config):</span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.self_attn <span class="op">=</span> MultiHeadAttention(config)</span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.cross_attn <span class="op">=</span> MultiHeadAttention(config)</span>
<span id="cb40-6"><a href="#cb40-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.feed_forward <span class="op">=</span> FeedForward(config)</span>
<span id="cb40-7"><a href="#cb40-7" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb40-8"><a href="#cb40-8" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Pre-LN을 위한 레이어 정규화</span></span>
<span id="cb40-9"><a href="#cb40-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.norm1 <span class="op">=</span> LayerNorm(config)</span>
<span id="cb40-10"><a href="#cb40-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.norm2 <span class="op">=</span> LayerNorm(config)</span>
<span id="cb40-11"><a href="#cb40-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.norm3 <span class="op">=</span> LayerNorm(config)</span>
<span id="cb40-12"><a href="#cb40-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(config.dropout_prob)</span>
<span id="cb40-13"><a href="#cb40-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-14"><a href="#cb40-14" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x, memory, src_mask<span class="op">=</span><span class="va">None</span>, tgt_mask<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb40-15"><a href="#cb40-15" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Pre-LN 구조</span></span>
<span id="cb40-16"><a href="#cb40-16" aria-hidden="true" tabindex="-1"></a>        m <span class="op">=</span> <span class="va">self</span>.norm1(x)</span>
<span id="cb40-17"><a href="#cb40-17" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.dropout(<span class="va">self</span>.self_attn(m, m, m, tgt_mask))</span>
<span id="cb40-18"><a href="#cb40-18" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb40-19"><a href="#cb40-19" aria-hidden="true" tabindex="-1"></a>        m <span class="op">=</span> <span class="va">self</span>.norm2(x)</span>
<span id="cb40-20"><a href="#cb40-20" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.dropout(<span class="va">self</span>.cross_attn(m, memory, memory, src_mask))</span>
<span id="cb40-21"><a href="#cb40-21" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb40-22"><a href="#cb40-22" aria-hidden="true" tabindex="-1"></a>        m <span class="op">=</span> <span class="va">self</span>.norm3(x)</span>
<span id="cb40-23"><a href="#cb40-23" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.dropout(<span class="va">self</span>.feed_forward(m))</span>
<span id="cb40-24"><a href="#cb40-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><strong>Key Components and Roles of the Decoder</strong></p>
<table class="caption-top table">
<colgroup>
<col style="width: 9%">
<col style="width: 43%">
<col style="width: 47%">
</colgroup>
<thead>
<tr class="header">
<th>Sublayer</th>
<th>Role</th>
<th>Implementation Features</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Masked Self-Attention</td>
<td>Understanding relationships between words in the output sequence generated so far, preventing reference to future information (self-recursively generated)</td>
<td><code>tgt_mask</code> (causal mask + padding mask) used, <code>self.self_attn</code></td>
</tr>
<tr class="even">
<td>Encoder-Decoder Attention (Cross-Attention)</td>
<td>The decoder references the encoder’s output (contextual information of the input sentence) to obtain information related to the word being generated</td>
<td><code>Q</code>: decoder, <code>K</code>, <code>V</code>: encoder, <code>src_mask</code> (padding mask) used, <code>self.cross_attn</code></td>
</tr>
<tr class="odd">
<td>Feed Forward Network</td>
<td>Independently transforming representations at each position to create richer representations</td>
<td>Same structure as the encoder, <code>self.feed_forward</code></td>
</tr>
<tr class="even">
<td>Layer Normalization (LayerNorm)</td>
<td>Normalizing inputs to each sublayer (Pre-LN), improving learning stability and performance</td>
<td><code>self.norm1</code>, <code>self.norm2</code>, <code>self.norm3</code></td>
</tr>
<tr class="odd">
<td>Dropout</td>
<td>Preventing overfitting, improving generalization performance</td>
<td>Applied to the output of each sublayer, <code>self.dropout</code></td>
</tr>
<tr class="even">
<td>Residual Connection</td>
<td>Mitigating gradient vanishing/exploding problems in deep networks, improving information flow</td>
<td>Adding the input and output of each sublayer</td>
</tr>
</tbody>
</table>
<p><strong>1. Masked Self-Attention (Masked Self-Attention)</strong> * <strong>Role:</strong> It makes the decoder generate output autoregressively, i.e., it prevents the model from referencing tokens that have not yet been generated. For example, when translating “I love you”, after generating “나는”, it cannot reference the token “사랑해” which has not been generated yet when generating “너를”. * <strong>Implementation:</strong> It uses a <code>tgt_mask</code> that combines causal masks and padding masks. The causal mask fills the upper triangular matrix with <code>-inf</code>, making the attention weights for future tokens 0. (Refer to section 8.2.5). This mask is applied in the <code>self.self_attn(m, m, m, tgt_mask)</code> part of the <code>forward</code> method of <code>TransformerDecoderLayer</code>.</p>
<p><strong>2. Encoder-Decoder Attention (Cross-Attention)</strong></p>
<ul>
<li><strong>Role:</strong> It allows the decoder to reference the output of the encoder (context information of the input sentence) to obtain relevant information for the word being generated. This plays a crucial role in translation tasks, as it enables the decoder to accurately understand the meaning of the original sentence and choose the correct translation words.</li>
<li><strong>Implementation:</strong>
<ul>
<li><strong>Query (Q):</strong> The current state of the decoder (output of masked self-attention)</li>
<li><strong>Key (K):</strong> Output of the encoder (<code>memory</code>)</li>
<li><strong>Value (V):</strong> Output of the encoder (<code>memory</code>)</li>
<li>It uses a <code>src_mask</code> (padding mask) to ignore padding tokens in the encoder output.</li>
<li>This attention is performed in the <code>self.cross_attn(m, memory, memory, src_mask)</code> part of the <code>forward</code> method of <code>TransformerDecoderLayer</code>, where <code>memory</code> represents the output of the encoder.</li>
</ul></li>
</ul>
<p><strong>3. Decoder Stack Structure</strong></p>
<div id="cell-76" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> TransformerDecoder(nn.Module):</span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config):</span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.embeddings <span class="op">=</span> Embeddings(config)</span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.layers <span class="op">=</span> nn.ModuleList([</span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a>            TransformerDecoderLayer(config)</span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(config.num_hidden_layers)</span>
<span id="cb41-8"><a href="#cb41-8" aria-hidden="true" tabindex="-1"></a>        ])</span>
<span id="cb41-9"><a href="#cb41-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.norm <span class="op">=</span> LayerNorm(config)</span>
<span id="cb41-10"><a href="#cb41-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-11"><a href="#cb41-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x, memory, src_mask<span class="op">=</span><span class="va">None</span>, tgt_mask<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb41-12"><a href="#cb41-12" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.embeddings(x)</span>
<span id="cb41-13"><a href="#cb41-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> layer <span class="kw">in</span> <span class="va">self</span>.layers:</span>
<span id="cb41-14"><a href="#cb41-14" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> layer(x, memory, src_mask, tgt_mask)</span>
<span id="cb41-15"><a href="#cb41-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.norm(x)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<ul>
<li>The decoder is composed of multiple layers (6 in the original paper) of <code>TransformerDecoderLayer</code>.</li>
<li>Each layer performs masked self-attention, encoder-decoder attention, and feedforward network sequentially.</li>
<li>Pre-LN structure and residual connection are applied to each sublayer, allowing for stable learning even in deep networks.</li>
<li>The <code>forward</code> method of the <code>TransformerDecoder</code> class takes input <code>x</code> (decoder input), <code>memory</code> (encoder output), <code>src_mask</code> (encoder padding mask), and <code>tgt_mask</code> (decoder mask) and returns the final output after passing through the decoder layers sequentially.</li>
</ul>
<p><strong>Number of Encoder/Decoder Layers by Model</strong></p>
<table class="caption-top table">
<colgroup>
<col style="width: 25%">
<col style="width: 8%">
<col style="width: 20%">
<col style="width: 13%">
<col style="width: 13%">
<col style="width: 17%">
</colgroup>
<thead>
<tr class="header">
<th>Model</th>
<th>Year</th>
<th>Structure</th>
<th>Encoder Layers</th>
<th>Decoder Layers</th>
<th>Total Parameters</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Original Transformer</td>
<td>2017</td>
<td>Encoder-Decoder</td>
<td>6</td>
<td>6</td>
<td>65M</td>
</tr>
<tr class="even">
<td>BERT-base</td>
<td>2018</td>
<td>Encoder-only</td>
<td>12</td>
<td>-</td>
<td>110M</td>
</tr>
<tr class="odd">
<td>GPT-2</td>
<td>2019</td>
<td>Decoder-only</td>
<td>-</td>
<td>48</td>
<td>1.5B</td>
</tr>
<tr class="even">
<td>T5-base</td>
<td>2020</td>
<td>Encoder-Decoder</td>
<td>12</td>
<td>12</td>
<td>220M</td>
</tr>
<tr class="odd">
<td>GPT-3</td>
<td>2020</td>
<td>Decoder-only</td>
<td>-</td>
<td>96</td>
<td>175B</td>
</tr>
<tr class="even">
<td>PaLM</td>
<td>2022</td>
<td>Decoder-only</td>
<td>-</td>
<td>118</td>
<td>540B</td>
</tr>
<tr class="odd">
<td>Gemma-2</td>
<td>2024</td>
<td>Decoder-only</td>
<td>-</td>
<td>18-36</td>
<td>2B-27B</td>
</tr>
</tbody>
</table>
<p>Recent models have been able to effectively learn much more layers thanks to advanced training techniques like Pre-LN. Deeper decoders can learn more abstract and complex language patterns, resulting in improved performance in various natural language processing tasks such as translation and text generation.</p>
<p><strong>4. Generating Decoder Output and Termination Conditions</strong></p>
<ul>
<li><strong>Output Generation:</strong> The <code>generator</code> (linear layer) of the <code>Transformer</code> class converts the final output of the decoder into a logit vector of size <code>vocab_size</code>, applies <code>log_softmax</code> to obtain the probability distribution of each token, and predicts the next token based on this distribution.</li>
</ul>
<div id="cell-78" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 최종 출력 생성 (설명용)</span></span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> <span class="va">self</span>.generator(decoder_output)</span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a><span class="cf">return</span> F.log_softmax(output, dim<span class="op">=-</span><span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<ul>
<li><strong>Termination Conditions</strong>
<ol type="1">
<li><strong>Maximum Length Reached:</strong> When the predetermined maximum output length is reached.</li>
<li><strong>Custom Termination Condition:</strong> When a specific condition (e.g., punctuation) is met.</li>
<li><strong>Special Token Generation:</strong> When a special token indicating the end of a sentence (<code>&lt;eos&gt;</code>, <code>&lt;/s&gt;</code>, etc.) is generated. The decoder learns to add this special token at the end of a sentence during training.</li>
</ol></li>
<li><strong>Token Generation Strategies</strong></li>
</ul>
<p>Although not typically included in the decoder, there are token generation strategies that affect the output generation results.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 16%">
<col style="width: 13%">
<col style="width: 32%">
<col style="width: 17%">
<col style="width: 20%">
</colgroup>
<thead>
<tr class="header">
<th>Generation Strategy</th>
<th>Mechanism</th>
<th>Advantages</th>
<th>Disadvantages</th>
<th>Example</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Greedy Search</strong></td>
<td>Selects the token with the highest probability at each step</td>
<td>Fast, simple to implement</td>
<td>May result in local optima, lacks diversity</td>
<td>“I” followed by → “school” (highest probability)</td>
</tr>
<tr class="even">
<td><strong>Beam Search</strong></td>
<td>Tracks the top <code>k</code> paths simultaneously</td>
<td>Wide exploration, potentially better results</td>
<td>High computational cost, limited diversity</td>
<td><code>k=2</code>: Maintains “I go to school” and “I go home”, then proceeds to the next step</td>
</tr>
<tr class="odd">
<td><strong>Top-k Sampling</strong></td>
<td>Selects a token from the top <code>k</code> probabilities in proportion to their probabilities</td>
<td>Appropriate diversity, prevents unusual tokens</td>
<td>Difficult to set the value of <code>k</code>, performance depends on context</td>
<td><code>k=3</code>: After “I”, selects from {“school”, “home”, “park”} based on probability</td>
</tr>
<tr class="even">
<td><strong>Nucleus Sampling</strong></td>
<td>Selects a token from the set of tokens with cumulative probability up to <code>p</code></td>
<td>Dynamic candidate set, flexible in context</td>
<td>Requires tuning of <code>p</code>, increased computational complexity</td>
<td><code>p=0.9</code>: After “I”, selects from {“school”, “home”, “park”, “meal”} without exceeding a cumulative probability of 0.9</td>
</tr>
<tr class="odd">
<td><strong>Temperature Sampling</strong></td>
<td>Adjusts the temperature of the probability distribution (lower means more certain, higher means more diverse)</td>
<td>Controls output creativity, simple to implement</td>
<td>Too high may result in inappropriate output, too low may result in repetitive text generation</td>
<td><code>T=0.5</code>: Emphasizes high probabilities, <code>T=1.5</code>: increases the possibility of selecting lower probabilities</td>
</tr>
</tbody>
</table>
<p>These token generation strategies are typically implemented as separate classes or functions from the decoder.</p>
</section>
<section id="description-of-the-overall-structure" class="level3">
<h3 class="anchored" data-anchor-id="description-of-the-overall-structure">8.4.4 Description of the Overall Structure</h3>
<p>So far, we have understood the design intention and operating principle of the transformer. Based on the contents described up to 8.4.3, let’s take a look at the overall structure of the transformer. The implementation was modified structurally, such as modularization, referencing Harvard NLP, and written as concisely as possible for learning purposes. In an actual production environment, additional requirements such as type hinting for code stability, efficient processing of multi-dimensional tensors, input validation and error handling, memory optimization, and extensibility to support various settings are necessary.</p>
<p>The code is in the <code>chapter_08/transformer</code> directory.</p>
<p><strong>Role and Implementation of the Embedding Layer</strong></p>
<p>The first step of the transformer is the embedding layer that converts input tokens into vector space. The input is a sequence of integer token IDs (e.g., [101, 2045, 3012, …]), where each token ID is a unique index in the vocabulary dictionary. The embedding layer maps this ID to a high-dimensional vector (embedding vector).</p>
<p>The embedding dimension has a significant impact on the model’s performance. A larger dimension can express richer semantic information but increases computational cost, while a smaller dimension is the opposite.</p>
<p>After passing through the embedding layer, the tensor dimension changes as follows:</p>
<ul>
<li>Input: (batch_size, seq_length) → Output: (batch_size, seq_length, hidden_size)</li>
<li>Example: (32, 50) → (32, 50, 768)</li>
</ul>
<p>The following is an example code for performing embedding in the transformer.</p>
<div id="cell-81" class="cell" data-execution_count="20">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_08.transformer.config <span class="im">import</span> TransformerConfig</span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_08.transformer.embeddings <span class="im">import</span> Embeddings</span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a configuration object</span></span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a>config <span class="op">=</span> TransformerConfig()</span>
<span id="cb43-7"><a href="#cb43-7" aria-hidden="true" tabindex="-1"></a>config.vocab_size <span class="op">=</span> <span class="dv">1000</span>  <span class="co"># Vocabulary size</span></span>
<span id="cb43-8"><a href="#cb43-8" aria-hidden="true" tabindex="-1"></a>config.hidden_size <span class="op">=</span> <span class="dv">768</span>  <span class="co"># Embedding dimension</span></span>
<span id="cb43-9"><a href="#cb43-9" aria-hidden="true" tabindex="-1"></a>config.max_position_embeddings <span class="op">=</span> <span class="dv">512</span>  <span class="co"># Maximum sequence length</span></span>
<span id="cb43-10"><a href="#cb43-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-11"><a href="#cb43-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Create an embedding layer</span></span>
<span id="cb43-12"><a href="#cb43-12" aria-hidden="true" tabindex="-1"></a>embedding_layer <span class="op">=</span> Embeddings(config)</span>
<span id="cb43-13"><a href="#cb43-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-14"><a href="#cb43-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate random input tokens</span></span>
<span id="cb43-15"><a href="#cb43-15" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb43-16"><a href="#cb43-16" aria-hidden="true" tabindex="-1"></a>seq_length <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb43-17"><a href="#cb43-17" aria-hidden="true" tabindex="-1"></a>input_ids <span class="op">=</span> torch.tensor([</span>
<span id="cb43-18"><a href="#cb43-18" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">1</span>, <span class="dv">5</span>, <span class="dv">9</span>, <span class="dv">2</span>],  <span class="co"># First sequence</span></span>
<span id="cb43-19"><a href="#cb43-19" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">6</span>, <span class="dv">3</span>, <span class="dv">7</span>, <span class="dv">4</span>]   <span class="co"># Second sequence</span></span>
<span id="cb43-20"><a href="#cb43-20" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb43-21"><a href="#cb43-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-22"><a href="#cb43-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform embedding</span></span>
<span id="cb43-23"><a href="#cb43-23" aria-hidden="true" tabindex="-1"></a>embedded <span class="op">=</span> embedding_layer(input_ids)</span>
<span id="cb43-24"><a href="#cb43-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-25"><a href="#cb43-25" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Input shape: </span><span class="sc">{</span>input_ids<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb43-26"><a href="#cb43-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Output: Input shape: torch.Size([2, 4])</span></span>
<span id="cb43-27"><a href="#cb43-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-28"><a href="#cb43-28" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Shape after embedding: </span><span class="sc">{</span>embedded<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb43-29"><a href="#cb43-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Output: Shape after embedding: torch.Size([2, 4, 768])</span></span>
<span id="cb43-30"><a href="#cb43-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-31"><a href="#cb43-31" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Part of the embedding vector for the first token of the first sequence:"</span>)</span>
<span id="cb43-32"><a href="#cb43-32" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(embedded[<span class="dv">0</span>, <span class="dv">0</span>, :<span class="dv">10</span>])  <span class="co"># Print only the first 10 dimensions</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Input shape: torch.Size([2, 4])
Shape after embedding: torch.Size([2, 4, 768])

Part of the embedding vector for the first token of the first sequence:
tensor([-0.7838, -0.9194,  0.4240, -0.8408, -0.0876,  2.0239,  1.3892, -0.4484,
        -0.6902,  1.1443], grad_fn=&lt;SliceBackward0&gt;)</code></pre>
</div>
</div>
<p><strong>Configuration Class</strong></p>
<p>The <code>TransformerConfig</code> class defines all hyperparameters of the model.</p>
<div id="cell-83" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> TransformerConfig:</span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.vocab_size <span class="op">=</span> <span class="dv">30000</span>          <span class="co"># Vocabulary size</span></span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hidden_size <span class="op">=</span> <span class="dv">768</span>           <span class="co"># Hidden layer dimension</span></span>
<span id="cb45-5"><a href="#cb45-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.num_hidden_layers <span class="op">=</span> <span class="dv">12</span>      <span class="co"># Number of encoder/decoder layers</span></span>
<span id="cb45-6"><a href="#cb45-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.num_attention_heads <span class="op">=</span> <span class="dv">12</span>    <span class="co"># Number of attention heads</span></span>
<span id="cb45-7"><a href="#cb45-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.intermediate_size <span class="op">=</span> <span class="dv">3072</span>    <span class="co"># FFN intermediate layer dimension</span></span>
<span id="cb45-8"><a href="#cb45-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hidden_dropout_prob <span class="op">=</span> <span class="fl">0.1</span>   <span class="co"># Hidden layer dropout probability</span></span>
<span id="cb45-9"><a href="#cb45-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.attention_probs_dropout_prob <span class="op">=</span> <span class="fl">0.1</span>  <span class="co"># Attention dropout probability</span></span>
<span id="cb45-10"><a href="#cb45-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.max_position_embeddings <span class="op">=</span> <span class="dv">512</span>  <span class="co"># Maximum sequence length</span></span>
<span id="cb45-11"><a href="#cb45-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.layer_norm_eps <span class="op">=</span> <span class="fl">1e-12</span>      <span class="co"># Layer normalization epsilon</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><code>vocab_size</code> is the total number of unique tokens that the model can process. Here, we assume word-level tokenization for simple implementation and set it to 30,000. In actual language models, various subword tokenizers such as BPE (Byte Pair Encoding), Unigram, and WordPiece are used, and in this case, <code>vocab_size</code> can be smaller. For example, the word ‘playing’ can be separated into ‘play’ and ‘ing’ and expressed with only two subwords.</p>
<p><strong>Tensor Dimension Change of Attention</strong></p>
<p>In multi-head attention, each head rearranges the dimension of the input tensor to calculate attention independently.</p>
<div id="cell-85" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MultiHeadAttention(nn.Module):</span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, query, key, value, mask<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a>        batch_size <span class="op">=</span> query.size(<span class="dv">0</span>)</span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb46-5"><a href="#cb46-5" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Linear transformations and head splitting</span></span>
<span id="cb46-6"><a href="#cb46-6" aria-hidden="true" tabindex="-1"></a>        query <span class="op">=</span> <span class="va">self</span>.linears[<span class="dv">0</span>](query).view(batch_size, <span class="op">-</span><span class="dv">1</span>, <span class="va">self</span>.h, <span class="va">self</span>.d_k).transpose(<span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb46-7"><a href="#cb46-7" aria-hidden="true" tabindex="-1"></a>        key <span class="op">=</span> <span class="va">self</span>.linears[<span class="dv">1</span>](key).view(batch_size, <span class="op">-</span><span class="dv">1</span>, <span class="va">self</span>.h, <span class="va">self</span>.d_k).transpose(<span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb46-8"><a href="#cb46-8" aria-hidden="true" tabindex="-1"></a>        value <span class="op">=</span> <span class="va">self</span>.linears[<span class="dv">2</span>](value).view(batch_size, <span class="op">-</span><span class="dv">1</span>, <span class="va">self</span>.h, <span class="va">self</span>.d_k).transpose(<span class="dv">1</span>, <span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>The dimension transformation process is as follows.</p>
<ol type="1">
<li>Input: (batch_size, seq_len, d_model)</li>
<li>Linear transformation: (batch_size, seq_len, d_model)</li>
<li><code>view</code>: (batch_size, seq_len, h, d_k)</li>
<li><code>transpose</code>: (batch_size, h, seq_len, d_k)</li>
</ol>
<p>Here, h is the number of heads, and d_k is the dimension of each head (d_model / h). Through this dimension rearrangement, each head calculates attention independently.</p>
<p><strong>Integrated Structure of Transformer</strong></p>
<p>Finally, let’s look at the <code>Transformer</code> class that integrates all the components.</p>
<div id="cell-87" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Transformer(nn.Module):</span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config: TransformerConfig):</span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb47-4"><a href="#cb47-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.encoder <span class="op">=</span> TransformerEncoder(config)</span>
<span id="cb47-5"><a href="#cb47-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.decoder <span class="op">=</span> TransformerDecoder(config)</span>
<span id="cb47-6"><a href="#cb47-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.generator <span class="op">=</span> nn.Linear(config.hidden_size, config.vocab_size)</span>
<span id="cb47-7"><a href="#cb47-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._init_weights()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>The transformer consists of three main components:</p>
<ol type="1">
<li>Encoder: processes the input sequence.</li>
<li>Decoder: generates the output sequence.</li>
<li>Generator: converts the decoder output to vocabulary probabilities.</li>
</ol>
<p>The <code>forward</code> method processes data in the following order:</p>
<div id="cell-89" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> forward(<span class="va">self</span>, src, tgt, src_mask<span class="op">=</span><span class="va">None</span>, tgt_mask<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Encoder-decoder processing</span></span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a>    encoder_output <span class="op">=</span> <span class="va">self</span>.encode(src, src_mask)</span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a>    decoder_output <span class="op">=</span> <span class="va">self</span>.decode(encoder_output, src_mask, tgt, tgt_mask)</span>
<span id="cb48-5"><a href="#cb48-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb48-6"><a href="#cb48-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Generate final output</span></span>
<span id="cb48-7"><a href="#cb48-7" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> <span class="va">self</span>.generator(decoder_output)</span>
<span id="cb48-8"><a href="#cb48-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> F.log_softmax(output, dim<span class="op">=-</span><span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>The dimension change of the tensor is as follows.</p>
<ol type="1">
<li>Input (<code>src</code>, <code>tgt</code>): (batch_size, seq_len)</li>
<li>Encoder output: (batch_size, src_len, hidden_size)</li>
<li>Decoder output: (batch_size, tgt_len, hidden_size)</li>
<li>Final output: (batch_size, tgt_len, vocab_size)</li>
</ol>
<p>In the next section, we will apply this structure to an actual example.</p>
</section>
</section>
<section id="transformer-examples" class="level2">
<h2 class="anchored" data-anchor-id="transformer-examples">8.5 Transformer Examples</h2>
<p>So far, we have examined the structure and operating principles of transformers. Now, let’s verify the operation of transformers through actual examples. The examples are organized in order of difficulty, and each example allows us to understand specific features of transformers. These examples show how to solve various data processing and model design problems encountered in real projects step by step. In particular, they cover practical topics such as data preprocessing, loss function design, and evaluation metric setting. The location of the examples is transformer/examples.</p>
<pre class="text"><code>examples
├── addition_task.py  # 8.5.2 Addition Task
├── copy_task.py      # 8.5.1 Simple Copy Task
└── parser_task.py    # 8.5.3 Parser Task</code></pre>
<p>What we learn from each example is as follows:</p>
<p>The <strong>Simple Copy Task</strong> allows us to understand the basic functionality of transformers. Through attention pattern visualization, we can clearly understand the operating principle of the model. Additionally, we can learn about basic sequence data processing, tensor dimension design for batch processing, basic padding and masking strategies, and task-specific loss function design.</p>
<p>The <strong>Addition Problem</strong> shows how self-regressive generation is possible. We can observe the sequential generation process of the decoder and the role of cross-attention. Along with this, we gain practical experience in tokenizing numerical data, creating valid datasets, evaluating partial/total accuracy, and testing generalization performance according to digit expansion.</p>
<p>The <strong>Parser Task</strong> shows how transformers learn and represent structural relationships. We can understand how attention mechanisms capture the hierarchical structure of input sequences. Additionally, we can learn various techniques necessary for actual parsing problems, such as sequence conversion of structural data, token dictionary design, linearization strategies for tree structures, and evaluation methods for structural accuracy.</p>
<p>The following is a table summarizing what we learn from each example:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 71%">
<col style="width: 28%">
</colgroup>
<thead>
<tr class="header">
<th>Example</th>
<th>Learning Content</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>8.5.1 Simple Copy Task (copy_task.py)</td>
<td>- Understanding transformer basics and operating principles<br> - Intuitive understanding through attention pattern visualization<br> - Sequence data processing and tensor dimension design for batch processing<br> - Padding and masking strategies<br> - Task-specific loss function design</td>
</tr>
<tr class="even">
<td>8.5.2 Addition Task (addition_task.py)</td>
<td>- Learning the self-regressive generation process of transformers<br> - Observing the sequential generation of decoders and the role of cross-attention<br> - Tokenizing numerical data, creating valid datasets<br> - Evaluating partial/total accuracy, testing generalization performance according to digit expansion</td>
</tr>
<tr class="odd">
<td>8.5.3 Parser Task (parser_task.py)</td>
<td>- Understanding how transformers learn and represent structural relationships<br> - Understanding attention mechanisms that capture the hierarchical structure of input sequences<br> - Sequence conversion of structural data, token dictionary design<br> - Linearization strategies for tree structures, evaluation methods for structural accuracy</td>
</tr>
</tbody>
</table>
<section id="simple-copy-task" class="level3">
<h3 class="anchored" data-anchor-id="simple-copy-task">8.5.1 Simple Copy Task</h3>
<p>The first example is a copy task that outputs the input sequence as is. This task is suitable for verifying the basic operation of transformers and visualizing attention patterns, and although it seems simple, it is very useful for understanding the core mechanisms of transformers.</p>
<p><strong>Data Preparation</strong></p>
<p>The data for the copy task consists of input and output sequences that are identical. The following is an example of data creation.</p>
<div id="cell-92" class="cell" data-execution_count="1">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_08.transformer.examples.copy_task <span class="im">import</span> explain_copy_data</span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a>explain_copy_data(seq_length<span class="op">=</span><span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
=== Copy Task Data Explanation ===
Sequence Length: 5

1. Input Sequence:
Original Tensor Shape: torch.Size([1, 5])
Input Sequence: [7, 15, 2, 3, 12]

2. Target Sequence:
Original Tensor Shape: torch.Size([1, 5])
Target Sequence: [7, 15, 2, 3, 12]

3. Task Description:
- Basic task of copying the input sequence as is
- Tokens at each position are integer values between 1-19
- Input and output have the same sequence length
- Current Example: [7, 15, 2, 3, 12] → [7, 15, 2, 3, 12]</code></pre>
</div>
</div>
<p>create_copy_data creates tensors with the same input and output for learning. It generates a 2D tensor (batch_size, seq_length) for batch processing, where each element is an integer value between 1 and 19.</p>
<div id="cell-94" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_copy_data(batch_size: <span class="bu">int</span> <span class="op">=</span> <span class="dv">32</span>, seq_length: <span class="bu">int</span> <span class="op">=</span> <span class="dv">5</span>) <span class="op">-&gt;</span> torch.Tensor:</span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""복사 태스크용 데이터 생성"""</span></span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a>    sequences <span class="op">=</span> torch.randint(<span class="dv">1</span>, <span class="dv">20</span>, (batch_size, seq_length))</span>
<span id="cb52-4"><a href="#cb52-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> sequences, sequences</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>The data in this example is the same as the tokenized input data used in natural language processing or sequence modeling. In language processing, each token is converted to a unique integer value and then fed into the model.</p>
<p><strong>Model Training</strong></p>
<p>We train the model with the following code.</p>
<div id="cell-96" class="cell" data-execution_count="1">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_08.transformer.config <span class="im">import</span> TransformerConfig</span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_08.transformer.examples.copy_task <span class="im">import</span> train_copy_task</span>
<span id="cb53-3"><a href="#cb53-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-4"><a href="#cb53-4" aria-hidden="true" tabindex="-1"></a>seq_length <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb53-5"><a href="#cb53-5" aria-hidden="true" tabindex="-1"></a>config <span class="op">=</span> TransformerConfig()</span>
<span id="cb53-6"><a href="#cb53-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Modify default values</span></span>
<span id="cb53-7"><a href="#cb53-7" aria-hidden="true" tabindex="-1"></a>config.vocab_size <span class="op">=</span> <span class="dv">20</span>           <span class="co"># Small vocabulary size (minimum size to represent integers 1-19)</span></span>
<span id="cb53-8"><a href="#cb53-8" aria-hidden="true" tabindex="-1"></a>config.hidden_size <span class="op">=</span> <span class="dv">64</span>          <span class="co"># Small hidden dimension (enough representation for a simple task)</span></span>
<span id="cb53-9"><a href="#cb53-9" aria-hidden="true" tabindex="-1"></a>config.num_hidden_layers <span class="op">=</span> <span class="dv">2</span>     <span class="co"># Minimum number of layers (considering the low complexity of the copy task)</span></span>
<span id="cb53-10"><a href="#cb53-10" aria-hidden="true" tabindex="-1"></a>config.num_attention_heads <span class="op">=</span> <span class="dv">2</span>   <span class="co"># Minimum number of heads (minimum configuration for attention from various perspectives)</span></span>
<span id="cb53-11"><a href="#cb53-11" aria-hidden="true" tabindex="-1"></a>config.intermediate_size <span class="op">=</span> <span class="dv">128</span>   <span class="co"># Small FFN dimension (set to twice the hidden dimension to ensure adequate transformation capacity)</span></span>
<span id="cb53-12"><a href="#cb53-12" aria-hidden="true" tabindex="-1"></a>config.max_position_embeddings <span class="op">=</span> seq_length  <span class="co"># Short sequence length (set to the same length as the input sequence)</span></span>
<span id="cb53-13"><a href="#cb53-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-14"><a href="#cb53-14" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> train_copy_task(config, num_epochs<span class="op">=</span><span class="dv">50</span>, batch_size<span class="op">=</span><span class="dv">40</span>, steps_per_epoch<span class="op">=</span><span class="dv">100</span>, seq_length<span class="op">=</span>seq_length)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
=== Start Training ==== 
Device: cuda:0
Model saved to saved_models/transformer_copy_task.pth</code></pre>
</div>
</div>
<p><strong>Model Test</strong></p>
<p>Read the saved training model and perform a test.</p>
<div id="cell-98" class="cell" data-execution_count="1">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_08.transformer.examples.copy_task <span class="im">import</span> test_copy</span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-3"><a href="#cb55-3" aria-hidden="true" tabindex="-1"></a>test_copy(seq_length<span class="op">=</span><span class="dv">20</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
=== Copy Test ===
Input: [10, 10, 2, 12, 1, 5, 3, 1, 8, 18, 2, 19, 2, 2, 8, 14, 7, 19, 5, 4]
Output: [10, 10, 2, 12, 1, 5, 3, 1, 8, 18, 2, 19, 2, 2, 8, 14, 7, 19, 5, 4]
Accuracy: True</code></pre>
</div>
</div>
<p><strong>Model Settings</strong></p>
<ul>
<li><code>hidden_size</code>: 64 (model design dimension, d_model).
<ul>
<li>Transformer design dimension (d_model) and the same value:
<ol type="1">
<li>Word embedding dimension</li>
<li>Positional embedding dimension</li>
<li>Attention Q, K, V vector dimensions</li>
<li>Output dimension of each sublayer in encoder/decoder</li>
</ol></li>
</ul></li>
<li><code>intermediate_size</code>: FFN size, which should be sufficiently larger than d_model.</li>
</ul>
<p><strong>Mask Implementation</strong></p>
<p>The transformer uses two types of masks.</p>
<ol type="1">
<li><strong>Padding Mask (Padding Mask)</strong>: Ignores padding tokens added for batch processing.
<ul>
<li>This example has a length of <code>seq_length</code> and is the same, so padding is not necessary, but it is included for general use of the transformer.</li>
<li>Implement the <code>create_pad_mask</code> function directly (PyTorch’s <code>nn.Transformer</code> or Hugging Face’s <code>transformers</code> library implements it internally).</li>
</ul></li>
</ol>
<div id="cell-100" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a>src_mask <span class="op">=</span> create_pad_mask(src).to(device)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<ol start="2" type="1">
<li><strong>Subsequent Mask</strong>: Used for the decoder’s self-regressive generation.
<ul>
<li>The <code>create_subsequent_mask</code> function generates an upper triangular matrix mask that masks tokens after the current position.</li>
<li>This allows the decoder to predict the next token by referencing only previously generated tokens.</li>
</ul></li>
</ol>
<div id="cell-102" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb58"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a>tgt_mask <span class="op">=</span> create_subsequent_mask(decoder_input.size(<span class="dv">1</span>)).to(device)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>This masking ensures the efficiency of batch processing and sequence causality.</p>
<p><strong>Design of Loss Function</strong></p>
<p>The <code>CopyLoss</code> class implements a loss function for copy tasks.</p>
<ul>
<li>It considers both the accuracy at each token position and whether the entire sequence is completely matched.</li>
<li>It monitors accuracy, loss value, and predicted/actual values in detail to finely grasp the progress of learning.</li>
</ul>
<div id="cell-104" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb59"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> CopyLoss(nn.Module):</span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, outputs: torch.Tensor, target: torch.Tensor, </span>
<span id="cb59-3"><a href="#cb59-3" aria-hidden="true" tabindex="-1"></a>                print_details: <span class="bu">bool</span> <span class="op">=</span> <span class="va">False</span>) <span class="op">-&gt;</span> Tuple[torch.Tensor, <span class="bu">float</span>]:</span>
<span id="cb59-4"><a href="#cb59-4" aria-hidden="true" tabindex="-1"></a>        batch_size <span class="op">=</span> outputs.size(<span class="dv">0</span>)</span>
<span id="cb59-5"><a href="#cb59-5" aria-hidden="true" tabindex="-1"></a>        predictions <span class="op">=</span> F.softmax(outputs, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb59-6"><a href="#cb59-6" aria-hidden="true" tabindex="-1"></a>        target_one_hot <span class="op">=</span> F.one_hot(target, num_classes<span class="op">=</span>outputs.size(<span class="op">-</span><span class="dv">1</span>)).<span class="bu">float</span>()</span>
<span id="cb59-7"><a href="#cb59-7" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb59-8"><a href="#cb59-8" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> <span class="op">-</span>torch.<span class="bu">sum</span>(target_one_hot <span class="op">*</span> torch.log(predictions <span class="op">+</span> <span class="fl">1e-10</span>)) <span class="op">/</span> batch_size</span>
<span id="cb59-9"><a href="#cb59-9" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb59-10"><a href="#cb59-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb59-11"><a href="#cb59-11" aria-hidden="true" tabindex="-1"></a>            pred_tokens <span class="op">=</span> predictions.argmax(dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb59-12"><a href="#cb59-12" aria-hidden="true" tabindex="-1"></a>            exact_match <span class="op">=</span> (pred_tokens <span class="op">==</span> target).<span class="bu">all</span>(dim<span class="op">=</span><span class="dv">1</span>).<span class="bu">float</span>()</span>
<span id="cb59-13"><a href="#cb59-13" aria-hidden="true" tabindex="-1"></a>            match_rate <span class="op">=</span> exact_match.mean().item()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<ul>
<li>Cross entropy alone is insufficient: evaluate individual token accuracy + whether the entire sequence matches.</li>
<li>Guide the model to learn the order accurately.</li>
</ul>
<p><strong>Operation example</strong> (<code>batch_size=2</code>, <code>sequence_length=3</code>, <code>vocab_size=5</code>):</p>
<ol type="1">
<li><strong>Model output (logits)</strong></li>
</ol>
<div id="cell-106" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb60"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Example: batch_size=2, sequence_length=3, vocab_size=5 (example is vocab_size=20)</span></span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-3"><a href="#cb60-3" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Model Output (logits)</span></span>
<span id="cb60-4"><a href="#cb60-4" aria-hidden="true" tabindex="-1"></a>outputs <span class="op">=</span> [</span>
<span id="cb60-5"><a href="#cb60-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># First batch</span></span>
<span id="cb60-6"><a href="#cb60-6" aria-hidden="true" tabindex="-1"></a>    [[<span class="fl">0.9</span>, <span class="fl">0.1</span>, <span class="fl">0.0</span>, <span class="fl">0.0</span>, <span class="fl">0.0</span>],  <span class="co"># First position: token 0 has the highest probability</span></span>
<span id="cb60-7"><a href="#cb60-7" aria-hidden="true" tabindex="-1"></a>     [<span class="fl">0.1</span>, <span class="fl">0.8</span>, <span class="fl">0.1</span>, <span class="fl">0.0</span>, <span class="fl">0.0</span>],  <span class="co"># Second position: token 1 has the highest probability</span></span>
<span id="cb60-8"><a href="#cb60-8" aria-hidden="true" tabindex="-1"></a>     [<span class="fl">0.0</span>, <span class="fl">0.1</span>, <span class="fl">0.9</span>, <span class="fl">0.0</span>, <span class="fl">0.0</span>]], <span class="co"># Third position: token 2 has the highest probability</span></span>
<span id="cb60-9"><a href="#cb60-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Second batch</span></span>
<span id="cb60-10"><a href="#cb60-10" aria-hidden="true" tabindex="-1"></a>    [[<span class="fl">0.8</span>, <span class="fl">0.2</span>, <span class="fl">0.0</span>, <span class="fl">0.0</span>, <span class="fl">0.0</span>],</span>
<span id="cb60-11"><a href="#cb60-11" aria-hidden="true" tabindex="-1"></a>     [<span class="fl">0.1</span>, <span class="fl">0.7</span>, <span class="fl">0.2</span>, <span class="fl">0.0</span>, <span class="fl">0.0</span>],</span>
<span id="cb60-12"><a href="#cb60-12" aria-hidden="true" tabindex="-1"></a>     [<span class="fl">0.1</span>, <span class="fl">0.1</span>, <span class="fl">0.8</span>, <span class="fl">0.0</span>, <span class="fl">0.0</span>]]</span>
<span id="cb60-13"><a href="#cb60-13" aria-hidden="true" tabindex="-1"></a>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<ol start="2" type="1">
<li><strong>Actual Target</strong></li>
</ol>
<div id="cell-108" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb61"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Actual Target</span></span>
<span id="cb61-2"><a href="#cb61-2" aria-hidden="true" tabindex="-1"></a>target <span class="op">=</span> [</span>
<span id="cb61-3"><a href="#cb61-3" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">2</span>],  <span class="co"># Correct sequence for the first batch</span></span>
<span id="cb61-4"><a href="#cb61-4" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">2</span>]   <span class="co"># Correct sequence for the second batch</span></span>
<span id="cb61-5"><a href="#cb61-5" aria-hidden="true" tabindex="-1"></a>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<ol start="3" type="1">
<li><strong>Loss Calculation Process</strong>
<ul>
<li><code>predictions = softmax(outputs)</code> (already converted to probability above)</li>
<li>Convert <code>target</code> to one-hot vector</li>
</ul></li>
</ol>
<div id="cell-110" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb62"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. Loss Calculation Process</span></span>
<span id="cb62-2"><a href="#cb62-2" aria-hidden="true" tabindex="-1"></a><span class="co"># predictions = softmax(outputs) (already converted to probabilities above)</span></span>
<span id="cb62-3"><a href="#cb62-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert target to one-hot vectors:</span></span>
<span id="cb62-4"><a href="#cb62-4" aria-hidden="true" tabindex="-1"></a>target_one_hot <span class="op">=</span> [</span>
<span id="cb62-5"><a href="#cb62-5" aria-hidden="true" tabindex="-1"></a>    [[<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>], [<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>], [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>]],  <span class="co"># First batch</span></span>
<span id="cb62-6"><a href="#cb62-6" aria-hidden="true" tabindex="-1"></a>    [[<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>], [<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>], [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>]]   <span class="co"># Second batch</span></span>
<span id="cb62-7"><a href="#cb62-7" aria-hidden="true" tabindex="-1"></a>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<ol start="4" type="1">
<li><strong>Accuracy Calculation</strong></li>
</ol>
<div id="cell-112" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb63"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 4. Accuracy Calculation</span></span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a>pred_tokens <span class="op">=</span> [</span>
<span id="cb63-3"><a href="#cb63-3" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">2</span>],  <span class="co"># First batch prediction</span></span>
<span id="cb63-4"><a href="#cb63-4" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">2</span>]   <span class="co"># Second batch prediction</span></span>
<span id="cb63-5"><a href="#cb63-5" aria-hidden="true" tabindex="-1"></a>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<ul>
<li>Sequence overall match: <code>exact_match = [True, True]</code> (both batches are accurate)</li>
<li>Average accuracy: <code>match_rate = 1.0</code> (100%)</li>
</ul>
<ol start="5" type="1">
<li><strong>Final loss value</strong>: average of cross entropy</li>
</ol>
<div id="cell-114" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb64"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Exact sequence match</span></span>
<span id="cb64-2"><a href="#cb64-2" aria-hidden="true" tabindex="-1"></a>exact_match <span class="op">=</span> [<span class="va">True</span>, <span class="va">True</span>]  <span class="co"># Both batches match exactly</span></span>
<span id="cb64-3"><a href="#cb64-3" aria-hidden="true" tabindex="-1"></a>match_rate <span class="op">=</span> <span class="fl">1.0</span>  <span class="co"># Average accuracy 100%</span></span>
<span id="cb64-4"><a href="#cb64-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-5"><a href="#cb64-5" aria-hidden="true" tabindex="-1"></a><span class="co"># The final loss value is the average of the cross-entropy</span></span>
<span id="cb64-6"><a href="#cb64-6" aria-hidden="true" tabindex="-1"></a><span class="co"># loss = -1/2 * (log(0.9) + log(0.8) + log(0.9) + log(0.8) + log(0.7) + log(0.8))</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><strong>Attention Visualization</strong></p>
<p>Attention visualization allows us to intuitively understand the behavior of transformers.</p>
<div id="cell-116" class="cell" data-execution_count="1">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb65"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_08.transformer.examples.copy_task <span class="im">import</span> visualize_attention</span>
<span id="cb65-2"><a href="#cb65-2" aria-hidden="true" tabindex="-1"></a>visualize_attention(seq_length<span class="op">=</span><span class="dv">20</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="08_The Birth of Transformer_files/figure-html/cell-52-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>It checks how each input token interacts with tokens at other positions.</p>
<p>Through this example of a copying task, we confirmed the core mechanism of the transformer. In the next example (addition problem), we will look at how the transformer learns arithmetic rules such as relationships between numbers and carrying.</p>
</section>
<section id="digit-addition-task" class="level3">
<h3 class="anchored" data-anchor-id="digit-addition-task">8.5.2 Digit Addition Task</h3>
<p>The second example is an addition task that adds two numbers. This task is suitable for understanding the autoregressive generation capability of the transformer and the sequential calculation process of the decoder. Through calculations with carry-over, we can observe how the transformer learns the relationship between numbers.</p>
<p><strong>Data Preparation</strong></p>
<p>The data for the addition task is generated from <code>create_addition_data()</code>.</p>
<div id="cell-119" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb66"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_addition_data(batch_size: <span class="bu">int</span> <span class="op">=</span> <span class="dv">32</span>, max_digits: <span class="bu">int</span> <span class="op">=</span> <span class="dv">3</span>) <span class="op">-&gt;</span> Tuple[torch.Tensor, torch.Tensor]:</span>
<span id="cb66-2"><a href="#cb66-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Create addition dataset"""</span></span>
<span id="cb66-3"><a href="#cb66-3" aria-hidden="true" tabindex="-1"></a>    max_value <span class="op">=</span> <span class="dv">10</span> <span class="op">**</span> max_digits <span class="op">-</span> <span class="dv">1</span></span>
<span id="cb66-4"><a href="#cb66-4" aria-hidden="true" tabindex="-1"></a>    num1 <span class="op">=</span> torch.randint(<span class="dv">0</span>, max_value <span class="op">//</span> <span class="dv">2</span> <span class="op">+</span> <span class="dv">1</span>, (batch_size,))</span>
<span id="cb66-5"><a href="#cb66-5" aria-hidden="true" tabindex="-1"></a>    num2 <span class="op">=</span> torch.randint(<span class="dv">0</span>, max_value <span class="op">//</span> <span class="dv">2</span> <span class="op">+</span> <span class="dv">1</span>, (batch_size,))</span>
<span id="cb66-6"><a href="#cb66-6" aria-hidden="true" tabindex="-1"></a>    result <span class="op">=</span> num1 <span class="op">+</span> num2</span>
<span id="cb66-7"><a href="#cb66-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-8"><a href="#cb66-8" aria-hidden="true" tabindex="-1"></a>    [See source below]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<ul>
<li>Generate two numbers that do not exceed the specified number of digits in sum.</li>
<li>Input: Two numbers + ‘+’ sign.</li>
<li>Includes digit limit validity check.</li>
</ul>
<p><strong>Learning Data Description</strong></p>
<div id="cell-121" class="cell" data-execution_count="4">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb67"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_08.transformer.config <span class="im">import</span> TransformerConfig</span>
<span id="cb67-2"><a href="#cb67-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_08.transformer.examples.addition_task <span class="im">import</span> explain_addition_data</span>
<span id="cb67-3"><a href="#cb67-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-4"><a href="#cb67-4" aria-hidden="true" tabindex="-1"></a>explain_addition_data()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
=== Addition Data Explanation ====
Maximum Digits: 3

1. Input Sequence:
Original Tensor Shape: torch.Size([1, 7])
First Number: 153 (Indices [np.int64(1), np.int64(5), np.int64(3)])
Plus Sign: '+' (Index 10)
Second Number: 391 (Indices [np.int64(3), np.int64(9), np.int64(1)])
Full Input: [1, 5, 3, 10, 3, 9, 1]

2. Target Sequence:
Original Tensor Shape: torch.Size([1, 3])
Actual Sum: 544
Target Sequence: [5, 4, 4]</code></pre>
</div>
</div>
<p>Model Training and Testing</p>
<div id="cell-123" class="cell" data-execution_count="1">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb69"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_08.transformer.config <span class="im">import</span> TransformerConfig</span>
<span id="cb69-2"><a href="#cb69-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_08.transformer.examples.addition_task <span class="im">import</span> train_addition_task</span>
<span id="cb69-3"><a href="#cb69-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-4"><a href="#cb69-4" aria-hidden="true" tabindex="-1"></a>config <span class="op">=</span> TransformerConfig()</span>
<span id="cb69-5"><a href="#cb69-5" aria-hidden="true" tabindex="-1"></a>config.vocab_size <span class="op">=</span> <span class="dv">11</span>       </span>
<span id="cb69-6"><a href="#cb69-6" aria-hidden="true" tabindex="-1"></a>config.hidden_size <span class="op">=</span> <span class="dv">256</span></span>
<span id="cb69-7"><a href="#cb69-7" aria-hidden="true" tabindex="-1"></a>config.num_hidden_layers <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb69-8"><a href="#cb69-8" aria-hidden="true" tabindex="-1"></a>config.num_attention_heads <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb69-9"><a href="#cb69-9" aria-hidden="true" tabindex="-1"></a>config.intermediate_size <span class="op">=</span> <span class="dv">512</span></span>
<span id="cb69-10"><a href="#cb69-10" aria-hidden="true" tabindex="-1"></a>config.max_position_embeddings <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb69-11"><a href="#cb69-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-12"><a href="#cb69-12" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> train_addition_task(config, num_epochs<span class="op">=</span><span class="dv">10</span>, batch_size<span class="op">=</span><span class="dv">128</span>, steps_per_epoch<span class="op">=</span><span class="dv">300</span>, max_digits<span class="op">=</span><span class="dv">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 0, Average Loss: 6.1352, Final Accuracy: 0.0073, Learning Rate: 0.000100
Epoch 5, Average Loss: 0.0552, Final Accuracy: 0.9852, Learning Rate: 0.000100

=== Loss Calculation Details (Step: 3000) ===
Predicted Sequences (First 10): tensor([[6, 5, 4],
        [5, 3, 3],
        [1, 7, 5],
        [6, 0, 6],
        [7, 5, 9],
        [5, 2, 8],
        [2, 8, 1],
        [3, 5, 8],
        [0, 7, 1],
        [6, 2, 1]], device='cuda:0')

Actual Target Sequences (First 10): tensor([[6, 5, 4],
        [5, 3, 3],
        [1, 7, 5],
        [6, 0, 6],
        [7, 5, 9],
        [5, 2, 8],
        [2, 8, 1],
        [3, 5, 8],
        [0, 7, 1],
        [6, 2, 1]], device='cuda:0')

Exact Match per Sequence (First 10): tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')

Calculated Loss: 0.0106
Calculated Accuracy: 1.0000
========================================
Model saved to saved_models/transformer_addition_task.pth</code></pre>
</div>
</div>
<p>After learning is finished, it performs a test by loading the saved model.</p>
<div id="cell-125" class="cell" data-execution_count="3">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb71"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_08.transformer.examples.addition_task <span class="im">import</span> test_addition</span>
<span id="cb71-2"><a href="#cb71-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-3"><a href="#cb71-3" aria-hidden="true" tabindex="-1"></a>test_addition(max_digits<span class="op">=</span><span class="dv">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
Addition Test (Digits: 3):
310 + 98 = 408 (Actual Answer: 408)
Correct: True</code></pre>
</div>
</div>
<p><strong>Model Settings</strong></p>
<p>The transformer settings for the addition task are as follows.</p>
<div id="cell-127" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb73"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a>config <span class="op">=</span> TransformerConfig()</span>
<span id="cb73-2"><a href="#cb73-2" aria-hidden="true" tabindex="-1"></a>config.vocab_size <span class="op">=</span> <span class="dv">11</span>          <span class="co"># 0-9 digits + '+' symbol</span></span>
<span id="cb73-3"><a href="#cb73-3" aria-hidden="true" tabindex="-1"></a>config.hidden_size <span class="op">=</span> <span class="dv">256</span>        <span class="co"># Larger hidden dimension than copy task (sufficient capacity for learning arithmetic operations)</span></span>
<span id="cb73-4"><a href="#cb73-4" aria-hidden="true" tabindex="-1"></a>config.num_hidden_layers <span class="op">=</span> <span class="dv">3</span>    <span class="co"># Deeper layers (hierarchical feature extraction for handling carry operations)</span></span>
<span id="cb73-5"><a href="#cb73-5" aria-hidden="true" tabindex="-1"></a>config.num_attention_heads <span class="op">=</span> <span class="dv">4</span>  <span class="co"># Increased number of heads (learning relationships between different digit positions)</span></span>
<span id="cb73-6"><a href="#cb73-6" aria-hidden="true" tabindex="-1"></a>config.intermediate_size <span class="op">=</span> <span class="dv">512</span>  <span class="co">#  FFN dimension: should be larger than hidden_size.</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Masking Implementation</p>
<p>In the addition task, padding masks are <em>essential</em>. Since the number of input digits can vary, ignoring the padding position is necessary for accurate calculations.</p>
<div id="cell-129" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb74"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb74-1"><a href="#cb74-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> _number_to_digits(number: torch.Tensor, max_digits: <span class="bu">int</span>) <span class="op">-&gt;</span> torch.Tensor:</span>
<span id="cb74-2"><a href="#cb74-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""숫자를 자릿수 시퀀스로 변환하며 패딩 적용"""</span></span>
<span id="cb74-3"><a href="#cb74-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> torch.tensor([[<span class="bu">int</span>(d) <span class="cf">for</span> d <span class="kw">in</span> <span class="bu">str</span>(n.item()).zfill(max_digits)] </span>
<span id="cb74-4"><a href="#cb74-4" aria-hidden="true" tabindex="-1"></a>                        <span class="cf">for</span> n <span class="kw">in</span> number])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>The operation of the above method is as follows.</p>
<div id="cell-131" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb75"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb75-1"><a href="#cb75-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb75-2"><a href="#cb75-2" aria-hidden="true" tabindex="-1"></a>number <span class="op">=</span> torch.tensor([<span class="dv">7</span>, <span class="dv">25</span>, <span class="dv">348</span>])</span>
<span id="cb75-3"><a href="#cb75-3" aria-hidden="true" tabindex="-1"></a>max_digits <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb75-4"><a href="#cb75-4" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> _number_to_digits(number, max_digits)</span>
<span id="cb75-5"><a href="#cb75-5" aria-hidden="true" tabindex="-1"></a><span class="co"># 입력: [7, 25, 348]</span></span>
<span id="cb75-6"><a href="#cb75-6" aria-hidden="true" tabindex="-1"></a><span class="co"># 과정: </span></span>
<span id="cb75-7"><a href="#cb75-7" aria-hidden="true" tabindex="-1"></a><span class="co">#   7   -&gt; "7"   -&gt; "007" -&gt; [0,0,7]</span></span>
<span id="cb75-8"><a href="#cb75-8" aria-hidden="true" tabindex="-1"></a><span class="co">#   25  -&gt; "25"  -&gt; "025" -&gt; [0,2,5]</span></span>
<span id="cb75-9"><a href="#cb75-9" aria-hidden="true" tabindex="-1"></a><span class="co">#   348 -&gt; "348" -&gt; "348" -&gt; [3,4,8]</span></span>
<span id="cb75-10"><a href="#cb75-10" aria-hidden="true" tabindex="-1"></a><span class="co"># 결과: tensor([[0, 0, 7],</span></span>
<span id="cb75-11"><a href="#cb75-11" aria-hidden="true" tabindex="-1"></a><span class="co">#               [0, 2, 5],</span></span>
<span id="cb75-12"><a href="#cb75-12" aria-hidden="true" tabindex="-1"></a><span class="co">#               [3, 4, 8]])</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>The design of the loss function</p>
<p>The <code>AdditionLoss</code> class implements the loss function for the addition task.</p>
<ul>
<li>Unlike the copying task, it evaluates the <em>digit-wise accuracy</em> and <em>overall answer accuracy</em> separately.</li>
</ul>
<div id="cell-133" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb76"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb76-1"><a href="#cb76-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> AdditionLoss(nn.Module):</span>
<span id="cb76-2"><a href="#cb76-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, outputs: torch.Tensor, target: torch.Tensor, </span>
<span id="cb76-3"><a href="#cb76-3" aria-hidden="true" tabindex="-1"></a>                print_details: <span class="bu">bool</span> <span class="op">=</span> <span class="va">False</span>) <span class="op">-&gt;</span> Tuple[torch.Tensor, <span class="bu">float</span>]:</span>
<span id="cb76-4"><a href="#cb76-4" aria-hidden="true" tabindex="-1"></a>        batch_size <span class="op">=</span> outputs.size(<span class="dv">0</span>)</span>
<span id="cb76-5"><a href="#cb76-5" aria-hidden="true" tabindex="-1"></a>        predictions <span class="op">=</span> F.softmax(outputs, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb76-6"><a href="#cb76-6" aria-hidden="true" tabindex="-1"></a>        target_one_hot <span class="op">=</span> F.one_hot(target, num_classes<span class="op">=</span>outputs.size(<span class="op">-</span><span class="dv">1</span>)).<span class="bu">float</span>()</span>
<span id="cb76-7"><a href="#cb76-7" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb76-8"><a href="#cb76-8" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> <span class="op">-</span>torch.<span class="bu">sum</span>(target_one_hot <span class="op">*</span> torch.log(predictions <span class="op">+</span> <span class="fl">1e-10</span>)) <span class="op">/</span> batch_size</span>
<span id="cb76-9"><a href="#cb76-9" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb76-10"><a href="#cb76-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb76-11"><a href="#cb76-11" aria-hidden="true" tabindex="-1"></a>            pred_digits <span class="op">=</span> predictions.argmax(dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb76-12"><a href="#cb76-12" aria-hidden="true" tabindex="-1"></a>            exact_match <span class="op">=</span> (pred_digits <span class="op">==</span> target).<span class="bu">all</span>(dim<span class="op">=</span><span class="dv">1</span>).<span class="bu">float</span>()</span>
<span id="cb76-13"><a href="#cb76-13" aria-hidden="true" tabindex="-1"></a>            match_rate <span class="op">=</span> exact_match.mean().item()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<ul>
<li>Loss calculation: accuracy of each digit prediction + verification of <em>place value</em> accuracy.</li>
<li>Induced to learn addition rules, not just simple digit mapping.</li>
</ul>
<p>Example of <code>AdditionLoss</code> behavior (<code>batch_size=2</code>, <code>sequence_length=3</code>, <code>vocab_size=10</code>)</p>
<div id="cell-135" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb77"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb77-1"><a href="#cb77-1" aria-hidden="true" tabindex="-1"></a>outputs <span class="op">=</span> [</span>
<span id="cb77-2"><a href="#cb77-2" aria-hidden="true" tabindex="-1"></a>    [[<span class="fl">0.1</span>, <span class="fl">0.8</span>, <span class="fl">0.1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>],  <span class="co"># 첫 번째 자리</span></span>
<span id="cb77-3"><a href="#cb77-3" aria-hidden="true" tabindex="-1"></a>     [<span class="fl">0.1</span>, <span class="fl">0.1</span>, <span class="fl">0.7</span>, <span class="fl">0.1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>], <span class="co"># 두 번째 자리</span></span>
<span id="cb77-4"><a href="#cb77-4" aria-hidden="true" tabindex="-1"></a>     [<span class="fl">0.8</span>, <span class="fl">0.1</span>, <span class="fl">0.1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>]]   <span class="co"># 세 번째 자리</span></span>
<span id="cb77-5"><a href="#cb77-5" aria-hidden="true" tabindex="-1"></a>]  <span class="co"># 첫 번째 배치</span></span>
<span id="cb77-6"><a href="#cb77-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-7"><a href="#cb77-7" aria-hidden="true" tabindex="-1"></a>target <span class="op">=</span> [</span>
<span id="cb77-8"><a href="#cb77-8" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">0</span>]  <span class="co"># 실제 정답: "120"</span></span>
<span id="cb77-9"><a href="#cb77-9" aria-hidden="true" tabindex="-1"></a>]  <span class="co"># 첫 번째 배치</span></span>
<span id="cb77-10"><a href="#cb77-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-11"><a href="#cb77-11" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. softmax는 이미 적용되어 있다고 가정 (outputs)</span></span>
<span id="cb77-12"><a href="#cb77-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-13"><a href="#cb77-13" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. target을 원-핫 인코딩으로 변환</span></span>
<span id="cb77-14"><a href="#cb77-14" aria-hidden="true" tabindex="-1"></a>target_one_hot <span class="op">=</span> [</span>
<span id="cb77-15"><a href="#cb77-15" aria-hidden="true" tabindex="-1"></a>    [[<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>],  <span class="co"># 1</span></span>
<span id="cb77-16"><a href="#cb77-16" aria-hidden="true" tabindex="-1"></a>     [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>],  <span class="co"># 2</span></span>
<span id="cb77-17"><a href="#cb77-17" aria-hidden="true" tabindex="-1"></a>     [<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>]]  <span class="co"># 0</span></span>
<span id="cb77-18"><a href="#cb77-18" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb77-19"><a href="#cb77-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-20"><a href="#cb77-20" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. 손실 계산</span></span>
<span id="cb77-21"><a href="#cb77-21" aria-hidden="true" tabindex="-1"></a><span class="co"># -log(0.8) - log(0.7) - log(0.8) = 0.223 + 0.357 + 0.223 = 0.803</span></span>
<span id="cb77-22"><a href="#cb77-22" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> <span class="fl">0.803</span> <span class="op">/</span> batch_size</span>
<span id="cb77-23"><a href="#cb77-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-24"><a href="#cb77-24" aria-hidden="true" tabindex="-1"></a><span class="co"># 4. 정확도 계산</span></span>
<span id="cb77-25"><a href="#cb77-25" aria-hidden="true" tabindex="-1"></a>pred_digits <span class="op">=</span> [<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">0</span>]  <span class="co"># argmax 적용</span></span>
<span id="cb77-26"><a href="#cb77-26" aria-hidden="true" tabindex="-1"></a>exact_match <span class="op">=</span> <span class="va">True</span>  <span class="co"># 모든 자릿수가 일치</span></span>
<span id="cb77-27"><a href="#cb77-27" aria-hidden="true" tabindex="-1"></a>match_rate <span class="op">=</span> <span class="fl">1.0</span>  <span class="co"># 배치의 평균 정확도</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>The output of the transformer decoder is linearly transformed by <code>vocab_size</code> in the last layer, so the logit has <code>vocab_size</code> dimensions.</p>
<p>In the next section, we will look at how the transformer learns more complex structural relationships through parser tasks.</p>
</section>
<section id="parser-task" class="level3">
<h3 class="anchored" data-anchor-id="parser-task">8.5.3 Parser Task</h3>
<p>The last example is the implementation of a parser task. This task takes formulas as input and converts them into parse trees, which is an example that can verify how well the transformer handles structural information.</p>
<p><strong>Description of Data Preparation Process</strong></p>
<p>The training data for the parser task is generated through the following steps:</p>
<ol type="1">
<li><strong>Formula Generation</strong>:
<ul>
<li>The <code>generate_random_expression()</code> function is used to combine variables (x, y, z), operators (+, -, *, /), and numbers (0-9) to create simple formulas like “x=1+2”.</li>
</ul></li>
<li><strong>Parse Tree Conversion</strong>:
<ul>
<li>The <code>parse_to_tree()</code> function is used to convert the generated formula into a nested list-type parse tree like <code>['ASSIGN', 'x', ['ADD', '1', '2']]</code>. This tree represents the hierarchical structure of the formula.</li>
</ul></li>
<li><strong>Tokenization Processing</strong>:
<ul>
<li>The formulas and parse trees are each converted into integer sequences.</li>
<li>According to the pre-defined <code>TOKEN_DICT</code>, each token is mapped to a unique integer ID.</li>
</ul></li>
</ol>
<div id="cell-138" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb78"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb78-1"><a href="#cb78-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_addition_data(batch_size: <span class="bu">int</span> <span class="op">=</span> <span class="dv">32</span>, max_digits: <span class="bu">int</span> <span class="op">=</span> <span class="dv">3</span>) <span class="op">-&gt;</span> Tuple[torch.Tensor, torch.Tensor]:</span>
<span id="cb78-2"><a href="#cb78-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Create addition dataset"""</span></span>
<span id="cb78-3"><a href="#cb78-3" aria-hidden="true" tabindex="-1"></a>    max_value <span class="op">=</span> <span class="dv">10</span> <span class="op">**</span> max_digits <span class="op">-</span> <span class="dv">1</span></span>
<span id="cb78-4"><a href="#cb78-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb78-5"><a href="#cb78-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Generate input numbers</span></span>
<span id="cb78-6"><a href="#cb78-6" aria-hidden="true" tabindex="-1"></a>    num1 <span class="op">=</span> torch.randint(<span class="dv">0</span>, max_value <span class="op">//</span> <span class="dv">2</span> <span class="op">+</span> <span class="dv">1</span>, (batch_size,))</span>
<span id="cb78-7"><a href="#cb78-7" aria-hidden="true" tabindex="-1"></a>    num2 <span class="op">=</span> torch.randint(<span class="dv">0</span>, max_value <span class="op">//</span> <span class="dv">2</span> <span class="op">+</span> <span class="dv">1</span>, (batch_size,))</span>
<span id="cb78-8"><a href="#cb78-8" aria-hidden="true" tabindex="-1"></a>    result <span class="op">=</span> num1 <span class="op">+</span> num2</span>
<span id="cb78-9"><a href="#cb78-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-10"><a href="#cb78-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># [이하 생략]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<ul>
<li>Generate two numbers that do not exceed the specified number of digits in sum.</li>
<li>Input: two numbers + ‘+’ sign.</li>
<li>Includes digit limit validation.</li>
</ul>
<p><strong>Training Data Description</strong> The following describes the structure of the training data. It shows what values are changed when expressed and tokenized.</p>
<div id="cell-140" class="cell" data-execution_count="2">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb79"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb79-1"><a href="#cb79-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_08.transformer.examples.parser_task <span class="im">import</span> explain_parser_data</span>
<span id="cb79-2"><a href="#cb79-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-3"><a href="#cb79-3" aria-hidden="true" tabindex="-1"></a>explain_parser_data()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
=== Parsing Data Explanation ===
Max Tokens: 5

1. Input Sequence:
Original Tensor Shape: torch.Size([1, 5])
Expression: x = 4 + 9
Tokenized Input: [11, 1, 17, 2, 22]

2. Target Sequence:
Original Tensor Shape: torch.Size([1, 5])
Parse Tree: ['ASSIGN', 'x', 'ADD', '4', '9']
Tokenized Output: [6, 11, 7, 17, 22]</code></pre>
</div>
</div>
<p>The following code is executed to explain how the parsing example data is constructed in an easy-to-understand manner, with explanations displayed in order.</p>
<div id="cell-142" class="cell" data-execution_count="4">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb81"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb81-1"><a href="#cb81-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_08.transformer.examples.parser_task <span class="im">import</span> show_parser_examples</span>
<span id="cb81-2"><a href="#cb81-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-3"><a href="#cb81-3" aria-hidden="true" tabindex="-1"></a>show_parser_examples(num_examples<span class="op">=</span><span class="dv">3</span> )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
=== Generating 3 Parsing Examples ===

Example 1:
Generated Expression: y=7/7
Parse Tree: ['ASSIGN', 'y', ['DIV', '7', '7']]
Expression Tokens: [12, 1, 21, 5, 21]
Tree Tokens: [6, 12, 10, 21, 21]
Padded Expression Tokens: [12, 1, 21, 5, 21]
Padded Tree Tokens: [6, 12, 10, 21, 21]

Example 2:
Generated Expression: x=4/3
Parse Tree: ['ASSIGN', 'x', ['DIV', '4', '3']]
Expression Tokens: [11, 1, 18, 5, 17]
Tree Tokens: [6, 11, 10, 18, 17]
Padded Expression Tokens: [11, 1, 18, 5, 17]
Padded Tree Tokens: [6, 11, 10, 18, 17]

Example 3:
Generated Expression: x=1*4
Parse Tree: ['ASSIGN', 'x', ['MUL', '1', '4']]
Expression Tokens: [11, 1, 15, 4, 18]
Tree Tokens: [6, 11, 9, 15, 18]
Padded Expression Tokens: [11, 1, 15, 4, 18]
Padded Tree Tokens: [6, 11, 9, 15, 18]
</code></pre>
</div>
</div>
<p>Model Training and Testing</p>
<div id="cell-144" class="cell" data-execution_count="2">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb83"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb83-1"><a href="#cb83-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_08.transformer.config <span class="im">import</span> TransformerConfig</span>
<span id="cb83-2"><a href="#cb83-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_08.transformer.examples.parser_task <span class="im">import</span> train_parser_task</span>
<span id="cb83-3"><a href="#cb83-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-4"><a href="#cb83-4" aria-hidden="true" tabindex="-1"></a>config <span class="op">=</span> TransformerConfig()</span>
<span id="cb83-5"><a href="#cb83-5" aria-hidden="true" tabindex="-1"></a>config.vocab_size <span class="op">=</span> <span class="dv">25</span>  <span class="co"># Adjusted to match the token dictionary size</span></span>
<span id="cb83-6"><a href="#cb83-6" aria-hidden="true" tabindex="-1"></a>config.hidden_size <span class="op">=</span> <span class="dv">128</span></span>
<span id="cb83-7"><a href="#cb83-7" aria-hidden="true" tabindex="-1"></a>config.num_hidden_layers <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb83-8"><a href="#cb83-8" aria-hidden="true" tabindex="-1"></a>config.num_attention_heads <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb83-9"><a href="#cb83-9" aria-hidden="true" tabindex="-1"></a>config.intermediate_size <span class="op">=</span> <span class="dv">512</span></span>
<span id="cb83-10"><a href="#cb83-10" aria-hidden="true" tabindex="-1"></a>config.max_position_embeddings <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb83-11"><a href="#cb83-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-12"><a href="#cb83-12" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> train_parser_task(config, num_epochs<span class="op">=</span><span class="dv">6</span>, batch_size<span class="op">=</span><span class="dv">64</span>, steps_per_epoch<span class="op">=</span><span class="dv">100</span>, max_tokens<span class="op">=</span><span class="dv">5</span>, print_progress<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
=== Start Training ===
Device: cuda:0
Batch Size: 64
Steps per Epoch: 100
Max Tokens: 5

Epoch 0, Average Loss: 6.3280, Final Accuracy: 0.2309, Learning Rate: 0.000100

=== Prediction Result Samples ===
Input: y = 8 * 8
Prediction: ['ASSIGN', 'y', 'MUL', '8', '8']
Truth: ['ASSIGN', 'y', 'MUL', '8', '8']
Result: Correct

Input: z = 6 / 5
Prediction: ['ASSIGN', 'z', 'DIV', '8', 'a']
Truth: ['ASSIGN', 'z', 'DIV', '6', '5']
Result: Incorrect

Epoch 5, Average Loss: 0.0030, Final Accuracy: 1.0000, Learning Rate: 0.000100

=== Prediction Result Samples ===
Input: z = 5 - 6
Prediction: ['ASSIGN', 'z', 'SUB', '5', '6']
Truth: ['ASSIGN', 'z', 'SUB', '5', '6']
Result: Correct

Input: y = 9 + 9
Prediction: ['ASSIGN', 'y', 'ADD', '9', '9']
Truth: ['ASSIGN', 'y', 'ADD', '9', '9']
Result: Correct

Model saved to saved_models/transformer_parser_task.pth</code></pre>
</div>
</div>
<p>I perform a test.</p>
<div id="cell-146" class="cell" data-execution_count="3">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb85"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb85-1"><a href="#cb85-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_08.transformer.config <span class="im">import</span> TransformerConfig</span>
<span id="cb85-2"><a href="#cb85-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_08.transformer.examples.parser_task <span class="im">import</span> test_parser</span>
<span id="cb85-3"><a href="#cb85-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-4"><a href="#cb85-4" aria-hidden="true" tabindex="-1"></a>test_parser()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
=== Parser Test ===
Input Expression: x = 8 * 3
Predicted Parse Tree: ['ASSIGN', 'x', 'MUL', '8', '3']
Actual Parse Tree: ['ASSIGN', 'x', 'MUL', '8', '3']
Correct: True

=== Additional Tests ===

Input: x=1+2
Predicted Parse Tree: ['ASSIGN', 'x', 'ADD', '2', '3']

Input: y=3*4
Predicted Parse Tree: ['ASSIGN', 'y', 'MUL', '4', '5']

Input: z=5-1
Predicted Parse Tree: ['ASSIGN', 'z', 'SUB', '6', '2']

Input: x=2/3
Predicted Parse Tree: ['ASSIGN', 'x', 'DIV', '3', '4']</code></pre>
</div>
</div>
<p><strong>Model Settings</strong> - <code>vocab_size</code>: 25 (size of the token dictionary) - <code>hidden_size</code>: 128 - <code>num_hidden_layers</code>: 3 - <code>num_attention_heads</code>: 4 - <code>intermediate_size</code>: 512 - <code>max_position_embeddings</code>: 10 (maximum number of tokens)</p>
<p><strong>Loss Function Design</strong></p>
<p>The loss function for the parser task uses cross entropy loss.</p>
<ol type="1">
<li><strong>Output Conversion</strong>: The model output is converted to a probability using the softmax function.</li>
<li><strong>Target Conversion</strong>: The target (answer) sequence is one-hot encoded.</li>
<li><strong>Loss Calculation</strong>: The loss is calculated by computing the negative mean of the log probability.</li>
<li><strong>Accuracy</strong>: Accuracy is calculated based on whether the predicted sequence and the answer sequence match exactly. This reflects the characteristics of this task, which requires the parse tree to be accurately generated.</li>
</ol>
<p><strong>Example of Loss Function Operation</strong></p>
<div id="cell-148" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb87"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb87-1"><a href="#cb87-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Example input values (batch_size=2, sequence_length=4, vocab_size=5)</span></span>
<span id="cb87-2"><a href="#cb87-2" aria-hidden="true" tabindex="-1"></a><span class="co"># vocab = {'=':0, 'x':1, '+':2, '1':3, '2':4}</span></span>
<span id="cb87-3"><a href="#cb87-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-4"><a href="#cb87-4" aria-hidden="true" tabindex="-1"></a>outputs <span class="op">=</span> [</span>
<span id="cb87-5"><a href="#cb87-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># First batch: prediction probabilities for "x=1+2"</span></span>
<span id="cb87-6"><a href="#cb87-6" aria-hidden="true" tabindex="-1"></a>    [[<span class="fl">0.1</span>, <span class="fl">0.7</span>, <span class="fl">0.1</span>, <span class="fl">0.1</span>, <span class="fl">0.0</span>],  <span class="co"># predicting x</span></span>
<span id="cb87-7"><a href="#cb87-7" aria-hidden="true" tabindex="-1"></a>     [<span class="fl">0.8</span>, <span class="fl">0.1</span>, <span class="fl">0.0</span>, <span class="fl">0.1</span>, <span class="fl">0.0</span>],  <span class="co"># predicting =</span></span>
<span id="cb87-8"><a href="#cb87-8" aria-hidden="true" tabindex="-1"></a>     [<span class="fl">0.1</span>, <span class="fl">0.0</span>, <span class="fl">0.1</span>, <span class="fl">0.7</span>, <span class="fl">0.1</span>],  <span class="co"># predicting 1</span></span>
<span id="cb87-9"><a href="#cb87-9" aria-hidden="true" tabindex="-1"></a>     [<span class="fl">0.0</span>, <span class="fl">0.1</span>, <span class="fl">0.8</span>, <span class="fl">0.0</span>, <span class="fl">0.1</span>]], <span class="co"># predicting +</span></span>
<span id="cb87-10"><a href="#cb87-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb87-11"><a href="#cb87-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Second batch: prediction probabilities for "x=2+1"</span></span>
<span id="cb87-12"><a href="#cb87-12" aria-hidden="true" tabindex="-1"></a>    [[<span class="fl">0.1</span>, <span class="fl">0.8</span>, <span class="fl">0.0</span>, <span class="fl">0.1</span>, <span class="fl">0.0</span>],  <span class="co"># predicting x</span></span>
<span id="cb87-13"><a href="#cb87-13" aria-hidden="true" tabindex="-1"></a>     [<span class="fl">0.7</span>, <span class="fl">0.1</span>, <span class="fl">0.1</span>, <span class="fl">0.0</span>, <span class="fl">0.1</span>],  <span class="co"># predicting =</span></span>
<span id="cb87-14"><a href="#cb87-14" aria-hidden="true" tabindex="-1"></a>     [<span class="fl">0.1</span>, <span class="fl">0.0</span>, <span class="fl">0.1</span>, <span class="fl">0.1</span>, <span class="fl">0.7</span>],  <span class="co"># predicting 2</span></span>
<span id="cb87-15"><a href="#cb87-15" aria-hidden="true" tabindex="-1"></a>     [<span class="fl">0.0</span>, <span class="fl">0.0</span>, <span class="fl">0.9</span>, <span class="fl">0.1</span>, <span class="fl">0.0</span>]]  <span class="co"># predicting +</span></span>
<span id="cb87-16"><a href="#cb87-16" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb87-17"><a href="#cb87-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-18"><a href="#cb87-18" aria-hidden="true" tabindex="-1"></a>target <span class="op">=</span> [</span>
<span id="cb87-19"><a href="#cb87-19" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">3</span>, <span class="dv">2</span>],  <span class="co"># Actual answer: "x=1+"</span></span>
<span id="cb87-20"><a href="#cb87-20" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">4</span>, <span class="dv">2</span>]   <span class="co"># Actual answer: "x=2+"</span></span>
<span id="cb87-21"><a href="#cb87-21" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb87-22"><a href="#cb87-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-23"><a href="#cb87-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert target to one-hot encoding</span></span>
<span id="cb87-24"><a href="#cb87-24" aria-hidden="true" tabindex="-1"></a>target_one_hot <span class="op">=</span> [</span>
<span id="cb87-25"><a href="#cb87-25" aria-hidden="true" tabindex="-1"></a>    [[<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>],  <span class="co"># x</span></span>
<span id="cb87-26"><a href="#cb87-26" aria-hidden="true" tabindex="-1"></a>     [<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>],  <span class="co"># =</span></span>
<span id="cb87-27"><a href="#cb87-27" aria-hidden="true" tabindex="-1"></a>     [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>],  <span class="co"># 1</span></span>
<span id="cb87-28"><a href="#cb87-28" aria-hidden="true" tabindex="-1"></a>     [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>]], <span class="co"># +</span></span>
<span id="cb87-29"><a href="#cb87-29" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb87-30"><a href="#cb87-30" aria-hidden="true" tabindex="-1"></a>    [[<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>],  <span class="co"># x</span></span>
<span id="cb87-31"><a href="#cb87-31" aria-hidden="true" tabindex="-1"></a>     [<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>],  <span class="co"># =</span></span>
<span id="cb87-32"><a href="#cb87-32" aria-hidden="true" tabindex="-1"></a>     [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>],  <span class="co"># 2</span></span>
<span id="cb87-33"><a href="#cb87-33" aria-hidden="true" tabindex="-1"></a>     [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>]]  <span class="co"># +</span></span>
<span id="cb87-34"><a href="#cb87-34" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb87-35"><a href="#cb87-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-36"><a href="#cb87-36" aria-hidden="true" tabindex="-1"></a><span class="co"># Loss calculation (first batch)</span></span>
<span id="cb87-37"><a href="#cb87-37" aria-hidden="true" tabindex="-1"></a><span class="co"># -log(0.7) - log(0.8) - log(0.7) - log(0.8) = 0.357 + 0.223 + 0.357 + 0.223 = 1.16</span></span>
<span id="cb87-38"><a href="#cb87-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-39"><a href="#cb87-39" aria-hidden="true" tabindex="-1"></a><span class="co"># Loss calculation (second batch)</span></span>
<span id="cb87-40"><a href="#cb87-40" aria-hidden="true" tabindex="-1"></a><span class="co"># -log(0.8) - log(0.7) - log(0.7) - log(0.9) = 0.223 + 0.357 + 0.357 + 0.105 = 1.042</span></span>
<span id="cb87-41"><a href="#cb87-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-42"><a href="#cb87-42" aria-hidden="true" tabindex="-1"></a><span class="co"># Total loss</span></span>
<span id="cb87-43"><a href="#cb87-43" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> (<span class="fl">1.16</span> <span class="op">+</span> <span class="fl">1.042</span>) <span class="op">/</span> <span class="dv">2</span> <span class="op">=</span> <span class="fl">1.101</span></span>
<span id="cb87-44"><a href="#cb87-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-45"><a href="#cb87-45" aria-hidden="true" tabindex="-1"></a><span class="co"># Accuracy calculation</span></span>
<span id="cb87-46"><a href="#cb87-46" aria-hidden="true" tabindex="-1"></a>pred_tokens <span class="op">=</span> [</span>
<span id="cb87-47"><a href="#cb87-47" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">3</span>, <span class="dv">2</span>],  <span class="co"># First batch prediction</span></span>
<span id="cb87-48"><a href="#cb87-48" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">4</span>, <span class="dv">2</span>]   <span class="co"># Second batch prediction</span></span>
<span id="cb87-49"><a href="#cb87-49" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb87-50"><a href="#cb87-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-51"><a href="#cb87-51" aria-hidden="true" tabindex="-1"></a>exact_match <span class="op">=</span> [<span class="va">True</span>, <span class="va">True</span>]  <span class="co"># Both batches match exactly</span></span>
<span id="cb87-52"><a href="#cb87-52" aria-hidden="true" tabindex="-1"></a>match_rate <span class="op">=</span> <span class="fl">1.0</span>  <span class="co"># Overall accuracy</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Through the examples so far, we have seen that transformers can effectively process structural information.</p>
</section>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>In Chapter 8, we deeply explored the background of the birth of transformers and their core components. We examined how the core ideas that make up transformers, such as researchers’ concerns to overcome the limitations of RNN-based models, the discovery and development of attention mechanisms, and parallel processing and capturing contextual information from various perspectives through Q, K, V vector space separation and multi-head attention, were gradually materialized. Additionally, we analyzed in detail positional encoding for effective expression of position information, sophisticated masking strategies to prevent information leakage, and the encoder-decoder structure and the role and operation of each component.</p>
<p>Through three examples (simple copy, digit addition, and parser), we intuitively understood how transformers actually work and what role each component plays. These examples demonstrate the basic functions of transformers, their self-recurrent generative capabilities, and their ability to process structural information, providing foundational knowledge for applying transformers to real natural language processing problems.</p>
<p>In Chapter 9, we will follow the evolution of transformers after the publication of the “Attention is All You Need” paper. We will look at how various transformer-based models such as BERT and GPT emerged and what innovations they brought to fields beyond natural language processing, including computer vision and speech recognition.</p>
</section>
<section id="practice-problems" class="level2">
<h2 class="anchored" data-anchor-id="practice-problems">Practice Problems</h2>
<section id="basic-problems" class="level3">
<h3 class="anchored" data-anchor-id="basic-problems">Basic Problems</h3>
<ol type="1">
<li>What are the two biggest advantages of transformers over RNNs?</li>
<li>What is the core idea of the attention mechanism, and what effects can be achieved through it?</li>
<li>What advantages does multi-head attention provide compared to self-attention?</li>
<li>Why is positional encoding necessary, and how does it express location information?</li>
<li>What roles do the encoder and decoder play in the transformer?</li>
</ol>
</section>
<section id="application-problems" class="level3">
<h3 class="anchored" data-anchor-id="application-problems">Application Problems</h3>
<ol type="1">
<li><strong>Text Summarization Task</strong>: Design a transformer model that takes a long text as input and generates a short summary containing the key content, and explain what evaluation metrics can be used to measure the model’s performance.</li>
<li><strong>Question Answering System Analysis</strong>: Explain the step-by-step process of how a transformer-based question answering system finds the correct answer to a given question, and analyze the core role that the attention mechanism plays in this process.</li>
<li><strong>Investigation of Applications in Other Domains</strong>: Investigate two or more cases where transformers have been successfully applied in domains other than natural language processing (e.g., images, speech, graphs), and explain how transformers were used in each case and what advantages they provided.</li>
</ol>
</section>
<section id="advanced-problems" class="level3">
<h3 class="anchored">Advanced Problems</h3>
<ol type="1">
<li><strong>Comparison Analysis of Computational Complexity Improvement Methods</strong>: Investigate two or more methods proposed to improve the computational complexity of transformers (e.g., Reformer, Performer, Longformer), and compare their core ideas, pros and cons, and applicable scenarios.</li>
<li><strong>Proposal and Evaluation of New Architecture</strong>: Propose a new transformer-based architecture specialized for a specific problem (e.g., long text classification, multilingual translation), and theoretically explain its advantages over existing transformer models and suggest ways to experimentally verify it.</li>
<li><strong>Analysis of Ethical and Social Impacts and Response Measures</strong>: Analyze the potential positive and negative impacts of the development of large language models based on transformers (e.g., GPT-3, BERT) on society, and propose technical and policy measures to mitigate negative impacts such as bias, fake news generation, and job loss.</li>
</ol>
<div class="callout callout-style-default callout-note callout-titled" title="Click to view contents (exercise answers)">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Click to view contents (exercise answers)
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<section id="practice-problem-solutions" class="level2 callout-body-container callout-body">
<h2 class="anchored" data-anchor-id="practice-problem-solutions">Practice Problem Solutions</h2>
<section id="basic-problems-1" class="level3">
<h3 class="anchored" data-anchor-id="basic-problems-1">Basic Problems</h3>
<ol type="1">
<li><p><strong>Transformer advantages over RNN:</strong> The transformer has two major advantages over RNN: <strong>parallel processing</strong> and <strong>solving long-term dependency problems</strong>. While RNN is slow due to sequential processing, the transformer can process all words simultaneously using attention, allowing for GPU parallel computation and faster learning. Additionally, while RNN suffers from information loss in long sequences, the transformer preserves important information regardless of distance by directly calculating word relationships through self-attention.</p></li>
<li><p><strong>Attention mechanism core &amp; effect:</strong> Attention calculates <strong>how important each part of the input sequence is for generating the output sequence</strong>. The decoder does not look at the entire input equally when predicting output words; instead, it focuses on relevant parts, understanding context better and making more accurate predictions.</p></li>
<li><p><strong>Multi-head attention advantages:</strong> Multi-head attention performs <strong>multiple self-attentions in parallel</strong>. Each head learns word relationships within the input sequence from different perspectives, helping the model capture richer and more diverse contextual information (similar to multiple detectives collaborating with their own specialties).</p></li>
<li><p><strong>Need for &amp; method of positional encoding:</strong> Since the transformer does not process sequentially, it needs to know the <strong>position of each word</strong>. Positional encoding works by adding a vector containing position information to the word embedding. This allows the transformer to consider both the meaning of words and their positions in the sentence when understanding context, typically using sine-cosine functions to represent position.</p></li>
<li><p><strong>Encoder &amp; decoder roles:</strong> The transformer has an encoder-decoder structure. The <strong>encoder</strong> generates contextual vectors reflecting each word’s context from the input sequence. The <strong>decoder</strong> predicts the next word based on the contextual vector generated by the encoder and previously generated output words, repeating this process to create the final output sequence.</p></li>
</ol>
</section>
<section id="application-problems-1" class="level3">
<h3 class="anchored" data-anchor-id="application-problems-1">Application Problems</h3>
<ol type="1">
<li><strong>Text Summarization Task:</strong>
<ul>
<li><strong>Model design:</strong> Uses an encoder-decoder transformer model. The encoder generates a contextual vector from long text, and the decoder creates a summary based on this vector. Masked self-attention is used in the decoder to prevent referencing future words during generation.</li>
<li><strong>Evaluation metric:</strong> Model performance can be mainly evaluated using ROUGE (Recall-Oriented Understudy for Gisting Evaluation) scores. ROUGE measures similarity based on overlapping n-grams between generated and reference summaries, with varieties like ROUGE-N, ROUGE-L, and ROUGE-S. BLEU (Bilingual Evaluation Understudy) score can also be considered.</li>
</ul></li>
<li><strong>Question Answering System Analysis:</strong> A transformer-based question answering system finds the correct answer in a document for a given question as follows:
<ol type="1">
<li>Inputs the question and document into separate transformer encoders to obtain embedding vectors.</li>
<li>Calculates attention weights between question and document embeddings (to understand which parts of the document relate to the question).</li>
<li>Uses these attention weights to compute a weighted average of the document embedding, serving as a contextual vector for the question.</li>
<li>Predicts the start and end positions of the answer based on this contextual vector and extracts the final answer. In this process, the <strong>attention mechanism</strong> plays a crucial role in identifying the most important parts of the document related to the question by understanding semantic relevance between the question and document.</li>
</ol></li>
<li><strong>Investigating Different Domain Application Cases:</strong>
<ul>
<li><strong>Image:</strong> Vision Transformer (ViT) divides an image into multiple patches and processes each patch like a transformer input sequence, showing excellent performance in image classification, object detection, and other tasks. This demonstrates that transformers can be effectively applied not only to sequential data but also to two-dimensional data like images.</li>
<li><strong>Speech:</strong> Conformer combines CNN and transformer to achieve high accuracy in speech recognition. By effectively modeling both local features and global features of speech signals, it improves speech recognition performance.</li>
</ul></li>
</ol>
</section>
<section id="advanced-problems-1" class="level3">
<h3 class="anchored" data-anchor-id="advanced-problems-1">Advanced Problems</h3>
<ol type="1">
<li><p><strong>Comparative Analysis of Computational Complexity Improvement Methods:</strong></p>
<p>Transformers have a quadratic computational complexity due to self-attention with respect to the input sequence length. Various methods have been proposed to improve this.</p>
<ul>
<li><strong>Reformer:</strong> Uses Locality-Sensitive Hashing (LSH) attention to approximate the similarity between queries and keys. LSH is a hashing technique that assigns similar vectors to the same bucket, allowing it to avoid calculating attention for the entire sequence and focus on nearby tokens, reducing computational complexity. Reformer can significantly reduce memory usage and computation time but may slightly decrease accuracy due to the approximative nature of LSH.</li>
<li><strong>Longformer:</strong> Combines sliding window attention and global attention to efficiently process long sequences. Each token performs attention only within a fixed-size window around it, and some tokens (e.g., sentence start tokens) perform attention over the entire sequence. Longformer is fast in processing long sequences and uses less memory but may have performance variations depending on the window size.</li>
</ul></li>
<li><p><strong>Proposing and Evaluating New Architectures:</strong></p>
<ul>
<li><strong>Problem Definition:</strong> When classifying long texts, existing transformers face high computational complexity and difficulty capturing long-range dependencies.</li>
<li><strong>Architecture Proposal:</strong> Divide text into multiple segments, apply a transformer encoder to each segment to obtain segment embeddings, and then input these embeddings into another transformer encoder to get the representation of the entire text for classification.</li>
<li><strong>Theoretical Advantages:</strong> Can effectively capture long-range dependencies with reduced computational complexity through its hierarchical structure.</li>
<li><strong>Experiment Design:</strong> Use long-text classification datasets (e.g., IMDB movie reviews) to compare the performance (accuracy, F1-score) of the proposed architecture with existing transformer models (e.g., BERT). Analyze performance changes by varying text length, segment size, and other hyperparameters to validate the effectiveness of the proposed architecture.</li>
</ul></li>
<li><p><strong>Analyzing Ethical and Social Impacts and Responding Measures:</strong></p>
<p>The development and application of transformer models raise several ethical and social concerns that need careful consideration and response. These include but are not limited to issues of bias in training data, privacy concerns, especially when dealing with sensitive information like personal medical records or financial data, and the potential for misuse in generating misleading or harmful content. To address these challenges, it is essential to develop and implement robust ethical guidelines, ensure transparency in model development and deployment, and continuously monitor and evaluate the social impact of transformer models. Moreover, incorporating diverse perspectives during the design phase and fostering a culture of accountability among developers and users are crucial steps towards mitigating the adverse effects and maximizing the benefits of these powerful technologies. The advancement of large-scale language models based on transformers (e.g.&nbsp;GPT-3, BERT) can have various positive and negative effects on society.</p></li>
</ol>
<ul>
<li><strong>Positive effects:</strong> It can lower the barrier to communication and increase information accessibility through automatic translation, chatbots, virtual assistants, etc. Additionally, it can improve productivity through content creation, code generation, automatic summarization, etc., and accelerate innovation by being applied to new fields such as scientific research (e.g.&nbsp;protein structure prediction), medical diagnosis, etc.</li>
<li><strong>Negative effects:</strong> It can learn biases (gender, race, religion, etc.) existing in the training data and lead to discriminatory results. Malicious users can generate fake news on a large scale to manipulate public opinion or damage the reputation of specific individuals/groups. Furthermore, automated writing, translation, customer service, etc. can reduce job opportunities in related fields, and problems such as personal information infringement and copyright infringement can also occur.</li>
<li><strong>Countermeasures:</strong> To mitigate these negative effects, technical and policy efforts are needed, such as removing data bias, developing fake news detection technology, preparing social discussions and re-education programs for job changes due to automation, strengthening algorithm transparency and accountability, and establishing ethical guidelines.</li>
</ul>
</section>
</section>
</div>
</div>
</section>
</section>
<section id="references-1" class="level2">
<h2 class="anchored" data-anchor-id="references-1">References</h2>
<ol type="1">
<li><a href="https://arxiv.org/abs/1706.03762">Attention is All You Need</a> (Vaswani et al., 2017) - The original transformer paper</li>
<li><a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html">The Annotated Transformer</a> (Harvard NLP) - A detailed explanation of the transformer with PyTorch implementation</li>
<li><a href="http://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a> (Jay Alammar) - A visual explanation of the transformer</li>
<li><a href="https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html">Transformer: A Novel Neural Network Architecture for Language Understanding</a> (Google AI Blog) - Introduction to the transformer</li>
<li><a href="https://lilianweng.github.io/posts/2020-04-07-the-transformer-family/">The Transformer Family</a> (Lilian Weng) - Introduction to various transformer models</li>
<li><a href="https://arxiv.org/abs/1810.04805">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a> (Devlin et al., 2018) - Introduction to BERT</li>
<li><a href="https://arxiv.org/abs/2005.14165">GPT-3: Language Models are Few-Shot Learners</a> (Brown et al., 2020) - Introduction to GPT-3</li>
<li><a href="https://huggingface.co/transformers/">Hugging Face Transformers</a> - Providing various transformer models and tools</li>
<li><a href="https://www.tensorflow.org/text/tutorials/transformer">TensorFlow Transformer Tutorial</a> - TensorFlow transformer implementation tutorial</li>
<li><a href="https://pytorch.org/docs/stable/nn.html#transformer-layers">PyTorch Transformer Documentation</a> - PyTorch transformer module explanation</li>
<li><a href="https://arxiv.org/abs/1904.02679">Visualizing Attention in Transformer-Based Language Representation Models</a> - Visualizing attention in transformer-based language models</li>
<li><a href="https://arxiv.org/abs/2107.03789">A Survey of Long-Term Context in Transformers</a> - Research trends for handling long-term context in transformers</li>
<li><a href="https://arxiv.org/abs/2001.04451">Reformer: The Efficient Transformer</a> - Reformer model for improving transformer efficiency</li>
<li><a href="https://arxiv.org/abs/2009.06732">Efficient Transformers: A Survey</a> - Research trends for efficient transformer models</li>
<li><a href="https://arxiv.org/abs/2011.04006">Long Range Arena: A Benchmark for Efficient Transformers</a> - Benchmark for efficient transformers handling long-term context</li>
</ol>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>