<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>multimodal-deep-learning-the-beginning-of-multisensory-convergence – Deep Learning DNA: Surviving Architectures and Essential Principles</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap-f507c7d0488cb7630e20aad62ad8c2aa.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script>window.MathJax = {loader: {load: ['[tex]/boldsymbol']},tex: {packages: {'[+]': ['boldsymbol']}}};</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../../notebooks/en/part_1/01_The Beginning of Deep Learning.html">part_1</a></li><li class="breadcrumb-item"><a href="../../../notebooks/en/part_1/10_Multimodal Deep Learning: The Beginning of Multisensory Convergence.html">10. Multimodal Deep Learning: The Beginning of Multisensory Convergence</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../../../">English</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Language</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index_de.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Deutsch</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index_en.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">English</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index_es.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Español</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">한국어</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index_zh.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">中文</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/en/part_1/00_Introduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">part_1</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/en/part_1/01_The Beginning of Deep Learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">1. The Beginning of Deep Learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/en/part_1/02_Mathematics of Deep Learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">2. Mathematics of Deep Learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/en/part_1/03_Deep Learning Framework.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">3. Deep Learning Framework</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/en/part_1/04_Activation Function.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">4. Activation Function</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/en/part_1/05_Optimization and Visualization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">5. Optimization and Visualization</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/en/part_1/06_Overfitting and Development of Solution Techniques.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">6. Overfitting and Development of Solution Techniques</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/en/part_1/07_Evolution of Convolutional Neural Networks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">7. Evolution of Convolutional Neural Networks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/en/part_1/08_The Birth of Transformer.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">8. The Birth of Transformer</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/en/part_1/09_The Evolution of Transformers.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">9. The Evolution of Transformers</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/en/part_1/10_Multimodal Deep Learning: The Beginning of Multisensory Convergence.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">10. Multimodal Deep Learning: The Beginning of Multisensory Convergence</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/en/part_1/11_Multimodal Deep Learning: Intelligence Beyond Limits.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">11. Multimodal Deep Learning: Intelligence Beyond Limits</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Deep Learning Frontier</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/en/Deep Learning Frontier/01_SLM: Small but Powerful Language Model.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">1. SLM: Small but Powerful Language Model</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notebooks/en/Deep Learning Frontier/02_Autonomous Driving.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">2. Autonomous Driving</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#multimodal-deep-learning-the-beginning-of-multisensory-convergence" id="toc-multimodal-deep-learning-the-beginning-of-multisensory-convergence" class="nav-link active" data-scroll-target="#multimodal-deep-learning-the-beginning-of-multisensory-convergence">10 Multimodal Deep Learning: The Beginning of Multisensory Convergence</a>
  <ul class="collapse">
  <li><a href="#multimodality-and-deep-learning" id="toc-multimodality-and-deep-learning" class="nav-link" data-scroll-target="#multimodality-and-deep-learning">10.1 Multimodality and Deep Learning</a>
  <ul class="collapse">
  <li><a href="#the-encounter-between-multimodal-data-and-deep-learning" id="toc-the-encounter-between-multimodal-data-and-deep-learning" class="nav-link" data-scroll-target="#the-encounter-between-multimodal-data-and-deep-learning">10.1.1 The Encounter Between Multimodal Data and Deep Learning</a></li>
  <li><a href="#importance-and-application-fields-of-multimodal-deep-learning" id="toc-importance-and-application-fields-of-multimodal-deep-learning" class="nav-link" data-scroll-target="#importance-and-application-fields-of-multimodal-deep-learning">10.1.2 Importance and Application Fields of Multimodal Deep Learning</a></li>
  <li><a href="#history-and-development-process-of-multimodal-deep-learning" id="toc-history-and-development-process-of-multimodal-deep-learning" class="nav-link" data-scroll-target="#history-and-development-process-of-multimodal-deep-learning">10.1.3 History and Development Process of Multimodal Deep Learning</a></li>
  </ul></li>
  <li><a href="#early-multimodal-approaches" id="toc-early-multimodal-approaches" class="nav-link" data-scroll-target="#early-multimodal-approaches">10.2 Early Multimodal Approaches</a>
  <ul class="collapse">
  <li><a href="#image-captioning-the-first-step-in-multimodal-fusion" id="toc-image-captioning-the-first-step-in-multimodal-fusion" class="nav-link" data-scroll-target="#image-captioning-the-first-step-in-multimodal-fusion">10.2.1 Image Captioning: The First Step in Multimodal Fusion</a></li>
  <li><a href="#visual-question-answering-vqa-image-understanding-and-inference" id="toc-visual-question-answering-vqa-image-understanding-and-inference" class="nav-link" data-scroll-target="#visual-question-answering-vqa-image-understanding-and-inference">10.2.2 Visual Question Answering (VQA): Image Understanding and Inference</a></li>
  </ul></li>
  <li><a href="#multimodal-fusion-theory-classification-based-on-cmu-lectures" id="toc-multimodal-fusion-theory-classification-based-on-cmu-lectures" class="nav-link" data-scroll-target="#multimodal-fusion-theory-classification-based-on-cmu-lectures">10.3 Multimodal Fusion Theory: Classification Based on CMU Lectures</a>
  <ul class="collapse">
  <li><a href="#joint-representations" id="toc-joint-representations" class="nav-link" data-scroll-target="#joint-representations">10.3.1 Joint Representations</a></li>
  <li><a href="#coordinated-representations" id="toc-coordinated-representations" class="nav-link" data-scroll-target="#coordinated-representations">10.3.2 Coordinated Representations</a></li>
  <li><a href="#encoder-decoder" id="toc-encoder-decoder" class="nav-link" data-scroll-target="#encoder-decoder">10.3.3 Encoder-Decoder</a></li>
  <li><a href="#modality-integration-strategies" id="toc-modality-integration-strategies" class="nav-link" data-scroll-target="#modality-integration-strategies">10.3.4 Modality Integration Strategies</a></li>
  </ul></li>
  <li><a href="#multimodal-representation-learning-techniques" id="toc-multimodal-representation-learning-techniques" class="nav-link" data-scroll-target="#multimodal-representation-learning-techniques">10.4 Multimodal Representation Learning Techniques</a>
  <ul class="collapse">
  <li><a href="#modality-to-modality-representation-learning" id="toc-modality-to-modality-representation-learning" class="nav-link" data-scroll-target="#modality-to-modality-representation-learning">10.4.1 Modality-to-Modality Representation Learning</a></li>
  <li><a href="#cross-modal-attention-structure" id="toc-cross-modal-attention-structure" class="nav-link" data-scroll-target="#cross-modal-attention-structure">10.4.2 Cross-Modal Attention Structure</a></li>
  <li><a href="#perceiver-architecture" id="toc-perceiver-architecture" class="nav-link" data-scroll-target="#perceiver-architecture">10.4.3 Perceiver Architecture</a></li>
  <li><a href="#cross-attention-implementation-and-training-stability" id="toc-cross-attention-implementation-and-training-stability" class="nav-link" data-scroll-target="#cross-attention-implementation-and-training-stability">10.4.4 Cross-Attention Implementation and Training Stability</a></li>
  </ul></li>
  <li><a href="#vision-transformer-vit" id="toc-vision-transformer-vit" class="nav-link" data-scroll-target="#vision-transformer-vit">10.5 Vision Transformer (ViT)</a>
  <ul class="collapse">
  <li><a href="#paradigm-shift-from-cnn-to-vit" id="toc-paradigm-shift-from-cnn-to-vit" class="nav-link" data-scroll-target="#paradigm-shift-from-cnn-to-vit">10.5.1 Paradigm Shift from CNN to ViT</a></li>
  <li><a href="#principle-of-image-patch-embedding" id="toc-principle-of-image-patch-embedding" class="nav-link" data-scroll-target="#principle-of-image-patch-embedding">10.5.2 Principle of Image Patch Embedding</a></li>
  <li><a href="#positional-encoding-mechanism" id="toc-positional-encoding-mechanism" class="nav-link" data-scroll-target="#positional-encoding-mechanism">10.5.3 Positional Encoding Mechanism</a></li>
  <li><a href="#structure-and-key-components-of-vit" id="toc-structure-and-key-components-of-vit" class="nav-link" data-scroll-target="#structure-and-key-components-of-vit">10.5.4 Structure and Key Components of ViT</a></li>
  <li><a href="#vit-training-example" id="toc-vit-training-example" class="nav-link" data-scroll-target="#vit-training-example">10.5.5 ViT Training Example</a></li>
  <li><a href="#vit-22b-the-extreme-scale" id="toc-vit-22b-the-extreme-scale" class="nav-link" data-scroll-target="#vit-22b-the-extreme-scale">10.5.6 ViT-22B: The Extreme Scale</a></li>
  <li><a href="#mae-v3-self-supervised-learning" id="toc-mae-v3-self-supervised-learning" class="nav-link" data-scroll-target="#mae-v3-self-supervised-learning">10.5.7 MAE v3: Self-Supervised Learning</a></li>
  </ul></li>
  <li><a href="#clip-a-milestone-in-multimodal-learning" id="toc-clip-a-milestone-in-multimodal-learning" class="nav-link" data-scroll-target="#clip-a-milestone-in-multimodal-learning">10.6 CLIP: A Milestone in Multimodal Learning</a>
  <ul class="collapse">
  <li><a href="#basic-structure-of-clip-dual-encoder" id="toc-basic-structure-of-clip-dual-encoder" class="nav-link" data-scroll-target="#basic-structure-of-clip-dual-encoder">10.6.1 Basic Structure of CLIP: Dual Encoder</a></li>
  <li><a href="#image-encoder" id="toc-image-encoder" class="nav-link" data-scroll-target="#image-encoder">10.6.2 Image Encoder</a></li>
  <li><a href="#text-encoder" id="toc-text-encoder" class="nav-link" data-scroll-target="#text-encoder">10.6.3 Text Encoder</a></li>
  <li><a href="#mechanism-of-zero-shot-transfer" id="toc-mechanism-of-zero-shot-transfer" class="nav-link" data-scroll-target="#mechanism-of-zero-shot-transfer">10.6.4 Mechanism of Zero-shot Transfer</a></li>
  </ul></li>
  <li><a href="#practice-problems" id="toc-practice-problems" class="nav-link" data-scroll-target="#practice-problems">Practice Problems</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../../notebooks/en/part_1/01_The Beginning of Deep Learning.html">part_1</a></li><li class="breadcrumb-item"><a href="../../../notebooks/en/part_1/10_Multimodal Deep Learning: The Beginning of Multisensory Convergence.html">10. Multimodal Deep Learning: The Beginning of Multisensory Convergence</a></li></ol></nav></header>




<p><a href="https://colab.research.google.com/github/Quantum-Intelligence-Frontier/dldna/blob/main/notebooks/en/part_1/10_Multimodal_Deep_Learning_A_Starting_Point_for_Multi-Sensory_Fusion.ipynb" target="_parent"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"> </a></p>
<section id="multimodal-deep-learning-the-beginning-of-multisensory-convergence" class="level1">
<h1>10 Multimodal Deep Learning: The Beginning of Multisensory Convergence</h1>
<blockquote class="blockquote">
<p>“Perception is not a collection of fragmented single senses, but a harmonious symphony of all senses.” - James Gibson, founder of ecological psychology.</p>
</blockquote>
<p>There has been a long-standing challenge in the history of artificial intelligence. It is “multimodality”. Humans perceive the world by using various senses (modalities) such as vision, hearing, and touch simultaneously, and integrate them organically. For example, when we drink coffee in a cafe, we receive various information such as the warmth of the cup (tactile), the smell of coffee (olfactory), the sound of people talking around us (auditory), and the scenery inside the cafe (visual), and form a comprehensive experience of “being in a cafe”.</p>
<p>However, early artificial intelligence models had difficulty processing this multimodal information. Artificial intelligence research, which began in the 1950s, focused primarily on processing single modalities (text, images, speech). While there were significant achievements in each field, such as translation and speech recognition, integrating them to understand like humans was another dimension of the problem.</p>
<p>In this chapter, we will delve into the core theories and surviving architectures of multimodal deep learning. We will examine how each architecture extends and evolves the DNA of deep learning, and how they contribute to solving complex problems in the real world.</p>
<section id="multimodality-and-deep-learning" class="level2">
<h2 class="anchored" data-anchor-id="multimodality-and-deep-learning">10.1 Multimodality and Deep Learning</h2>
<blockquote class="blockquote">
<p><strong>Challenge:</strong> How can different forms of data such as text, images, and audio be integrated and processed in a single model? These data have different representation methods, dimensions, and statistical characteristics. How can heterogeneous information be fused to learn meaningful representations?</p>
<p><strong>Researcher’s Concerns:</strong> Researchers had to find new methods that could effectively model the interactions between modalities while maintaining their unique characteristics, i.e., a new DNA for deep learning. A true fusion was needed, where each modality understands the context of others and provides complementary information, beyond simple concatenation.</p>
</blockquote>
<section id="the-encounter-between-multimodal-data-and-deep-learning" class="level3">
<h3 class="anchored" data-anchor-id="the-encounter-between-multimodal-data-and-deep-learning">10.1.1 The Encounter Between Multimodal Data and Deep Learning</h3>
<p>Multimodal data refers to the combination of two or more different forms of data such as text, images, audio, and video. For example, news articles consist of text and images, while movies are composed of video and audio. Humans naturally integrate this multimodal information to understand the world. It is perfectly normal for humans to read words while looking at pictures, listen to sounds while understanding situations.</p>
<p><strong>Why was multimodal deep learning a difficult problem?</strong></p>
<ol type="1">
<li><p><strong>Heterogeneous data representation:</strong> Text, images, and audio have different representation methods, dimensions, and statistical characteristics. It was a challenging problem to effectively represent and process these heterogeneous data in a single model.</p></li>
<li><p><strong>Complexity of information fusion:</strong> Simply concatenating each modality’s information does not constitute true fusion. Complex interactions need to be modeled where each modality understands the context of others, provides complementary information, and sometimes reconciles conflicting information.</p></li>
<li><p><strong>Data scarcity and imbalance:</strong> Multimodal data is relatively scarce compared to single-modality data, and there is also an issue of data imbalance between modalities. For example, there are many datasets that pair images with text, but fewer datasets that include all three: images, text, and audio.</p></li>
</ol>
<p>Despite these challenges, deep learning has presented new possibilities for processing multimodal data. Since the 2010s, the advancement of deep learning technology, particularly the emergence of the Transformer architecture, has played a crucial role in the development of multimodal deep learning. This was an important turning point in the evolution of deep learning DNA. The self-attention mechanism of Transformers enabled effective modeling not only of relationships between elements within each modality but also of complex interactions between different modalities. Previously, CNNs were specialized for image processing and RNNs for sequence data, while Transformers provided a universal architecture that could be applied to various modalities with flexibility.</p>
</section>
<section id="importance-and-application-fields-of-multimodal-deep-learning" class="level3">
<h3 class="anchored" data-anchor-id="importance-and-application-fields-of-multimodal-deep-learning">10.1.2 Importance and Application Fields of Multimodal Deep Learning</h3>
<p>Multimodal deep learning is an essential technology for artificial intelligence to understand the world like humans and interact with it. It goes beyond simply processing multiple forms of data, organically connecting the meanings contained in each data to enable richer and more accurate inferences. Just as different areas of the brain cooperate to perform complex cognitive functions, multimodal deep learning is a core driving force that elevates the intelligence of artificial intelligence.</p>
<p><strong>Main Application Fields</strong></p>
<ul>
<li><p><strong>Visual Question Answering (VQA):</strong> Takes an image and a question (text) as input and generates an answer to the question. It must comprehensively understand the meaning of the image and the question, beyond simply recognizing objects in the image. For example, to answer the question “What color hat is the man in the picture wearing?”, a complex process of finding the man, recognizing the hat, and determining the color is necessary.</p></li>
<li><p><strong>Image Captioning:</strong> Automatically generates text that describes an image. It must accurately grasp the content of the image and express it in natural sentences.</p></li>
<li><p><strong>Multimodal Sentiment Analysis:</strong> Integrates text, voice, facial expressions, and other information to understand a user’s emotions. It can detect subtle changes in emotions or sarcastic tones through voice tone or facial expression changes that might be difficult to discern from text alone.</p></li>
<li><p><strong>Autonomous Driving:</strong> Integrates data from cameras (images), LiDAR (3D sensors), GPS (location information), radar, and other sensors to recognize the surroundings and make driving decisions. Each sensor provides different information, and fusing them is crucial for safe and accurate driving.</p></li>
<li><p><strong>Robotics:</strong> Robots integrate visual, tactile, auditory, and other sensory information to perform complex tasks. For instance, for a robot to grasp an object, it must visually identify the object’s location and shape and adjust its grip force based on tactile feedback upon contact.</p></li>
<li><p><strong>Medical Diagnosis:</strong> Combines X-ray, MRI (images), patient records (text), bio-signals (time-series data), genomic information, and more to diagnose and predict diseases. Each type of data provides different clues about the disease, and integrating them is necessary for accurate diagnosis.</p></li>
</ul>
</section>
<section id="history-and-development-process-of-multimodal-deep-learning" class="level3">
<h3 class="anchored" data-anchor-id="history-and-development-process-of-multimodal-deep-learning">10.1.3 History and Development Process of Multimodal Deep Learning</h3>
<p>The research on multimodal deep learning is an interesting journey that shows the evolution of deep learning DNA. This journey can be broadly divided into the following major stages:</p>
<section id="early-stage-early-2010s" class="level4">
<h4 class="anchored" data-anchor-id="early-stage-early-2010s">Early Stage (Early 2010s)</h4>
<p>In the early 2010s, initial research on multimodal deep learning focused primarily on image captioning and VQA (Visual Question Answering). During this period, CNN-RNN based models were prevalent, using CNNs to extract features from images and RNNs to process text. CNNs effectively captured spatial features in images, while RNNs processed sequential information in text with strength. However, early models mainly used the <em>late fusion</em> approach, which processed each modality independently and then combined the results in the final stage. This method had the advantage of preserving the unique characteristics of each modality, but it had the limitation of not fully reflecting the interaction between modalities in the early stages.</p>
<p>Representative models of this period include <strong>DeViSE (Frome et al., 2013)</strong>, which projected images and word embeddings into the same space to calculate image-text similarity, and <strong>m-RNN (Mao et al., 2014)</strong>, which combined CNN and RNN for image captioning and added a multimodal layer to fuse multimodal information.</p>
</section>
<section id="introduction-of-attention-mechanism-mid-2010s" class="level4">
<h4 class="anchored" data-anchor-id="introduction-of-attention-mechanism-mid-2010s">Introduction of Attention Mechanism (Mid-2010s)</h4>
<p>In the mid-2010s, the introduction of the attention mechanism brought about a significant turning point in multimodal deep learning research. The attention mechanism allowed for more sophisticated modeling of the relationship between images and text. For example, in image captioning, attention enabled the model to learn which region of the image to “focus” on when generating a specific word, and in VQA, it helped determine which part of the image to look at to answer a question.</p>
<p>The introduction of the attention mechanism greatly improved the performance of image captioning and VQA models. Representative models include <strong>Show, Attend and Tell (Xu et al., 2015)</strong>, which introduced attention to image captioning to focus on the relevant image region when generating words, and <strong>Stacked Attention Networks (Yang et al., 2016)</strong>, which applied attention multiple times to the image to generate answers to questions in VQA.</p>
</section>
<section id="emergence-of-transformers-and-multimodal-innovation-2017-and-later" class="level4">
<h4 class="anchored" data-anchor-id="emergence-of-transformers-and-multimodal-innovation-2017-and-later">Emergence of Transformers and Multimodal Innovation (2017 and Later)</h4>
<p>In 2017, the introduction of the Transformer architecture in the paper “Attention is All You Need” marked a new era for multimodal deep learning. The Transformer had the advantage of directly modeling the relationships between all elements in the input sequence using self-attention mechanisms.</p>
<ul>
<li><strong>ViT (Vision Transformer, 2020):</strong> ViT, which divides images into patches and inputs them into the Transformer, emerged as a strong alternative to CNNs in the image processing field. ViT effectively modeled long-range dependencies within images, achieving excellent performance in various tasks such as image classification.</li>
<li><strong>CLIP (Contrastive Language-Image Pre-training, 2021):</strong> CLIP learned to embed images and text into the same space using a large number of image-text pairs. This achieved remarkable results, including zero-shot performance on various downstream tasks such as image classification and object detection without additional fine-tuning.</li>
<li><strong>DALL-E (2021), Imagen (2022), Stable Diffusion (2022):</strong> Models that generated high-quality images based on text descriptions demonstrated the remarkable capabilities of Transformer-based generative models. They learned complex relationships between text and images, producing image generation results that were previously unimaginable.</li>
<li><strong>GPT-4V (2023), Gemini (2023):</strong> The emergence of large multimodal models (LMM, Large Multimodal Model) that can understand and process text and images simultaneously has opened up new possibilities for multimodal deep learning. These gigantic models, with billions of parameters, achieve human-level performance in various multimodal tasks and are at the forefront of AI research.</li>
</ul>
</section>
<section id="recent-trends-expansion-and-convergence-of-intelligence" class="level4">
<h4 class="anchored" data-anchor-id="recent-trends-expansion-and-convergence-of-intelligence">Recent Trends: Expansion and Convergence of Intelligence</h4>
<p>Recent multimodal deep learning research is advancing beyond simple information fusion to enhance the ability to generate new knowledge and inference based on the information from each modality.</p>
<ul>
<li><p><strong>Advancements in LMM (Large Multimodal Model):</strong> More advanced LMMs are emerging, integrating more modalities (such as audio, video, 3D sensor data) and possessing more complex reasoning capabilities.</p></li>
<li><p><strong>Research on Efficient Fusion Techniques:</strong> On the other hand, research is also being actively conducted on efficient fusion techniques that can maximize the effect of information fusion while reducing computational costs, allowing multimodal models to be effectively utilized even with limited computing resources.</p></li>
<li><p><strong>Explainability (XAI) and Ethical Issues:</strong> As the complexity of multimodal models increases, the importance of research on understanding the decision-making process of models and addressing ethical issues such as bias is also growing.</p></li>
</ul>
<p>In the next section, we will take a closer look at the early approaches to multimodal deep learning and the major architectures that have “survived” the process.</p>
</section>
</section>
</section>
<section id="early-multimodal-approaches" class="level2">
<h2 class="anchored" data-anchor-id="early-multimodal-approaches">10.2 Early Multimodal Approaches</h2>
<p>As seen in Section 10.1.3, Transformers and CLIP have brought innovations to multimodal deep learning. However, these advances were not sudden. Prior to this, there were numerous attempts to combine images and text, and further, various modalities, which laid the solid foundation for modern multimodal deep learning. In this section, we will explore the major approaches that led the early days of deep learning-based multimodal research in the early 2010s and their significance.</p>
<section id="image-captioning-the-first-step-in-multimodal-fusion" class="level3">
<h3 class="anchored" data-anchor-id="image-captioning-the-first-step-in-multimodal-fusion">10.2.1 Image Captioning: The First Step in Multimodal Fusion</h3>
<p>Image captioning is a task that automatically generates a natural language sentence (caption) describing a given image. This is a representative multimodal problem that converts visual information (image) into linguistic information (text), which was the primary research target in the early days of deep learning-based multimodal research. Image captioning is similar to when a child looks at a picture book and says, “There’s a dog here, and there’s a ball!”</p>
<section id="early-cnn-rnn-structure-before-2014" class="level4">
<h4 class="anchored" data-anchor-id="early-cnn-rnn-structure-before-2014">Early CNN-RNN Structure (Before 2014)</h4>
<p>In the early days of image captioning research, models that combined CNNs and RNNs were dominant. It was similar to connecting two hemispheres of the brain: CNN for vision and RNN for language. CNNs were used as image encoders, such as VGGNet and AlexNet, to extract feature vectors from images, while RNNs were used as text decoders, such as LSTMs, to generate caption sentences based on the image feature vectors.</p>
<p>A representative model is Show and Tell (Vinyals et al., 2015), which proposed an end-to-end approach that inputs the image features extracted by CNN into the initial hidden state of LSTM to generate captions. However, this CNN-RNN structure had limitations in that it could not clearly model the correspondence between specific regions of the image and specific words in the text, although it grasped the overall content of the image.</p>
</section>
<section id="introduction-of-attention-mechanism-after-2015" class="level4">
<h4 class="anchored" data-anchor-id="introduction-of-attention-mechanism-after-2015">Introduction of Attention Mechanism (After 2015)</h4>
<p>The attention mechanism, which “focuses” on specific regions of the image, greatly improved the performance of image captioning models. Attention is similar to when our gaze naturally moves to important parts of a picture.</p>
<p>There are Soft Attention and Hard Attention mechanisms. Soft Attention calculates weights for all regions of the image and uses a weighted average of feature vectors, while Hard Attention selects only one specific region of the image to generate captions.</p>
<p>Show, Attend and Tell (Xu et al., 2015) was the first model to introduce the Soft Attention mechanism to image captioning, which learned to focus on specific regions of the image when generating each word in the caption, resulting in more accurate and detailed captions.</p>
</section>
<section id="bottom-up-and-top-down-attention-after-2017" class="level4">
<h4 class="anchored" data-anchor-id="bottom-up-and-top-down-attention-after-2017">Bottom-Up and Top-Down Attention (After 2017)</h4>
<p>Since 2017, the Bottom-Up and Top-Down Attention approach has emerged, which utilizes both the overall context (top-down) and individual object (bottom-up) information of the image. The bottom-up approach uses object detection models such as Faster R-CNN to identify major objects in the image, while the top-down approach calculates attention weights for these object features during caption generation.</p>
<p>The Bottom-Up and Top-Down Attention (Anderson et al., 2018) model combined these two approaches, significantly improving image captioning performance. This is similar to considering the overall flow of a story while detailing the objects in each scene.</p>
</section>
<section id="evolution-of-image-captioning-from-a-deep-learning-dna-perspective" class="level4">
<h4 class="anchored" data-anchor-id="evolution-of-image-captioning-from-a-deep-learning-dna-perspective">Evolution of Image Captioning from a Deep Learning DNA Perspective</h4>
<p>Image captioning research added important elements to the DNA of deep learning. The combination of CNN-RNN presented a basic framework for effectively combining different modalities, and attention mechanisms became a key technology in multimodal deep learning. Additionally, Bottom-Up and Top-Down Attention further enhanced the image understanding capabilities of deep learning models.</p>
<p>These advancements have become the foundation for extending beyond image captioning to various multimodal tasks such as VQA and multimodal machine translation. Recently, transformer-based models like BLIP have emerged, demonstrating good performance not only in image captioning but also in various multimodal tasks.</p>
</section>
<section id="image-captioning-model-blip-example" class="level4">
<h4 class="anchored" data-anchor-id="image-captioning-model-blip-example">Image Captioning Model (BLIP) Example</h4>
<p>BLIP (Bootstrapping Language-Image Pre-training) is a transformer-based model for image captioning. BLIP pre-trains images and text together, showing good performance not only in image captioning but also in various multimodal tasks such as VQA and image-text retrieval.</p>
<p>The following is an example code that generates image captions using the BLIP model with the Hugging Face Transformers library.</p>
<div id="cell-3" class="cell" data-execution_count="4">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># !pip install dldna[colab] # in Colab</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="co"># !pip install dldna[all] # in your local</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>load_ext autoreload</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>autoreload <span class="dv">2</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="cell-4" class="cell" data-execution_count="5">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> BlipProcessor, BlipForConditionalGeneration</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> Image</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> requests</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the model and processor</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>processor <span class="op">=</span> BlipProcessor.from_pretrained(<span class="st">"Salesforce/blip-image-captioning-base"</span>)</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> BlipForConditionalGeneration.from_pretrained(<span class="st">"Salesforce/blip-image-captioning-base"</span>)</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Download the image</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>url <span class="op">=</span> <span class="st">"http://images.cocodataset.org/val2017/000000000632.jpg"</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>image <span class="op">=</span> Image.<span class="bu">open</span>(requests.get(url, stream<span class="op">=</span><span class="va">True</span>).raw)</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the image</span></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>plt.imshow(image)</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>plt.axis(<span class="st">'off'</span>)</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Preprocess the input</span></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>inputs <span class="op">=</span> processor(image, return_tensors<span class="op">=</span><span class="st">"pt"</span>)</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate the caption</span></span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>outputs <span class="op">=</span> model.generate(<span class="op">**</span>inputs)</span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Decode and print the caption</span></span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>caption <span class="op">=</span> processor.decode(outputs[<span class="dv">0</span>], skip_special_tokens<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Generated caption:"</span>, caption)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="10_Multimodal Deep Learning: The Beginning of Multisensory Convergence_files/figure-html/cell-3-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Generated caption: a bedroom with a bed and a window</code></pre>
</div>
</div>
</section>
</section>
<section id="visual-question-answering-vqa-image-understanding-and-inference" class="level3">
<h3 class="anchored" data-anchor-id="visual-question-answering-vqa-image-understanding-and-inference">10.2.2 Visual Question Answering (VQA): Image Understanding and Inference</h3>
<p>Visual Question Answering (VQA) is a task that generates answers to questions about an image based on its content. If image captioning describes the content of an image, VQA is a question-answering process for images. For example, it answers questions like “What is the cat eating?” VQA requires more complex and high-dimensional image understanding capabilities than image captioning, especially the ability to understand and infer the relationship between the image and the question (text).</p>
<section id="early-vqa-models-cnn-rnn-before-2015" class="level4">
<h4 class="anchored" data-anchor-id="early-vqa-models-cnn-rnn-before-2015"><strong>Early VQA Models (CNN + RNN) (Before 2015)</strong></h4>
<p>Like image captioning, early VQA models used a structure that combined CNN and RNN. They extracted image features using CNN, encoded questions using RNN, and then combined these two features to generate answers. However, simply combining image features and question features made it difficult to answer complex questions.</p>
</section>
<section id="multimodal-attention-mechanism-after-2016" class="level4">
<h4 class="anchored" data-anchor-id="multimodal-attention-mechanism-after-2016"><strong>Multimodal Attention Mechanism (After 2016)</strong></h4>
<p>As the attention mechanism was successful in image captioning, it was also introduced to VQA. Co-Attention applies attention to both images and questions, calculating the relevance between each word of the question and each region of the image. This allows for more accurate identification of image regions related to the question.</p>
<p>Stacked Attention repeats attention multiple times to gradually understand the complex relationship between the image and the question. It’s similar to a detective looking at a picture multiple times to deeply understand its relevance to the question.</p>
<p>Representative models include Stacked Attention Networks (SAN) (Yang et al., 2016) and Dual Attention Networks (DAN) (Nam et al., 2017). SAN is a model that applies attention to images multiple times to generate answers to questions, while DAN calculates attention separately for images and questions and combines them to generate answers.</p>
</section>
<section id="external-knowledge-integration-after-2018" class="level4">
<h4 class="anchored" data-anchor-id="external-knowledge-integration-after-2018">External Knowledge Integration (After 2018)</h4>
<p>The biggest difference between image captioning and VQA is the integration of external knowledge. To further improve the inference capabilities of VQA models, research has been conducted to utilize external knowledge (common sense, encyclopedia knowledge, etc.). The Knowledge Base (KB) uses structured knowledge bases such as Wikipedia and ConceptNet to provide information needed to find answers to questions.</p>
<p>Memory Networks store external knowledge in memory form and use it to generate answers by searching for relevant information in memory according to the question. However, effectively utilizing external knowledge is still a challenging task. There are many problems to be solved, including the imperfection of the knowledge base, judgment of relevance to questions, and complexity of the inference process.</p>
</section>
<section id="vqa-evolution-from-a-deep-learning-dna-perspective" class="level4">
<h4 class="anchored" data-anchor-id="vqa-evolution-from-a-deep-learning-dna-perspective">VQA Evolution from a Deep Learning DNA Perspective</h4>
<p>VQA research has added important genes to deep learning DNA. The combination of CNN-RNN shares a basic framework with image captioning for combining images and text. Multimodal attention gives deep learning models the ability to model complex relationships between images and questions. This means that deep learning models can understand and infer interactions between information, rather than just combining it.</p>
<p>External knowledge integration has opened up the possibility for deep learning models to perform higher-level inferences using external knowledge. This shows that deep learning models can utilize human knowledge and experience, rather than just relying on data. 10.2.1 and 10.2.2 sections reviewed image captioning and VQA, which were two important pillars of early multimodal deep learning research. These studies greatly contributed to applying and advancing the core technologies of deep learning, such as CNN, RNN, and attention mechanisms, to multimodal problems, and became an important foundation for the emergence of more powerful multimodal models based on transformers (CLIP, DALL-E, GPT-4V, Gemini, etc.).</p>
<p>Recently, transformer-based VQA models like ViLT (Vision-and-Language Transformer) have emerged, showing good performance. ViLT inputs image patches and text tokens into the same transformer model, effectively modeling complex interactions between images and text.</p>
</section>
<section id="vqa-model-vilt-example" class="level4">
<h4 class="anchored" data-anchor-id="vqa-model-vilt-example">VQA Model (ViLT) Example</h4>
<p>ViLT (Vision-and-Language Transformer) is one of the representative transformer-based VQA models. ViLT inputs image patches and text tokens into the same transformer model, effectively modeling complex interactions between images and text.</p>
<p>The following is an example code for performing VQA using the ViLT model with the Hugging Face Transformers library.</p>
<div id="cell-6" class="cell" data-execution_count="6">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> ViltProcessor, ViltForQuestionAnswering</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> Image</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> requests</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="co"># 모델과 프로세서 로드</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>processor <span class="op">=</span> ViltProcessor.from_pretrained(<span class="st">"dandelin/vilt-b32-finetuned-vqa"</span>)</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> ViltForQuestionAnswering.from_pretrained(<span class="st">"dandelin/vilt-b32-finetuned-vqa"</span>)</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a><span class="co"># 이미지 다운로드</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>url <span class="op">=</span> <span class="st">"http://images.cocodataset.org/val2017/000000039769.jpg"</span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>image <span class="op">=</span> Image.<span class="bu">open</span>(requests.get(url, stream<span class="op">=</span><span class="va">True</span>).raw)</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a><span class="co"># 이미지 출력</span></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>plt.imshow(image)</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>plt.axis(<span class="st">'off'</span>)  <span class="co"># 축 제거</span></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a><span class="co"># 질문 설정</span></span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>question <span class="op">=</span> <span class="st">"How many cats are in the image?"</span></span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Question:"</span>, question)</span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a><span class="co"># 입력 전처리</span></span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>encoding <span class="op">=</span> processor(image, question, return_tensors<span class="op">=</span><span class="st">"pt"</span>)</span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a><span class="co"># 추론</span></span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>outputs <span class="op">=</span> model(<span class="op">**</span>encoding)</span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a>logits <span class="op">=</span> outputs.logits</span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a>idx <span class="op">=</span> logits.argmax(<span class="op">-</span><span class="dv">1</span>).item()</span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Predicted answer:"</span>, model.config.id2label[idx])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="10_Multimodal Deep Learning: The Beginning of Multisensory Convergence_files/figure-html/cell-4-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Question: How many cats are in the image?
Predicted answer: 2</code></pre>
</div>
</div>
</section>
</section>
</section>
<section id="multimodal-fusion-theory-classification-based-on-cmu-lectures" class="level2">
<h2 class="anchored" data-anchor-id="multimodal-fusion-theory-classification-based-on-cmu-lectures">10.3 Multimodal Fusion Theory: Classification Based on CMU Lectures</h2>
<p>Let’s say we have two types of information: images and text. How can we combine these two pieces of information? The simplest way is to concatenate the image vector with the text vector to create a new vector. Connecting information from heterogeneous data sources is called fusion. Efficiently fusing information from two heterogeneous data characteristics is the core of multimodal learning. One reason why it’s difficult to start multimodal deep learning is that it’s a <strong>rapidly evolving field, making systematic organization lacking</strong>.</p>
<p>In this section, we will explain multimodal fusion in three main categories based on the methods presented in Carnegie Mellon University (CMU) Multimodal Machine Learning lectures. Although this classification is not the standard for current multimodal research, it is very useful for understanding various fusion techniques systematically.</p>
<section id="joint-representations" class="level3">
<h3 class="anchored" data-anchor-id="joint-representations">10.3.1 Joint Representations</h3>
<p>Joint Representations is a method of representing multiple modalities of data in a single common vector space. It’s like drawing text and images together on one canvas.</p>
<p>Instead of processing each modality’s data separately, they are fused into one integrated feature vector. This vector contains the information of each modality, allowing the model to learn deep correlations between modalities. One model can process multiple modalities, and the model structure is relatively simple and efficient because it compresses and represents multiple modalities’ information in one vector. However, each modality’s unique characteristics may be diluted or lost during the fusion process. If a particular modality has much more information than other modalities, an information imbalance problem can occur. Fusing data from different modalities into one meaningful vector is a very difficult problem.</p>
<p>One simple method is to concatenate each modality’s feature vectors. Additionally, the Multi-modal Factorization Model (MFM) combines multiple types of data through matrix factorization to create a common representation space. Multi-modal Discriminative Binary Embedding (MDBE) is a method that represents multimodal data such as images and text as binary codes.</p>
<p>Recent research has proposed methods like COSA (Concatenated Sample), which sequentially connects multiple image-text pairs and applies a transformer-based model to jointly learn visual content and temporal cues. Attentional Concatenation is also used to generate high-resolution images from text, using a multi-level hierarchical structure and utilizing the results of previous layers and word vectors as input for the next layer.</p>
<p><strong>Structural Example</strong></p>
<p>The following diagram illustrates the fusion of three methods (Concatenation, MFM, MDBF).</p>
<p><img src="../../../assets/images/10_01.png" width="800"></p>
<p><strong>Example</strong></p>
<div id="cell-8" class="cell" data-execution_count="7">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoModel, AutoProcessor, AutoTokenizer</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> Image</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> requests</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Load pre-trained models and processor/tokenizer for image and text</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>image_model_name <span class="op">=</span> <span class="st">"google/vit-base-patch16-224-in21k"</span>  <span class="co"># ViT (Vision Transformer)</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>text_model_name <span class="op">=</span> <span class="st">"bert-base-uncased"</span>  <span class="co"># BERT</span></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>image_processor <span class="op">=</span> AutoProcessor.from_pretrained(image_model_name)</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>image_model <span class="op">=</span> AutoModel.from_pretrained(image_model_name)</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(text_model_name)</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>text_model <span class="op">=</span> AutoModel.from_pretrained(text_model_name)</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Example image and text</span></span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>url <span class="op">=</span> <span class="st">"http://images.cocodataset.org/val2017/000000039769.jpg"</span></span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>image <span class="op">=</span> Image.<span class="bu">open</span>(requests.get(url, stream<span class="op">=</span><span class="va">True</span>).raw)</span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>text <span class="op">=</span> <span class="st">"Two cats sleeping on a couch."</span></span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the image</span></span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a>plt.imshow(image)</span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a>plt.axis(<span class="st">'off'</span>)  <span class="co"># Remove axes</span></span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Preprocess image and text</span></span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a>image_inputs <span class="op">=</span> image_processor(images<span class="op">=</span>image, return_tensors<span class="op">=</span><span class="st">"pt"</span>)</span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a>text_inputs <span class="op">=</span> tokenizer(text, return_tensors<span class="op">=</span><span class="st">"pt"</span>)</span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-31"><a href="#cb6-31" aria-hidden="true" tabindex="-1"></a><span class="co"># Feature extraction (embeddings) for each modality</span></span>
<span id="cb6-32"><a href="#cb6-32" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():  <span class="co"># Disable gradient calculation (inference mode)</span></span>
<span id="cb6-33"><a href="#cb6-33" aria-hidden="true" tabindex="-1"></a>    image_features <span class="op">=</span> image_model(<span class="op">**</span>image_inputs).last_hidden_state[:, <span class="dv">0</span>, :]  <span class="co"># [CLS] token embedding</span></span>
<span id="cb6-34"><a href="#cb6-34" aria-hidden="true" tabindex="-1"></a>    text_features <span class="op">=</span> text_model(<span class="op">**</span>text_inputs).last_hidden_state[:, <span class="dv">0</span>, :]   <span class="co"># [CLS] token embedding</span></span>
<span id="cb6-35"><a href="#cb6-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-36"><a href="#cb6-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-37"><a href="#cb6-37" aria-hidden="true" tabindex="-1"></a><span class="co"># Create Joint Representation (Concatenation)</span></span>
<span id="cb6-38"><a href="#cb6-38" aria-hidden="true" tabindex="-1"></a>joint_representation <span class="op">=</span> torch.cat((image_features, text_features), dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb6-39"><a href="#cb6-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-40"><a href="#cb6-40" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Image Features Shape:"</span>, image_features.shape)  <span class="co"># Image feature vector size</span></span>
<span id="cb6-41"><a href="#cb6-41" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Text Features Shape:"</span>, text_features.shape)     <span class="co"># Text feature vector size</span></span>
<span id="cb6-42"><a href="#cb6-42" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Joint Representation Shape:"</span>, joint_representation.shape) <span class="co"># Combined feature vector size (image + text)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stderr">
<pre><code>Fast image processor class &lt;class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'&gt; is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="10_Multimodal Deep Learning: The Beginning of Multisensory Convergence_files/figure-html/cell-5-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Image Features Shape: torch.Size([1, 768])
Text Features Shape: torch.Size([1, 768])
Joint Representation Shape: torch.Size([1, 1536])</code></pre>
</div>
</div>
</section>
<section id="coordinated-representations" class="level3">
<h3 class="anchored" data-anchor-id="coordinated-representations">10.3.2 Coordinated Representations</h3>
<p>Coordinated Representations is a method that represents each modality in a separate space and explicitly learns the relationship between them. It’s similar to having multiple canvas paintings, where each canvas harmonizes with the others.</p>
<p>Each modality is represented as a separate feature vector, but these vectors are learned to “coordinate” with each other. In other words, the feature space of each modality is independent, but their similarities, order relationships, and other meaningful connections are learned to establish a meaningful relationship between them. The advantage of this approach is that it preserves the unique characteristics of each modality while considering its relevance to other modalities. Additionally, it can learn the relationships between various forms of modalities, making it applicable to diverse multi-modal problems.</p>
<p>However, since each modality must be processed separately, the model structure may become more complex than Joint Representations. This can make model design and training more difficult. Furthermore, explicitly learning the relationship between each modality is not an easy task.</p>
<p>A representative example is CLIP (Contrastive Language-Image Pre-training). CLIP processes images and text through separate encoders to obtain feature vectors and learns their similarity. CLIP learns to pair images and text, allowing it to understand the meaningful relationship between them.</p>
<p>CLIP’s success is particularly notable in its zero-shot learning ability. A pre-trained CLIP model can classify or search for new images without additional training for a specific task. This is possible because it effectively learns the semantic connection between text and images.</p>
<p><strong>Structure Example</strong></p>
<p>The following is a diagram of CLIP’s fusion.</p>
<p><img src="../../../assets/images/10_02.png" width="800"></p>
<ul>
<li>Image encoder: ViT (Vision Transformer) or ResNet</li>
<li>Text encoder: Transformer</li>
</ul>
<p><strong>Example</strong></p>
<div id="cell-10" class="cell" data-execution_count="8">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> CLIPProcessor, CLIPModel</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> Image</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> requests</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Load CLIP model and processor</span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> CLIPModel.from_pretrained(<span class="st">"openai/clip-vit-base-patch32"</span>)</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>processor <span class="op">=</span> CLIPProcessor.from_pretrained(<span class="st">"openai/clip-vit-base-patch32"</span>)</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Example image and text</span></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>url <span class="op">=</span> <span class="st">"http://images.cocodataset.org/val2017/000000039769.jpg"</span></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>image <span class="op">=</span> Image.<span class="bu">open</span>(requests.get(url, stream<span class="op">=</span><span class="va">True</span>).raw)</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>text <span class="op">=</span> <span class="st">"Two cats sleeping on a couch."</span></span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Display image</span></span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>plt.imshow(image)</span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>plt.axis(<span class="st">'off'</span>)  <span class="co"># Remove axes</span></span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Preprocess image and text</span></span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>inputs <span class="op">=</span> processor(text<span class="op">=</span>[text], images<span class="op">=</span>image, return_tensors<span class="op">=</span><span class="st">"pt"</span>, padding<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract image and text features (embeddings)</span></span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a>    outputs <span class="op">=</span> model(<span class="op">**</span>inputs)</span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a>    image_features <span class="op">=</span> outputs.image_embeds</span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a>    text_features <span class="op">=</span> outputs.text_embeds</span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-30"><a href="#cb9-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Coordinated Representation: Keep features of each modality separate</span></span>
<span id="cb9-31"><a href="#cb9-31" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Image Features Shape:"</span>, image_features.shape)</span>
<span id="cb9-32"><a href="#cb9-32" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Text Features Shape:"</span>, text_features.shape)</span>
<span id="cb9-33"><a href="#cb9-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-34"><a href="#cb9-34" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate similarity between image and text (dot product)</span></span>
<span id="cb9-35"><a href="#cb9-35" aria-hidden="true" tabindex="-1"></a>similarity <span class="op">=</span> torch.matmul(image_features, text_features.T)  <span class="co"># Or text_features @ image_features.T</span></span>
<span id="cb9-36"><a href="#cb9-36" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Image-Text Similarity:"</span>, similarity.item())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="10_Multimodal Deep Learning: The Beginning of Multisensory Convergence_files/figure-html/cell-6-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Image Features Shape: torch.Size([1, 512])
Text Features Shape: torch.Size([1, 512])
Image-Text Similarity: 0.29803216457366943</code></pre>
</div>
</div>
<p>By applying the above method, a simple zero-shot test is possible as follows.</p>
<div id="cell-12" class="cell" data-execution_count="9">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Zero-shot 이미지 분류</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="co">#   - 여러 텍스트 후보군을 만들고, 각 텍스트와 이미지 간의 유사도를 계산하여 가장 높은 유사도를 갖는 텍스트를 선택</span></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>candidate_texts <span class="op">=</span> [<span class="st">"a photo of a cat"</span>, <span class="st">"a photo of a dog"</span>, <span class="st">"a photo of a bird"</span>]</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>inputs <span class="op">=</span> processor(text<span class="op">=</span>candidate_texts, images<span class="op">=</span>image, return_tensors<span class="op">=</span><span class="st">"pt"</span>, padding<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>    outputs <span class="op">=</span> model(<span class="op">**</span>inputs)</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>    image_features <span class="op">=</span> outputs.image_embeds</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>    text_features <span class="op">=</span> outputs.text_embeds</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>    logits_per_image <span class="op">=</span> outputs.logits_per_image <span class="co"># 유사도 점수</span></span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>probs <span class="op">=</span> logits_per_image.softmax(dim<span class="op">=</span><span class="dv">1</span>)  <span class="co">#  확률</span></span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>predicted_class_idx <span class="op">=</span> probs.argmax().item()</span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a>predicted_class <span class="op">=</span> candidate_texts[predicted_class_idx]</span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Predicted Class:"</span>, predicted_class)</span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Probabilities:"</span>, probs)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Predicted Class: a photo of a cat
Probabilities: tensor([[9.9403e-01, 5.1377e-03, 8.3070e-04]])</code></pre>
</div>
</div>
</section>
<section id="encoder-decoder" class="level3">
<h3 class="anchored">10.3.3 Encoder-Decoder</h3>
<p>The Encoder-Decoder is a method of converting data from one modality to another. It is a technique commonly used in language translation.</p>
<p>In this structure, the encoder converts the input modality data (e.g., image) into a feature vector. This feature vector compactly represents the core information of the input data. The decoder generates data of another modality (e.g., text) based on the feature vector created by the encoder. The decoder “interprets” the output of the encoder to create new data. Additionally, through the attention mechanism, the decoder learns which part of the encoder’s feature vector to “pay attention” to when generating output data.</p>
<p>The advantage of this method is that it can be applied to various tasks that connect different forms of data, such as image captioning, VQA, and machine translation. It can also be applied even if the input and output modalities are different, and various combinations such as text-image, image-text, and audio-text are possible.</p>
<p>Representative examples include image captioning and VQA (Visual Question Answering). Image captioning processes an image with an encoder to obtain a feature vector and uses a decoder to generate a caption (text). VQA processes an image and a question (text) separately with encoders, uses an attention mechanism to understand the relationship between the image and the question, and then uses a decoder to generate an answer (text).</p>
<p>However, if the input or output data becomes longer, information loss may occur or the amount of computation may increase. In particular, in the case of RNN-based models, it may be difficult to learn long-distance dependencies due to the gradient vanishing problem. Additionally, since the encoder and decoder must be trained simultaneously, training can be unstable or difficult.</p>
<p><strong>Structure Example</strong></p>
<p>The following is a diagram of the Encoder-Decoder fusion.</p>
<p><img src="../../../assets/images/10_03.png" width="800"></p>
<ul>
<li>Image Input, Text Input: Represent image and text inputs (questions or other text information), respectively.</li>
<li>Image Encoder, Text Encoder: Encoders for each modality. Image encoders typically use CNN or ViT (Vision Transformer), while text encoders use RNN or Transformer.</li>
<li>Attention: A mechanism that determines which part of the image features (Image Encoder’s Features) to “pay attention” to when the decoder generates text. Text Encoder’s Features can also be used for Attention (Cross-Modal Attention).</li>
</ul>
<p><strong>Example</strong></p>
<div id="cell-14" class="cell" data-execution_count="10">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> BlipProcessor, BlipForConditionalGeneration</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> Image</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> requests</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Load model and processor</span></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>processor <span class="op">=</span> BlipProcessor.from_pretrained(<span class="st">"Salesforce/blip-image-captioning-base"</span>)</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> BlipForConditionalGeneration.from_pretrained(<span class="st">"Salesforce/blip-image-captioning-base"</span>)</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Download image</span></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>url <span class="op">=</span> <span class="st">"http://images.cocodataset.org/val2017/000000000139.jpg"</span></span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>image <span class="op">=</span> Image.<span class="bu">open</span>(requests.get(url, stream<span class="op">=</span><span class="va">True</span>).raw)</span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Display image</span></span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>plt.imshow(image)</span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>plt.axis(<span class="st">'off'</span>)</span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Input text (optional - Conditional Generation)</span></span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a><span class="co"># text = "describe this image:"  # Prompt (guide image description)</span></span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a>text <span class="op">=</span> <span class="st">"a photo of"</span></span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Preprocess image and text (optional)</span></span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a><span class="co"># If text is provided, it uses the text as a prompt to generate the caption.</span></span>
<span id="cb13-25"><a href="#cb13-25" aria-hidden="true" tabindex="-1"></a>inputs <span class="op">=</span> processor(image, text<span class="op">=</span>text, return_tensors<span class="op">=</span><span class="st">"pt"</span>)</span>
<span id="cb13-26"><a href="#cb13-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-27"><a href="#cb13-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate caption</span></span>
<span id="cb13-28"><a href="#cb13-28" aria-hidden="true" tabindex="-1"></a>outputs <span class="op">=</span> model.generate(<span class="op">**</span>inputs)</span>
<span id="cb13-29"><a href="#cb13-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-30"><a href="#cb13-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Decode and print caption</span></span>
<span id="cb13-31"><a href="#cb13-31" aria-hidden="true" tabindex="-1"></a>caption <span class="op">=</span> processor.decode(outputs[<span class="dv">0</span>], skip_special_tokens<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb13-32"><a href="#cb13-32" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Generated caption:"</span>, caption)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="10_Multimodal Deep Learning: The Beginning of Multisensory Convergence_files/figure-html/cell-8-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Generated caption: a photo of a living room with a television and a fireplace</code></pre>
</div>
</div>
<p>This example shows image captioning, a representative example of the Encoder-Decoder structure. The encoder takes an image (BLIP’s visual encoder) as input and extracts feature vectors. The decoder generates text (BLIP’s text decoder). It determines which part of the image feature vector to focus on through the attention mechanism while generating captions. You can specify a prompt that affects the caption generated as text. Although BLIP can use both images and text as inputs, here we only use images as inputs and generate text in the decoder.</p>
<p>In sections 10.3.1, 10.3.2, and 10.3.3, we looked at the three core theories of multimodal fusion: Joint Representations, Coordinated Representations, and Encoder-Decoder. Each method has its own characteristics and advantages and disadvantages, so it is important to select the appropriate method according to the application field.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Click to view content (Deep Dive: Multimodal Fusion and Latest Research Trends)">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Click to view content (Deep Dive: Multimodal Fusion and Latest Research Trends)
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<section id="multimodal-fusion-and-latest-research-trends" class="level2 callout-body-container callout-body">
<h2 class="anchored" data-anchor-id="multimodal-fusion-and-latest-research-trends">Multimodal Fusion and Latest Research Trends</h2>
<p>In multimodal deep learning, “fusion” refers to the core process of combining information from different modalities to create a richer and more powerful representation. Although we briefly reviewed CMU lecture-based fusion theory in Section 10.3, actual multimodal fusion research has been much more diverse and dynamic. In this deep dive, we will analyze various classification systems for fusion and the latest research trends in depth, and examine which technologies are being noted as of 2025.</p>
<section id="various-classifications-of-multimodal-fusion" class="level3">
<h3 class="anchored" data-anchor-id="various-classifications-of-multimodal-fusion">1. Various Classifications of Multimodal Fusion</h3>
<p>Multimodal fusion is difficult to classify based on a single criterion. Researchers classify fusion methods from various perspectives, and each classification is not mutually exclusive but complementary.</p>
<section id="classification-by-fusion-point-early-late-hybrid-fusion" class="level4">
<h4 class="anchored" data-anchor-id="classification-by-fusion-point-early-late-hybrid-fusion">1.1 Classification by Fusion Point (Early, Late, Hybrid Fusion)</h4>
<p>This classification focuses on “which stage” of the multimodal deep learning model the fusion occurs. (Refer to Section 10.3.4)</p>
<ul>
<li><p><strong>Early Fusion:</strong> Combines “raw” data (or features processed very early) from each modality at the input stage of the model.</p>
<ul>
<li><strong>Latest Research:</strong> The LUMA dataset (announced in June 2024) is a multimodal benchmark with 50 classes, including audio, image, and text data. It provides an environment to verify the effectiveness of early fusion in uncertain and diverse modality data. This dataset extends CIFAR 10/100 and includes audio samples extracted from three audio corpora and text data generated by Gemma-7B LLM, allowing for controlled injection of various types and degrees of uncertainty.</li>
</ul></li>
<li><p><strong>Late Fusion:</strong> Processes each modality with a separate model and combines the outputs (e.g., prediction results) at the final stage.</p>
<ul>
<li><strong>Latest Research:</strong> According to a study published in January 2024, late fusion (also known as model fusion) is an approach that combines decisions obtained from various modalities, which is effectively utilized in skin disease diagnosis. This method allows for the use of models specialized for each modality, maximizing the strengths of individual modalities.</li>
</ul></li>
<li><p><strong>Hybrid Fusion:</strong> Combines Early Fusion and Late Fusion. It performs fusion at multiple stages of the model to utilize information at various levels.</p>
<ul>
<li><strong>Latest Research:</strong> CAST (Cross Attention-based multimodal fusion of Structure and Text) is a study published on February 6, 2025, which proposes a hybrid approach to effectively fuse structure and text data in materials science. The model combines node-level and token-level features, allowing for close interaction, and uses a linked query key-value-based cross-attention mechanism to capture the relationship between nodes and text.</li>
</ul></li>
</ul>
</section>
<section id="classification-by-model-structure" class="level4">
<h4 class="anchored" data-anchor-id="classification-by-model-structure">1.2 Classification by Model Structure</h4>
<ul>
<li><p><strong>Model-Agnostic Fusion:</strong> General fusion techniques that do not depend on specific models (e.g., Early, Late, Hybrid Fusion).</p></li>
<li><p><strong>Model-Specific Fusion:</strong> Fusion techniques specialized for specific model structures.</p>
<ul>
<li><strong>Transformer’s Cross-Modal Attention:</strong> (Detailed in Section 10.4.2)</li>
</ul></li>
<li><p><strong>Latest Research:</strong> The MULA 2025 workshop, to be held on June 11-12, 2025, will discuss model structures for effectively fusing various sensor data (camera, LiDAR, radar, etc.) in the field of autonomous driving. This workshop aims to encourage interdisciplinary interactions and collaborations between computer vision, multimedia, remote sensing, and robotics communities, with a particular focus on multimodal approaches in autonomous driving.</p></li>
</ul>
</section>
<section id="other-classifications" class="level4">
<h4 class="anchored" data-anchor-id="other-classifications">1.3 Other Classifications</h4>
<ul>
<li><p><strong>Symmetric vs.&nbsp;Asymmetric Fusion:</strong></p>
<ul>
<li><p><strong>Symmetric:</strong> Treats all modalities equally.</p></li>
<li><p><strong>Asymmetric:</strong> Gives more weight to certain modalities or assigns different roles.</p></li>
<li><p><strong>Latest Research:</strong> “Learning Deep Multimodal Feature Representation with Asymmetric Multi-layer Fusion” proposed an effective framework for fusing multimodal features in multiple layers within a single network. This study introduced two asymmetric fusion operations, channel shuffle and pixel shift, to learn different features according to various fusion directions. Additionally, “Multimodal sentiment analysis based on multi-layer feature fusion,” published in January 2025, presented a new approach for accurate sentiment analysis under modal imbalance and implicit expression conditions.</p></li>
</ul></li>
<li><p><strong>Explicit vs.&nbsp;Implicit Fusion:</strong></p>
<ul>
<li><p><strong>Explicit:</strong> Explicitly defines or models the relationships between modalities (e.g., attention mechanisms).</p></li>
<li><p><strong>Implicit:</strong> Does not directly define the relationships between modalities, but rather allows the model to learn them through training (e.g., simple concatenation).</p></li>
<li><p><strong>Latest Research:</strong> The HCI International 2025 conference (June 2025) will feature a study comparing the pros and cons of explicit and implicit fusion.</p></li>
</ul></li>
</ul>
</section>
</section>
<section id="latest-trends-attention-based-fusion-and-self-supervised-learning" class="level3">
<h3 class="anchored" data-anchor-id="latest-trends-attention-based-fusion-and-self-supervised-learning">2. Latest Trends: Attention-Based Fusion and Self-Supervised Learning</h3>
<p>The most notable fusion method in 2024-2025 research is the <strong>attention-based mechanism</strong>.</p>
<section id="cross-modal-attention" class="level4">
<h4 class="anchored" data-anchor-id="cross-modal-attention">2.1 Cross-Modal Attention</h4>
<ul>
<li><p><strong>Concept:</strong> Applies attention to the features of one modality (key-value) using the features of another modality as a query. (Refer to Section 10.4.2) This allows the model to finely capture the relationships between specific elements of different modalities.</p></li>
<li><p><strong>Advantages:</strong> Can capture fine-grained and flexible relationships between modalities. For example, in image captioning, it can focus on the “running” action of a dog in the image when generating the word “running.”</p></li>
<li><p><strong>Latest Research</strong></p>
<ul>
<li><p>The “Bi-Att3DDet” study, published in January 2025, introduced a bidirectional attention-based fusion method for 3D object detection in autonomous driving. This study proposed a bidirectional interaction approach to maximize the utilization of complementary information between LiDAR and camera data.</p></li>
<li><p>The “LANMSFF” study, published in March 2024 and revised in February 2025, combined a lightweight attention-based network with multi-scale feature fusion for multi-view facial expression recognition. This approach simultaneously generates channel and spatial attention maps to emphasize important features and suppress irrelevant ones.</p></li>
<li><p>A recent neuroscience study (2025) investigated the impact of cross-modal congruency on sensory information processing and accumulation. The study showed that congruence between auditory and visual stimuli plays a crucial role in the early stages of sensory processing. #### 2.2 Multi-head Attention</p></li>
</ul></li>
<li><p><strong>Concept:</strong> Uses multiple attention heads to capture relationships between modalities from different perspectives. Each head uses different weight matrices (W_Q, W_K, W_V) to transform the input data and calculate attention, allowing each head to focus on different aspects of the input data (e.g., meaning, grammatical structure, style).</p></li>
<li><p><strong>Advantages:</strong> Can model various types of relationships simultaneously, enabling the learning of richer and more complex representations. For example, when fusing images and text, some heads can focus on the relationship between objects in the image and words in the text, while others focus on the relationship between the overall atmosphere of the image and the tone of the text.</p></li>
<li><p><strong>Recent Research:</strong> Recent large-scale multi-modal models (LMMs) have further expanded and refined this technique, effectively modeling complex interactions between various modalities such as images, text, audio, and video.</p></li>
</ul>
</section>
<section id="self-supervised-learning-and-multi-modal-fusion" class="level4">
<h4 class="anchored" data-anchor-id="self-supervised-learning-and-multi-modal-fusion">2.3 Self-Supervised Learning and Multi-Modal Fusion</h4>
<ul>
<li><p><strong>Contrastive Learning:</strong></p>
<ul>
<li><p><strong>Concept:</strong> Learns to place related modality pairs (e.g., an image and its caption) close together in the embedding space and unrelated pairs far apart.</p></li>
<li><p><strong>Advantages:</strong> Can be effectively learned from large-scale unlabeled datasets, helping to alleviate data scarcity issues.</p></li>
<li><p><strong>Recent Research:</strong> “Dual-Level Cross-Modal Contrastive Clustering” (2024) proposes a new contrastive learning method to bridge the gap between visual representations and text semantics.</p></li>
</ul></li>
<li><p><strong>Masking-Based Learning:</strong></p>
<ul>
<li><p><strong>Concept:</strong> Masks parts of the input and learns to recover them using information from other modalities.</p></li>
<li><p><strong>Advantages:</strong> Can learn complementary relationships between modalities. For example, it can learn to predict masked parts of an image using text descriptions or predict masked words in text using images.</p></li>
<li><p><strong>Recent Research:</strong> CAST (2025) improved the alignment between graph structure nodes and text tokens through a Masked Node Prediction (MNP) pre-training strategy.</p></li>
</ul></li>
</ul>
</section>
</section>
<section id="token-level-vs-instance-level-fusion-2025-research" class="level3">
<h3 class="anchored" data-anchor-id="token-level-vs-instance-level-fusion-2025-research">3. Token-Level vs Instance-Level Fusion (2025 Research)</h3>
<ul>
<li><p><strong>Token-Level Fusion:</strong> Models fine-grained interactions between individual tokens of each modality (e.g., image patches, text tokens).</p>
<ul>
<li><p><strong>Advantages:</strong> Enables more nuanced capture of relationships between modalities. For example, it can learn direct correspondence relationships between specific objects in an image and specific words in text.</p></li>
<li><p><strong>Recent Research:</strong> CAST (2025) demonstrated that token-level fusion outperforms instance-level fusion in the field of materials science, particularly in graph node and text token fusion.</p></li>
</ul></li>
<li><p><strong>Instance-Level Fusion:</strong> Treats each modality’s entire instance (e.g., an entire image, entire text) as a single unit for fusion.</p>
<ul>
<li><p><strong>Advantages:</strong> High computational efficiency and simple implementation.</p></li>
<li><p><strong>Disadvantages:</strong> May fail to capture detailed relationships within modalities.</p></li>
</ul></li>
</ul>
</section>
<section id="conclusion" class="level3">
<h3 class="anchored" data-anchor-id="conclusion">4. Conclusion</h3>
<p>Multi-modal fusion can be classified in various ways, each providing different perspectives. In actual research, these classifications are often combined and used together. As of 2025, multimodal fusion research focuses on developing efficient fusion techniques using fine-grained interactions at the token level, cross-attention mechanisms, and self-supervised learning methods. In particular, major academic events such as the CVPR 2025 workshop (June 2025, Nashville) will actively discuss advances in multimodal fusion technology in various application fields, including autonomous driving, medical diagnosis, and materials science.</p>
<p>Through this deep dive, we will understand the various classification systems of multimodal fusion and grasp the characteristics of each approach, allowing us to analyze the various multimodal models introduced later more in-depth.</p>
</section>
</section>
</div>
</div>
<p>There is no original text provided to translate.</p>
</section>
<section id="modality-integration-strategies" class="level3">
<h3 class="anchored" data-anchor-id="modality-integration-strategies">10.3.4 Modality Integration Strategies</h3>
<p>From Sections 10.3.1 to 10.3.3, we examined ways to fuse multimodal data. This is a theoretical classification. When actually designing a multimodal model, it is necessary to strategically decide which fusion method to apply, when, and how, according to the characteristics of the given problem and data. In this section, we will look at sophisticated modality integration strategies adopted by state-of-the-art multimodal models.</p>
<section id="early-fusion" class="level4">
<h4 class="anchored" data-anchor-id="early-fusion">10.3.4.1 Early Fusion</h4>
<p>Early fusion combines the inputs of multiple modalities in the early stages of the model. The simplest form is to concatenate the feature vectors of each modality. The advantage of early fusion is that it is easy to capture low-level interactions between modalities. For example, if the color of an image and a specific word in text are strongly related, early fusion can easily learn this relationship. However, it may not fully utilize the characteristics of each modality. In particular, when specialized processing is required for each modality (e.g., CNN for images and RNN for text), early fusion can be inefficient.</p>
<p>Recent studies have also presented benchmarks that validate the effectiveness of early fusion in environments with noisy multimodal data, beyond simple concatenation.</p>
<p>Let’s take a look at a simple example of early fusion. This is an example where joint representation uses concatenation for early fusion. The same code is used. Finally, a simple linear classifier is used to determine whether there is a cat or not.</p>
<div id="cell-19" class="cell" data-execution_count="11">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoModel, AutoProcessor, AutoTokenizer</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> Image</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> requests</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a><span class="co"># 이미지와 텍스트를 위한 사전 학습된 모델 및 프로세서/토크나이저 로드</span></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>image_model_name <span class="op">=</span> <span class="st">"google/vit-base-patch16-224-in21k"</span>  <span class="co">#  ViT (Vision Transformer)</span></span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>text_model_name <span class="op">=</span> <span class="st">"bert-base-uncased"</span>  <span class="co"># BERT</span></span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>image_processor <span class="op">=</span> AutoProcessor.from_pretrained(image_model_name)</span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a>image_model <span class="op">=</span> AutoModel.from_pretrained(image_model_name)</span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(text_model_name)</span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a>text_model <span class="op">=</span> AutoModel.from_pretrained(text_model_name)</span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a><span class="co"># 예제 이미지 및 텍스트</span></span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a>url <span class="op">=</span> <span class="st">"http://images.cocodataset.org/val2017/000000039769.jpg"</span></span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a>image <span class="op">=</span> Image.<span class="bu">open</span>(requests.get(url, stream<span class="op">=</span><span class="va">True</span>).raw)</span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a>text <span class="op">=</span> <span class="st">"Two cats sleeping on a couch."</span></span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-22"><a href="#cb15-22" aria-hidden="true" tabindex="-1"></a><span class="co"># 이미지 출력</span></span>
<span id="cb15-23"><a href="#cb15-23" aria-hidden="true" tabindex="-1"></a>plt.imshow(image)</span>
<span id="cb15-24"><a href="#cb15-24" aria-hidden="true" tabindex="-1"></a>plt.axis(<span class="st">'off'</span>)  <span class="co"># 축 제거</span></span>
<span id="cb15-25"><a href="#cb15-25" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb15-26"><a href="#cb15-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-27"><a href="#cb15-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-28"><a href="#cb15-28" aria-hidden="true" tabindex="-1"></a><span class="co"># 이미지와 텍스트 전처리</span></span>
<span id="cb15-29"><a href="#cb15-29" aria-hidden="true" tabindex="-1"></a>image_inputs <span class="op">=</span> image_processor(images<span class="op">=</span>image, return_tensors<span class="op">=</span><span class="st">"pt"</span>)</span>
<span id="cb15-30"><a href="#cb15-30" aria-hidden="true" tabindex="-1"></a>text_inputs <span class="op">=</span> tokenizer(text, return_tensors<span class="op">=</span><span class="st">"pt"</span>)</span>
<span id="cb15-31"><a href="#cb15-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-32"><a href="#cb15-32" aria-hidden="true" tabindex="-1"></a><span class="co"># 각 모달리티에 대한 특징 추출 (임베딩)</span></span>
<span id="cb15-33"><a href="#cb15-33" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():  <span class="co"># 기울기 계산 비활성화 (추론 모드)</span></span>
<span id="cb15-34"><a href="#cb15-34" aria-hidden="true" tabindex="-1"></a>    image_features <span class="op">=</span> image_model(<span class="op">**</span>image_inputs).last_hidden_state[:, <span class="dv">0</span>, :]  <span class="co"># [CLS] 토큰 임베딩</span></span>
<span id="cb15-35"><a href="#cb15-35" aria-hidden="true" tabindex="-1"></a>    text_features <span class="op">=</span> text_model(<span class="op">**</span>text_inputs).last_hidden_state[:, <span class="dv">0</span>, :]   <span class="co"># [CLS] 토큰 임베딩</span></span>
<span id="cb15-36"><a href="#cb15-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-37"><a href="#cb15-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-38"><a href="#cb15-38" aria-hidden="true" tabindex="-1"></a><span class="co"># Joint Representation 생성 (Concatenation)</span></span>
<span id="cb15-39"><a href="#cb15-39" aria-hidden="true" tabindex="-1"></a>joint_representation <span class="op">=</span> torch.cat((image_features, text_features), dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb15-40"><a href="#cb15-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-41"><a href="#cb15-41" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Image Features Shape:"</span>, image_features.shape)  <span class="co"># 이미지 특징 벡터 크기</span></span>
<span id="cb15-42"><a href="#cb15-42" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Text Features Shape:"</span>, text_features.shape)     <span class="co"># 텍스트 특징 벡터 크기</span></span>
<span id="cb15-43"><a href="#cb15-43" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Joint Representation Shape:"</span>, joint_representation.shape) <span class="co"># 결합된 특징 벡터 크기 (image + text)</span></span>
<span id="cb15-44"><a href="#cb15-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-45"><a href="#cb15-45" aria-hidden="true" tabindex="-1"></a><span class="co">#  Joint Representation을 활용한 추가 작업 (예: 분류)</span></span>
<span id="cb15-46"><a href="#cb15-46" aria-hidden="true" tabindex="-1"></a>num_labels <span class="op">=</span> <span class="dv">2</span>  <span class="co">#  예: "고양이 없음(0)" "고양이 있음(1)", 두 가지 클래스로 분류</span></span>
<span id="cb15-47"><a href="#cb15-47" aria-hidden="true" tabindex="-1"></a>classifier <span class="op">=</span> torch.nn.Linear(joint_representation.size(<span class="dv">1</span>), num_labels) <span class="co"># 간단한 선형 분류기</span></span>
<span id="cb15-48"><a href="#cb15-48" aria-hidden="true" tabindex="-1"></a>outputs <span class="op">=</span> classifier(joint_representation)</span>
<span id="cb15-49"><a href="#cb15-49" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Classification Outputs:"</span>, outputs)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stderr">
<pre><code>Fast image processor class &lt;class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'&gt; is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="10_Multimodal Deep Learning: The Beginning of Multisensory Convergence_files/figure-html/cell-9-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Image Features Shape: torch.Size([1, 768])
Text Features Shape: torch.Size([1, 768])
Joint Representation Shape: torch.Size([1, 1536])
Classification Outputs: tensor([[0.1817, 0.0355]], grad_fn=&lt;AddmmBackward0&gt;)</code></pre>
</div>
</div>
<p>In the above example, the image and text are directly combined as the output of separate models, ViT and BERT. No additional processing (attention, complex transformation, etc.) is performed on these two vectors before combining the image features and text features. Therefore, this corresponds to early fusion.</p>
</section>
<section id="late-fusion" class="level4">
<h4 class="anchored" data-anchor-id="late-fusion">10.3.4.2 Late Fusion</h4>
<p>Late fusion processes each modality with a separate model and combines the outputs of each model (e.g., prediction results) in the final stage. The advantage of this approach is that it can use models specialized for each modality. For example, pre-trained CNN can be used for images and pre-trained Transformer can be used for text to effectively extract complex features from each modality. However, it only considers high-level interactions between modalities and has a disadvantage that information exchange in the middle stage is difficult.</p>
<p>Late fusion is similar to ensemble techniques, and there are active studies on combining the outputs of models for different modalities to improve performance.</p>
</section>
<section id="hybrid-fusion" class="level4">
<h4 class="anchored" data-anchor-id="hybrid-fusion">10.3.4.3 Hybrid Fusion</h4>
<p>Hybrid fusion combines early fusion and late fusion. It performs fusion at multiple stages of the model to utilize various levels of information. The advantage of this approach is that it can take advantage of both early fusion and late fusion. In other words, it can consider both low-level and high-level interactions between modalities. However, the model structure becomes complex, and there are many hyperparameters to tune.</p>
<p>A representative example of hybrid fusion is Cross-Modal Attention. This method uses the features of one modality as a query to apply attention to the features (key-value) of another modality. This is a representative method for performing fusion in the middle stage.</p>
<p>Recently, in addition to attention, various methods such as gated mechanisms and bilinear pooling have been attempted for mid-level fusion.</p>
</section>
<section id="refined-integration-strategies-for-recent-models-2023-and-later" class="level4">
<h4 class="anchored" data-anchor-id="refined-integration-strategies-for-recent-models-2023-and-later">10.3.4.4 Refined Integration Strategies for Recent Models (2023 and later)</h4>
<p>Since 2023, large-scale multi-modal models (LMMs) such as Gemini and GPT-4V have introduced more refined modality integration strategies to greatly improve performance.</p>
<p><strong>Selective Fusion Mechanism</strong> dynamically determines the importance of each modality and selectively integrates information. For example, when text is included in an image, it strongly associates the visual features of the text area with the text content. This is similar to how humans adjust the importance of visual and textual information according to the situation.</p>
<p><strong>Dynamic Weighting</strong> automatically adjusts the contribution of each modality based on the characteristics of the task and input. For example, in visual question answering (VQA) tasks, it assigns different weights to image and text information depending on the nature of the question. For a question like “What is the color of the image?”, it gives more weight to visual information, while for a question like “What does the image mean?”, it gives more weight to textual information.</p>
<p><strong>Task-Specific Fusion</strong> optimizes the modality integration method according to the requirements of a specific task. In image captioning, it focuses on one-way transfer from visual to text information, while in visual question answering, it enhances two-way interaction.</p>
<p>These refined integration strategies have greatly improved the performance of multi-modal models. In particular, by dynamically adjusting the role and importance of each modality and optimizing the fusion method according to the task characteristics, they have shown excellent results in tasks that require complex inference, beyond simple information combination. These integrated strategies require large datasets and computational resources, so it is difficult to implement and experiment directly through learning examples. Instead, it is desirable to understand conceptually through the papers and technical documents of each model.</p>
</section>
</section>
</section>
<section id="multimodal-representation-learning-techniques" class="level2">
<h2 class="anchored" data-anchor-id="multimodal-representation-learning-techniques">10.4 Multimodal Representation Learning Techniques</h2>
<p>In Section 10.3, we examined various theoretical methods and strategies for fusing multimodal data. Based on this, let’s take a look at the specific techniques that actual multimodal models use to effectively represent information from each modality and learn relationships between different modalities. The entire implementation is in <code>chapter_10/multimodal_embeding.py</code>.</p>
<section id="modality-to-modality-representation-learning" class="level3">
<h3 class="anchored" data-anchor-id="modality-to-modality-representation-learning">10.4.1 Modality-to-Modality Representation Learning</h3>
<p>One of the core tasks in multimodal learning is how to represent modalities with different characteristics in a meaningful <em>common space</em>. Images are 2D arrays of pixel values, text is a 1D sequence of tokens, and audio is amplitude values over time, each with its own unique representation method. To effectively process these heterogeneous data, a representation learning technique is needed that captures the essential characteristics of each modality while maintaining the semantic relationships between them.</p>
<p><strong>Early Approach: Individual Encoders + Projection</strong></p>
<p>Early multimodal models used specialized encoders for each modality (e.g., CNN for images, RNN for text) to extract feature vectors, which were then projected into a common vector space using linear transformation or shallow MLP. (Refer to Joint Representation and Concatenation methods in Section 10.3.1)</p>
<p><strong>Recent Approach: Semantic Alignment</strong></p>
<p>Recently, the main approach is to learn semantic alignment between modality-specific feature vectors, rather than simply matching dimensions. In other words, related images and text are learned to be close in the embedding space, while unrelated images and text are learned to be far apart.</p>
<ul>
<li><p><strong>Contrastive Learning:</strong> (Refer to Coordinated Representation and CLIP example in Section 10.3.2) Image-text pairs are considered “positive” samples, and randomly mixed image-text pairs are considered “negative” samples. The model learns to increase the similarity between positive samples and decrease the similarity between negative samples.</p></li>
<li><p><strong>Triplet Loss:</strong> Using an image anchor, a positive text (the caption of the anchor image), and a negative text (the caption of another image), the model learns to make the distance between the anchor image and the positive text close, and the distance between the anchor image and the negative text far.</p></li>
</ul>
<p><strong>Implementation Example (Contrastive Learning)</strong></p>
<div id="cell-22" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MultimodalEmbedding(nn.Module):</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, embedding_dim<span class="op">=</span><span class="dv">512</span>):</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.image_encoder <span class="op">=</span> models.resnet18(pretrained<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.image_encoder.fc <span class="op">=</span> nn.Sequential(</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">512</span>, embedding_dim),</span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>            nn.LayerNorm(embedding_dim)</span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.text_encoder <span class="op">=</span> BertModel.from_pretrained(<span class="st">'bert-base-uncased'</span>)</span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.text_projection <span class="op">=</span> nn.Sequential(</span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">768</span>, embedding_dim),  <span class="co"># BERT output dimension is 768</span></span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a>            nn.LayerNorm(embedding_dim)</span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-17"><a href="#cb18-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.logit_scale <span class="op">=</span> nn.Parameter(torch.ones([]) <span class="op">*</span> np.log(<span class="dv">1</span> <span class="op">/</span> <span class="fl">0.07</span>))</span>
<span id="cb18-18"><a href="#cb18-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-19"><a href="#cb18-19" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> encode_image(<span class="va">self</span>, image):</span>
<span id="cb18-20"><a href="#cb18-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.image_encoder(image)</span>
<span id="cb18-21"><a href="#cb18-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-22"><a href="#cb18-22" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> encode_text(<span class="va">self</span>, input_ids, attention_mask):</span>
<span id="cb18-23"><a href="#cb18-23" aria-hidden="true" tabindex="-1"></a>        text_features <span class="op">=</span> <span class="va">self</span>.text_encoder(input_ids, attention_mask)[<span class="dv">0</span>][:, <span class="dv">0</span>, :]  <span class="co"># [CLS] token, keep batch dim</span></span>
<span id="cb18-24"><a href="#cb18-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.text_projection(text_features)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<ul>
<li><strong><code>MultimodalEmbedding</code> class:</strong>
<ul>
<li><code>image_encoder</code>: Uses ResNet18 to convert an image into a feature vector of size <code>embedding_dim</code>.</li>
<li><code>text_encoder</code>: Employs the BERT model to convert text into a feature vector and aligns it to the size of <code>embedding_dim</code> through the <code>text_projection</code> layer.</li>
<li><code>logit_scale</code>: A learnable temperature parameter used in CLIP.</li>
</ul></li>
</ul>
<p><strong>Semantic Alignment Mechanism</strong></p>
<p>The semantic alignment is largely implemented in two parts: the forward method of the MultimodalEmbedding class and the constrasive_loss().</p>
<div id="cell-24" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> forward(<span class="va">self</span>, image, input_ids, attention_mask):</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>    image_features <span class="op">=</span> <span class="va">self</span>.encode_image(image)</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>    text_features <span class="op">=</span> <span class="va">self</span>.encode_text(input_ids, attention_mask)</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>    image_features <span class="op">=</span> image_features <span class="op">/</span> image_features.norm(dim<span class="op">=-</span><span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>    text_features <span class="op">=</span> text_features <span class="op">/</span> text_features.norm(dim<span class="op">=-</span><span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>    logit_scale <span class="op">=</span> <span class="va">self</span>.logit_scale.exp()</span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>    logits <span class="op">=</span> logit_scale <span class="op">*</span> image_features <span class="op">@</span> text_features.transpose(<span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">2</span>)</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># print("logits:", logits.shape)</span></span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> logits   <span class="co"># Return a single value</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<ul>
<li><strong><code>forward</code> method:</strong>
<ol type="1">
<li><p>Uses <code>encode_image</code> and <code>encode_text</code> to encode the image and text, respectively.</p></li>
<li><p><strong>Feature Normalization:</strong> Applies L2 normalization to set the magnitude of <code>image_features</code> and <code>text_features</code> vectors to 1. This is done to consider only the direction of the vectors when calculating similarity.</p></li>
<li><p><strong>Temperature Scaling:</strong> Utilizes <code>logit_scale</code> to adjust the distribution of similarity scores. The logit scale is applied to an exponential function to obtain a scaling value, which is then multiplied with the matrix product of the image feature matrix and the transposed text feature matrix. The matrix product calculates the dot product between each image feature vector and all text feature vectors to generate similarity scores.</p></li>
<li><p><code>logits</code>: Calculates the similarity (dot product) between image feature vectors and text feature vectors. Instead of using <code>text_features.t()</code>, <code>text_features.transpose(-1, -2)</code> is used for transposition. This swaps the last two dimensions of the text feature matrix from (batch, text feature dimension) to (batch, feature dimension, text), allowing multiplication with the image feature matrix of shape (batch, image feature dimension).</p></li>
</ol></li>
</ul>
<div id="cell-26" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> contrastive_loss(logits): <span class="co"># removed enhanced_similarity</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>    labels <span class="op">=</span> torch.arange(logits.size(<span class="dv">0</span>), device<span class="op">=</span>logits.device) <span class="co"># Use logits.size(0)</span></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Image-to-text and text-to-image contrastive loss</span></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>    img_txt_loss <span class="op">=</span> nn.CrossEntropyLoss()(logits, labels)</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>    txt_img_loss <span class="op">=</span> nn.CrossEntropyLoss()(logits.T, labels)</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Average loss</span></span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (img_txt_loss <span class="op">+</span> txt_img_loss) <span class="op">/</span> <span class="dv">2</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>In the <code>contrastive_loss</code> function, <code>labels</code> are generated as integers from 0 to (batch size - 1) to match the size of the <code>logits</code> matrix. The diagonal elements (i, i) in the <code>logits</code> matrix represent the similarity between the i-th image and the i-th text, which is the similarity of the positive pair (image-text pair), so the labels are set to make these diagonal elements correct. Additionally, <code>img_txt_loss</code> calculates the loss of image-to-text similarity, and <code>txt_img_loss</code> calculates the loss of text-to-image similarity. By averaging these two losses, both image-to-text and text-to-image semantic alignments are considered.</p>
<p>The semantic alignment mechanism maps features from different modalities to a semantically consistent space. First, all feature vectors are projected onto a unit sphere using L2 normalization to remove scale differences between modalities. A temperature scaling parameter is introduced to adjust the distribution of similarity values. High temperatures produce softer distributions, while low temperatures produce sharper distributions, increasing learning stability. Additionally, through contrastive learning, related image-text pairs are learned to be close in the embedding space, and unrelated pairs are learned to be far apart. In particular, both image-to-text and text-to-image mappings are simultaneously optimized to achieve bidirectional semantic alignment.</p>
<p>Like CLIP’s contrastive learning, related content is learned to be close, and unrelated content is learned to be far apart. This contrastive learning-based semantic alignment strategy has evolved from OpenAI’s CLIP in 2021 to Google’s PaLM-E, Anthropic’s Claude, and DeepMind’s Gemini. While early CLIP focused on simple contrastive learning of image-text pairs, newer models capture more nuanced relationships between multiple modalities. In particular, Gemini learns semantic alignments between various modalities such as images, text, audio, and video simultaneously, preserving the unique characteristics of each modality while building an integrated semantic space.</p>
<p><strong>Example Execution</strong></p>
<p>The data used for training is Flicker8k. The <code>EnhancedMultimodalEmbedding</code> (or <code>EnhancedMultimodalEmbedding_no_p</code>) model can be trained on the Flickr8k dataset using the <code>train_multimodal_embedding</code> function. In the <code>main</code> function, the model, data loader, optimizer, etc. are set up, and calling the <code>train_multimodal_embedding</code> function starts the training.</p>
<div id="cell-28" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="co"># download flickr8k.</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>mkdir data<span class="op">;</span>cd data<span class="op">;</span>wget <span class="st">"https://github.com/awsaf49/flickr-dataset/releases/download/v1.0/flickr8k.zip"</span><span class="op">;</span>unzip <span class="op">-</span>q flickr8k.<span class="bu">zip</span> <span class="op">-</span>d .<span class="op">/</span>flickr8k</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>mkdir: cannot create directory ‘data’: File exists
--2025-03-09 16:33:12--  https://github.com/awsaf49/flickr-dataset/releases/download/v1.0/flickr8k.zip
Resolving github.com (github.com)... 20.200.245.247
Connecting to github.com (github.com)|20.200.245.247|:443... connected.
HTTP request sent, awaiting response... 302 Found
Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/753516996/d7c62b13-1e50-40ea-8fae-f34a44b1695f?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=releaseassetproduction%2F20250309%2Fus-east-1%2Fs3%2Faws4_request&amp;X-Amz-Date=20250309T073156Z&amp;X-Amz-Expires=300&amp;X-Amz-Signature=ff62cf7df8ac3deba8bd6f4f775e164abf03c6d2d6d86d740e5407e52702c6a3&amp;X-Amz-SignedHeaders=host&amp;response-content-disposition=attachment%3B%20filename%3Dflickr8k.zip&amp;response-content-type=application%2Foctet-stream [following]
--2025-03-09 16:33:12--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/753516996/d7c62b13-1e50-40ea-8fae-f34a44b1695f?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=releaseassetproduction%2F20250309%2Fus-east-1%2Fs3%2Faws4_request&amp;X-Amz-Date=20250309T073156Z&amp;X-Amz-Expires=300&amp;X-Amz-Signature=ff62cf7df8ac3deba8bd6f4f775e164abf03c6d2d6d86d740e5407e52702c6a3&amp;X-Amz-SignedHeaders=host&amp;response-content-disposition=attachment%3B%20filename%3Dflickr8k.zip&amp;response-content-type=application%2Foctet-stream
Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.110.133, ...
Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.109.133|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 1112971163 (1.0G) [application/octet-stream]
Saving to: ‘flickr8k.zip’

flickr8k.zip        100%[===================&gt;]   1.04G  56.8MB/s    in 19s     

2025-03-09 16:33:32 (56.9 MB/s) - ‘flickr8k.zip’ saved [1112971163/1112971163]
</code></pre>
</div>
</div>
<div id="cell-29" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision <span class="im">import</span> models, transforms</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> Dataset, DataLoader</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Assuming dldna.chapter_10.multimodal_embedding is in the same directory or Python path.</span></span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Adjust if necessary (e.g., from multimodal_embedding import ...).</span></span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_10.multimodal_embedding <span class="im">import</span> Flickr8kDataset, MultimodalEmbedding, train_multimodal_embedding, generate_example</span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Data transformation setup</span></span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a>transform <span class="op">=</span> transforms.Compose([</span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a>    transforms.Resize((<span class="dv">224</span>, <span class="dv">224</span>)),</span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a>    transforms.ToTensor(),</span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a>    transforms.Normalize(mean<span class="op">=</span>[<span class="fl">0.485</span>, <span class="fl">0.456</span>, <span class="fl">0.406</span>], std<span class="op">=</span>[<span class="fl">0.229</span>, <span class="fl">0.224</span>, <span class="fl">0.225</span>])</span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-15"><a href="#cb23-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Dataset and DataLoader setup</span></span>
<span id="cb23-16"><a href="#cb23-16" aria-hidden="true" tabindex="-1"></a>image_dir <span class="op">=</span> <span class="st">'./data/flickr8k/Images'</span>  <span class="co"># Replace with the actual path to your image directory</span></span>
<span id="cb23-17"><a href="#cb23-17" aria-hidden="true" tabindex="-1"></a>caption_file <span class="op">=</span> <span class="st">'./data/flickr8k/captions.txt'</span>  <span class="co"># Replace with the actual path to your caption file</span></span>
<span id="cb23-18"><a href="#cb23-18" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> Flickr8kDataset(image_dir, caption_file, transform<span class="op">=</span>transform)</span>
<span id="cb23-19"><a href="#cb23-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-20"><a href="#cb23-20" aria-hidden="true" tabindex="-1"></a>train_size <span class="op">=</span> <span class="bu">int</span>(<span class="fl">0.8</span> <span class="op">*</span> <span class="bu">len</span>(dataset))</span>
<span id="cb23-21"><a href="#cb23-21" aria-hidden="true" tabindex="-1"></a>val_size <span class="op">=</span> <span class="bu">len</span>(dataset) <span class="op">-</span> train_size</span>
<span id="cb23-22"><a href="#cb23-22" aria-hidden="true" tabindex="-1"></a>train_dataset, val_dataset <span class="op">=</span> torch.utils.data.random_split(dataset, [train_size, val_size])</span>
<span id="cb23-23"><a href="#cb23-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-24"><a href="#cb23-24" aria-hidden="true" tabindex="-1"></a>train_loader <span class="op">=</span> DataLoader(train_dataset, batch_size<span class="op">=</span><span class="dv">32</span>, shuffle<span class="op">=</span><span class="va">True</span>, num_workers<span class="op">=</span><span class="dv">4</span>)</span>
<span id="cb23-25"><a href="#cb23-25" aria-hidden="true" tabindex="-1"></a>val_loader <span class="op">=</span> DataLoader(val_dataset, batch_size<span class="op">=</span><span class="dv">32</span>, shuffle<span class="op">=</span><span class="va">False</span>, num_workers<span class="op">=</span><span class="dv">4</span>)</span>
<span id="cb23-26"><a href="#cb23-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-27"><a href="#cb23-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Model initialization</span></span>
<span id="cb23-28"><a href="#cb23-28" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> MultimodalEmbedding()</span>
<span id="cb23-29"><a href="#cb23-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-30"><a href="#cb23-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Model training</span></span>
<span id="cb23-31"><a href="#cb23-31" aria-hidden="true" tabindex="-1"></a>train_multimodal_embedding(model, train_loader, val_loader, num_epochs<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb23-32"><a href="#cb23-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-33"><a href="#cb23-33" aria-hidden="true" tabindex="-1"></a><span class="co"># Model saving</span></span>
<span id="cb23-34"><a href="#cb23-34" aria-hidden="true" tabindex="-1"></a>torch.save(model.state_dict(), <span class="st">'multimodal_embedding_model.pth'</span>)</span>
<span id="cb23-35"><a href="#cb23-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-36"><a href="#cb23-36" aria-hidden="true" tabindex="-1"></a><span class="co"># Example generation</span></span>
<span id="cb23-37"><a href="#cb23-37" aria-hidden="true" tabindex="-1"></a>model_path <span class="op">=</span> <span class="st">'multimodal_embedding_model.pth'</span></span>
<span id="cb23-38"><a href="#cb23-38" aria-hidden="true" tabindex="-1"></a>generate_example(model_path, image_dir, caption_file)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 1/3:  15%|█▍        | 147/1012 [00:16&lt;01:36,  8.96it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Image file not found: ./data/flickr8k/Images/image</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 1/3: 100%|██████████| 1012/1012 [01:53&lt;00:00,  8.90it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/3 - Train Loss: 0.9618</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code></code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/3 - Validation Loss: 0.5212
Epoch 1: Saved best model with Validation Loss = 0.5212</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 2/3:  52%|█████▏    | 525/1012 [00:59&lt;00:55,  8.84it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Image file not found: ./data/flickr8k/Images/image</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 2/3: 100%|██████████| 1012/1012 [01:54&lt;00:00,  8.83it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 2/3 - Train Loss: 0.3393</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code></code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 2/3 - Validation Loss: 0.4240
Epoch 2: Saved best model with Validation Loss = 0.4240</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 3/3:  34%|███▍      | 347/1012 [00:39&lt;01:15,  8.85it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Image file not found: ./data/flickr8k/Images/image</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 3/3: 100%|██████████| 1012/1012 [01:54&lt;00:00,  8.83it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 3/3 - Train Loss: 0.2313</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code></code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 3/3 - Validation Loss: 0.3891
Epoch 3: Saved best model with Validation Loss = 0.3891
Image 0:</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="10_Multimodal Deep Learning: The Beginning of Multisensory Convergence_files/figure-html/cell-14-output-19.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Top 3 Captions (Image -&gt; Text):
  - football players in red congratulate each other as crowds in red cheer behind. (prob: 0.9970)
  - a man in black holds up an obama 08 sign. (prob: 0.0023)
  - a large group of bicycles racing on the street (prob: 0.0004)

Caption: football players in red congratulate each other as crowds in red cheer behind.

Top 3 Images (Text -&gt; Image):
 - Image 0 (prob: 0.9983)</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="10_Multimodal Deep Learning: The Beginning of Multisensory Convergence_files/figure-html/cell-14-output-21.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> - Image 17 (prob: 0.0013)</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="10_Multimodal Deep Learning: The Beginning of Multisensory Convergence_files/figure-html/cell-14-output-23.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> - Image 2 (prob: 0.0001)</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="10_Multimodal Deep Learning: The Beginning of Multisensory Convergence_files/figure-html/cell-14-output-25.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="cross-modal-attention-structure" class="level3">
<h3 class="anchored" data-anchor-id="cross-modal-attention-structure">10.4.2 Cross-Modal Attention Structure</h3>
<p>Cross-modal attention is used to effectively model the relationship between different modalities. This extends ViT’s self-attention to enable interaction between heterogeneous data such as images and text.</p>
<p><strong>Modal Attention Design</strong></p>
<p>Cross-modal attention has an asymmetric structure considering the characteristics of each modality.</p>
<div id="cell-31" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> CrossModalAttention(nn.Module):</span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config):</span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.image_proj <span class="op">=</span> nn.Linear(config.image_dim, config.hidden_dim)</span>
<span id="cb45-5"><a href="#cb45-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.text_proj <span class="op">=</span> nn.Linear(config.text_dim, config.hidden_dim)</span>
<span id="cb45-6"><a href="#cb45-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.attention <span class="op">=</span> nn.MultiheadAttention(config.hidden_dim, config.num_heads)</span>
<span id="cb45-7"><a href="#cb45-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb45-8"><a href="#cb45-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, image_features, text_features):</span>
<span id="cb45-9"><a href="#cb45-9" aria-hidden="true" tabindex="-1"></a>        image_proj <span class="op">=</span> <span class="va">self</span>.image_proj(image_features)</span>
<span id="cb45-10"><a href="#cb45-10" aria-hidden="true" tabindex="-1"></a>        text_proj <span class="op">=</span> <span class="va">self</span>.text_proj(text_features)</span>
<span id="cb45-11"><a href="#cb45-11" aria-hidden="true" tabindex="-1"></a>        attn_output, _ <span class="op">=</span> <span class="va">self</span>.attention(text_proj, image_proj, image_proj)</span>
<span id="cb45-12"><a href="#cb45-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> attn_output</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>After projecting image and text features into a common latent space, the relationship between the two modalities is learned through a multi-head attention mechanism. The text feature is used as a query, and the image feature is used as a key and value, allowing the text to pay attention to the relevant part of the image.</p>
<p><strong>Asymmetric Attention Pattern</strong></p>
<p>An asymmetric attention pattern is used to preserve the unique characteristics of each modality while facilitating effective information exchange.</p>
<div id="cell-33" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> HierarchicalCrossModalAttention(nn.Module):</span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config):</span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.local_image_attention <span class="op">=</span> nn.MultiheadAttention(config.hidden_dim, config.num_heads)</span>
<span id="cb46-5"><a href="#cb46-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.local_text_attention <span class="op">=</span> nn.MultiheadAttention(config.hidden_dim, config.num_heads)</span>
<span id="cb46-6"><a href="#cb46-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.image_to_text_attention <span class="op">=</span> CrossModalAttention(config)</span>
<span id="cb46-7"><a href="#cb46-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.text_to_image_attention <span class="op">=</span> CrossModalAttention(config)</span>
<span id="cb46-8"><a href="#cb46-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.output_layer <span class="op">=</span> nn.Linear(config.hidden_dim <span class="op">*</span> <span class="dv">2</span>, config.hidden_dim)</span>
<span id="cb46-9"><a href="#cb46-9" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb46-10"><a href="#cb46-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, image_features, text_features):</span>
<span id="cb46-11"><a href="#cb46-11" aria-hidden="true" tabindex="-1"></a>        local_image <span class="op">=</span> <span class="va">self</span>.local_image_attention(image_features, image_features, image_features)[<span class="dv">0</span>]</span>
<span id="cb46-12"><a href="#cb46-12" aria-hidden="true" tabindex="-1"></a>        local_text <span class="op">=</span> <span class="va">self</span>.local_text_attention(text_features, text_features, text_features)[<span class="dv">0</span>]</span>
<span id="cb46-13"><a href="#cb46-13" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb46-14"><a href="#cb46-14" aria-hidden="true" tabindex="-1"></a>        image_attended_text <span class="op">=</span> <span class="va">self</span>.image_to_text_attention(image_features, local_text)</span>
<span id="cb46-15"><a href="#cb46-15" aria-hidden="true" tabindex="-1"></a>        text_attended_image <span class="op">=</span> <span class="va">self</span>.text_to_image_attention(text_features, local_image)</span>
<span id="cb46-16"><a href="#cb46-16" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb46-17"><a href="#cb46-17" aria-hidden="true" tabindex="-1"></a>        combined_features <span class="op">=</span> torch.cat([image_attended_text, text_attended_image], dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb46-18"><a href="#cb46-18" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> <span class="va">self</span>.output_layer(combined_features)</span>
<span id="cb46-19"><a href="#cb46-19" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb46-20"><a href="#cb46-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> output</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Here, bidirectional attention is performed separately from images to text and from text to images. This allows each modality to selectively focus on relevant information from the other modality.</p>
<p><strong>Hierarchical Attention Structure</strong></p>
<p>To capture complex multimodal relationships, multiple layers of attention are hierarchically constructed. In the lower layers, local features within each modality are processed, and in the upper layers, global relationships between modalities are modeled. This hierarchical structure plays a key role in models such as GPT-4V and Gemini.</p>
<div id="cell-35" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> EnhancedMultimodalEmbedding_no_p(MultimodalEmbedding):</span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, image, input_ids, attention_mask):</span>
<span id="cb47-4"><a href="#cb47-4" aria-hidden="true" tabindex="-1"></a>        image_features <span class="op">=</span> <span class="va">self</span>.encode_image(image)</span>
<span id="cb47-5"><a href="#cb47-5" aria-hidden="true" tabindex="-1"></a>        text_features <span class="op">=</span> <span class="va">self</span>.encode_text(input_ids, attention_mask)</span>
<span id="cb47-6"><a href="#cb47-6" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb47-7"><a href="#cb47-7" aria-hidden="true" tabindex="-1"></a>        image_features <span class="op">=</span> <span class="va">self</span>.image_preserve(image_features)</span>
<span id="cb47-8"><a href="#cb47-8" aria-hidden="true" tabindex="-1"></a>        text_features <span class="op">=</span> <span class="va">self</span>.text_preserve(text_features)</span>
<span id="cb47-9"><a href="#cb47-9" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb47-10"><a href="#cb47-10" aria-hidden="true" tabindex="-1"></a>        combined_features <span class="op">=</span> <span class="va">self</span>.cross_modal_attention(image_features, text_features)</span>
<span id="cb47-11"><a href="#cb47-11" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb47-12"><a href="#cb47-12" aria-hidden="true" tabindex="-1"></a>        combined_features <span class="op">=</span> combined_features <span class="op">/</span> combined_features.norm(dim<span class="op">=-</span><span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb47-13"><a href="#cb47-13" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb47-14"><a href="#cb47-14" aria-hidden="true" tabindex="-1"></a>        logit_scale <span class="op">=</span> <span class="va">self</span>.logit_scale.exp()</span>
<span id="cb47-15"><a href="#cb47-15" aria-hidden="true" tabindex="-1"></a>        logits <span class="op">=</span> logit_scale <span class="op">*</span> combined_features <span class="op">@</span> combined_features.t()</span>
<span id="cb47-16"><a href="#cb47-16" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb47-17"><a href="#cb47-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> logits</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="cell-36" class="cell" data-execution_count="2">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision <span class="im">import</span> models, transforms</span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> Dataset, DataLoader</span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-5"><a href="#cb48-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> namedtuple</span>
<span id="cb48-6"><a href="#cb48-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-7"><a href="#cb48-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_10.crossmodal_attention <span class="im">import</span> Flickr8kDataset, CrossModalEmbedding, train_crossmodal_embedding, generate_example</span>
<span id="cb48-8"><a href="#cb48-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-9"><a href="#cb48-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Configuration</span></span>
<span id="cb48-10"><a href="#cb48-10" aria-hidden="true" tabindex="-1"></a>config <span class="op">=</span> namedtuple(<span class="st">'Config'</span>, [<span class="st">'embedding_dim'</span>, <span class="st">'image_dim'</span>, <span class="st">'text_dim'</span>, <span class="st">'hidden_dim'</span>, <span class="st">'num_heads'</span>])(</span>
<span id="cb48-11"><a href="#cb48-11" aria-hidden="true" tabindex="-1"></a>                    embedding_dim<span class="op">=</span><span class="dv">512</span>, <span class="co"># Output embedding dimension</span></span>
<span id="cb48-12"><a href="#cb48-12" aria-hidden="true" tabindex="-1"></a>                    image_dim<span class="op">=</span><span class="dv">512</span>, <span class="co"># ResNet18 image encoder output dimension</span></span>
<span id="cb48-13"><a href="#cb48-13" aria-hidden="true" tabindex="-1"></a>                    text_dim<span class="op">=</span><span class="dv">512</span>, <span class="co"># Text feature (768 from BERT -&gt; 512 after projection)</span></span>
<span id="cb48-14"><a href="#cb48-14" aria-hidden="true" tabindex="-1"></a>                    hidden_dim<span class="op">=</span><span class="dv">512</span>, <span class="co"># Cross-modal attention internal hidden dimension</span></span>
<span id="cb48-15"><a href="#cb48-15" aria-hidden="true" tabindex="-1"></a>                    num_heads<span class="op">=</span><span class="dv">8</span> <span class="co"># Number of multi-head attention heads</span></span>
<span id="cb48-16"><a href="#cb48-16" aria-hidden="true" tabindex="-1"></a>                    )</span>
<span id="cb48-17"><a href="#cb48-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Data transformation setup</span></span>
<span id="cb48-18"><a href="#cb48-18" aria-hidden="true" tabindex="-1"></a>transform <span class="op">=</span> transforms.Compose([</span>
<span id="cb48-19"><a href="#cb48-19" aria-hidden="true" tabindex="-1"></a>    transforms.Resize((<span class="dv">224</span>, <span class="dv">224</span>)),</span>
<span id="cb48-20"><a href="#cb48-20" aria-hidden="true" tabindex="-1"></a>    transforms.ToTensor(),</span>
<span id="cb48-21"><a href="#cb48-21" aria-hidden="true" tabindex="-1"></a>    transforms.Normalize(mean<span class="op">=</span>[<span class="fl">0.485</span>, <span class="fl">0.456</span>, <span class="fl">0.406</span>], std<span class="op">=</span>[<span class="fl">0.229</span>, <span class="fl">0.224</span>, <span class="fl">0.225</span>])</span>
<span id="cb48-22"><a href="#cb48-22" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb48-23"><a href="#cb48-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-24"><a href="#cb48-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Dataset and DataLoader setup</span></span>
<span id="cb48-25"><a href="#cb48-25" aria-hidden="true" tabindex="-1"></a>image_dir <span class="op">=</span> <span class="st">'./data/flickr8k/Images'</span>  <span class="co"># Change to the actual path</span></span>
<span id="cb48-26"><a href="#cb48-26" aria-hidden="true" tabindex="-1"></a>caption_file <span class="op">=</span> <span class="st">'./data/flickr8k/captions.txt'</span>  <span class="co"># Change to the actual path</span></span>
<span id="cb48-27"><a href="#cb48-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-28"><a href="#cb48-28" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> Flickr8kDataset(image_dir, caption_file, transform<span class="op">=</span>transform)</span>
<span id="cb48-29"><a href="#cb48-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-30"><a href="#cb48-30" aria-hidden="true" tabindex="-1"></a>train_size <span class="op">=</span> <span class="bu">int</span>(<span class="fl">0.8</span> <span class="op">*</span> <span class="bu">len</span>(dataset))</span>
<span id="cb48-31"><a href="#cb48-31" aria-hidden="true" tabindex="-1"></a>val_size <span class="op">=</span> <span class="bu">len</span>(dataset) <span class="op">-</span> train_size</span>
<span id="cb48-32"><a href="#cb48-32" aria-hidden="true" tabindex="-1"></a>train_dataset, val_dataset <span class="op">=</span> torch.utils.data.random_split(dataset, [train_size, val_size])</span>
<span id="cb48-33"><a href="#cb48-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-34"><a href="#cb48-34" aria-hidden="true" tabindex="-1"></a>train_loader <span class="op">=</span> DataLoader(train_dataset, batch_size<span class="op">=</span><span class="dv">32</span>, shuffle<span class="op">=</span><span class="va">True</span>, num_workers<span class="op">=</span><span class="dv">4</span>, pin_memory<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb48-35"><a href="#cb48-35" aria-hidden="true" tabindex="-1"></a>val_loader <span class="op">=</span> DataLoader(val_dataset, batch_size<span class="op">=</span><span class="dv">32</span>, shuffle<span class="op">=</span><span class="va">False</span>, num_workers<span class="op">=</span><span class="dv">4</span>, pin_memory<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb48-36"><a href="#cb48-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-37"><a href="#cb48-37" aria-hidden="true" tabindex="-1"></a><span class="co"># Model initialization</span></span>
<span id="cb48-38"><a href="#cb48-38" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> CrossModalEmbedding(config)</span>
<span id="cb48-39"><a href="#cb48-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-40"><a href="#cb48-40" aria-hidden="true" tabindex="-1"></a><span class="co"># Model training</span></span>
<span id="cb48-41"><a href="#cb48-41" aria-hidden="true" tabindex="-1"></a>train_crossmodal_embedding(model, train_loader, val_loader, num_epochs<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb48-42"><a href="#cb48-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-43"><a href="#cb48-43" aria-hidden="true" tabindex="-1"></a><span class="co"># Model saving</span></span>
<span id="cb48-44"><a href="#cb48-44" aria-hidden="true" tabindex="-1"></a>torch.save(model.state_dict(), <span class="st">'crossmodal_embedding_model.pth'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 1/3:   4%|▍         | 40/1012 [00:04&lt;01:41,  9.53it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Image file not found: ./data/flickr8k/Images/image</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 1/3: 100%|██████████| 1012/1012 [01:47&lt;00:00,  9.41it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/3 - Train Loss: 0.9663</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code></code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/3 - Validation Loss: 0.5378</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 2/3:  58%|█████▊    | 582/1012 [01:02&lt;00:45,  9.36it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Image file not found: ./data/flickr8k/Images/image</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 2/3: 100%|██████████| 1012/1012 [01:48&lt;00:00,  9.31it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 2/3 - Train Loss: 0.3381</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code></code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 2/3 - Validation Loss: 0.4452</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 3/3:   0%|          | 4/1012 [00:00&lt;02:27,  6.82it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Image file not found: ./data/flickr8k/Images/image</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 3/3: 100%|██████████| 1012/1012 [01:48&lt;00:00,  9.35it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 3/3 - Train Loss: 0.2288</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code></code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 3/3 - Validation Loss: 0.3743</code></pre>
</div>
</div>
<div id="cell-37" class="cell" data-execution_count="3">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb67"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Example generation</span></span>
<span id="cb67-2"><a href="#cb67-2" aria-hidden="true" tabindex="-1"></a>model_path <span class="op">=</span> <span class="st">'crossmodal_embedding_model.pth'</span></span>
<span id="cb67-3"><a href="#cb67-3" aria-hidden="true" tabindex="-1"></a>generate_example(model_path, image_dir, caption_file)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Image 0:</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="10_Multimodal Deep Learning: The Beginning of Multisensory Convergence_files/figure-html/cell-19-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Top 3 Captions (Image -&gt; Text):
  - two people walk out onto the desert sand. (prob: 0.9862)
  - a man takes a picture of him and his friend with his phone. (prob: 0.0092)
  - the little boy wearing the blue shirt is putting dirt in his mouth. (prob: 0.0013)

Caption: two people walk out onto the desert sand.

Top 3 Images (Text -&gt; Image):
 - Image 0 (prob: 0.9898)</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="10_Multimodal Deep Learning: The Beginning of Multisensory Convergence_files/figure-html/cell-19-output-4.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> - Image 2 (prob: 0.0089)</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="10_Multimodal Deep Learning: The Beginning of Multisensory Convergence_files/figure-html/cell-19-output-6.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> - Image 4 (prob: 0.0005)</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="10_Multimodal Deep Learning: The Beginning of Multisensory Convergence_files/figure-html/cell-19-output-8.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="perceiver-architecture" class="level3">
<h3 class="anchored" data-anchor-id="perceiver-architecture">10.4.3 Perceiver Architecture</h3>
<p>Perceiver is a multimodal architecture proposed by DeepMind in 2021. It addresses the quadratic complexity issue of existing transformers (where computation increases with the square of the input sequence length) while effectively handling various modalities (such as images, text, audio, and point clouds). The Perceiver is particularly advantageous when the input data size is very large (e.g., high-resolution images, long texts). Here, we describe the overall architecture and omit examples. The code is example code for explanation purposes.</p>
<p><strong>Core Idea of Perceiver</strong></p>
<p>Perceiver is based on the following ideas:</p>
<ol type="1">
<li><strong>Bottleneck Architecture:</strong></li>
</ol>
<p>Perceiver uses a fixed-size latent array regardless of the input sequence length. This latent array compresses and represents the information of the input data, summarizing a large amount of input information into a small number of latent vectors, like a bottleneck. Therefore, even if the input data size is very large (e.g., 10,000 tokens), the number of latent vectors is fixed (e.g., 256), which can greatly reduce computational complexity and memory usage.</p>
<div id="cell-39" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb72"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Perceiver(nn.Module):</span>
<span id="cb72-2"><a href="#cb72-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, ..., num_latents<span class="op">=</span><span class="dv">256</span>, latent_dim<span class="op">=</span><span class="dv">512</span>, ...):</span>
<span id="cb72-3"><a href="#cb72-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb72-4"><a href="#cb72-4" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Latent vector initialization (key!)</span></span>
<span id="cb72-5"><a href="#cb72-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.latents <span class="op">=</span> nn.Parameter(torch.randn(num_latents, latent_dim))</span>
<span id="cb72-6"><a href="#cb72-6" aria-hidden="true" tabindex="-1"></a>        <span class="co"># ...</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>In the above code, <code>self.latents</code> represents that latent vector. It is defined as <code>nn.Parameter</code>, which means it is a learnable parameter.</p>
<ol start="2" type="1">
<li><strong>Modality-Agnostic Processing:</strong></li>
</ol>
<p>The Perceiver does not use modality-specific processing methods (e.g., CNN, RNN) for input modalities such as images, text, and audio. Instead, each modality undergoes simple preprocessing (e.g., image patches, text tokenization) to be converted into a common format (sequence of vectors). Subsequent processing uses the same transformer-based architecture (Cross-Attention, Self-Attention) regardless of the modality type. This allows for flexible handling of various modalities and easy addition of new modalities.</p>
<ol start="3" type="1">
<li><strong>Adaptive Latent Representation:</strong></li>
</ol>
<p>The Perceiver uses multiple layers of self-attention to gradually update the latent vectors. At each layer, the latent vectors exchange information with each other and learn complex patterns from the input data. Initially, the latent vectors that represented simple features come to express abstract and high-level meanings as they pass through multiple layers.</p>
<p><strong>How Perceiver Works (Simplified Code Example)</strong></p>
<div id="cell-41" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb73"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb73-2"><a href="#cb73-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb73-3"><a href="#cb73-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-4"><a href="#cb73-4" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Perceiver(nn.Module):</span>
<span id="cb73-5"><a href="#cb73-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>,</span>
<span id="cb73-6"><a href="#cb73-6" aria-hidden="true" tabindex="-1"></a>                 input_channels<span class="op">=</span><span class="dv">3</span>,  <span class="co"># Input channels (e.g., RGB image)</span></span>
<span id="cb73-7"><a href="#cb73-7" aria-hidden="true" tabindex="-1"></a>                 input_axis<span class="op">=</span><span class="dv">2</span>,      <span class="co"># Input dimension (image=2, video=3)</span></span>
<span id="cb73-8"><a href="#cb73-8" aria-hidden="true" tabindex="-1"></a>                 num_latents<span class="op">=</span><span class="dv">256</span>,  <span class="co"># Number of latent vectors</span></span>
<span id="cb73-9"><a href="#cb73-9" aria-hidden="true" tabindex="-1"></a>                 latent_dim<span class="op">=</span><span class="dv">512</span>,    <span class="co"># Latent vector dimension</span></span>
<span id="cb73-10"><a href="#cb73-10" aria-hidden="true" tabindex="-1"></a>                 num_heads<span class="op">=</span><span class="dv">8</span>,       <span class="co"># Number of attention heads</span></span>
<span id="cb73-11"><a href="#cb73-11" aria-hidden="true" tabindex="-1"></a>                 depth<span class="op">=</span><span class="dv">6</span>):          <span class="co"># Model depth (number of self-attention layers)</span></span>
<span id="cb73-12"><a href="#cb73-12" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb73-13"><a href="#cb73-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-14"><a href="#cb73-14" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 1. Latent vector initialization (key!)</span></span>
<span id="cb73-15"><a href="#cb73-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.latents <span class="op">=</span> nn.Parameter(torch.randn(num_latents, latent_dim))</span>
<span id="cb73-16"><a href="#cb73-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-17"><a href="#cb73-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 2. Input projection (matches input dimension to latent dimension)</span></span>
<span id="cb73-18"><a href="#cb73-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.input_proj <span class="op">=</span> nn.Linear(input_dim, latent_dim)</span>
<span id="cb73-19"><a href="#cb73-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-20"><a href="#cb73-20" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 3. Cross-Attention (learns relationships between input and latent vectors)</span></span>
<span id="cb73-21"><a href="#cb73-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># self.cross_attention = nn.MultiheadAttention(latent_dim, num_heads, batch_first=True)</span></span>
<span id="cb73-22"><a href="#cb73-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-23"><a href="#cb73-23" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 4. Self-Attention (learns relationships between latent vectors) - repeated multiple times</span></span>
<span id="cb73-24"><a href="#cb73-24" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.self_attention_layers <span class="op">=</span> nn.ModuleList([</span>
<span id="cb73-25"><a href="#cb73-25" aria-hidden="true" tabindex="-1"></a>            nn.MultiheadAttention(latent_dim, num_heads, batch_first<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb73-26"><a href="#cb73-26" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(depth)</span>
<span id="cb73-27"><a href="#cb73-27" aria-hidden="true" tabindex="-1"></a>        ])</span>
<span id="cb73-28"><a href="#cb73-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-29"><a href="#cb73-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-30"><a href="#cb73-30" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):  <span class="co"># x: Input data (image, text, ...)</span></span>
<span id="cb73-31"><a href="#cb73-31" aria-hidden="true" tabindex="-1"></a>        batch_size <span class="op">=</span> x.shape[<span class="dv">0</span>]</span>
<span id="cb73-32"><a href="#cb73-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-33"><a href="#cb73-33" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 1. Input projection</span></span>
<span id="cb73-34"><a href="#cb73-34" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.input_proj(x)</span>
<span id="cb73-35"><a href="#cb73-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-36"><a href="#cb73-36" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 2. Latent vector replication (for each item in the batch)</span></span>
<span id="cb73-37"><a href="#cb73-37" aria-hidden="true" tabindex="-1"></a>        latents <span class="op">=</span> <span class="va">self</span>.latents.unsqueeze(<span class="dv">0</span>).expand(batch_size, <span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>)  <span class="co"># (B, num_latents, latent_dim)</span></span>
<span id="cb73-38"><a href="#cb73-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-39"><a href="#cb73-39" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 3. (Optional) Cross-attention (between input and latent vectors)</span></span>
<span id="cb73-40"><a href="#cb73-40" aria-hidden="true" tabindex="-1"></a>        <span class="co"># latents, _ = self.cross_attention(latents, x, x)  # query, key, value</span></span>
<span id="cb73-41"><a href="#cb73-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-42"><a href="#cb73-42" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 4. Self-attention (between latent vectors) - repeated multiple times</span></span>
<span id="cb73-43"><a href="#cb73-43" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> layer <span class="kw">in</span> <span class="va">self</span>.self_attention_layers:</span>
<span id="cb73-44"><a href="#cb73-44" aria-hidden="true" tabindex="-1"></a>            latents, _ <span class="op">=</span> layer(latents, latents, latents) <span class="co"># query, key, value</span></span>
<span id="cb73-45"><a href="#cb73-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-46"><a href="#cb73-46" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> latents  <span class="co"># Return the processed latent vectors</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><strong>Advantages and Disadvantages of Perceiver</strong></p>
<p>Perceiver has the efficiency of having a computational complexity that is almost constant regardless of the input size, and provides flexibility to process various modalities in the same way. Additionally, the expandability of Perceiver, which can easily add new modalities, is also an advantage. However, since Perceiver is still based on a transformer, it has the disadvantage of having a complex structure, and the model can become very large as the dimension of the latent vector and the number of layers increase. Additionally, in specific tasks such as image classification, its performance may be inferior to models specialized for those tasks, such as CNNs.</p>
<p><strong>Perceiver IO</strong></p>
<p>Perceiver IO, a follow-up study to Perceiver, proposed a method to process not only inputs but also outputs through latent vectors. This allows for flexible handling of various output forms (classification, regression, sequence generation, etc.). Perceiver IO is evaluated as a more general and powerful model than Perceiver.</p>
</section>
<section id="cross-attention-implementation-and-training-stability" class="level3">
<h3 class="anchored" data-anchor-id="cross-attention-implementation-and-training-stability">10.4.4 Cross-Attention Implementation and Training Stability</h3>
<p>Here, we start with the basic structure of cross-attention and gradually add mechanisms to compare trainability and performance. Through this process, we aim to understand the issues that arise in multimodal learning and explore practical approaches to address them.</p>
<p>When designing cross-attention mechanisms, it is a common and recommended approach to gradually increase complexity as described in this section and experiment with it. This method, known as an ablation study, effectively identifies the importance of each component mechanism and the key elements contributing to the final model’s performance. Many papers proposing new architectures use this approach. Moreover, discussing not only the final performance but also stability issues during training is crucial from a practical perspective.</p>
<section id="structure-of-training" class="level4">
<h4 class="anchored" data-anchor-id="structure-of-training">10.4.4.1 Structure of Training</h4>
<p><strong>Comparative Training Methods</strong></p>
<p>Experiments are conducted using the flickr8k dataset, which was previously explored, with text and image as two inputs to train mutual similarity. The training involves versions of cross-attention with increasing complexity. For each version, a cross-attention mechanism is added one by one, and training is performed for comparison. All trainings use the same hyperparameters. The training epoch is fixed at 5.</p>
<p><strong>Structure of Examples</strong></p>
<p>Examples are composed of the following structure:</p>
<pre class="text"><code>
chapter_10/mm
├── cat_resized.png
├── cross_attention
│&nbsp;&nbsp; ├── v0.py
│&nbsp;&nbsp; ├── v1.py
│&nbsp;&nbsp; ├── v2.py
│&nbsp;&nbsp; ├── v3.py 
│&nbsp;&nbsp; .... (continue to exist)
├── train_multimodal.py
└── evaluate_models.py
</code></pre>
<p>The cross_attention folder increases the complexity of cross-attention sequentially from v1 to v11. <code>train_mulimodal.py</code> dynamically generates and trains the next version of the model after one training is completed. During training, metrics such as accuracy, contrast loss, and execution time are stored to generate a final comparison table. It is not desirable to determine trainability based on loss values and accuracy. The easiest way to check if the training has been done correctly due to the nature of contrastive learning is to evaluate it with data that did not exist before. The file that evaluates the model in a zero-shot manner is <code>evalute_models.py</code>.</p>
<p>The image being evaluated is as follows.</p>
<p><img src="../../../assets/images/cat_resized.png" class="img-fluid"></p>
<p>The evaluation is done by measuring the similarity between the above image and five texts.</p>
<pre><code>test_captions = [
    "A dog playing in the park",
    "A cat sleeping on a couch",
    "Children playing soccer",
    "A sunset over the ocean",
    "A person cooking in the kitchen"
]</code></pre>
<p>If the model training is done correctly, the second caption “A cat sleeping on a couch” should have the highest similarity among the five captions. The above image is something that was not in the training data, which corresponds to a typical zero-shot test.</p>
<p>Cross-attention dynamic allocation</p>
<p>Changing the version of cross_attention is done through dynamic allocation.</p>
<div id="cell-47" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb76"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb76-1"><a href="#cb76-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_10.mm.cross_attention.v0 <span class="im">import</span> CrossAttention <span class="im">as</span> v0</span>
<span id="cb76-2"><a href="#cb76-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_10.mm.cross_attention.v1 <span class="im">import</span> CrossAttention <span class="im">as</span> v1</span>
<span id="cb76-3"><a href="#cb76-3" aria-hidden="true" tabindex="-1"></a><span class="co"># ... (import other versions) ...</span></span>
<span id="cb76-4"><a href="#cb76-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_10.mm.cross_attention.v11 <span class="im">import</span> CrossAttention <span class="im">as</span> v11</span>
<span id="cb76-5"><a href="#cb76-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-6"><a href="#cb76-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_cross_attention(version, config<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb76-7"><a href="#cb76-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> config <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb76-8"><a href="#cb76-8" aria-hidden="true" tabindex="-1"></a>        config <span class="op">=</span> {}</span>
<span id="cb76-9"><a href="#cb76-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-10"><a href="#cb76-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> version <span class="op">==</span> <span class="st">'v0'</span>:</span>
<span id="cb76-11"><a href="#cb76-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> v0(<span class="op">**</span>config)</span>
<span id="cb76-12"><a href="#cb76-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> version <span class="op">==</span> <span class="st">'v1'</span>:</span>
<span id="cb76-13"><a href="#cb76-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> v1(<span class="op">**</span>config)</span>
<span id="cb76-14"><a href="#cb76-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ... (other version conditions) ...</span></span>
<span id="cb76-15"><a href="#cb76-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> version <span class="op">==</span> <span class="st">'v11'</span>:</span>
<span id="cb76-16"><a href="#cb76-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> v11(<span class="op">**</span>config)</span>
<span id="cb76-17"><a href="#cb76-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb76-18"><a href="#cb76-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="ss">f"Invalid cross-attention version: </span><span class="sc">{</span>version<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb76-19"><a href="#cb76-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-20"><a href="#cb76-20" aria-hidden="true" tabindex="-1"></a><span class="co"># ...</span></span>
<span id="cb76-21"><a href="#cb76-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-22"><a href="#cb76-22" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ImageTextMatchingModel(nn.Module):</span>
<span id="cb76-23"><a href="#cb76-23" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, image_encoder_dim<span class="op">=</span><span class="dv">2048</span>, text_encoder_dim<span class="op">=</span><span class="dv">768</span>, projection_dim<span class="op">=</span><span class="dv">256</span>):</span>
<span id="cb76-24"><a href="#cb76-24" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb76-25"><a href="#cb76-25" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.image_encoder <span class="op">=</span> ImageEncoder(image_encoder_dim, projection_dim)</span>
<span id="cb76-26"><a href="#cb76-26" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.text_encoder <span class="op">=</span> TextEncoder(text_encoder_dim, projection_dim)</span>
<span id="cb76-27"><a href="#cb76-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-28"><a href="#cb76-28" aria-hidden="true" tabindex="-1"></a>        <span class="co"># The CrossAttention module is dynamically assigned in main().</span></span>
<span id="cb76-29"><a href="#cb76-29" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.cross_attention <span class="op">=</span> <span class="va">None</span>  <span class="co"># CrossAttention(projection_dim)</span></span>
<span id="cb76-30"><a href="#cb76-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-31"><a href="#cb76-31" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, image, input_ids, attention_mask):</span>
<span id="cb76-32"><a href="#cb76-32" aria-hidden="true" tabindex="-1"></a>        <span class="co"># ...</span></span>
<span id="cb76-33"><a href="#cb76-33" aria-hidden="true" tabindex="-1"></a>        image_attended, text_attended <span class="op">=</span> <span class="va">self</span>.cross_attention(</span>
<span id="cb76-34"><a href="#cb76-34" aria-hidden="true" tabindex="-1"></a>            image_features.unsqueeze(<span class="dv">1</span>),</span>
<span id="cb76-35"><a href="#cb76-35" aria-hidden="true" tabindex="-1"></a>            text_features.unsqueeze(<span class="dv">1</span>)</span>
<span id="cb76-36"><a href="#cb76-36" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb76-37"><a href="#cb76-37" aria-hidden="true" tabindex="-1"></a>        <span class="co"># ...</span></span>
<span id="cb76-38"><a href="#cb76-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-39"><a href="#cb76-39" aria-hidden="true" tabindex="-1"></a><span class="co"># ...</span></span>
<span id="cb76-40"><a href="#cb76-40" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> run_training(model_versions, ...):</span>
<span id="cb76-41"><a href="#cb76-41" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ...</span></span>
<span id="cb76-42"><a href="#cb76-42" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> model_version <span class="kw">in</span> model_versions:</span>
<span id="cb76-43"><a href="#cb76-43" aria-hidden="true" tabindex="-1"></a>        <span class="co"># ...</span></span>
<span id="cb76-44"><a href="#cb76-44" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Model initialization</span></span>
<span id="cb76-45"><a href="#cb76-45" aria-hidden="true" tabindex="-1"></a>        model <span class="op">=</span> ImageTextMatchingModel()</span>
<span id="cb76-46"><a href="#cb76-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-47"><a href="#cb76-47" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Dynamically load the CrossAttention module</span></span>
<span id="cb76-48"><a href="#cb76-48" aria-hidden="true" tabindex="-1"></a>        model.cross_attention <span class="op">=</span> get_cross_attention(model_version, config<span class="op">=</span>config)</span>
<span id="cb76-49"><a href="#cb76-49" aria-hidden="true" tabindex="-1"></a>        <span class="co"># ...</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>This part implements the logic to dynamically load and apply various versions of the <code>Cross-Attention</code> module, which is the core of the experiment. The <code>get_cross_attention</code> function takes a string version (v0, v1, …, v11) as input and returns an instance of the <code>CrossAttention</code> class corresponding to that version. Inside the <code>run_training</code> function, for each version specified in the <code>model_versions</code> list, the ImageTextMatchingModel is initialized, and the <code>get_cross_attention</code> function is called to assign the <code>Cross-Attention</code> module of the corresponding version to <code>model.cross_attention</code>.</p>
<p>This dynamic assignment method increases code reusability and makes experiment management easier. When adding a new version of <code>Cross-Attention</code>, you only need to add that version to the <code>get_cross_attention</code> function, so there is no need to greatly modify the training code. Additionally, it is easy to control which versions to train through the <code>model_versions</code> list in the <code>run_training</code> function.</p>
<p><strong>Contrastive Loss calculation and training loop</strong></p>
<div id="cell-49" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb77"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb77-1"><a href="#cb77-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> contrastive_loss(logits):</span>
<span id="cb77-2"><a href="#cb77-2" aria-hidden="true" tabindex="-1"></a>    labels <span class="op">=</span> torch.arange(<span class="bu">len</span>(logits), device<span class="op">=</span>logits.device)</span>
<span id="cb77-3"><a href="#cb77-3" aria-hidden="true" tabindex="-1"></a>    loss_i <span class="op">=</span> nn.CrossEntropyLoss()(logits, labels)</span>
<span id="cb77-4"><a href="#cb77-4" aria-hidden="true" tabindex="-1"></a>    loss_t <span class="op">=</span> nn.CrossEntropyLoss()(logits.t(), labels)</span>
<span id="cb77-5"><a href="#cb77-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (loss_i <span class="op">+</span> loss_t) <span class="op">/</span> <span class="dv">2</span></span>
<span id="cb77-6"><a href="#cb77-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-7"><a href="#cb77-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train(model, train_loader, val_loader, epochs<span class="op">=</span><span class="dv">10</span>, lr<span class="op">=</span><span class="fl">1e-4</span>, model_version<span class="op">=</span><span class="st">'v0'</span>):</span>
<span id="cb77-8"><a href="#cb77-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ...</span></span>
<span id="cb77-9"><a href="#cb77-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb77-10"><a href="#cb77-10" aria-hidden="true" tabindex="-1"></a>        model.train()</span>
<span id="cb77-11"><a href="#cb77-11" aria-hidden="true" tabindex="-1"></a>        total_loss <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb77-12"><a href="#cb77-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># ...</span></span>
<span id="cb77-13"><a href="#cb77-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> batch <span class="kw">in</span> tqdm(train_loader, ...):</span>
<span id="cb77-14"><a href="#cb77-14" aria-hidden="true" tabindex="-1"></a>            images, input_ids, attention_mask <span class="op">=</span> [x.to(device) <span class="cf">for</span> x <span class="kw">in</span> batch]</span>
<span id="cb77-15"><a href="#cb77-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-16"><a href="#cb77-16" aria-hidden="true" tabindex="-1"></a>            optimizer.zero_grad()</span>
<span id="cb77-17"><a href="#cb77-17" aria-hidden="true" tabindex="-1"></a>            logits <span class="op">=</span> model(images, input_ids, attention_mask)</span>
<span id="cb77-18"><a href="#cb77-18" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">=</span> contrastive_loss(logits)</span>
<span id="cb77-19"><a href="#cb77-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-20"><a href="#cb77-20" aria-hidden="true" tabindex="-1"></a>            loss.backward()</span>
<span id="cb77-21"><a href="#cb77-21" aria-hidden="true" tabindex="-1"></a>            optimizer.step()</span>
<span id="cb77-22"><a href="#cb77-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-23"><a href="#cb77-23" aria-hidden="true" tabindex="-1"></a>            total_loss <span class="op">+=</span> loss.item()</span>
<span id="cb77-24"><a href="#cb77-24" aria-hidden="true" tabindex="-1"></a>        <span class="co"># ... (validation 및 지표 계산) ...</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>This part defines the calculation of Contrastive Loss used for model training and the training loop. The <code>contrastive_loss</code> function calculates the Contrastive Loss by taking the similarity score (logits) of image-text pairs as input. At this time, the correct label is generated so that the elements corresponding to the diagonal (i.e., the image-text pair with the same index) of the logits are 1 (similar) and the rest are 0 (not similar) (using torch.arange). Both the image-based Cross-Entropy Loss (loss_i) and the text-based Cross-Entropy Loss (loss_t) are calculated, and their average is used as the final loss.</p>
<p><strong>Training method: adding mechanisms</strong></p>
<p>We will test by adding functions one by one from the simplest attention structure. Let’s call the added functions “mechanisms.” As each mechanism is added, we will look at which mechanism affects the multimodal attention design. First, we will take a look at part of the training code, and then we will look directly at the training results. After that, we will also look at which mechanisms determined the success or failure of the training in each cross-modal attention.</p>
<p>The following is the training code. When trained, each model is saved as <code>model_final_{version}.pth</code>. This saved model is used for evaluation.</p>
<div id="cell-51" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb78"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb78-1"><a href="#cb78-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_10.mm.train_multimodal <span class="im">import</span> run_training</span>
<span id="cb78-2"><a href="#cb78-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-3"><a href="#cb78-3" aria-hidden="true" tabindex="-1"></a><span class="co"># model_versions = ['v0', 'v1']  # List of model versions to train</span></span>
<span id="cb78-4"><a href="#cb78-4" aria-hidden="true" tabindex="-1"></a>model_versions <span class="op">=</span> [<span class="st">'v0'</span>, <span class="st">'v1'</span>, <span class="st">'v2'</span>, <span class="st">'v3'</span>, <span class="st">'v4'</span>, <span class="st">'v5'</span>, <span class="st">'v6'</span>, <span class="st">'v7'</span>, <span class="st">'v8'</span>, <span class="st">'v9'</span>, <span class="st">'v10_1'</span>, <span class="st">'v10_2'</span>, <span class="st">'v10_3'</span>, <span class="st">'v10_4'</span>, <span class="st">'v10_5'</span>, <span class="st">'v10_6'</span>, <span class="st">'v11'</span>]</span>
<span id="cb78-5"><a href="#cb78-5" aria-hidden="true" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb78-6"><a href="#cb78-6" aria-hidden="true" tabindex="-1"></a>lr <span class="op">=</span> <span class="fl">1e-4</span></span>
<span id="cb78-7"><a href="#cb78-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-8"><a href="#cb78-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Dataset </span></span>
<span id="cb78-9"><a href="#cb78-9" aria-hidden="true" tabindex="-1"></a>image_dir <span class="op">=</span> <span class="st">'./data/flickr8k/Images'</span>  <span class="co"># Change to the actual path</span></span>
<span id="cb78-10"><a href="#cb78-10" aria-hidden="true" tabindex="-1"></a>caption_file <span class="op">=</span> <span class="st">'./data/flickr8k/captions.txt'</span>  <span class="co"># Change to the actual path</span></span>
<span id="cb78-11"><a href="#cb78-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-12"><a href="#cb78-12" aria-hidden="true" tabindex="-1"></a>results_df <span class="op">=</span> run_training(model_versions, epochs<span class="op">=</span>epochs, lr<span class="op">=</span>lr, image_dir<span class="op">=</span>image_dir, caption_file<span class="op">=</span>caption_file) <span class="co"># Train multiple versions</span></span>
<span id="cb78-13"><a href="#cb78-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-14"><a href="#cb78-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Print results</span></span>
<span id="cb78-15"><a href="#cb78-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Training Results:"</span>)</span>
<span id="cb78-16"><a href="#cb78-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Print results in Markdown table format</span></span>
<span id="cb78-17"><a href="#cb78-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(results_df.to_markdown(index<span class="op">=</span><span class="va">False</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Evaluate with the model.</p>
<div id="cell-53" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb79"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb79-1"><a href="#cb79-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dldna.chapter_10.mm.evaluate_models <span class="im">import</span> evaluate_all_models</span>
<span id="cb79-2"><a href="#cb79-2" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb79-3"><a href="#cb79-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Test captions (fixed)</span></span>
<span id="cb79-4"><a href="#cb79-4" aria-hidden="true" tabindex="-1"></a>test_captions <span class="op">=</span> [</span>
<span id="cb79-5"><a href="#cb79-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">"A dog playing in the park"</span>,</span>
<span id="cb79-6"><a href="#cb79-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">"A cat sleeping on a couch"</span>,</span>
<span id="cb79-7"><a href="#cb79-7" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Children playing soccer"</span>,</span>
<span id="cb79-8"><a href="#cb79-8" aria-hidden="true" tabindex="-1"></a>    <span class="st">"A sunset over the ocean"</span>,</span>
<span id="cb79-9"><a href="#cb79-9" aria-hidden="true" tabindex="-1"></a>    <span class="st">"A person cooking in the kitchen"</span></span>
<span id="cb79-10"><a href="#cb79-10" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb79-11"><a href="#cb79-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-12"><a href="#cb79-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Run model evaluation</span></span>
<span id="cb79-13"><a href="#cb79-13" aria-hidden="true" tabindex="-1"></a>image_path <span class="op">=</span> <span class="st">'./cat_resized.png'</span></span>
<span id="cb79-14"><a href="#cb79-14" aria-hidden="true" tabindex="-1"></a>model_dir <span class="op">=</span> <span class="st">'.'</span></span>
<span id="cb79-15"><a href="#cb79-15" aria-hidden="true" tabindex="-1"></a>model_versions <span class="op">=</span> [<span class="st">'v0'</span>, <span class="st">'v1'</span>, <span class="st">'v2'</span>, <span class="st">'v3'</span>, <span class="st">'v4'</span>, <span class="st">'v5'</span>, <span class="st">'v6'</span>, <span class="st">'v7'</span>, <span class="st">'v8'</span>, <span class="st">'v9'</span>, <span class="st">'v10_1'</span>, <span class="st">'v10_2'</span>, <span class="st">'v10_3'</span>, <span class="st">'v10_4'</span>, <span class="st">'v10_5'</span>, <span class="st">'v10_6'</span>, <span class="st">'v11'</span>]</span>
<span id="cb79-16"><a href="#cb79-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-17"><a href="#cb79-17" aria-hidden="true" tabindex="-1"></a>results_df <span class="op">=</span> evaluate_all_models(model_dir, image_path, test_captions, model_versions)</span>
<span id="cb79-18"><a href="#cb79-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-19"><a href="#cb79-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Print results (Markdown table)</span></span>
<span id="cb79-20"><a href="#cb79-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(results_df.to_markdown(index<span class="op">=</span><span class="va">False</span>))</span>
<span id="cb79-21"><a href="#cb79-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-22"><a href="#cb79-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Print results (detailed)</span></span>
<span id="cb79-23"><a href="#cb79-23" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> _, row <span class="kw">in</span> results_df.iterrows():</span>
<span id="cb79-24"><a href="#cb79-24" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Model: </span><span class="sc">{</span>row[<span class="st">'model_version'</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb79-25"><a href="#cb79-25" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  Best Caption: </span><span class="sc">{</span>row[<span class="st">'best_caption'</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb79-26"><a href="#cb79-26" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  Trained Well: </span><span class="sc">{</span>row[<span class="st">'trained_well'</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb79-27"><a href="#cb79-27" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  Similarity Ratio: </span><span class="sc">{</span>row[<span class="st">'similarity_ratio'</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb79-28"><a href="#cb79-28" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  Similarity Gap: </span><span class="sc">{</span>row[<span class="st">'similarity_gap'</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb79-29"><a href="#cb79-29" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"  All Similarities:"</span>)</span>
<span id="cb79-30"><a href="#cb79-30" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> caption, sim <span class="kw">in</span> <span class="bu">zip</span>(test_captions, row[<span class="st">'all_similarities'</span>]):</span>
<span id="cb79-31"><a href="#cb79-31" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"    - </span><span class="sc">{</span>caption<span class="sc">:&lt;30}</span><span class="ss">: </span><span class="sc">{</span>sim<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="experimental-results" class="level4">
<h4 class="anchored" data-anchor-id="experimental-results">10.4.4.2 Experimental Results</h4>
</section>
<section id="experimental-result-table" class="level4">
<h4 class="anchored" data-anchor-id="experimental-result-table">Experimental Result Table</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 9%">
<col style="width: 18%">
<col style="width: 29%">
<col style="width: 10%">
<col style="width: 9%">
<col style="width: 8%">
<col style="width: 13%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">model_version</th>
<th style="text-align: left;">best_caption</th>
<th style="text-align: left;">all_similarities</th>
<th style="text-align: right;">similarity_ratio</th>
<th style="text-align: right;">similarity_gap</th>
<th style="text-align: left;">trained_well</th>
<th style="text-align: right;">similarity_ratio_rank</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">v0</td>
<td style="text-align: left;">A cat sleeping on a couch</td>
<td style="text-align: left;">[‘5.322’, ‘15.477’, ‘-4.509’, ‘-6.609’, ‘2.107’]</td>
<td style="text-align: right;">2.908</td>
<td style="text-align: right;">10.155</td>
<td style="text-align: left;">True</td>
<td style="text-align: right;">1</td>
</tr>
<tr class="even">
<td style="text-align: left;">v1</td>
<td style="text-align: left;">A cat sleeping on a couch</td>
<td style="text-align: left;">[‘3.117’, ‘18.174’, ‘-6.475’, ‘-1.825’, ‘8.705’]</td>
<td style="text-align: right;">2.088</td>
<td style="text-align: right;">9.469</td>
<td style="text-align: left;">True</td>
<td style="text-align: right;">3</td>
</tr>
<tr class="odd">
<td style="text-align: left;">v2</td>
<td style="text-align: left;">A cat sleeping on a couch</td>
<td style="text-align: left;">[‘3.085’, ‘12.541’, ‘-4.252’, ‘0.924’, ‘6.849’]</td>
<td style="text-align: right;">1.831</td>
<td style="text-align: right;">5.692</td>
<td style="text-align: left;">True</td>
<td style="text-align: right;">5</td>
</tr>
<tr class="even">
<td style="text-align: left;">v3</td>
<td style="text-align: left;">Children playing soccer</td>
<td style="text-align: left;">[‘34.882’, ‘34.882’, ‘34.882’, ‘34.882’, ‘34.882’]</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">0</td>
<td style="text-align: left;">False</td>
<td style="text-align: right;">14</td>
</tr>
<tr class="odd">
<td style="text-align: left;">v4</td>
<td style="text-align: left;">A cat sleeping on a couch</td>
<td style="text-align: left;">[‘7.385’, ‘8.301’, ‘-1.038’, ‘-6.262’, ‘1.240’]</td>
<td style="text-align: right;">1.124</td>
<td style="text-align: right;">0.915</td>
<td style="text-align: left;">True</td>
<td style="text-align: right;">12</td>
</tr>
<tr class="even">
<td style="text-align: left;">v5</td>
<td style="text-align: left;">Children playing soccer</td>
<td style="text-align: left;">[‘27.357’, ‘27.357’, ‘27.357’, ‘27.357’, ‘27.357’]</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">0</td>
<td style="text-align: left;">False</td>
<td style="text-align: right;">14</td>
</tr>
<tr class="odd">
<td style="text-align: left;">v6</td>
<td style="text-align: left;">A cat sleeping on a couch</td>
<td style="text-align: left;">[‘5.022’, ‘14.861’, ‘-5.370’, ‘-8.630’, ‘9.063’]</td>
<td style="text-align: right;">1.64</td>
<td style="text-align: right;">5.798</td>
<td style="text-align: left;">True</td>
<td style="text-align: right;">7</td>
</tr>
<tr class="even">
<td style="text-align: left;">v7</td>
<td style="text-align: left;">A dog playing in the park</td>
<td style="text-align: left;">[‘16.300’, ‘16.300’, ‘16.300’, ‘16.300’, ‘16.300’]</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">0</td>
<td style="text-align: left;">False</td>
<td style="text-align: right;">14</td>
</tr>
<tr class="odd">
<td style="text-align: left;">v8</td>
<td style="text-align: left;">A cat sleeping on a couch</td>
<td style="text-align: left;">[‘9.841’, ‘15.442’, ‘-7.350’, ‘-1.249’, ‘11.023’]</td>
<td style="text-align: right;">1.401</td>
<td style="text-align: right;">4.419</td>
<td style="text-align: left;">True</td>
<td style="text-align: right;">10</td>
</tr>
<tr class="even">
<td style="text-align: left;">v9</td>
<td style="text-align: left;">A cat sleeping on a couch</td>
<td style="text-align: left;">[‘10.382’, ‘15.192’, ‘-5.582’, ‘-1.594’, ‘5.953’]</td>
<td style="text-align: right;">1.463</td>
<td style="text-align: right;">4.81</td>
<td style="text-align: left;">True</td>
<td style="text-align: right;">9</td>
</tr>
<tr class="odd">
<td style="text-align: left;">v10_1</td>
<td style="text-align: left;">A dog playing in the park</td>
<td style="text-align: left;">[‘0.940’, ‘0.472’, ‘-0.554’, ‘0.334’, ‘-0.111’]</td>
<td style="text-align: right;">1.991</td>
<td style="text-align: right;">0.468</td>
<td style="text-align: left;">False</td>
<td style="text-align: right;">4</td>
</tr>
<tr class="even">
<td style="text-align: left;">v10_2</td>
<td style="text-align: left;">A cat sleeping on a couch</td>
<td style="text-align: left;">[‘17.720’, ‘17.720’, ‘17.720’, ‘17.720’, ‘17.720’]</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">0</td>
<td style="text-align: left;">True</td>
<td style="text-align: right;">14</td>
</tr>
<tr class="odd">
<td style="text-align: left;">v10_3</td>
<td style="text-align: left;">A cat sleeping on a couch</td>
<td style="text-align: left;">[‘0.516’, ‘1.479’, ‘-0.989’, ‘-5.989’, ‘5.151’]</td>
<td style="text-align: right;">1.748</td>
<td style="text-align: right;">4.421</td>
<td style="text-align: left;">True</td>
<td style="text-align: right;">6</td>
</tr>
<tr class="even">
<td style="text-align: left;">v10_4</td>
<td style="text-align: left;">A cat sleeping on a couch</td>
<td style="text-align: left;">[‘5.913’, ‘10.334’, ‘-5.989’, ‘-1.024’, ‘5.151’]</td>
<td style="text-align: right;">1.748</td>
<td style="text-align: right;">4.421</td>
<td style="text-align: left;">True</td>
<td style="text-align: right;">6</td>
</tr>
<tr class="odd">
<td style="text-align: left;">v10_5</td>
<td style="text-align: left;">A cat sleeping on a couch</td>
<td style="text-align: left;">[‘6.601’, ‘9.990’, ‘-5.984’, ‘-2.988’, ‘-0.070’]</td>
<td style="text-align: right;">1.513</td>
<td style="text-align: right;">3.389</td>
<td style="text-align: left;">True</td>
<td style="text-align: right;">8</td>
</tr>
<tr class="even">
<td style="text-align: left;">v10_6</td>
<td style="text-align: left;">A dog playing in the park</td>
<td style="text-align: left;">[‘33.967’, ‘33.302’, ‘31.580’, ‘32.710’, ‘31.384’]</td>
<td style="text-align: right;">1.02</td>
<td style="text-align: right;">0.665</td>
<td style="text-align: left;">False</td>
<td style="text-align: right;">13</td>
</tr>
<tr class="odd">
<td style="text-align: left;">v11</td>
<td style="text-align: left;">A cat sleeping on a couch</td>
<td style="text-align: left;">[‘11.315’, ‘15.491’, ‘-10.428’, ‘-0.004’, ‘10.014’]</td>
<td style="text-align: right;">1.369</td>
<td style="text-align: right;">4.175</td>
<td style="text-align: left;">True</td>
<td style="text-align: right;">11</td>
</tr>
</tbody>
</table>
</section>
<section id="analysis-of-model-structures-and-training-outcomes" class="level4">
<h4 class="anchored" data-anchor-id="analysis-of-model-structures-and-training-outcomes"><strong>Analysis of Model Structures and Training Outcomes</strong></h4>
<p>Based on these experimental results, we can analyze the training outcomes for each cross-attention version and summarize the causes of success/failure as follows.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
</colgroup>
<thead>
<tr class="header">
<th>Version</th>
<th>Attention Structure</th>
<th>Key Features</th>
<th>Training Result</th>
<th>Detailed Explanation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>v0</td>
<td>Independent Bidirectional Attention</td>
<td>Only scaled dot-product attention</td>
<td>Training Success</td>
<td>Most basic structure. Calculates attention independently for images and text. No normalization/transformation other than scaling. Without separate normalization, it’s sensitive to changes in input feature scale.</td>
</tr>
<tr class="even">
<td>v1</td>
<td>Shared Attention</td>
<td>Single attention matrix and transpose matrix</td>
<td>Training Success</td>
<td>Shares the same attention matrix for image→text and text→image attention calculations. Attempts bidirectional information exchange but remains sensitive to input scale due to lack of normalization and fails to properly reflect asymmetric relationships between the two modalities.</td>
</tr>
<tr class="odd">
<td>v2</td>
<td>Shared Attention + LN</td>
<td>LayerNorm applied to inputs</td>
<td>Training Success</td>
<td>Applies LayerNorm to input features to stabilize feature scaling. Resolves v1’s issue (input scale sensitivity). Attention matrix is still shared.</td>
</tr>
<tr class="even">
<td>v3</td>
<td>v2 + Residual Connection</td>
<td>Adds residual connection to output</td>
<td>Training Failure</td>
<td>Adds residual connection that directly adds original features (image_features, text_features) after attention calculation. This excessively preserves original features, hindering the creation of new features through interaction between modalities. This phenomenon is particularly pronounced in shallow network structures.</td>
</tr>
<tr class="odd">
<td>v4</td>
<td>v2 + Projection</td>
<td>Modality-specific linear transformations</td>
<td>Training Success</td>
<td>Applies independent linear projections (self.image_proj, self.text_proj) to each modality. By applying separate linear transformations to normalized inputs (image_norm, text_norm), it more flexibly adjusts each modality’s feature space and transforms it into a form suitable for attention calculation.</td>
</tr>
<tr class="even">
<td>v5</td>
<td>v2 + Mixing Ratio</td>
<td>Fixed 0.5 mixing ratio</td>
<td>Training Failure</td>
<td>Mixes original features (image_norm, text_norm) and attention outputs (image_attended, text_attended) at a fixed ratio (0.5). Similar to residual connection (v3) in preserving original features, but the fixed mixing ratio limits the model’s ability to flexibly adjust weights according to data.</td>
</tr>
<tr class="odd">
<td>v6</td>
<td>Shared Attention + Q/K/V</td>
<td>Q/K/V transformations and single LayerNorm</td>
<td>Training Success</td>
<td>Adds separate linear transformations (self.to_q, self.to_k, self.to_v) that generate Query (Q), Key (K), and Value (V) for inputs (image_norm, text_norm). This allows the attention mechanism to learn richer feature representations. Still uses a shared attention matrix.</td>
</tr>
<tr class="even">
<td>v7</td>
<td>Shared Multi-head</td>
<td>Multi-head + output normalization</td>
<td>Training Failure</td>
<td>Extends shared attention matrix to multi-head attention. Maintains LayerNorm for inputs (v2). Although each head can learn different features, it still uses shared attention, failing to properly model the asymmetric relationship between image→text and text→image. Despite applying LayerNorm to the output, training fails.</td>
</tr>
<tr class="odd">
<td>v8</td>
<td>Independent Multi-head</td>
<td>Independent bidirectional multi-head + dual normalization</td>
<td>Training Success</td>
<td>Separates image→text and text→image attention into independent multi-head attention. Applies LayerNorm to both input and output. Effectively performs bidirectional information exchange while preserving each modality’s characteristics.</td>
</tr>
<tr class="even">
<td>v9</td>
<td>v8 + Pre-LN + FFN</td>
<td>Adds gated FFN and dropout</td>
<td>Training Success</td>
<td>Adds Pre-LayerNorm, gated Feed-Forward Network (FFN), and dropout to v8’s structure. Pre-LN applies LayerNorm before attention and FFN to increase training stability. Gated FFN uses GELU activation function and dropout to enhance non-linearity and prevent overfitting. Applies residual connection only to FFN output to improve information flow.</td>
</tr>
<tr class="odd">
<td>v10_1</td>
<td>v9 + Modality-specific Q/K/V</td>
<td>Specialized transformations for each modality</td>
<td>Training Failure</td>
<td>Based on v9, uses separate Q, K, V projections (self.image_to_q, self.image_to_k, …, self.text_to_v) for each modality. This greatly increases model complexity but excessively separates each modality’s characteristics, making it difficult to learn interactions between the two modalities.</td>
</tr>
<tr class="even">
<td>v10_2</td>
<td>v9 + Cross Gate</td>
<td>Controls information flow between modalities</td>
<td>Training Failure</td>
<td>Adds cross-gate mechanism to v9. Applies gate layer (sigmoid) after concatenating attention output and original features to control information exchange between modalities. However, without normalization for the gate layer and with very small initial gate values (self.gate_scale = 0.1), it fails to effectively control information flow and hinders learning.</td>
</tr>
<tr class="odd">
<td>v10_3</td>
<td>v9 + Context Layer</td>
<td>Processes modality-specific contextual information</td>
<td>Training Success</td>
<td>Adds modality-specific context layers (self.image_context, self.text_context) to v9. This additionally processes each modality’s features to provide richer contextual information before attention calculation.</td>
</tr>
<tr class="even">
<td>v10_4</td>
<td>v9 + Multi-query</td>
<td>K,V shared attention approach</td>
<td>Training Success</td>
<td>Introduces Multi-Query Attention mechanism to v9. Maintains queries independently for each head while sharing keys and values across all heads (self.to_kv). This reduces parameter count while allowing each head to generate queries from different perspectives to capture diverse features.</td>
</tr>
<tr class="odd">
<td>v10_5</td>
<td>v9 + Hierarchical Multi-head</td>
<td>3-level feature processing, weight-based fusion</td>
<td>Training Success</td>
<td>Introduces hierarchical multi-head attention structure to v9. Processes input features at 3 levels (self.level_projections, self.level_norms). Performs independent multi-head attention at each level and fuses outputs using learnable weights (self.level_weights). This allows the model to learn features at various levels of abstraction and effectively combine them.</td>
</tr>
<tr class="even">
<td>v10_6</td>
<td>v9 + Contrastive Learning Multi-head</td>
<td>Contrastive learning-based similarity constraints, feature enhancement</td>
<td>Training Failure</td>
<td>Adds separate projection layer (self.contrast_proj) for contrastive learning to v9. Calculates similarity between normalized contrastive learning features and enhances attention output by directly adding to original features. However, this distorts original features and, similar to v3, hinders interaction between modalities, leading to training failure.</td>
</tr>
<tr class="odd">
<td>v11</td>
<td>v9 + Multi-query + Hierarchical Fusion</td>
<td>Combines K,V sharing with 3-level feature processing</td>
<td>Training Success</td>
<td>Combines advantages of v10_4 (multi-query) and v10_5 (hierarchical multi-head). Increases parameter efficiency through multi-query attention and integrates features at various levels through hierarchical fusion. Maintains stabilization techniques from v9 such as Pre-LN, gated FFN, and dropout.</td>
</tr>
</tbody>
</table>
</section>
<section id="explanation-by-attention-structure" class="level4">
<h4 class="anchored" data-anchor-id="explanation-by-attention-structure">10.4.4.3 Explanation by Attention Structure</h4>
<p><strong>1. v0:</strong> Independent Bidirectional Attention - Basic Structure</p>
<p>v0 implements the most basic form of Cross-Modal Attention. It calculates independent attention for images and text, respectively, and uses only scaled dot-product attention without any other normalization or transformation.</p>
<div id="cell-55" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb80"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb80-1"><a href="#cb80-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb80-2"><a href="#cb80-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb80-3"><a href="#cb80-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb80-4"><a href="#cb80-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-5"><a href="#cb80-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-6"><a href="#cb80-6" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> CrossAttention(nn.Module):</span>
<span id="cb80-7"><a href="#cb80-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, dim):</span>
<span id="cb80-8"><a href="#cb80-8" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb80-9"><a href="#cb80-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.scale <span class="op">=</span> dim <span class="op">**</span> <span class="op">-</span><span class="fl">0.5</span></span>
<span id="cb80-10"><a href="#cb80-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-11"><a href="#cb80-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, image_features, text_features):</span>
<span id="cb80-12"><a href="#cb80-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Image -&gt; Text attention</span></span>
<span id="cb80-13"><a href="#cb80-13" aria-hidden="true" tabindex="-1"></a>        attn_i2t <span class="op">=</span> torch.matmul(image_features, text_features.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>)) <span class="op">*</span> <span class="va">self</span>.scale</span>
<span id="cb80-14"><a href="#cb80-14" aria-hidden="true" tabindex="-1"></a>        attn_i2t <span class="op">=</span> attn_i2t.softmax(dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb80-15"><a href="#cb80-15" aria-hidden="true" tabindex="-1"></a>        image_attended <span class="op">=</span> torch.matmul(attn_i2t, text_features)</span>
<span id="cb80-16"><a href="#cb80-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-17"><a href="#cb80-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Text -&gt; Image attention</span></span>
<span id="cb80-18"><a href="#cb80-18" aria-hidden="true" tabindex="-1"></a>        attn_t2i <span class="op">=</span> torch.matmul(text_features, image_features.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>)) <span class="op">*</span> <span class="va">self</span>.scale</span>
<span id="cb80-19"><a href="#cb80-19" aria-hidden="true" tabindex="-1"></a>        attn_t2i <span class="op">=</span> attn_t2i.softmax(dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb80-20"><a href="#cb80-20" aria-hidden="true" tabindex="-1"></a>        text_attended <span class="op">=</span> torch.matmul(attn_t2i, image_features)</span>
<span id="cb80-21"><a href="#cb80-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-22"><a href="#cb80-22" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> image_attended, text_attended</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>v0 is sensitive to the scale change of input features because it does not have a separate normalization process. If the scale of the input data changes greatly during the learning process, the attention weights become unstable and training may not be done properly.</p>
<p><strong>2. v2:</strong> Shared Attention + Layer Normalization</p>
<p>v2 is a version that applies Layer Normalization (LN) to the input features in v1 to stabilize the feature scale. v1 used the same attention matrix (weight matrix) and its transpose for image-to-text and text-to-image attention calculations, but it had the drawback of being sensitive to input scales.</p>
<div id="cell-57" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb81"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb81-1"><a href="#cb81-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb81-2"><a href="#cb81-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb81-3"><a href="#cb81-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb81-4"><a href="#cb81-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-5"><a href="#cb81-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Co-attention + added LN</span></span>
<span id="cb81-6"><a href="#cb81-6" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> CrossAttention(nn.Module):</span>
<span id="cb81-7"><a href="#cb81-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, dim):</span>
<span id="cb81-8"><a href="#cb81-8" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb81-9"><a href="#cb81-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.scale <span class="op">=</span> dim <span class="op">**</span> <span class="op">-</span><span class="fl">0.5</span></span>
<span id="cb81-10"><a href="#cb81-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.norm <span class="op">=</span> nn.LayerNorm(dim)  <span class="co"># Use a single LayerNorm</span></span>
<span id="cb81-11"><a href="#cb81-11" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb81-12"><a href="#cb81-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, image_features, text_features):</span>
<span id="cb81-13"><a href="#cb81-13" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Input normalization</span></span>
<span id="cb81-14"><a href="#cb81-14" aria-hidden="true" tabindex="-1"></a>        image_norm <span class="op">=</span> <span class="va">self</span>.norm(image_features)</span>
<span id="cb81-15"><a href="#cb81-15" aria-hidden="true" tabindex="-1"></a>        text_norm <span class="op">=</span> <span class="va">self</span>.norm(text_features)</span>
<span id="cb81-16"><a href="#cb81-16" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb81-17"><a href="#cb81-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Simple attention calculation</span></span>
<span id="cb81-18"><a href="#cb81-18" aria-hidden="true" tabindex="-1"></a>        attn <span class="op">=</span> torch.matmul(image_norm, text_norm.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>)) <span class="op">*</span> <span class="va">self</span>.scale</span>
<span id="cb81-19"><a href="#cb81-19" aria-hidden="true" tabindex="-1"></a>        attn <span class="op">=</span> attn.softmax(dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb81-20"><a href="#cb81-20" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb81-21"><a href="#cb81-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Bidirectional feature fusion (without residual connection)</span></span>
<span id="cb81-22"><a href="#cb81-22" aria-hidden="true" tabindex="-1"></a>        image_out <span class="op">=</span> torch.matmul(attn, text_norm)</span>
<span id="cb81-23"><a href="#cb81-23" aria-hidden="true" tabindex="-1"></a>        text_out <span class="op">=</span> torch.matmul(attn.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>), image_norm)</span>
<span id="cb81-24"><a href="#cb81-24" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb81-25"><a href="#cb81-25" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> image_out, text_out</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><code>image_norm = self.norm(image_features)</code> and <code>text_norm = self.norm(text_features)</code> apply Layer Normalization to the input features. Layer Normalization performs normalization independently for each sample (each image or text in a mini-batch). That is, it calculates the mean and variance of the feature vector of each sample and makes them 0 and 1. This prevents the attention weights from diverging even if the scale of the input features changes greatly, stabilizing learning.</p>
<p>However, there are still limitations. v2 solved the input scale problem through Layer Normalization, but it uses the same attention matrix for image-to-text and text-to-image attention. This may not fully reflect the asymmetric relationship between the two modalities. Generating text from an image and generating an image from text can have different complexities. Processing them with the same attention mechanism can be inefficient.</p>
<p><strong>3. v3:</strong> v2 + Residual Connection - Failure Case</p>
<p>After the ResNet model architecture, residual connections, which had been widely used, became the cause of failure here. Residual connections are generally used to alleviate the gradient vanishing problem that can occur as the network deepens and to effectively learn deeper networks. However, in this experiment, residual connections showed a failure case where they actually degraded performance.</p>
<p>This is a very important observation.</p>
<div id="cell-59" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb82"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb82-1"><a href="#cb82-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb82-2"><a href="#cb82-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb82-3"><a href="#cb82-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb82-4"><a href="#cb82-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-5"><a href="#cb82-5" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> CrossAttention(nn.Module):</span>
<span id="cb82-6"><a href="#cb82-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, dim):</span>
<span id="cb82-7"><a href="#cb82-7" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb82-8"><a href="#cb82-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.scale <span class="op">=</span> dim <span class="op">**</span> <span class="op">-</span><span class="fl">0.5</span></span>
<span id="cb82-9"><a href="#cb82-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.norm <span class="op">=</span> nn.LayerNorm(dim)  <span class="co"># Use a single LayerNorm</span></span>
<span id="cb82-10"><a href="#cb82-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-11"><a href="#cb82-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, image_features, text_features):</span>
<span id="cb82-12"><a href="#cb82-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Input normalization</span></span>
<span id="cb82-13"><a href="#cb82-13" aria-hidden="true" tabindex="-1"></a>        image_norm <span class="op">=</span> <span class="va">self</span>.norm(image_features)</span>
<span id="cb82-14"><a href="#cb82-14" aria-hidden="true" tabindex="-1"></a>        text_norm <span class="op">=</span> <span class="va">self</span>.norm(text_features)</span>
<span id="cb82-15"><a href="#cb82-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-16"><a href="#cb82-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Simple attention calculation</span></span>
<span id="cb82-17"><a href="#cb82-17" aria-hidden="true" tabindex="-1"></a>        attn <span class="op">=</span> torch.matmul(image_norm, text_norm.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>)) <span class="op">*</span> <span class="va">self</span>.scale</span>
<span id="cb82-18"><a href="#cb82-18" aria-hidden="true" tabindex="-1"></a>        attn <span class="op">=</span> attn.softmax(dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb82-19"><a href="#cb82-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-20"><a href="#cb82-20" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Bidirectional feature fusion</span></span>
<span id="cb82-21"><a href="#cb82-21" aria-hidden="true" tabindex="-1"></a>        image_attended <span class="op">=</span> torch.matmul(attn, text_norm)</span>
<span id="cb82-22"><a href="#cb82-22" aria-hidden="true" tabindex="-1"></a>        text_attended <span class="op">=</span> torch.matmul(attn.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>), image_norm)</span>
<span id="cb82-23"><a href="#cb82-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-24"><a href="#cb82-24" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Add residual connection</span></span>
<span id="cb82-25"><a href="#cb82-25" aria-hidden="true" tabindex="-1"></a>        image_out <span class="op">=</span> image_features <span class="op">+</span> image_attended</span>
<span id="cb82-26"><a href="#cb82-26" aria-hidden="true" tabindex="-1"></a>        text_out <span class="op">=</span> text_features <span class="op">+</span> text_attended</span>
<span id="cb82-27"><a href="#cb82-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-28"><a href="#cb82-28" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> image_out, text_out</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Generally, residual connections are effective in solving the problem that learning becomes more difficult as the network deepens. However, in v3, it resulted in a performance degradation for the following reasons:</p>
<p><strong>Relatively shallow network:</strong> The v3 model has a relatively shallow network structure that is not very deep. Residual connections are of great help in mitigating the gradient vanishing problem in deep networks, but their effect is minimal in shallow networks and may even hinder the flow of information.</p>
<p><strong>Excessive preservation of original features:</strong> The core of Cross-Modal Attention is to generate new features through interactions between two different modalities, images and text. However, in v3, by directly adding the original feature vectors to the attention operation results, it diluted the important information obtained through the attention mechanism and hindered the generation of features through interactions between the two modalities. In other words, the model focused on maintaining existing information rather than learning new information.</p>
<p>The experimental results of v3 provide an important lesson that residual connections are not a <strong>panacea that always improves performance</strong>. Residual connections should be used carefully considering the depth of the network, the location of application, and the characteristics of the problem. v3 can be considered a representative failure case where the performance was degraded due to the improper use of residual connections.</p>
<p><strong>4. v8:</strong> Independent Multi-Head Attention</p>
<p>v8 introduced important changes to solve the problems of the previous version (v7) and improve the performance of Cross-Modal Attention. Specifically, it separated image-to-text attention and text-to-image attention into independent multi-head attentions. Additionally, Layer Normalization was applied not only to the input but also to the output of the attention operation to further strengthen training stability.</p>
<div id="cell-61" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb83"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb83-1"><a href="#cb83-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb83-2"><a href="#cb83-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb83-3"><a href="#cb83-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb83-4"><a href="#cb83-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-5"><a href="#cb83-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-6"><a href="#cb83-6" aria-hidden="true" tabindex="-1"></a><span class="co"># v8 - Independent multi-head</span></span>
<span id="cb83-7"><a href="#cb83-7" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> CrossAttention(nn.Module):</span>
<span id="cb83-8"><a href="#cb83-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, dim, num_heads<span class="op">=</span><span class="dv">8</span>):</span>
<span id="cb83-9"><a href="#cb83-9" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb83-10"><a href="#cb83-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.num_heads <span class="op">=</span> num_heads</span>
<span id="cb83-11"><a href="#cb83-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.head_dim <span class="op">=</span> dim <span class="op">//</span> num_heads</span>
<span id="cb83-12"><a href="#cb83-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.scale <span class="op">=</span> <span class="va">self</span>.head_dim <span class="op">**</span> <span class="op">-</span><span class="fl">0.5</span></span>
<span id="cb83-13"><a href="#cb83-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.norm <span class="op">=</span> nn.LayerNorm(dim)</span>
<span id="cb83-14"><a href="#cb83-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-15"><a href="#cb83-15" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Projections for multi-head attention</span></span>
<span id="cb83-16"><a href="#cb83-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.to_q <span class="op">=</span> nn.Linear(dim, dim)</span>
<span id="cb83-17"><a href="#cb83-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.to_k <span class="op">=</span> nn.Linear(dim, dim)</span>
<span id="cb83-18"><a href="#cb83-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.to_v <span class="op">=</span> nn.Linear(dim, dim)</span>
<span id="cb83-19"><a href="#cb83-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-20"><a href="#cb83-20" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Output projection</span></span>
<span id="cb83-21"><a href="#cb83-21" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.to_out <span class="op">=</span> nn.Linear(dim, dim)</span>
<span id="cb83-22"><a href="#cb83-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-23"><a href="#cb83-23" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Add output normalization</span></span>
<span id="cb83-24"><a href="#cb83-24" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.out_norm <span class="op">=</span> nn.LayerNorm(dim)</span>
<span id="cb83-25"><a href="#cb83-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-26"><a href="#cb83-26" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, image_features, text_features):</span>
<span id="cb83-27"><a href="#cb83-27" aria-hidden="true" tabindex="-1"></a>        B, N_i, _ <span class="op">=</span> image_features.shape</span>
<span id="cb83-28"><a href="#cb83-28" aria-hidden="true" tabindex="-1"></a>        _, N_t, _ <span class="op">=</span> text_features.shape</span>
<span id="cb83-29"><a href="#cb83-29" aria-hidden="true" tabindex="-1"></a>        H <span class="op">=</span> <span class="va">self</span>.num_heads</span>
<span id="cb83-30"><a href="#cb83-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-31"><a href="#cb83-31" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Input normalization</span></span>
<span id="cb83-32"><a href="#cb83-32" aria-hidden="true" tabindex="-1"></a>        image_norm <span class="op">=</span> <span class="va">self</span>.norm(image_features)</span>
<span id="cb83-33"><a href="#cb83-33" aria-hidden="true" tabindex="-1"></a>        text_norm <span class="op">=</span> <span class="va">self</span>.norm(text_features)</span>
<span id="cb83-34"><a href="#cb83-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-35"><a href="#cb83-35" aria-hidden="true" tabindex="-1"></a>        <span class="kw">def</span> split_heads(x):</span>
<span id="cb83-36"><a href="#cb83-36" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> x.reshape(B, <span class="op">-</span><span class="dv">1</span>, H, <span class="va">self</span>.head_dim).transpose(<span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb83-37"><a href="#cb83-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-38"><a href="#cb83-38" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Image -&gt; Text direction attention</span></span>
<span id="cb83-39"><a href="#cb83-39" aria-hidden="true" tabindex="-1"></a>        q_img <span class="op">=</span> split_heads(<span class="va">self</span>.to_q(image_norm))</span>
<span id="cb83-40"><a href="#cb83-40" aria-hidden="true" tabindex="-1"></a>        k_txt <span class="op">=</span> split_heads(<span class="va">self</span>.to_k(text_norm))</span>
<span id="cb83-41"><a href="#cb83-41" aria-hidden="true" tabindex="-1"></a>        v_txt <span class="op">=</span> split_heads(<span class="va">self</span>.to_v(text_norm))</span>
<span id="cb83-42"><a href="#cb83-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-43"><a href="#cb83-43" aria-hidden="true" tabindex="-1"></a>        attn_i2t <span class="op">=</span> torch.matmul(q_img, k_txt.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>)) <span class="op">*</span> <span class="va">self</span>.scale</span>
<span id="cb83-44"><a href="#cb83-44" aria-hidden="true" tabindex="-1"></a>        attn_i2t <span class="op">=</span> attn_i2t.softmax(dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb83-45"><a href="#cb83-45" aria-hidden="true" tabindex="-1"></a>        image_attended <span class="op">=</span> torch.matmul(attn_i2t, v_txt)</span>
<span id="cb83-46"><a href="#cb83-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-47"><a href="#cb83-47" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Text -&gt; Image direction attention</span></span>
<span id="cb83-48"><a href="#cb83-48" aria-hidden="true" tabindex="-1"></a>        q_txt <span class="op">=</span> split_heads(<span class="va">self</span>.to_q(text_norm))</span>
<span id="cb83-49"><a href="#cb83-49" aria-hidden="true" tabindex="-1"></a>        k_img <span class="op">=</span> split_heads(<span class="va">self</span>.to_k(image_norm))</span>
<span id="cb83-50"><a href="#cb83-50" aria-hidden="true" tabindex="-1"></a>        v_img <span class="op">=</span> split_heads(<span class="va">self</span>.to_v(image_norm))</span>
<span id="cb83-51"><a href="#cb83-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-52"><a href="#cb83-52" aria-hidden="true" tabindex="-1"></a>        attn_t2i <span class="op">=</span> torch.matmul(q_txt, k_img.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>)) <span class="op">*</span> <span class="va">self</span>.scale</span>
<span id="cb83-53"><a href="#cb83-53" aria-hidden="true" tabindex="-1"></a>        attn_t2i <span class="op">=</span> attn_t2i.softmax(dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb83-54"><a href="#cb83-54" aria-hidden="true" tabindex="-1"></a>        text_attended <span class="op">=</span> torch.matmul(attn_t2i, v_img)</span>
<span id="cb83-55"><a href="#cb83-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-56"><a href="#cb83-56" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Combine heads and output projection</span></span>
<span id="cb83-57"><a href="#cb83-57" aria-hidden="true" tabindex="-1"></a>        image_attended <span class="op">=</span> image_attended.transpose(<span class="dv">1</span>, <span class="dv">2</span>).reshape(B, N_i, <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb83-58"><a href="#cb83-58" aria-hidden="true" tabindex="-1"></a>        text_attended <span class="op">=</span> text_attended.transpose(<span class="dv">1</span>, <span class="dv">2</span>).reshape(B, N_t, <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb83-59"><a href="#cb83-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-60"><a href="#cb83-60" aria-hidden="true" tabindex="-1"></a>        image_out <span class="op">=</span> <span class="va">self</span>.out_norm(<span class="va">self</span>.to_out(image_attended))</span>
<span id="cb83-61"><a href="#cb83-61" aria-hidden="true" tabindex="-1"></a>        text_out <span class="op">=</span> <span class="va">self</span>.out_norm(<span class="va">self</span>.to_out(text_attended))</span>
<span id="cb83-62"><a href="#cb83-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-63"><a href="#cb83-63" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> image_out, text_out</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>In v7, multi-head attention was introduced, but the same Q, K, V transformation was still used for both image-to-text and text-to-image attention. That is, since all heads shared the same Q, K, V matrix, there was a constraint on each head learning different features, which became a factor limiting the model’s expressiveness. v8 solved this problem by applying independent Q, K, V transformations to each direction (image-to-text, text-to-image) and each head, allowing the model to learn much more flexible and rich feature representations.</p>
<p><strong>5. v9:</strong> v8 + Pre-LN + FFN (Gate FFN + Dropout)</p>
<p>v9 is based on the structure of v8 and adds three important mechanisms to further improve training stability and performance: Pre-Layer Normalization, Gate Feed-Forward Network (FFN), and Dropout.</p>
<div id="cell-63" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb84"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb84-1"><a href="#cb84-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb84-2"><a href="#cb84-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb84-3"><a href="#cb84-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb84-4"><a href="#cb84-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-5"><a href="#cb84-5" aria-hidden="true" tabindex="-1"></a><span class="co"># v9 - Dropout before gated FFN, pass through norm at the end -&gt; trainable</span></span>
<span id="cb84-6"><a href="#cb84-6" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> CrossAttention(nn.Module):</span>
<span id="cb84-7"><a href="#cb84-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, dim, num_heads<span class="op">=</span><span class="dv">8</span>, dropout<span class="op">=</span><span class="fl">0.1</span>, ff_dim<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb84-8"><a href="#cb84-8" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb84-9"><a href="#cb84-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.num_heads <span class="op">=</span> num_heads</span>
<span id="cb84-10"><a href="#cb84-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.head_dim <span class="op">=</span> dim <span class="op">//</span> num_heads</span>
<span id="cb84-11"><a href="#cb84-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.scale <span class="op">=</span> <span class="va">self</span>.head_dim <span class="op">**</span> <span class="op">-</span><span class="fl">0.5</span></span>
<span id="cb84-12"><a href="#cb84-12" aria-hidden="true" tabindex="-1"></a>        ff_dim <span class="op">=</span> ff_dim <span class="kw">or</span> dim <span class="op">*</span> <span class="dv">4</span></span>
<span id="cb84-13"><a href="#cb84-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-14"><a href="#cb84-14" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Normalization layers for Pre-LN</span></span>
<span id="cb84-15"><a href="#cb84-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.attn_norm <span class="op">=</span> nn.LayerNorm(dim)</span>
<span id="cb84-16"><a href="#cb84-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ff_norm <span class="op">=</span> nn.LayerNorm(dim)</span>
<span id="cb84-17"><a href="#cb84-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-18"><a href="#cb84-18" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Projections for multi-head attention</span></span>
<span id="cb84-19"><a href="#cb84-19" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.to_q <span class="op">=</span> nn.Linear(dim, dim)</span>
<span id="cb84-20"><a href="#cb84-20" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.to_k <span class="op">=</span> nn.Linear(dim, dim)</span>
<span id="cb84-21"><a href="#cb84-21" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.to_v <span class="op">=</span> nn.Linear(dim, dim)</span>
<span id="cb84-22"><a href="#cb84-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-23"><a href="#cb84-23" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Output projection</span></span>
<span id="cb84-24"><a href="#cb84-24" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.to_out <span class="op">=</span> nn.Linear(dim, dim)</span>
<span id="cb84-25"><a href="#cb84-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-26"><a href="#cb84-26" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Dropout</span></span>
<span id="cb84-27"><a href="#cb84-27" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(dropout)</span>
<span id="cb84-28"><a href="#cb84-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-29"><a href="#cb84-29" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Gated feedforward network</span></span>
<span id="cb84-30"><a href="#cb84-30" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ff_gate <span class="op">=</span> nn.Sequential(</span>
<span id="cb84-31"><a href="#cb84-31" aria-hidden="true" tabindex="-1"></a>            nn.Linear(dim, ff_dim),</span>
<span id="cb84-32"><a href="#cb84-32" aria-hidden="true" tabindex="-1"></a>            nn.GELU(),</span>
<span id="cb84-33"><a href="#cb84-33" aria-hidden="true" tabindex="-1"></a>            nn.Dropout(dropout)</span>
<span id="cb84-34"><a href="#cb84-34" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb84-35"><a href="#cb84-35" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ff_value <span class="op">=</span> nn.Sequential(</span>
<span id="cb84-36"><a href="#cb84-36" aria-hidden="true" tabindex="-1"></a>            nn.Linear(dim, ff_dim),</span>
<span id="cb84-37"><a href="#cb84-37" aria-hidden="true" tabindex="-1"></a>            nn.GELU(),</span>
<span id="cb84-38"><a href="#cb84-38" aria-hidden="true" tabindex="-1"></a>            nn.Dropout(dropout)</span>
<span id="cb84-39"><a href="#cb84-39" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb84-40"><a href="#cb84-40" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ff_out <span class="op">=</span> nn.Linear(ff_dim, dim)</span>
<span id="cb84-41"><a href="#cb84-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-42"><a href="#cb84-42" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, image_features, text_features):</span>
<span id="cb84-43"><a href="#cb84-43" aria-hidden="true" tabindex="-1"></a>        B, N_i, _ <span class="op">=</span> image_features.shape</span>
<span id="cb84-44"><a href="#cb84-44" aria-hidden="true" tabindex="-1"></a>        _, N_t, _ <span class="op">=</span> text_features.shape</span>
<span id="cb84-45"><a href="#cb84-45" aria-hidden="true" tabindex="-1"></a>        H <span class="op">=</span> <span class="va">self</span>.num_heads</span>
<span id="cb84-46"><a href="#cb84-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-47"><a href="#cb84-47" aria-hidden="true" tabindex="-1"></a>        <span class="kw">def</span> split_heads(x):</span>
<span id="cb84-48"><a href="#cb84-48" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> x.reshape(B, <span class="op">-</span><span class="dv">1</span>, H, <span class="va">self</span>.head_dim).transpose(<span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb84-49"><a href="#cb84-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-50"><a href="#cb84-50" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Pre-LN: Normalize before attention</span></span>
<span id="cb84-51"><a href="#cb84-51" aria-hidden="true" tabindex="-1"></a>        image_norm <span class="op">=</span> <span class="va">self</span>.attn_norm(image_features)</span>
<span id="cb84-52"><a href="#cb84-52" aria-hidden="true" tabindex="-1"></a>        text_norm <span class="op">=</span> <span class="va">self</span>.attn_norm(text_features)</span>
<span id="cb84-53"><a href="#cb84-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-54"><a href="#cb84-54" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Image -&gt; Text direction attention</span></span>
<span id="cb84-55"><a href="#cb84-55" aria-hidden="true" tabindex="-1"></a>        q_img <span class="op">=</span> split_heads(<span class="va">self</span>.to_q(image_norm))</span>
<span id="cb84-56"><a href="#cb84-56" aria-hidden="true" tabindex="-1"></a>        k_txt <span class="op">=</span> split_heads(<span class="va">self</span>.to_k(text_norm))</span>
<span id="cb84-57"><a href="#cb84-57" aria-hidden="true" tabindex="-1"></a>        v_txt <span class="op">=</span> split_heads(<span class="va">self</span>.to_v(text_norm))</span>
<span id="cb84-58"><a href="#cb84-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-59"><a href="#cb84-59" aria-hidden="true" tabindex="-1"></a>        attn_i2t <span class="op">=</span> torch.matmul(q_img, k_txt.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>)) <span class="op">*</span> <span class="va">self</span>.scale</span>
<span id="cb84-60"><a href="#cb84-60" aria-hidden="true" tabindex="-1"></a>        attn_i2t <span class="op">=</span> attn_i2t.softmax(dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb84-61"><a href="#cb84-61" aria-hidden="true" tabindex="-1"></a>        attn_i2t <span class="op">=</span> <span class="va">self</span>.dropout(attn_i2t)  <span class="co"># Apply dropout to attention weights</span></span>
<span id="cb84-62"><a href="#cb84-62" aria-hidden="true" tabindex="-1"></a>        image_attended <span class="op">=</span> torch.matmul(attn_i2t, v_txt)</span>
<span id="cb84-63"><a href="#cb84-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-64"><a href="#cb84-64" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Text -&gt; Image direction attention</span></span>
<span id="cb84-65"><a href="#cb84-65" aria-hidden="true" tabindex="-1"></a>        q_txt <span class="op">=</span> split_heads(<span class="va">self</span>.to_q(text_norm))</span>
<span id="cb84-66"><a href="#cb84-66" aria-hidden="true" tabindex="-1"></a>        k_img <span class="op">=</span> split_heads(<span class="va">self</span>.to_k(image_norm))</span>
<span id="cb84-67"><a href="#cb84-67" aria-hidden="true" tabindex="-1"></a>        v_img <span class="op">=</span> split_heads(<span class="va">self</span>.to_v(image_norm))</span>
<span id="cb84-68"><a href="#cb84-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-69"><a href="#cb84-69" aria-hidden="true" tabindex="-1"></a>        attn_t2i <span class="op">=</span> torch.matmul(q_txt, k_img.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>)) <span class="op">*</span> <span class="va">self</span>.scale</span>
<span id="cb84-70"><a href="#cb84-70" aria-hidden="true" tabindex="-1"></a>        attn_t2i <span class="op">=</span> attn_t2i.softmax(dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb84-71"><a href="#cb84-71" aria-hidden="true" tabindex="-1"></a>        attn_t2i <span class="op">=</span> <span class="va">self</span>.dropout(attn_t2i)  <span class="co"># Apply dropout to attention weights</span></span>
<span id="cb84-72"><a href="#cb84-72" aria-hidden="true" tabindex="-1"></a>        text_attended <span class="op">=</span> torch.matmul(attn_t2i, v_img)</span>
<span id="cb84-73"><a href="#cb84-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-74"><a href="#cb84-74" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Combine heads and output projection</span></span>
<span id="cb84-75"><a href="#cb84-75" aria-hidden="true" tabindex="-1"></a>        image_attended <span class="op">=</span> image_attended.transpose(<span class="dv">1</span>, <span class="dv">2</span>).reshape(B, N_i, <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb84-76"><a href="#cb84-76" aria-hidden="true" tabindex="-1"></a>        text_attended <span class="op">=</span> text_attended.transpose(<span class="dv">1</span>, <span class="dv">2</span>).reshape(B, N_t, <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb84-77"><a href="#cb84-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-78"><a href="#cb84-78" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Output projection and dropout</span></span>
<span id="cb84-79"><a href="#cb84-79" aria-hidden="true" tabindex="-1"></a>        image_attended <span class="op">=</span> <span class="va">self</span>.dropout(<span class="va">self</span>.to_out(image_attended))</span>
<span id="cb84-80"><a href="#cb84-80" aria-hidden="true" tabindex="-1"></a>        text_attended <span class="op">=</span> <span class="va">self</span>.dropout(<span class="va">self</span>.to_out(text_attended))</span>
<span id="cb84-81"><a href="#cb84-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-82"><a href="#cb84-82" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Residual connection - connecting the original image features makes training impossible.</span></span>
<span id="cb84-83"><a href="#cb84-83" aria-hidden="true" tabindex="-1"></a>        <span class="co"># image_attended = image_attended + image_features</span></span>
<span id="cb84-84"><a href="#cb84-84" aria-hidden="true" tabindex="-1"></a>        <span class="co"># text_attended = text_attended + text_features</span></span>
<span id="cb84-85"><a href="#cb84-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-86"><a href="#cb84-86" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Pre-LN: Normalize before FFN</span></span>
<span id="cb84-87"><a href="#cb84-87" aria-hidden="true" tabindex="-1"></a>        image_ff <span class="op">=</span> <span class="va">self</span>.ff_norm(image_attended)</span>
<span id="cb84-88"><a href="#cb84-88" aria-hidden="true" tabindex="-1"></a>        text_ff <span class="op">=</span> <span class="va">self</span>.ff_norm(text_attended)</span>
<span id="cb84-89"><a href="#cb84-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-90"><a href="#cb84-90" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Gated feedforward processing</span></span>
<span id="cb84-91"><a href="#cb84-91" aria-hidden="true" tabindex="-1"></a>        <span class="kw">def</span> apply_ff(x):</span>
<span id="cb84-92"><a href="#cb84-92" aria-hidden="true" tabindex="-1"></a>            gate <span class="op">=</span> <span class="va">self</span>.ff_gate(x)</span>
<span id="cb84-93"><a href="#cb84-93" aria-hidden="true" tabindex="-1"></a>            value <span class="op">=</span> <span class="va">self</span>.ff_value(x)</span>
<span id="cb84-94"><a href="#cb84-94" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> <span class="va">self</span>.dropout(<span class="va">self</span>.ff_out(gate <span class="op">*</span> value))</span>
<span id="cb84-95"><a href="#cb84-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-96"><a href="#cb84-96" aria-hidden="true" tabindex="-1"></a>        <span class="co"># FFN output and residual connection - this type of residual connection is possible.</span></span>
<span id="cb84-97"><a href="#cb84-97" aria-hidden="true" tabindex="-1"></a>        image_out <span class="op">=</span> apply_ff(image_ff) <span class="op">+</span> image_attended</span>
<span id="cb84-98"><a href="#cb84-98" aria-hidden="true" tabindex="-1"></a>        text_out <span class="op">=</span> apply_ff(text_ff) <span class="op">+</span> text_attended</span>
<span id="cb84-99"><a href="#cb84-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-100"><a href="#cb84-100" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> image_out, text_out</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<ul>
<li><p><strong>Pre-Layer Normalization:</strong> In v8, Layer Normalization was applied <em>after</em> the attention operation (Post-LN), but in v9, it is applied <em>before</em> (Pre-LN). This includes <code>self.image_norm_q</code>, <code>self.image_norm_k</code>, …, <code>self.text_norm_v</code>. Pre-LN has higher training stability than Post-LN and does not require separate warmup, so it is widely used in recent Transformer-based models.</p></li>
<li><p><strong>Gated Feed-Forward Network (FFN):</strong> An FFN is added after the attention operation in v8 to enhance non-linearity and increase the model’s expressiveness.</p>
<ul>
<li><code>self.image_ffn</code> and <code>self.text_ffn</code> define the FFN, which consists of two linear layers and a GELU (Gaussian Error Linear Unit) activation function with dropout in between.</li>
<li><strong>GELU Activation Function:</strong> A smoother non-linear function than ReLU, which tends to perform better.</li>
<li><strong>Residual Connection &amp; LayerNorm after FFN:</strong> Residual connection is applied to the output of the FFN, and Layer Normalization (<code>self.image_ffn_norm</code>, <code>self.text_ffn_norm</code>) is applied. Unlike v3, residual connection is applied <em>after</em> passing through the FFN, allowing for information combination after non-linear processing, which improves information flow and contributes to performance improvement.</li>
</ul></li>
<li><p><strong>Dropout:</strong> <code>self.dropout</code> defines the dropout applied to attention weights and within the FFN. Dropout is an effective regularization technique that prevents model overfitting by randomly deactivating neurons during training.</p></li>
</ul>
<p><strong>Effects of Added Mechanisms</strong></p>
<ul>
<li>v9 maintains the independent multi-head attention structure of v8 while adding Pre-LN, gated FFN, and dropout to further improve training stability and performance.</li>
<li><strong>Pre-LN:</strong> Makes the initial training stage more stable and allows the model to be effectively trained without a separate learning rate warmup.</li>
<li><strong>Gated FFN:</strong> Adds non-linear transformation after attention operation to increase the model’s expressiveness. The GELU activation function and dropout further enhance the performance of the FFN.</li>
<li><strong>Dropout:</strong> Prevents model overfitting and improves generalization performance.</li>
</ul>
<p>v9 greatly improves the performance of Cross-Modal Attention through the combination of these techniques, serving as the basis for subsequent versions.</p>
</section>
<section id="key-results-analysis" class="level4">
<h4 class="anchored" data-anchor-id="key-results-analysis">10.4.4.4 Key Results Analysis</h4>
<ul>
<li><p><strong>v0, v1 (Basic Structure):</strong> v0 and v1, which used simple attention without normalization, were <em>successful</em> in training. However, v1 showed higher similarity to “cat”-related captions in both the training and validation datasets, indicating the importance of normalization.</p></li>
<li><p><strong>v2 (LayerNorm):</strong> Applying LayerNorm to the input, v2 was successful in training, demonstrating that stabilizing the scale of input features is crucial.</p></li>
<li><p><strong>v3 (Residual Connection):</strong> Adding residual connections to v2, v3 failed in training. This shows that residual connections are not always helpful in multimodal learning and may hinder learning interactions between modalities by overly preserving original features.</p></li>
<li><p><strong>v4 (Projection):</strong> Adding independent linear transformations (projections) to each modality, v4 was successful in training. This suggests that properly transforming the feature space of each modality is important.</p></li>
<li><p><strong>v7 (Shared Multi-Head):</strong> Extending the shared attention matrix to multi-head, v7 failed in training. This is interpreted as each head not properly reflecting the characteristics of different modalities.</p></li>
<li><p><strong>v8 (Independent Multi-Head):</strong> Using independent multi-head attention for each direction (image-to-text, text-to-image) and applying separate LayerNorm to inputs and outputs, v8 was successful in training. This demonstrates that preserving modality-specific features while exchanging information is important.</p></li>
<li><p><strong>v10_1 (Modality-Specific Q/K/V):</strong> Introducing modality-specific Q/K/V transformations based on v9, v10_1 had unstable training. This is due to increased model complexity and a higher risk of overfitting.</p></li>
<li><p><strong>v10_2 (Cross-Gate):</strong> Adding a cross-modal gating mechanism to v9, v10_2 failed in training. The gating mechanism likely failed to properly control the flow of information between modalities and instead hindered learning, possibly due to restricting information exchange too early.</p></li>
<li><p><strong>v10_3 (Context Layer):</strong> Adding separate context processing layers to each modality, v10_3 was successful in training. These layers are expected to contribute to performance improvement by further refining modality-specific features and providing additional contextual information.</p></li>
<li><p><strong>v10_4 (Multi-Query Attention):</strong> Applying multi-query attention where queries (Q) are independent but keys (K) and values (V) are shared, v10_4 was successful in training. This is interpreted as enabling efficient information exchange while reducing parameters, thus improving generalization performance.</p></li>
<li><p><strong>v10_5 (Hierarchical Multi-Head):</strong> Introducing a three-stage hierarchical structure with independent multi-head attention at each level and fusing them through weights, v10_5 was successful in training. This analysis suggests that gradually integrating features and effectively utilizing information at each level improves performance.</p></li>
<li><p><strong>v10_6 (Contrastive Learning Multi-Head):</strong> Adding a separate projection layer for contrastive learning and training by directly adding similarity information to the original features, v10_6 had unstable training. This may be because the similarity information distorted the original features, hindering learning.</p></li>
<li><p><strong>v11 (Multi-Query + Hierarchical Fusion):</strong> Combining the advantages of multi-query attention (v10_4) and hierarchical multi-head (v10_5), v11 was successful in training. This means it leveraged both parameter efficiency and gradual feature integration to achieve stable learning.</p></li>
</ul>
<p><strong>Conclusion</strong></p>
<p>Through this ablation study, the following conclusions can be drawn. 1. <strong>Importance of Normalization:</strong> Applying LayerNorm to the input features is crucial for training stability (v2). 2. <strong>Duality of Residual Connections:</strong> Residual connections are a useful mechanism, but they can be harmful in the early stages of multimodal learning (v3). Overly preserving the original features hinders learning interactions between the two modalities. 3. <strong>Independent Feature Transformation:</strong> Applying independent linear transformations (projections) to each modality can improve performance (v4). 4. <strong>Multi-Head Attention:</strong> When using multi-head attention, each head should be configured independently to reflect the characteristics of different modalities (v7, v8). 5. <strong>Proper Complexity:</strong> Excessively increasing model complexity can make training unstable (v10_1, v10_2, v10_6). 6. <strong>Efficient Mechanisms:</strong> Multi-query attention (v10_4) and hierarchical fusion (v10_5) provide the benefits of parameter efficiency and progressive feature integration, respectively. 7. <strong>Importance of Optimal Combination:</strong> As seen in v11, combining effective mechanisms properly can build a more stable and high-performance multimodal learning model.</p>
<p>These ablation experiments are very useful for understanding the role and importance of each component in multimodal learning. Furthermore, they provide important guidelines when designing new models. By systematically analyzing performance changes with or without specific mechanisms, we can identify which elements are effective for multimodal fusion and which combinations produce optimal results.</p>
<p>Designing more systematic experiment cases and project frameworks can also facilitate experiments on large-scale models and various mechanisms. We hope this research is helpful.</p>
</section>
</section>
</section>
<section id="vision-transformer-vit" class="level2">
<h2 class="anchored" data-anchor-id="vision-transformer-vit">10.5 Vision Transformer (ViT)</h2>
<p>In this section, we will briefly explore the Vision Transformer (ViT) and its extensions, ViT-22B and MAE, which have brought innovation to the field of image processing.</p>
<section id="paradigm-shift-from-cnn-to-vit" class="level3">
<h3 class="anchored" data-anchor-id="paradigm-shift-from-cnn-to-vit">10.5.1 Paradigm Shift from CNN to ViT</h3>
<p>In 2020, the Google Research team introduced ViT to the world through a paper titled “An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.” ViT marked the beginning of the end of the era of Convolutional Neural Networks (CNNs), which had long dominated the field of image processing, and signaled the arrival of a new era based on Transformers.</p>
<p>The core idea of ViT is simple. It divides an image into multiple small patches and treats each patch as if it were a word in a text sentence. This transforms the image into a sequence of patches, which is then processed by the Transformer.</p>
<p>ViT has several key differences compared to CNNs:</p>
<ol type="1">
<li><p><strong>Locality vs.&nbsp;Globality:</strong> CNNs focus on extracting local features from an image using convolutional filters. In contrast, ViT uses an attention mechanism that allows each patch to directly consider its relationship with all other patches in the entire image. This makes it better suited for capturing the context of the entire image.</p></li>
<li><p><strong>Hierarchical Structure vs.&nbsp;Flat Structure:</strong> CNNs have a hierarchical structure that progressively abstracts features through multiple layers of convolution and pooling operations. In contrast, ViT divides an image into patches, transforms all patches into vectors of the same dimension, and processes them at a single scale. This flat structure makes the model easier to implement and optimize.</p></li>
<li><p><strong>Data Dependence:</strong> CNNs tend to work well with relatively small amounts of data. However, ViT, being a Transformer-based model, requires a sufficient amount of data to perform well. When pre-trained on large datasets, ViT outperforms CNNs in various vision tasks such as image classification and object detection.</p></li>
</ol>
<p>The emergence of ViT has completely changed the direction of research in the field of image processing. Following ViT, numerous subsequent studies have been conducted based on ideas such as image patch embedding, attention mechanisms, and large-scale pre-training.</p>
</section>
<section id="principle-of-image-patch-embedding" class="level3">
<h3 class="anchored" data-anchor-id="principle-of-image-patch-embedding">10.5.2 Principle of Image Patch Embedding</h3>
<p>Image patch embedding is the first step in ViT, which transforms a 2D image into a 1D sequence. In PyTorch, the <code>torchvision.models.vision_transformer.PatchEmbed</code> class is responsible for this process.</p>
<div id="cell-67" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb85"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb85-1"><a href="#cb85-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb85-2"><a href="#cb85-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb85-3"><a href="#cb85-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb85-4"><a href="#cb85-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-5"><a href="#cb85-5" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> PatchEmbed(nn.Module):</span>
<span id="cb85-6"><a href="#cb85-6" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb85-7"><a href="#cb85-7" aria-hidden="true" tabindex="-1"></a><span class="co">    Transforms a 2D image into a sequence of patch embeddings.</span></span>
<span id="cb85-8"><a href="#cb85-8" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb85-9"><a href="#cb85-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-10"><a href="#cb85-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(</span>
<span id="cb85-11"><a href="#cb85-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>,</span>
<span id="cb85-12"><a href="#cb85-12" aria-hidden="true" tabindex="-1"></a>        img_size: <span class="bu">int</span> <span class="op">=</span> <span class="dv">224</span>,</span>
<span id="cb85-13"><a href="#cb85-13" aria-hidden="true" tabindex="-1"></a>        patch_size: <span class="bu">int</span> <span class="op">=</span> <span class="dv">16</span>,</span>
<span id="cb85-14"><a href="#cb85-14" aria-hidden="true" tabindex="-1"></a>        in_chans: <span class="bu">int</span> <span class="op">=</span> <span class="dv">3</span>,</span>
<span id="cb85-15"><a href="#cb85-15" aria-hidden="true" tabindex="-1"></a>        embed_dim: <span class="bu">int</span> <span class="op">=</span> <span class="dv">768</span>,</span>
<span id="cb85-16"><a href="#cb85-16" aria-hidden="true" tabindex="-1"></a>    ) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb85-17"><a href="#cb85-17" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb85-18"><a href="#cb85-18" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb85-19"><a href="#cb85-19" aria-hidden="true" tabindex="-1"></a><span class="co">            img_size: The size of the input image (assuming a square image)</span></span>
<span id="cb85-20"><a href="#cb85-20" aria-hidden="true" tabindex="-1"></a><span class="co">            patch_size: The patch size (assuming square patches)</span></span>
<span id="cb85-21"><a href="#cb85-21" aria-hidden="true" tabindex="-1"></a><span class="co">            in_chans: The number of input image channels (e.g., 3 for RGB images)</span></span>
<span id="cb85-22"><a href="#cb85-22" aria-hidden="true" tabindex="-1"></a><span class="co">            embed_dim: The dimension of the patch embedding vector</span></span>
<span id="cb85-23"><a href="#cb85-23" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb85-24"><a href="#cb85-24" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb85-25"><a href="#cb85-25" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.img_size <span class="op">=</span> img_size</span>
<span id="cb85-26"><a href="#cb85-26" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.patch_size <span class="op">=</span> patch_size</span>
<span id="cb85-27"><a href="#cb85-27" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.num_patches <span class="op">=</span> (img_size <span class="op">//</span> patch_size) <span class="op">*</span> (img_size <span class="op">//</span> patch_size)</span>
<span id="cb85-28"><a href="#cb85-28" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.projection <span class="op">=</span> nn.Conv2d(in_chans, embed_dim, kernel_size<span class="op">=</span>patch_size, stride<span class="op">=</span>patch_size)</span>
<span id="cb85-29"><a href="#cb85-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-30"><a href="#cb85-30" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x: torch.Tensor) <span class="op">-&gt;</span> torch.Tensor:</span>
<span id="cb85-31"><a href="#cb85-31" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb85-32"><a href="#cb85-32" aria-hidden="true" tabindex="-1"></a><span class="co">        Transforms the input image into a sequence of patch embeddings.</span></span>
<span id="cb85-33"><a href="#cb85-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-34"><a href="#cb85-34" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb85-35"><a href="#cb85-35" aria-hidden="true" tabindex="-1"></a><span class="co">            x: Input image (shape: [batch_size, in_chans, img_size, img_size])</span></span>
<span id="cb85-36"><a href="#cb85-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-37"><a href="#cb85-37" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb85-38"><a href="#cb85-38" aria-hidden="true" tabindex="-1"></a><span class="co">            Sequence of patch embeddings (shape: [batch_size, num_patches, embed_dim])</span></span>
<span id="cb85-39"><a href="#cb85-39" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb85-40"><a href="#cb85-40" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.projection(x)  <span class="co"># [batch_size, embed_dim, num_patches_h, num_patches_w]</span></span>
<span id="cb85-41"><a href="#cb85-41" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.flatten(<span class="dv">2</span>)       <span class="co"># [batch_size, embed_dim, num_patches]</span></span>
<span id="cb85-42"><a href="#cb85-42" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.transpose(<span class="dv">1</span>, <span class="dv">2</span>)  <span class="co"># [batch_size, num_patches, embed_dim]</span></span>
<span id="cb85-43"><a href="#cb85-43" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<section id="image-patch-division" class="level4">
<h4 class="anchored" data-anchor-id="image-patch-division">10.5.2.1 Image Patch Division</h4>
<p>The most important part of the <code>PatchEmbed</code> class’s <code>__init__</code> method is <code>self.projection = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)</code>. This line of code performs image patch division and embedding simultaneously.</p>
<ul>
<li><strong><code>nn.Conv2d</code></strong>: A PyTorch layer that performs 2D convolution operations.</li>
<li><strong><code>in_chans</code></strong>: The number of channels in the input image (3 for RGB images).</li>
<li><strong><code>embed_dim</code></strong>: The dimension of the output embedding vector (768 for ViT-Base models).</li>
<li><strong><code>kernel_size=patch_size</code></strong>: Sets the size of the convolution filter (kernel) to be the same as the patch size.</li>
<li><strong><code>stride=patch_size</code></strong>: Sets the stride (interval) at which the filter moves over the image to be the same as the patch size.</li>
</ul>
<p>By setting <code>kernel_size</code> and <code>stride</code> to be the same as <code>patch_size</code>, the convolution filter divides the image into patches of the exact size, without overlap, like a checkerboard. Each convolution filter compresses the information of one patch into a single embedding vector.</p>
</section>
<section id="linear-projection" class="level4">
<h4 class="anchored" data-anchor-id="linear-projection">10.5.2.2 Linear Projection</h4>
<p>In the <code>PatchEmbed</code> class’s <code>forward</code> method, actual image patch embedding is performed through <code>self.projection(x)</code>.</p>
<ol type="1">
<li><p><strong><code>self.projection(x)</code></strong>: Applies the <code>Conv2d</code> operation to the input image <code>x</code> (<code>[batch_size, in_chans, img_size, img_size]</code>). The output becomes <code>[batch_size, embed_dim, num_patches_h, num_patches_w]</code> (<code>num_patches_h</code> and <code>num_patches_w</code> are the number of patches in the height and width of the image, respectively).</p></li>
<li><p><strong><code>x.flatten(2)</code></strong>: Flattens the output of <code>Conv2d</code> into <code>[batch_size, embed_dim, num_patches]</code>. <code>num_patches</code> is the total number of patches (<code>num_patches_h * num_patches_w</code>).</p></li>
<li><p><strong><code>x.transpose(1, 2)</code></strong>: Changes the tensor dimensions to <code>[batch_size, num_patches, embed_dim]</code>, which is the format used as input for the transformer encoder. Each patch embedding vector is treated like an element in a sequence.</p></li>
</ol>
<p>As a result, the <code>PatchEmbed</code> class divides the image into patches and linearly transforms each patch into an <code>embed_dim</code>-dimensional vector, creating a sequence that can be used as input for the transformer encoder.</p>
</section>
</section>
<section id="positional-encoding-mechanism" class="level3">
<h3 class="anchored" data-anchor-id="positional-encoding-mechanism">10.5.3 Positional Encoding Mechanism</h3>
<p>ViT divides the image into patches and treats each patch like a word in text when inputting it into the transformer. However, the transformer does not inherently recognize the order of the input sequence. Therefore, it is necessary to inform the model which location in the image each patch corresponds to. This role is performed by <strong>positional encoding</strong>.</p>
<p>In PyTorch’s <code>VisionTransformer</code> class, <em>learnable</em> positional embeddings are used. That is, a unique embedding vector corresponding to the position of each patch is learned and optimized together during the training process.</p>
<div id="cell-69" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb86"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb86-1"><a href="#cb86-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> VisionTransformer(nn.Module):</span>
<span id="cb86-2"><a href="#cb86-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, ..., num_patches, embed_dim, ...):</span>
<span id="cb86-3"><a href="#cb86-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb86-4"><a href="#cb86-4" aria-hidden="true" tabindex="-1"></a>        <span class="co"># ...</span></span>
<span id="cb86-5"><a href="#cb86-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.cls_token <span class="op">=</span> nn.Parameter(torch.zeros(<span class="dv">1</span>, <span class="dv">1</span>, embed_dim))  <span class="co"># Class token</span></span>
<span id="cb86-6"><a href="#cb86-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.pos_embed <span class="op">=</span> nn.Parameter(torch.zeros(<span class="dv">1</span>, num_patches <span class="op">+</span> <span class="dv">1</span>, embed_dim)) <span class="co"># Positional embedding</span></span>
<span id="cb86-7"><a href="#cb86-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.pos_drop <span class="op">=</span> nn.Dropout(p<span class="op">=</span>drop_rate)</span>
<span id="cb86-8"><a href="#cb86-8" aria-hidden="true" tabindex="-1"></a>        <span class="co"># ...</span></span>
<span id="cb86-9"><a href="#cb86-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb86-10"><a href="#cb86-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _pos_embed(<span class="va">self</span>, x):</span>
<span id="cb86-11"><a href="#cb86-11" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torch.cat((<span class="va">self</span>.cls_token.expand(x.shape[<span class="dv">0</span>], <span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>), x), dim<span class="op">=</span><span class="dv">1</span>) <span class="co"># Prepend class token</span></span>
<span id="cb86-12"><a href="#cb86-12" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.pos_embed  <span class="co"># Add positional embedding</span></span>
<span id="cb86-13"><a href="#cb86-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.pos_drop(x)</span>
<span id="cb86-14"><a href="#cb86-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb86-15"><a href="#cb86-15" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb86-16"><a href="#cb86-16" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.patch_embed(x)  <span class="co"># Patch embedding</span></span>
<span id="cb86-17"><a href="#cb86-17" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>._pos_embed(x)  <span class="co"># Add positional embedding</span></span>
<span id="cb86-18"><a href="#cb86-18" aria-hidden="true" tabindex="-1"></a>        <span class="co"># ... (Transformer Encoder etc.) ...</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><strong>Code Explanation</strong></p>
<ol type="1">
<li><strong><code>self.pos_embed</code> (learnable parameter):</strong> Defined as <code>nn.Parameter</code>, it gets updated during the learning process. It has a size of <code>(1, num_patches + 1, embed_dim)</code>.
<ul>
<li><code>num_patches + 1</code>: Adding 1 to the number of patches secures space for the <strong>class token</strong>, which has a special role other than image patches.</li>
<li><code>embed_dim</code>: It has the same dimension as the patch embedding.</li>
<li>That is, each patch (and class token) is assigned a unique positional embedding vector of size <code>embed_dim</code>.</li>
</ul></li>
<li><strong><code>_pos_embed</code> method:</strong>
<ul>
<li><strong>Add class token:</strong> Adds <code>self.cls_token</code> to the front of the input <code>x</code> (patch embedding sequence). <code>cls_token</code> is replicated (<code>expand</code>) batch size times and applied identically to all images.</li>
<li><strong>Add positional embedding:</strong> Adds the corresponding <code>self.pos_embed</code> value to the patch embedding (and class token embedding). Following PyTorch’s broadcasting rule, each positional embedding vector of <code>self.pos_embed</code> is automatically added to the patch embedding vector at the corresponding position.</li>
<li><strong>Dropout:</strong> Applies dropout to prevent overfitting.</li>
</ul></li>
<li><strong><code>forward</code> method:</strong> The <code>forward</code> method converts an image into a patch embedding through <code>self.patch_embed(x)</code> and then calls <code>self._pos_embed(x)</code> to add positional embedding.</li>
</ol>
<p><strong>Summary</strong></p>
<p>ViT uses <em>learnable</em> positional embeddings for each patch (and class token) and adds them to the patch embeddings to inject location information into the model. Since the positional embeddings are optimized along with other weights during model training, they can represent location information in a form that best fits the data.</p>
</section>
<section id="structure-and-key-components-of-vit" class="level3">
<h3 class="anchored" data-anchor-id="structure-and-key-components-of-vit">10.5.4 Structure and Key Components of ViT</h3>
<p>ViT (Vision Transformer) is a model that processes images like text to perform vision tasks such as classification. In PyTorch, you can use the ViT model through the <code>torchvision.models.VisionTransformer</code> class.</p>
<div id="cell-71" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb87"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb87-1"><a href="#cb87-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> VisionTransformer(nn.Module):</span>
<span id="cb87-2"><a href="#cb87-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, ..., embed_dim, depth, num_heads, ...):</span>
<span id="cb87-3"><a href="#cb87-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb87-4"><a href="#cb87-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-5"><a href="#cb87-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.patch_embed <span class="op">=</span> PatchEmbed(...)  <span class="co"># Image patch embedding</span></span>
<span id="cb87-6"><a href="#cb87-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.cls_token <span class="op">=</span> nn.Parameter(...)   <span class="co"># Class token</span></span>
<span id="cb87-7"><a href="#cb87-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.pos_embed <span class="op">=</span> nn.Parameter(...)   <span class="co"># Positional embedding</span></span>
<span id="cb87-8"><a href="#cb87-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.pos_drop <span class="op">=</span> nn.Dropout(...)</span>
<span id="cb87-9"><a href="#cb87-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-10"><a href="#cb87-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.blocks <span class="op">=</span> nn.Sequential(<span class="op">*</span>[</span>
<span id="cb87-11"><a href="#cb87-11" aria-hidden="true" tabindex="-1"></a>            TransformerEncoderLayer(...) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(depth) <span class="co"># Transformer Encoder blocks</span></span>
<span id="cb87-12"><a href="#cb87-12" aria-hidden="true" tabindex="-1"></a>        ])</span>
<span id="cb87-13"><a href="#cb87-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.norm <span class="op">=</span> nn.LayerNorm(embed_dim) <span class="co"># Layer Normalization</span></span>
<span id="cb87-14"><a href="#cb87-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.head <span class="op">=</span> nn.Linear(embed_dim, num_classes) <span class="co"># Classification Head</span></span>
<span id="cb87-15"><a href="#cb87-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-16"><a href="#cb87-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-17"><a href="#cb87-17" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward_features(<span class="va">self</span>, x):</span>
<span id="cb87-18"><a href="#cb87-18" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.patch_embed(x)       <span class="co"># 1. Patch embedding</span></span>
<span id="cb87-19"><a href="#cb87-19" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torch.cat((<span class="va">self</span>.cls_token.expand(x.shape[<span class="dv">0</span>], <span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>), x), dim<span class="op">=</span><span class="dv">1</span>) <span class="co"># 2. Prepend class token</span></span>
<span id="cb87-20"><a href="#cb87-20" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.pos_embed       <span class="co"># 3. Add positional embedding</span></span>
<span id="cb87-21"><a href="#cb87-21" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.pos_drop(x)</span>
<span id="cb87-22"><a href="#cb87-22" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.blocks(x)            <span class="co"># 4. Transformer Encoder</span></span>
<span id="cb87-23"><a href="#cb87-23" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.norm(x)              <span class="co"># 5. LayerNorm</span></span>
<span id="cb87-24"><a href="#cb87-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x[:, <span class="dv">0</span>]                <span class="co"># 6. Return class token</span></span>
<span id="cb87-25"><a href="#cb87-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-26"><a href="#cb87-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-27"><a href="#cb87-27" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb87-28"><a href="#cb87-28" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.forward_features(x)   <span class="co"># Feature extraction</span></span>
<span id="cb87-29"><a href="#cb87-29" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.head(x)               <span class="co"># Classification</span></span>
<span id="cb87-30"><a href="#cb87-30" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><strong>Core Components of ViT:</strong></p>
<ol type="1">
<li><strong><code>PatchEmbed</code> (Patch Embedding):</strong> Divides an image into multiple small patches and converts each patch into a fixed-dimensional vector (embedding). (Refer to section 10.5.2)</li>
<li><strong><code>cls_token</code> (Class Token):</strong> A learnable parameter that is added to the beginning of the patch embedding sequence. After passing through the transformer encoder, this class token contains information representing the entire image and is used for final classification.</li>
<li><strong><code>pos_embed</code> (Positional Embedding):</strong> Learnable parameters that represent the position information of each patch (and class token). Since transformers cannot inherently understand the order of the input sequence, positional embedding explicitly provides location information. (Refer to section 10.5.3)</li>
<li><strong><code>blocks</code> (Transformer Encoder):</strong> Composed of multiple <code>TransformerEncoderLayer</code> stacked together.
<ul>
<li><strong><code>TransformerEncoderLayer</code>:</strong> The core block of ViT, consisting of Multi-Head Self-Attention and Feed-Forward Network (FFN).
<ul>
<li><strong>Multi-Head Self-Attention:</strong> Considers the relationship between each patch and all other patches (including itself) to understand the contextual information of the entire image.</li>
<li><strong>FFN:</strong> Individually processes each patch embedding, adding non-linearity and allowing for more complex feature learning.</li>
<li><strong>(Pre-LN, Residual Connection, Dropout, etc.):</strong> Various techniques applied for stable training and performance improvement, as discussed in chapters 9 and 10.</li>
</ul></li>
</ul></li>
<li><strong><code>norm</code> (Layer Normalization) :</strong> Applies Layer Normalization to the output of the Transformer Encoder</li>
<li><strong><code>head</code> (Classification Head):</strong> A fully-connected layer that takes the class token passed through the transformer encoder as input and predicts the image’s class.</li>
</ol>
<p><strong><code>forward</code> Method (Overall Processing Flow):</strong></p>
<ol type="1">
<li><code>forward_features</code> method:
<ul>
<li><code>self.patch_embed(x)</code>: Converts the input image into a patch embedding sequence.</li>
<li>Adds the class token (<code>self.cls_token</code>) to the beginning of the patch embedding sequence.</li>
<li>Adds positional embedding (<code>self.pos_embed</code>).</li>
<li>Passes through the transformer encoder (<code>self.blocks</code>).</li>
<li>Applies Layer Normalization (<code>self.norm</code>).</li>
<li>Returns only the part corresponding to the class token (<code>x[:, 0]</code>).</li>
</ul></li>
<li><code>self.head(x)</code>: Passes the class token returned by <code>forward_features</code> through the classification head to obtain the final prediction (classification) result.</li>
</ol>
<p><strong>Summary:</strong></p>
<p>ViT divides an image into patches and inputs each patch into a transformer encoder to extract features from the entire image. It uses class tokens and positional embeddings to consider both global information of the image and location information of the patches. Finally, it classifies images using the class token.</p>
</section>
<section id="vit-training-example" class="level3">
<h3 class="anchored" data-anchor-id="vit-training-example">10.5.5 ViT Training Example</h3>
<p>Let’s look at a simple example of training ViT using the CIFAR-10 dataset. The code below uses PyTorch to train a ViT model and prints epoch-wise loss and accuracy.</p>
<div class="sourceCode" id="cb88"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb88-1"><a href="#cb88-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb88-2"><a href="#cb88-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb88-3"><a href="#cb88-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torchvision</span>
<span id="cb88-4"><a href="#cb88-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torchvision.transforms <span class="im">as</span> transforms</span>
<span id="cb88-5"><a href="#cb88-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-6"><a href="#cb88-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Set device (GPU if available, else CPU)</span></span>
<span id="cb88-7"><a href="#cb88-7" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> torch.device(<span class="st">"cuda:0"</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">"cpu"</span>)</span>
<span id="cb88-8"><a href="#cb88-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-9"><a href="#cb88-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Hyperparameters</span></span>
<span id="cb88-10"><a href="#cb88-10" aria-hidden="true" tabindex="-1"></a>num_epochs <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb88-11"><a href="#cb88-11" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb88-12"><a href="#cb88-12" aria-hidden="true" tabindex="-1"></a>learning_rate <span class="op">=</span> <span class="fl">0.001</span></span>
<span id="cb88-13"><a href="#cb88-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-14"><a href="#cb88-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Data augmentation and normalization for training</span></span>
<span id="cb88-15"><a href="#cb88-15" aria-hidden="true" tabindex="-1"></a>transform <span class="op">=</span> transforms.Compose([transforms.ToTensor(), transforms.Normalize((<span class="fl">0.5</span>, <span class="fl">0.5</span>, <span class="fl">0.5</span>), (<span class="fl">0.5</span>, <span class="fl">0.5</span>, <span class="fl">0.5</span>))])</span>
<span id="cb88-16"><a href="#cb88-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-17"><a href="#cb88-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Load CIFAR-10 dataset</span></span>
<span id="cb88-18"><a href="#cb88-18" aria-hidden="true" tabindex="-1"></a>train_dataset <span class="op">=</span> torchvision.datasets.CIFAR10(root<span class="op">=</span><span class="st">'./data'</span>, train<span class="op">=</span><span class="va">True</span>, download<span class="op">=</span><span class="va">True</span>, transform<span class="op">=</span>transform)</span>
<span id="cb88-19"><a href="#cb88-19" aria-hidden="true" tabindex="-1"></a>train_loader <span class="op">=</span> torch.utils.data.DataLoader(dataset<span class="op">=</span>train_dataset, batch_size<span class="op">=</span>batch_size, shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb88-20"><a href="#cb88-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-21"><a href="#cb88-21" aria-hidden="true" tabindex="-1"></a>test_dataset <span class="op">=</span> torchvision.datasets.CIFAR10(root<span class="op">=</span><span class="st">'./data'</span>, train<span class="op">=</span><span class="va">False</span>, download<span class="op">=</span><span class="va">True</span>, transform<span class="op">=</span>transform)</span>
<span id="cb88-22"><a href="#cb88-22" aria-hidden="true" tabindex="-1"></a>test_loader <span class="op">=</span> torch.utils.data.DataLoader(dataset<span class="op">=</span>test_dataset, batch_size<span class="op">=</span>batch_size, shuffle<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb88-23"><a href="#cb88-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-24"><a href="#cb88-24" aria-hidden="true" tabindex="-1"></a><span class="co"># ViT model</span></span>
<span id="cb88-25"><a href="#cb88-25" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ViT(nn.Module):</span>
<span id="cb88-26"><a href="#cb88-26" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb88-27"><a href="#cb88-27" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(ViT, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb88-28"><a href="#cb88-28" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.patch_embed <span class="op">=</span> PatchEmbed()</span>
<span id="cb88-29"><a href="#cb88-29" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.cls_token <span class="op">=</span> nn.Parameter(torch.randn(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">512</span>))</span>
<span id="cb88-30"><a href="#cb88-30" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.pos_embed <span class="op">=</span> nn.Parameter(torch.randn(<span class="dv">1</span>, <span class="dv">196</span>, <span class="dv">512</span>))  <span class="co"># Assuming 14x14 patches</span></span>
<span id="cb88-31"><a href="#cb88-31" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.blocks <span class="op">=</span> nn.ModuleList([TransformerEncoderLayer() <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">6</span>)])</span>
<span id="cb88-32"><a href="#cb88-32" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.norm <span class="op">=</span> nn.LayerNorm(<span class="dv">512</span>)</span>
<span id="cb88-33"><a href="#cb88-33" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.head <span class="op">=</span> nn.Linear(<span class="dv">512</span>, <span class="dv">10</span>)</span>
<span id="cb88-34"><a href="#cb88-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-35"><a href="#cb88-35" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb88-36"><a href="#cb88-36" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Patch embedding</span></span>
<span id="cb88-37"><a href="#cb88-37" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.patch_embed(x)</span>
<span id="cb88-38"><a href="#cb88-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-39"><a href="#cb88-39" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Add class token and positional embedding</span></span>
<span id="cb88-40"><a href="#cb88-40" aria-hidden="true" tabindex="-1"></a>        cls_token <span class="op">=</span> <span class="va">self</span>.cls_token.repeat(x.size(<span class="dv">0</span>), <span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb88-41"><a href="#cb88-41" aria-hidden="true" tabindex="-1"></a>        pos_embed <span class="op">=</span> <span class="va">self</span>.pos_embed.repeat(x.size(<span class="dv">0</span>), <span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb88-42"><a href="#cb88-42" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torch.cat((cls_token, x), dim<span class="op">=</span><span class="dv">1</span>) <span class="op">+</span> pos_embed</span>
<span id="cb88-43"><a href="#cb88-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-44"><a href="#cb88-44" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Transformer encoder</span></span>
<span id="cb88-45"><a href="#cb88-45" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> block <span class="kw">in</span> <span class="va">self</span>.blocks:</span>
<span id="cb88-46"><a href="#cb88-46" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> block(x)</span>
<span id="cb88-47"><a href="#cb88-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-48"><a href="#cb88-48" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Layer normalization and classification head</span></span>
<span id="cb88-49"><a href="#cb88-49" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.norm(x[:, <span class="dv">0</span>, :])  <span class="co"># Take only the class token</span></span>
<span id="cb88-50"><a href="#cb88-50" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.head(x)</span>
<span id="cb88-51"><a href="#cb88-51" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb88-52"><a href="#cb88-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-53"><a href="#cb88-53" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize ViT model, optimizer, and loss function</span></span>
<span id="cb88-54"><a href="#cb88-54" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> ViT().to(device)</span>
<span id="cb88-55"><a href="#cb88-55" aria-hidden="true" tabindex="-1"></a>criterion <span class="op">=</span> nn.CrossEntropyLoss()</span>
<span id="cb88-56"><a href="#cb88-56" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.Adam(model.parameters(), lr<span class="op">=</span>learning_rate)</span>
<span id="cb88-57"><a href="#cb88-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-58"><a href="#cb88-58" aria-hidden="true" tabindex="-1"></a><span class="co"># Train ViT model</span></span>
<span id="cb88-59"><a href="#cb88-59" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(num_epochs):</span>
<span id="cb88-60"><a href="#cb88-60" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, (images, labels) <span class="kw">in</span> <span class="bu">enumerate</span>(train_loader):</span>
<span id="cb88-61"><a href="#cb88-61" aria-hidden="true" tabindex="-1"></a>        images, labels <span class="op">=</span> images.to(device), labels.to(device)</span>
<span id="cb88-62"><a href="#cb88-62" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()</span>
<span id="cb88-63"><a href="#cb88-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-64"><a href="#cb88-64" aria-hidden="true" tabindex="-1"></a>        outputs <span class="op">=</span> model(images)</span>
<span id="cb88-65"><a href="#cb88-65" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> criterion(outputs, labels)</span>
<span id="cb88-66"><a href="#cb88-66" aria-hidden="true" tabindex="-1"></a>        loss.backward()</span>
<span id="cb88-67"><a href="#cb88-67" aria-hidden="true" tabindex="-1"></a>        optimizer.step()</span>
<span id="cb88-68"><a href="#cb88-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-69"><a href="#cb88-69" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'Epoch [</span><span class="sc">{</span>epoch<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">/</span><span class="sc">{</span>num_epochs<span class="sc">}</span><span class="ss">], Loss: </span><span class="sc">{</span>loss<span class="sc">.</span>item()<span class="sc">:.4f}</span><span class="ss">'</span>)</span>
<span id="cb88-70"><a href="#cb88-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-71"><a href="#cb88-71" aria-hidden="true" tabindex="-1"></a><span class="co"># Test ViT model</span></span>
<span id="cb88-72"><a href="#cb88-72" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">eval</span>()</span>
<span id="cb88-73"><a href="#cb88-73" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb88-74"><a href="#cb88-74" aria-hidden="true" tabindex="-1"></a>    correct <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb88-75"><a href="#cb88-75" aria-hidden="true" tabindex="-1"></a>    total <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb88-76"><a href="#cb88-76" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> images, labels <span class="kw">in</span> test_loader:</span>
<span id="cb88-77"><a href="#cb88-77" aria-hidden="true" tabindex="-1"></a>        images, labels <span class="op">=</span> images.to(device), labels.to(device)</span>
<span id="cb88-78"><a href="#cb88-78" aria-hidden="true" tabindex="-1"></a>        outputs <span class="op">=</span> model(images)</span>
<span id="cb88-79"><a href="#cb88-79" aria-hidden="true" tabindex="-1"></a>        _, predicted <span class="op">=</span> torch.<span class="bu">max</span>(outputs.data, <span class="dv">1</span>)</span>
<span id="cb88-80"><a href="#cb88-80" aria-hidden="true" tabindex="-1"></a>        total <span class="op">+=</span> labels.size(<span class="dv">0</span>)</span>
<span id="cb88-81"><a href="#cb88-81" aria-hidden="true" tabindex="-1"></a>        correct <span class="op">+=</span> (predicted <span class="op">==</span> labels).<span class="bu">sum</span>().item()</span>
<span id="cb88-82"><a href="#cb88-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-83"><a href="#cb88-83" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'Test Accuracy: </span><span class="sc">{</span><span class="dv">100</span> <span class="op">*</span> correct <span class="op">/</span> total<span class="sc">}</span><span class="ss">%'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div id="cell-73" class="cell" data-execution_count="1">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb89"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb89-1"><a href="#cb89-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb89-2"><a href="#cb89-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb89-3"><a href="#cb89-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.optim <span class="im">as</span> optim</span>
<span id="cb89-4"><a href="#cb89-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision <span class="im">import</span> datasets, transforms</span>
<span id="cb89-5"><a href="#cb89-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision.models <span class="im">import</span> vit_b_16  <span class="co"># Using vit_b_16 model as an example</span></span>
<span id="cb89-6"><a href="#cb89-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> DataLoader</span>
<span id="cb89-7"><a href="#cb89-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb89-8"><a href="#cb89-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Hyperparameter setup for a simple training run</span></span>
<span id="cb89-9"><a href="#cb89-9" aria-hidden="true" tabindex="-1"></a>num_epochs <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb89-10"><a href="#cb89-10" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb89-11"><a href="#cb89-11" aria-hidden="true" tabindex="-1"></a>learning_rate <span class="op">=</span> <span class="fl">1e-4</span></span>
<span id="cb89-12"><a href="#cb89-12" aria-hidden="true" tabindex="-1"></a>image_size <span class="op">=</span> <span class="dv">224</span>  <span class="co"># ViT input image size</span></span>
<span id="cb89-13"><a href="#cb89-13" aria-hidden="true" tabindex="-1"></a>num_classes <span class="op">=</span> <span class="dv">10</span>   <span class="co"># Number of classes in the CIFAR-10 dataset</span></span>
<span id="cb89-14"><a href="#cb89-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb89-15"><a href="#cb89-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Use GPU if available</span></span>
<span id="cb89-16"><a href="#cb89-16" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> torch.device(<span class="st">"cuda"</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">"cpu"</span>)</span>
<span id="cb89-17"><a href="#cb89-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb89-18"><a href="#cb89-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Data loading and preprocessing (using CIFAR-10 dataset)</span></span>
<span id="cb89-19"><a href="#cb89-19" aria-hidden="true" tabindex="-1"></a>transform <span class="op">=</span> transforms.Compose([</span>
<span id="cb89-20"><a href="#cb89-20" aria-hidden="true" tabindex="-1"></a>    transforms.Resize((image_size, image_size)),</span>
<span id="cb89-21"><a href="#cb89-21" aria-hidden="true" tabindex="-1"></a>    transforms.ToTensor(),</span>
<span id="cb89-22"><a href="#cb89-22" aria-hidden="true" tabindex="-1"></a>    transforms.Normalize((<span class="fl">0.5</span>, <span class="fl">0.5</span>, <span class="fl">0.5</span>), (<span class="fl">0.5</span>, <span class="fl">0.5</span>, <span class="fl">0.5</span>)), <span class="co"># Normalize with CIFAR-10 statistics</span></span>
<span id="cb89-23"><a href="#cb89-23" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb89-24"><a href="#cb89-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb89-25"><a href="#cb89-25" aria-hidden="true" tabindex="-1"></a>train_dataset <span class="op">=</span> datasets.CIFAR10(root<span class="op">=</span><span class="st">'./data'</span>, train<span class="op">=</span><span class="va">True</span>, download<span class="op">=</span><span class="va">True</span>, transform<span class="op">=</span>transform)</span>
<span id="cb89-26"><a href="#cb89-26" aria-hidden="true" tabindex="-1"></a>train_loader <span class="op">=</span> DataLoader(train_dataset, batch_size<span class="op">=</span>batch_size, shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb89-27"><a href="#cb89-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb89-28"><a href="#cb89-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Create ViT model (not using pretrained weights)</span></span>
<span id="cb89-29"><a href="#cb89-29" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> vit_b_16(pretrained<span class="op">=</span><span class="va">False</span>, num_classes<span class="op">=</span>num_classes).to(device)</span>
<span id="cb89-30"><a href="#cb89-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb89-31"><a href="#cb89-31" aria-hidden="true" tabindex="-1"></a><span class="co"># Define loss function and optimizer</span></span>
<span id="cb89-32"><a href="#cb89-32" aria-hidden="true" tabindex="-1"></a>criterion <span class="op">=</span> nn.CrossEntropyLoss()</span>
<span id="cb89-33"><a href="#cb89-33" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> optim.AdamW(model.parameters(), lr<span class="op">=</span>learning_rate)</span>
<span id="cb89-34"><a href="#cb89-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb89-35"><a href="#cb89-35" aria-hidden="true" tabindex="-1"></a><span class="co"># Training loop</span></span>
<span id="cb89-36"><a href="#cb89-36" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(num_epochs):</span>
<span id="cb89-37"><a href="#cb89-37" aria-hidden="true" tabindex="-1"></a>    model.train()  <span class="co"># Set the model to training mode</span></span>
<span id="cb89-38"><a href="#cb89-38" aria-hidden="true" tabindex="-1"></a>    running_loss <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb89-39"><a href="#cb89-39" aria-hidden="true" tabindex="-1"></a>    correct_predictions <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb89-40"><a href="#cb89-40" aria-hidden="true" tabindex="-1"></a>    total_samples <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb89-41"><a href="#cb89-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb89-42"><a href="#cb89-42" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, (images, labels) <span class="kw">in</span> <span class="bu">enumerate</span>(train_loader):</span>
<span id="cb89-43"><a href="#cb89-43" aria-hidden="true" tabindex="-1"></a>        images <span class="op">=</span> images.to(device)</span>
<span id="cb89-44"><a href="#cb89-44" aria-hidden="true" tabindex="-1"></a>        labels <span class="op">=</span> labels.to(device)</span>
<span id="cb89-45"><a href="#cb89-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb89-46"><a href="#cb89-46" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Forward and backward passes</span></span>
<span id="cb89-47"><a href="#cb89-47" aria-hidden="true" tabindex="-1"></a>        outputs <span class="op">=</span> model(images)</span>
<span id="cb89-48"><a href="#cb89-48" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> criterion(outputs, labels)</span>
<span id="cb89-49"><a href="#cb89-49" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()</span>
<span id="cb89-50"><a href="#cb89-50" aria-hidden="true" tabindex="-1"></a>        loss.backward()</span>
<span id="cb89-51"><a href="#cb89-51" aria-hidden="true" tabindex="-1"></a>        optimizer.step()</span>
<span id="cb89-52"><a href="#cb89-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb89-53"><a href="#cb89-53" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculate statistics</span></span>
<span id="cb89-54"><a href="#cb89-54" aria-hidden="true" tabindex="-1"></a>        running_loss <span class="op">+=</span> loss.item()</span>
<span id="cb89-55"><a href="#cb89-55" aria-hidden="true" tabindex="-1"></a>        _, predicted <span class="op">=</span> torch.<span class="bu">max</span>(outputs, <span class="dv">1</span>)  <span class="co"># Select the class with the highest probability</span></span>
<span id="cb89-56"><a href="#cb89-56" aria-hidden="true" tabindex="-1"></a>        total_samples <span class="op">+=</span> labels.size(<span class="dv">0</span>)</span>
<span id="cb89-57"><a href="#cb89-57" aria-hidden="true" tabindex="-1"></a>        correct_predictions <span class="op">+=</span> (predicted <span class="op">==</span> labels).<span class="bu">sum</span>().item()</span>
<span id="cb89-58"><a href="#cb89-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb89-59"><a href="#cb89-59" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Print every 100 batches.</span></span>
<span id="cb89-60"><a href="#cb89-60" aria-hidden="true" tabindex="-1"></a>        <span class="co"># if (i + 1) % 100 == 0:</span></span>
<span id="cb89-61"><a href="#cb89-61" aria-hidden="true" tabindex="-1"></a>        <span class="co">#     print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')</span></span>
<span id="cb89-62"><a href="#cb89-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb89-63"><a href="#cb89-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb89-64"><a href="#cb89-64" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Print epoch statistics</span></span>
<span id="cb89-65"><a href="#cb89-65" aria-hidden="true" tabindex="-1"></a>    epoch_loss <span class="op">=</span> running_loss <span class="op">/</span> <span class="bu">len</span>(train_loader)</span>
<span id="cb89-66"><a href="#cb89-66" aria-hidden="true" tabindex="-1"></a>    epoch_accuracy <span class="op">=</span> correct_predictions <span class="op">/</span> total_samples <span class="op">*</span> <span class="dv">100</span></span>
<span id="cb89-67"><a href="#cb89-67" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'Epoch [</span><span class="sc">{</span>epoch<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">/</span><span class="sc">{</span>num_epochs<span class="sc">}</span><span class="ss">], Loss: </span><span class="sc">{</span>epoch_loss<span class="sc">:.4f}</span><span class="ss">, Accuracy: </span><span class="sc">{</span>epoch_accuracy<span class="sc">:.2f}</span><span class="ss">%'</span>)</span>
<span id="cb89-68"><a href="#cb89-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb89-69"><a href="#cb89-69" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Training finished!'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stderr">
<pre><code>100%|██████████| 170M/170M [00:21&lt;00:00, 8.09MB/s] 
/home/sean/anaconda3/envs/DL/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/sean/anaconda3/envs/DL/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch [1/5], Step [100/1563], Loss: 2.1349
Epoch [1/5], Step [200/1563], Loss: 1.8978
Epoch [1/5], Step [300/1563], Loss: 1.9483
Epoch [1/5], Step [400/1563], Loss: 2.0783
Epoch [1/5], Step [500/1563], Loss: 1.7614
Epoch [1/5], Step [600/1563], Loss: 1.8051
Epoch [1/5], Step [700/1563], Loss: 1.7448
Epoch [1/5], Step [800/1563], Loss: 1.8347
Epoch [1/5], Step [900/1563], Loss: 1.8127
Epoch [1/5], Step [1000/1563], Loss: 1.7755
Epoch [1/5], Step [1100/1563], Loss: 1.6506
Epoch [1/5], Step [1200/1563], Loss: 1.7523
Epoch [1/5], Step [1300/1563], Loss: 1.5987
Epoch [1/5], Step [1400/1563], Loss: 1.6078
Epoch [1/5], Step [1500/1563], Loss: 1.7110
Epoch [1/5], Loss: 1.8429, Accuracy: 29.66%
Epoch [2/5], Step [100/1563], Loss: 1.4902
Epoch [2/5], Step [200/1563], Loss: 1.5161
Epoch [2/5], Step [300/1563], Loss: 1.4563
Epoch [2/5], Step [400/1563], Loss: 1.5858
Epoch [2/5], Step [500/1563], Loss: 1.6702
Epoch [2/5], Step [600/1563], Loss: 1.5833
Epoch [2/5], Step [700/1563], Loss: 1.4790
Epoch [2/5], Step [800/1563], Loss: 1.6507
Epoch [2/5], Step [900/1563], Loss: 1.6017
Epoch [2/5], Step [1000/1563], Loss: 1.5102
Epoch [2/5], Step [1100/1563], Loss: 1.2946
Epoch [2/5], Step [1200/1563], Loss: 1.3225
Epoch [2/5], Step [1300/1563], Loss: 1.9922
Epoch [2/5], Step [1400/1563], Loss: 1.3685
Epoch [2/5], Step [1500/1563], Loss: 1.4852
Epoch [2/5], Loss: 1.5410, Accuracy: 42.69%
Epoch [3/5], Step [100/1563], Loss: 1.2692
Epoch [3/5], Step [200/1563], Loss: 1.1648
Epoch [3/5], Step [300/1563], Loss: 1.2412
Epoch [3/5], Step [400/1563], Loss: 1.6217
Epoch [3/5], Step [500/1563], Loss: 1.3776
Epoch [3/5], Step [600/1563], Loss: 1.2591
Epoch [3/5], Step [700/1563], Loss: 1.4333
Epoch [3/5], Step [800/1563], Loss: 1.3301
Epoch [3/5], Step [900/1563], Loss: 1.3536
Epoch [3/5], Step [1000/1563], Loss: 1.4488
Epoch [3/5], Step [1100/1563], Loss: 1.3179
Epoch [3/5], Step [1200/1563], Loss: 1.0684
Epoch [3/5], Step [1300/1563], Loss: 1.6526
Epoch [3/5], Step [1400/1563], Loss: 1.1815
Epoch [3/5], Step [1500/1563], Loss: 1.3683
Epoch [3/5], Loss: 1.3836, Accuracy: 49.23%
Epoch [4/5], Step [100/1563], Loss: 1.2601
Epoch [4/5], Step [200/1563], Loss: 1.3277
Epoch [4/5], Step [300/1563], Loss: 1.1337
Epoch [4/5], Step [400/1563], Loss: 1.2273
Epoch [4/5], Step [500/1563], Loss: 1.7351
Epoch [4/5], Step [600/1563], Loss: 1.3826
Epoch [4/5], Step [700/1563], Loss: 1.2639
Epoch [4/5], Step [800/1563], Loss: 1.5757
Epoch [4/5], Step [900/1563], Loss: 1.0702
Epoch [4/5], Step [1000/1563], Loss: 1.3986
Epoch [4/5], Step [1100/1563], Loss: 1.1105
Epoch [4/5], Step [1200/1563], Loss: 1.2621
Epoch [4/5], Step [1300/1563], Loss: 1.4261
Epoch [4/5], Step [1400/1563], Loss: 1.3028
Epoch [4/5], Step [1500/1563], Loss: 1.9051
Epoch [4/5], Loss: 1.2850, Accuracy: 52.98%
Epoch [5/5], Step [100/1563], Loss: 0.9517
Epoch [5/5], Step [200/1563], Loss: 0.9844
Epoch [5/5], Step [300/1563], Loss: 1.2391
Epoch [5/5], Step [400/1563], Loss: 1.3588
Epoch [5/5], Step [500/1563], Loss: 0.9441
Epoch [5/5], Step [600/1563], Loss: 1.1711
Epoch [5/5], Step [700/1563], Loss: 1.1687
Epoch [5/5], Step [800/1563], Loss: 1.0097
Epoch [5/5], Step [900/1563], Loss: 0.9899
Epoch [5/5], Step [1000/1563], Loss: 1.3289
Epoch [5/5], Step [1100/1563], Loss: 1.5510
Epoch [5/5], Step [1200/1563], Loss: 0.9139
Epoch [5/5], Step [1300/1563], Loss: 0.9221
Epoch [5/5], Step [1400/1563], Loss: 1.3378
Epoch [5/5], Step [1500/1563], Loss: 1.1785
Epoch [5/5], Loss: 1.2116, Accuracy: 55.78%
Training finished!</code></pre>
</div>
</div>
<p>This code is a simple example to show the operation of the ViT model. The actual ViT shows much better performance when used in a way that pre-trains on large datasets like ImageNet and then fine-tunes for specific tasks (e.g., CIFAR-10 classification). Here, we simply check if training is possible.</p>
<p><strong>Meaning and Impact of ViT</strong></p>
<p>ViT showed superior performance to CNNs in image classification tasks, causing a big sensation in the computer vision field. In particular, it demonstrated its value when pre-trained on large-scale image datasets like JFT-300M with over 300 million images. This threw out two important implications.</p>
<ol type="1">
<li><p><strong>Scalability:</strong> ViT showed excellent scalability where performance continuously improves as the dataset size increases. This is in contrast to CNN-based models where performance improvement stagnates or even deteriorates beyond a certain scale of datasets. ViT’s characteristic opens up possibilities for building more powerful vision models using more data in the future.</p></li>
<li><p><strong>Universality of Transformer:</strong> ViT proved that the transformer architecture, widely used in natural language processing (NLP), can also be effective in image processing fields. This became a catalyst for research on multimodal models that can process various modalities (text, images, voices, etc.) with one architecture.</p></li>
</ol>
<p>The success of ViT became an important foundation for the development of subsequent multimodal models like CLIP (Contrastive Language-Image Pre-training). CLIP learns to express images and text in one integrated space by combining ViT’s image encoder and a transformer-based text encoder. This enables various applications such as generating text descriptions for images or searching images based on text descriptions.</p>
<p>No text was provided to translate.</p>
</section>
<section id="vit-22b-the-extreme-scale" class="level3">
<h3 class="anchored" data-anchor-id="vit-22b-the-extreme-scale">10.5.6 ViT-22B: The Extreme Scale</h3>
<p>The ViT-22B proposed and trained by the <strong>Google Research</strong> team has shown performance that surpasses CNNs in image classification, causing a huge sensation in the field of computer vision. ViT-22B has proven that expanding the size of models and data is one of the key factors for improving performance. With an overwhelming size of 22 billion parameters and trained on a massive dataset of tens of billions of images, ViT-22B has achieved unprecedented levels of performance, opening up new horizons in vision AI.</p>
<p><strong>Background: Scaling Law and Success of Large Language Models</strong></p>
<p>The emergence of ViT-22B is closely related to the remarkable success of large language models (LLMs) in the field of natural language processing (NLP). LLMs like GPT-3 have shown that their performance improves steadily as the size of the model (number of parameters) and the amount of data increase, following a <em>scaling law</em>. This trend has spread the belief that “bigger is better,” leading to similar attempts in the vision field.</p>
<p>Since ViT is based on the transformer architecture, it was easy to apply the scaling strategy validated in LLMs. Because ViT processes image patches like text tokens, it was possible to increase the number of parameters and use more data for training without greatly changing the model’s structure.</p>
<p><strong>Structure and Characteristics of ViT-22B</strong></p>
<p>ViT-22B basically follows the architecture of ViT but differs in <em>scale</em>.</p>
<ul>
<li><strong>Enormous Model Size:</strong> With 22 billion parameters, ViT-22B has an overwhelmingly large scale compared to ViT-Base (86 million), ViT-Large (370 million), and ViT-Huge (632 million). This means the model can capture much more complex and subtle image features and internalize more knowledge.</li>
<li><strong>Massive Dataset:</strong> ViT-22B was trained on a <em>private</em> dataset (such as JFT-4B) consisting of tens of billions of images. Such large-scale data is essential for maximizing generalization performance and comprehensively learning various image distributions.</li>
<li><strong>Improved Performance:</strong> ViT-22B has recorded superior performance (State-Of-The-Art, SOTA) in various vision benchmarks, including image classification, object detection, and image segmentation, clearly showing the positive impact of model size and data amount on performance.</li>
</ul>
<p><strong>Challenges and Implications of Training ViT-22B</strong></p>
<p>Training a massive model like ViT-22B is almost impossible in a typical research environment. It requires hundreds or thousands of GPUs or TPUs, which are expensive specialized hardware, and training time can take from several weeks to months. Additionally, building infrastructure to store and process enormous amounts of data is a significant challenge.</p>
<p>The emergence of ViT-22B has proven the scalability of the ViT architecture but has also raised concerns about <em>efficiency</em>. As model size increases, performance improves, but the computing resources and energy consumption required for training and inference also increase exponentially. Therefore, future research is expected to focus on improving efficiency while maintaining model performance.</p>
</section>
<section id="mae-v3-self-supervised-learning" class="level3">
<h3 class="anchored" data-anchor-id="mae-v3-self-supervised-learning">10.5.7 MAE v3: Self-Supervised Learning</h3>
<p>The Meta AI (Facebook AI Research, FAIR) team proposed MAE (Masked Autoencoder), a self-supervised learning method that learns powerful image representations by leveraging large-scale unlabeled image datasets. Based on ViT, MAE randomly masks a significant portion of the image and trains the model to restore the masked parts. MAE v3 is the latest version of MAE, which has improved performance and efficiency through various enhancements.</p>
<p><strong>How MAE Works</strong></p>
<p>The core idea of MAE is to train the model to understand and restore the entire image by only seeing a part of it, similar to how humans solve “fill-in-the-blank” problems.</p>
<ol type="1">
<li><p><strong>Random Masking of Input Images:</strong> A significant portion (e.g., 75%) of the input image is randomly masked, with masking performed at the image patch level.</p></li>
<li><p><strong>Encoding:</strong> Only the unmasked (i.e., visible) patches are fed into the ViT encoder to extract feature vectors.</p></li>
<li><p><strong>Decoding:</strong> The decoder uses the output of the encoder (features of visible patches) and information about the masked patches to restore the original image. The decoder consists of lightweight Transformer blocks to improve computational efficiency.</p></li>
<li><p><strong>Reconstruction Loss:</strong> The pixel-level difference (e.g., Mean Squared Error, MSE) between the restored image and the original image is calculated, and the model (encoder and decoder) is trained to minimize this difference.</p></li>
</ol>
<p><strong>Structural Improvements of MAE v3</strong></p>
<p>MAE v3 achieves better performance and efficiency than its predecessors through the following key improvements:</p>
<ol type="1">
<li><p><strong>Enhanced Masking Strategy:</strong> Unlike the initial MAE, which simply masked patches randomly, MAE v3 employs a more sophisticated masking strategy. For example, it can preserve meaningful areas of the image (such as object boundaries) or mask patches of various sizes.</p></li>
<li><p><strong>Optimized Encoder-Decoder Structure:</strong></p>
<ul>
<li>Encoder: Larger ViT models like ViT-Large and ViT-Huge are used to extract richer features from visible patches.</li>
<li>Decoder: Shallow and lightweight Transformer blocks are used to maintain computational efficiency while improving restoration performance.</li>
</ul></li>
<li><p><strong>Scale Expansion:</strong> The model scale has been expanded from ViT-L/16 and ViT-H/16 to ViT-g/14 (with 2.5 billion parameters).</p></li>
</ol>
<p><strong>Advantages and Significance of MAE</strong></p>
<p>MAE has the following advantages, making it notable in the field of self-supervised learning:</p>
<ol type="1">
<li><p><strong>Label-Free Learning:</strong> MAE can perform pre-training using large-scale unlabeled image datasets, saving the cost and time required for manual labeling and allowing for the use of more data.</p></li>
<li><p><strong>Powerful Representation Learning:</strong> By restoring images with significant portions masked, MAE develops the ability to understand the structure, meaning, and context of images. This ability helps achieve good performance in various downstream tasks such as image classification, object detection, and segmentation.</p></li>
<li><p><strong>Ease of Transfer Learning:</strong> Models pre-trained with MAE can be fine-tuned for different tasks, enabling good performance even in tasks with limited labels.</p></li>
</ol>
<p><strong>Conclusion</strong> MAE presents an effective approach to learning powerful image representations without labels through the intuitive idea of “filling in the blanks”. MAE v3 further develops the advantages of MAE, achieving higher performance and efficiency, and leading the development of self-supervised learning research.</p>
</section>
</section>
<section id="clip-a-milestone-in-multimodal-learning" class="level2">
<h2 class="anchored" data-anchor-id="clip-a-milestone-in-multimodal-learning">10.6 CLIP: A Milestone in Multimodal Learning</h2>
<p>In 2021, OpenAI introduced the <strong>CLIP (Contrastive Language-Image Pre-training)</strong> model through the paper “Learning Transferable Visual Models From Natural Language Supervision”. CLIP brought innovative advancements to the field of multimodal learning by learning to represent images and text, two different modalities, in <em>a shared space</em>.</p>
<section id="basic-structure-of-clip-dual-encoder" class="level3">
<h3 class="anchored" data-anchor-id="basic-structure-of-clip-dual-encoder">10.6.1 Basic Structure of CLIP: Dual Encoder</h3>
<p>The core of CLIP is a <em>dual encoder</em> structure consisting of two independent encoders: <strong>Image Encoder</strong> and <strong>Text Encoder</strong>.</p>
<ul>
<li><strong>Image Encoder:</strong> Transforms the input image into a fixed-dimensional vector (image embedding).</li>
<li><strong>Text Encoder:</strong> Transforms the input text (description of the image) into a vector (text embedding) of the <em>same dimension</em> as the image encoder.</li>
</ul>
<p>These two encoders are trained using <em>contrastive learning</em>.</p>
<p><strong>Core of CLIP Training: Contrastive Learning</strong></p>
<p>The core of CLIP training is <em>contrastive learning</em> using a <em>large-scale image-text pair dataset</em>.</p>
<ol type="1">
<li><strong>Data:</strong> Uses a dataset consisting of hundreds of millions of (image, text) pairs collected from the internet. In each pair, the text describes the corresponding image.</li>
<li><strong>Objective:</strong> Trains the encoders so that the embeddings of images and texts belonging to the <em>same pair</em> are <em>close</em>, and those belonging to <em>different pairs</em> are <em>far apart</em>.</li>
<li><strong>Loss Function:</strong> Employs a contrastive loss function. This loss function operates by <em>increasing</em> the similarity (e.g., cosine similarity) between embeddings of the <em>same pair</em> and <em>decreasing</em> the similarity between embeddings of <em>different pairs</em>. (Refer to Section 10.4 for contrastive learning)</li>
</ol>
<p><strong>Code Example</strong></p>
<div id="cell-78" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb92"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb92-1"><a href="#cb92-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb92-2"><a href="#cb92-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb92-3"><a href="#cb92-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb92-4"><a href="#cb92-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb92-5"><a href="#cb92-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb92-6"><a href="#cb92-6" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> CLIP(nn.Module):</span>
<span id="cb92-7"><a href="#cb92-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, image_encoder, text_encoder, embed_dim):</span>
<span id="cb92-8"><a href="#cb92-8" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb92-9"><a href="#cb92-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.image_encoder <span class="op">=</span> image_encoder</span>
<span id="cb92-10"><a href="#cb92-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.text_encoder <span class="op">=</span> text_encoder</span>
<span id="cb92-11"><a href="#cb92-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.image_projection <span class="op">=</span> nn.Linear(image_encoder.output_dim, embed_dim)</span>
<span id="cb92-12"><a href="#cb92-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.text_projection <span class="op">=</span> nn.Linear(text_encoder.output_dim, embed_dim)</span>
<span id="cb92-13"><a href="#cb92-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.logit_scale <span class="op">=</span> nn.Parameter(torch.ones([]) <span class="op">*</span> np.log(<span class="dv">1</span> <span class="op">/</span> <span class="fl">0.07</span>)) <span class="co"># Learnable scale parameter</span></span>
<span id="cb92-14"><a href="#cb92-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb92-15"><a href="#cb92-15" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, images, texts):</span>
<span id="cb92-16"><a href="#cb92-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 1. Image encoding</span></span>
<span id="cb92-17"><a href="#cb92-17" aria-hidden="true" tabindex="-1"></a>        image_features <span class="op">=</span> <span class="va">self</span>.image_encoder(images)  <span class="co"># [batch_size, image_encoder.output_dim]</span></span>
<span id="cb92-18"><a href="#cb92-18" aria-hidden="true" tabindex="-1"></a>        image_embeddings <span class="op">=</span> <span class="va">self</span>.image_projection(image_features)  <span class="co"># [batch_size, embed_dim]</span></span>
<span id="cb92-19"><a href="#cb92-19" aria-hidden="true" tabindex="-1"></a>        image_embeddings <span class="op">=</span> F.normalize(image_embeddings, dim<span class="op">=-</span><span class="dv">1</span>) <span class="co"># L2 normalization</span></span>
<span id="cb92-20"><a href="#cb92-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb92-21"><a href="#cb92-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 2. Text encoding</span></span>
<span id="cb92-22"><a href="#cb92-22" aria-hidden="true" tabindex="-1"></a>        text_features <span class="op">=</span> <span class="va">self</span>.text_encoder(texts)   <span class="co"># [batch_size, text_encoder.output_dim]</span></span>
<span id="cb92-23"><a href="#cb92-23" aria-hidden="true" tabindex="-1"></a>        text_embeddings <span class="op">=</span> <span class="va">self</span>.text_projection(text_features)    <span class="co"># [batch_size, embed_dim]</span></span>
<span id="cb92-24"><a href="#cb92-24" aria-hidden="true" tabindex="-1"></a>        text_embeddings <span class="op">=</span> F.normalize(text_embeddings, dim<span class="op">=-</span><span class="dv">1</span>) <span class="co"># L2 normalization</span></span>
<span id="cb92-25"><a href="#cb92-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb92-26"><a href="#cb92-26" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 3. Similarity calculation</span></span>
<span id="cb92-27"><a href="#cb92-27" aria-hidden="true" tabindex="-1"></a>        logits_per_image <span class="op">=</span> <span class="va">self</span>.logit_scale.exp() <span class="op">*</span> image_embeddings <span class="op">@</span> text_embeddings.t()  <span class="co"># [batch_size, batch_size]</span></span>
<span id="cb92-28"><a href="#cb92-28" aria-hidden="true" tabindex="-1"></a>        logits_per_text <span class="op">=</span> logits_per_image.t() <span class="co"># [batch_size, batch_size]</span></span>
<span id="cb92-29"><a href="#cb92-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb92-30"><a href="#cb92-30" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> logits_per_image, logits_per_text</span>
<span id="cb92-31"><a href="#cb92-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb92-32"><a href="#cb92-32" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> contrastive_loss(logits_per_image, logits_per_text):</span>
<span id="cb92-33"><a href="#cb92-33" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb92-34"><a href="#cb92-34" aria-hidden="true" tabindex="-1"></a><span class="co">    Calculates the Contrastive Loss</span></span>
<span id="cb92-35"><a href="#cb92-35" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb92-36"><a href="#cb92-36" aria-hidden="true" tabindex="-1"></a>    batch_size <span class="op">=</span> logits_per_image.shape[<span class="dv">0</span>]</span>
<span id="cb92-37"><a href="#cb92-37" aria-hidden="true" tabindex="-1"></a>    labels <span class="op">=</span> torch.arange(batch_size).to(logits_per_image.device) <span class="co"># Correct labels (diagonal: same pair)</span></span>
<span id="cb92-38"><a href="#cb92-38" aria-hidden="true" tabindex="-1"></a>    loss_i <span class="op">=</span> F.cross_entropy(logits_per_image, labels)  <span class="co"># Loss based on image</span></span>
<span id="cb92-39"><a href="#cb92-39" aria-hidden="true" tabindex="-1"></a>    loss_t <span class="op">=</span> F.cross_entropy(logits_per_text, labels)   <span class="co"># Loss based on text</span></span>
<span id="cb92-40"><a href="#cb92-40" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (loss_i <span class="op">+</span> loss_t) <span class="op">/</span> <span class="dv">2</span>  <span class="co"># Average loss</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="image-encoder" class="level3">
<h3 class="anchored" data-anchor-id="image-encoder">10.6.2 Image Encoder</h3>
<p>The image encoder of CLIP takes an image as input and converts it into a fixed-dimensional embedding vector. The initial CLIP paper experimented with both ResNet and ViT (Vision Transformer).</p>
<ul>
<li><strong>ResNet-based encoder</strong>: It uses existing CNN-based image classification models (e.g., ResNet-50, ResNet-101).</li>
<li><strong>ViT-based encoder</strong>: It uses ViT (Vision Transformer) as the image encoder (refer to section 10.5). ViT divides an image into patches and uses each patch as input to the transformer.</li>
</ul>
<p>The experimental results showed that the ViT-based encoder performed better than the ResNet-based encoder. In particular, as the model and data size increased, the performance improvement of ViT was greater.</p>
</section>
<section id="text-encoder" class="level3">
<h3 class="anchored" data-anchor-id="text-encoder">10.6.3 Text Encoder</h3>
<p>The text encoder of CLIP takes a text description as input and converts it into an embedding vector of the <em>same dimension</em> as the image encoder. The initial CLIP paper used a Transformer-based text encoder.</p>
<ul>
<li>The text encoder uses Byte Pair Encoding (BPE) to tokenize the text and embed each token.</li>
<li>It stacks multiple layers of Transformer blocks to capture contextual information from the text and ultimately generates a single embedding vector representing the entire text.</li>
</ul>
</section>
<section id="mechanism-of-zero-shot-transfer" class="level3">
<h3 class="anchored">10.6.4 Mechanism of Zero-shot Transfer</h3>
<p>One of the most significant features of CLIP is its ability to perform <strong>zero-shot transfer</strong> in various image classification tasks without <em>fine-tuning</em>, which means it can achieve excellent performance.</p>
<p><strong>Why zero-shot transfer is possible</strong></p>
<p>CLIP learns to represent images and text in the <em>same semantic space</em> through contrastive learning using a large-scale image-text pair dataset. In other words, CLIP acquires the ability to understand the <em>semantic relationship</em> between images and text.</p>
<p><strong>Zero-shot classification process</strong></p>
<ol type="1">
<li><p>Prepare text descriptions for the classes to be classified. For example, for the CIFAR-10 dataset, prepare text descriptions such as “a photo of a cat”, “a photo of a dog”, …, “a photo of a truck”.</p></li>
<li><p>Use the text encoder to embed each text description.</p></li>
<li><p>Embed the given image using the image encoder.</p></li>
<li><p>Calculate the similarity (e.g., cosine similarity) between the image embedding and each text embedding.</p></li>
<li><p>Select the class corresponding to the text description with the highest similarity as the predicted class for the image.</p></li>
</ol>
<p><strong>Meaning of zero-shot transfer</strong></p>
<p>Zero-shot transfer refers to the ability of a model to be applied directly to new classes or tasks that it has never seen during training, without additional learning or fine-tuning. This is in contrast to traditional supervised learning methods, which require specialized labels for specific tasks.</p>
<p>The core of zero-shot transfer is flexibility. For example, when training an image classification model using only data for “cat” and “dog” classes, if a new image of a “giraffe” or “elephant” (which was not in the training data) is given, the model can correctly classify the image simply by providing a natural language description such as “giraffe photo” or “elephant photo”. This ability to maximize the model’s generalization capability even when there is no data for new classes or tasks is the greatest strength of zero-shot transfer. Additionally, zero-shot transfer provides versatility. It can be applied to various multimodal tasks such as image classification, image retrieval, image captioning, object detection, and Visual Question Answering (VQA), beyond just image classification. For example, when a text query like “red sports car” is input into an image search system, the model can find the corresponding images in the database. This is possible because the model understands the semantic connection between images and texts. The fact that one model can be used for various tasks greatly contributes to saving time and resources and increasing the usability of AI systems.</p>
<p><strong>Impact of CLIP</strong></p>
<p>CLIP presented new possibilities for multimodal learning through its zero-shot transfer ability. Since then, various follow-up studies based on CLIP’s idea have been conducted, greatly influencing the development of image generation models such as DALL-E and Stable Diffusion, as well as large-scale multimodal models like GPT-4V.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Click to view content (Deep Dive: Contrastive Learning and CLIP)">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Click to view content (Deep Dive: Contrastive Learning and CLIP)
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<section id="deep-dive-contrastive-learning-and-clip" class="level2 callout-body-container callout-body">
<h2 class="anchored" data-anchor-id="deep-dive-contrastive-learning-and-clip">Deep Dive: Contrastive Learning and CLIP</h2>
<p>Contrastive Learning is a powerful methodology for learning representations from unlabeled data. It has shown outstanding performance in multi-modal learning, which connects different modalities such as images and text. In this deep dive, we will analyze the basic principles of Contrastive Learning, various methodologies, and CLIP (Contrastive Language-Image Pre-training), a groundbreaking model that connects images and text based on Contrastive Learning.</p>
<section id="basic-principles-of-contrastive-learning" class="level3">
<h3 class="anchored" data-anchor-id="basic-principles-of-contrastive-learning">1. Basic Principles of Contrastive Learning</h3>
<p>The core idea of Contrastive Learning is to learn representations by making <strong>similar sample pairs (positive pairs) close in the embedding space and dissimilar sample pairs (negative pairs) far apart</strong>.</p>
<ul>
<li><strong>Anchor:</strong> The reference sample.</li>
<li><strong>Positive Sample:</strong> A sample that is semantically similar to the anchor. (e.g., different augmentations of the same image, different translations of the same sentence)</li>
<li><strong>Negative Sample:</strong> A sample that is semantically different from the anchor.</li>
</ul>
<p>Contrastive Learning typically involves the following steps:</p>
<ol type="1">
<li><strong>Data Augmentation:</strong> Apply various data augmentation techniques to generate anchors and positive samples. (e.g., random cropping, color jittering, rotation for images)</li>
<li><strong>Encoding:</strong> Convert anchors, positive samples, and negative samples into embedding vectors using an encoder.</li>
<li><strong>Contrastive Loss:</strong> Use a contrastive loss function to train the encoder so that the embeddings of positive pairs are close and those of negative pairs are far apart.</li>
</ol>
</section>
<section id="contrastive-loss-functions" class="level3">
<h3 class="anchored" data-anchor-id="contrastive-loss-functions">2. Contrastive Loss Functions</h3>
<p>Several contrastive loss functions have been proposed, including:</p>
<ul>
<li><p><strong>InfoNCE Loss (Noise Contrastive Estimation):</strong> Similar to cross-entropy loss, it maximizes the softmax probability of positive pairs.</p>
<p><span class="math inline">\(L = -\log \frac{\exp(\text{sim}(z_i, z_j) / \tau)}{\sum_{k=1}^{2N} \mathbb{1}_{[k \neq i]} \exp(\text{sim}(z_i, z_k) / \tau)}\)</span></p>
<ul>
<li><span class="math inline">\(z_i\)</span>: The embedding of the anchor</li>
<li><span class="math inline">\(z_j\)</span>: The embedding of the positive sample</li>
<li><span class="math inline">\(z_k\)</span>: The embedding of the negative sample (k ≠ i)</li>
<li><span class="math inline">\(\text{sim}(\cdot, \cdot)\)</span>: A similarity function (e.g., cosine similarity)</li>
<li><span class="math inline">\(\tau\)</span>: Temperature parameter (controls the distribution of similarities)</li>
<li><span class="math inline">\(N\)</span>: Mini-batch size</li>
</ul></li>
<li><p><strong>NT-Xent Loss (Normalized Temperature-scaled Cross Entropy Loss):</strong> A variant of InfoNCE Loss proposed in the SimCLR paper.</p></li>
<li><p><strong>Triplet Loss:</strong> Uses anchor, positive, and negative samples to train the model so that the distance between the anchor and positive sample is smaller than the distance between the anchor and negative sample.</p>
<p><span class="math inline">\(L = \max(0, d(a, p) - d(a, n) + m)\)</span></p></li>
<li><p><span class="math inline">\(a\)</span>: Anchor</p></li>
<li><p><span class="math inline">\(p\)</span>: Positive sample</p></li>
<li><p><span class="math inline">\(n\)</span>: Negative sample</p></li>
<li><p><span class="math inline">\(d(\cdot, \cdot)\)</span>: Distance function (e.g., Euclidean distance)</p></li>
<li><p><span class="math inline">\(m\)</span>: Margin (determines how far apart to push)</p></li>
</ul>
</section>
<section id="contrastive-learning-methodologies" class="level3">
<h3 class="anchored" data-anchor-id="contrastive-learning-methodologies">3. Contrastive Learning Methodologies</h3>
<ul>
<li><strong>SimCLR (A Simple Framework for Contrastive Learning of Visual Representations):</strong> Learns image representations using data augmentation, large batch size, and projection head (MLP).</li>
<li><strong>MoCo (Momentum Contrast):</strong> Uses momentum encoder to maintain negative samples stably and achieves good performance without large batch sizes.</li>
<li><strong>SwAV (Swapping Assignments between multiple Views):</strong> Learns representation without explicitly defining positive/negative samples using online clustering.</li>
<li><strong>BYOL (Bootstrap Your Own Latent):</strong> Learns without negative samples by predicting between target network and online network.</li>
</ul>
</section>
<section id="clip-contrastive-language-image-pre-training" class="level3">
<h3 class="anchored" data-anchor-id="clip-contrastive-language-image-pre-training">4. CLIP (Contrastive Language-Image Pre-training)</h3>
<p>CLIP is a model developed by OpenAI that learns powerful multimodal representations connecting images and text using Contrastive Learning.</p>
<section id="clip-training" class="level4">
<h4 class="anchored" data-anchor-id="clip-training">4.1 CLIP Training</h4>
<ul>
<li><strong>Data:</strong> Large-scale image-text pair dataset (400 million)</li>
<li><strong>Model:</strong>
<ul>
<li><strong>Image Encoder:</strong> Extracts feature vectors from images (e.g., ResNet, ViT)</li>
<li><strong>Text Encoder:</strong> Extracts feature vectors from text (e.g., Transformer)</li>
</ul></li>
<li><strong>Training:</strong>
<ol type="1">
<li>Encodes images and text separately to obtain embedding vectors.</li>
<li>Uses Contrastive Loss (InfoNCE) to increase cosine similarity between image-text embeddings of the same pair (positive pair) and decrease cosine similarity between embeddings of different pairs (negative pairs).</li>
</ol>
<ul>
<li>Each image has one positive text and (N-1) negative texts within a batch</li>
<li>Similarly, each text has one positive image and (N-1) negative images</li>
</ul></li>
</ul>
</section>
<section id="clip-characteristics" class="level4">
<h4 class="anchored" data-anchor-id="clip-characteristics">4.2 CLIP Characteristics</h4>
<ul>
<li><strong>Zero-shot learning:</strong> Can perform new tasks (e.g., image classification, image search) without additional fine-tuning using the learned image-text representation.
<ul>
<li>Zero-shot Image Classification example:
<ol type="1">
<li>Expresses class names as text (e.g., “a photo of a cat”, “a photo of a dog”).</li>
<li>Encodes each text using Text Encoder.</li>
<li>Encodes the given image using Image Encoder.</li>
<li>Calculates cosine similarity between image embedding and each text embedding.</li>
<li>Classifies the image into the class corresponding to the text with the highest similarity.</li>
</ol></li>
</ul></li>
<li><strong>Powerful representation learning:</strong> Learns general image/text representations that can be transferred to various tasks.</li>
</ul>
</section>
<section id="clip-applications" class="level4">
<h4 class="anchored" data-anchor-id="clip-applications">4.3 CLIP Applications</h4>
<ul>
<li><strong>Image Classification:</strong> Zero-shot classification, few-shot classification.</li>
<li><strong>Image Retrieval:</strong> Image search using text queries.</li>
<li><strong>Image Generation:</strong> Used as the underlying technology for text-based image generation models such as DALL-E, Stable Diffusion.</li>
<li><strong>Visual Question Answering (VQA):</strong> Generating answers by taking images and question texts as input together.</li>
<li><strong>Object Detection:</strong> Performing open-vocabulary object detection by integrating CLIP into an object detection model.</li>
</ul>
</section>
</section>
<section id="limitations-of-contrastive-learning-and-clip-and-future-research-directions" class="level3">
<h3 class="anchored" data-anchor-id="limitations-of-contrastive-learning-and-clip-and-future-research-directions">5. Limitations of Contrastive Learning and CLIP, and Future Research Directions</h3>
<ul>
<li><strong>Dependence on Data Augmentation:</strong> Contrastive Learning is sensitive to data augmentation techniques. Research is needed on what augmentations are effective.</li>
<li><strong>Negative Sampling Bias:</strong> The outcome of learning can vary depending on how negative samples are selected. Techniques such as hard negative mining are being researched.</li>
<li><strong>Mode Collapse:</strong> A phenomenon where all samples converge to a single representation.</li>
<li><strong>Fine-grained Understanding:</strong> While CLIP learns coarse-grained alignment between images and texts well, it may lack fine-grained understanding (e.g., relationships between objects in an image, subtle nuances of text).</li>
<li><strong>Computational Cost:</strong> Requires large datasets and large batch sizes.</li>
</ul>
</section>
<section id="conclusion-1" class="level3">
<h3 class="anchored" data-anchor-id="conclusion-1">6. Conclusion</h3>
<p>Contrastive Learning is an effective methodology for learning powerful representations using unlabeled data. In particular, CLIP has successfully applied Contrastive Learning to multimodal learning, opening up a new horizon for connecting images and texts. It is expected that Contrastive Learning and CLIP will be used in various fields in the future.</p>
<p><strong>References:</strong> * Chen, T., Kornblith, S., Norouzi, M., &amp; Hinton, G. (2020). A simple framework for contrastive learning of visual representations. International conference on machine learning. PMLR. * Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., … &amp; Sutskever, I. (2021). Learning transferable visual models from natural language supervision. International Conference on Machine Learning. PMLR. * He, K., Fan, H., Wu, Y., Xie, S., &amp; Girshick, R. (2020). Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp.&nbsp;9729-9738). * Grill, J. B., Strub, F., Altché, F., Tallec, C., Richemond, P., Buchatskaya, E., … &amp; Valko, M. (2020). Bootstrap your own latent-a new approach to self-supervised learning. Advances in neural information processing systems, 33, 21271-21284.</p>
</section>
</section>
</div>
</div>
</section>
</section>
<section id="practice-problems" class="level2">
<h2 class="anchored" data-anchor-id="practice-problems">Practice Problems</h2>
<p><strong>Basic Problems</strong></p>
<ol type="1">
<li>Explain what multimodal data is and provide more than three examples of multimodal data.</li>
<li>Describe the differences between Joint Representation and Coordinated Representation, and compare their advantages and disadvantages.</li>
<li>Define the Image Captioning task and explain the general structure (encoder-decoder) of deep learning models used to solve this task.</li>
</ol>
<p><strong>Application Problems</strong></p>
<ol type="1">
<li>Design a simple model structure (using block diagrams, etc.) that takes an image and question text as input to generate an answer for the Visual Question Answering (VQA) task, and explain the role of each component.</li>
<li>Explain the training method of the CLIP model and describe its advantages compared to traditional image-text supervised learning methods.</li>
<li>Write code using the Hugging Face Transformers library to generate image-related text captions (e.g., using the <code>blip-image-captioning-base</code> model).</li>
</ol>
<p><strong>In-Depth Problems</strong></p>
<ol type="1">
<li>Investigate various multimodal fusion methods (early fusion, late fusion, hybrid fusion), explain their advantages and disadvantages, and describe which fusion method is suitable for different situations.</li>
<li>Explain the working principle of the Cross-Modal Attention mechanism and its role in multimodal learning, along with specific examples (e.g., VQA, Image Captioning).</li>
<li>Investigate the working principle of models that generate images based on text descriptions (e.g., DALL-E, Stable Diffusion) and discuss their potential positive and negative impacts on society (idea proposal and discussion level).</li>
</ol>
<div class="callout callout-style-default callout-note callout-titled" title="Click to view contents (exercise answers)">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Click to view contents (exercise answers)
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<section id="exercise-answers" class="level2 callout-body-container callout-body">
<h2 class="anchored" data-anchor-id="exercise-answers">Exercise Answers</h2>
<section id="basic-problems" class="level3">
<h3 class="anchored" data-anchor-id="basic-problems">Basic Problems</h3>
<ol type="1">
<li><strong>Multimodal Data:</strong> Refers to data that combines two or more different forms of data (modalities). Examples include:
<ul>
<li>Images and text captions</li>
<li>Videos and audio tracks</li>
<li>Sensor data (e.g., accelerometers, gyroscopes) and text descriptions</li>
</ul></li>
<li><strong>Joint Representation vs.&nbsp;Coordinated Representation:</strong>
<ul>
<li><strong>Joint Representation:</strong> Represents information from multiple modalities in a single unified vector space.
<ul>
<li>Advantage: Can directly model correlations between different modalities.</li>
<li>Disadvantage: One modality may dominate others.</li>
</ul></li>
<li><strong>Coordinated Representation:</strong> Represents each modality in its own separate vector space but learns these spaces to be related (e.g., through similarity constraints).
<ul>
<li>Advantage: Preserves unique characteristics of each modality while allowing interaction.</li>
<li>Disadvantage: Modeling interactions between modalities is less direct compared to Joint Representation.</li>
</ul></li>
</ul></li>
<li><strong>Image Captioning:</strong> The task of generating text descriptions for given images.
<ul>
<li><strong>Common Structure (Encoder-Decoder):</strong>
<ul>
<li><strong>Encoder:</strong> Extracts features from the image (typically using CNN).</li>
<li><strong>Decoder:</strong> Predicts the next word based on the image features extracted by the encoder and previously generated words (typically using RNN or Transformer). Can use attention mechanisms to focus on specific areas of the image.</li>
</ul></li>
</ul></li>
</ol>
</section>
<section id="application-problems" class="level3">
<h3 class="anchored" data-anchor-id="application-problems">Application Problems</h3>
<ol type="1">
<li><p><strong>VQA Model Structure:</strong></p>
<pre class="mermaid"><code>graph LR
    subgraph VQA Model
        A[Image] --&gt; B(Image Encoder - CNN)
        C[Question Text] --&gt; D(Text Encoder - RNN/Transformer)
        B --&gt; E(Fusion Module)
        D --&gt; E
        E --&gt; F(Decoder - RNN/Transformer)
        F --&gt; G(Answer)
    end</code></pre>
<ul>
<li><strong>Image Encoder (CNN):</strong> Takes an image as input and extracts a feature vector.</li>
<li><strong>Text Encoder (RNN/Transformer):</strong> Takes question text as input and extracts a feature vector.</li>
<li><strong>Fusion Module:</strong> Combines the image feature vector and the question text feature vector (e.g., through concatenation, element-wise multiplication, cross-modal attention).</li>
<li><strong>Decoder (RNN/Transformer):</strong> Generates an answer based on the fused feature vector.</li>
</ul></li>
<li><p><strong>CLIP Training Method and Advantages:</strong></p>
<ul>
<li><strong>Training Method:</strong> CLIP uses a large-scale image-text pair dataset to train by encoding images and texts separately and using contrastive loss to make embeddings from the same pair closer and those from different pairs farther apart.</li>
<li><strong>Advantages:</strong>
<ul>
<li><strong>Zero-shot learning:</strong> Can be applied to new tasks (e.g., image classification) without additional fine-tuning.</li>
<li><strong>Strong representation learning:</strong> Learns generalizable image/text representations that can be transferred across various tasks.</li>
<li><strong>Data efficiency:</strong> Can utilize unlabeled image-text pairs.</li>
</ul></li>
</ul></li>
<li><p><strong>Hugging Face Transformers Image Captioning Code:</strong></p></li>
</ol>
<div class="sourceCode" id="cb94"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb94-1"><a href="#cb94-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> pipeline</span>
<span id="cb94-2"><a href="#cb94-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-3"><a href="#cb94-3" aria-hidden="true" tabindex="-1"></a>captioner <span class="op">=</span> pipeline(<span class="st">"image-to-text"</span>, model<span class="op">=</span><span class="st">"nlpconnect/vit-gpt2-image-captioning"</span>) <span class="co"># or "blip-image-captioning-base"</span></span>
<span id="cb94-4"><a href="#cb94-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-5"><a href="#cb94-5" aria-hidden="true" tabindex="-1"></a>image_path <span class="op">=</span> <span class="st">"path/to/your/image.jpg"</span>  <span class="co"># image file path</span></span>
<span id="cb94-6"><a href="#cb94-6" aria-hidden="true" tabindex="-1"></a>caption <span class="op">=</span> captioner(image_path)[<span class="dv">0</span>][<span class="st">'generated_text'</span>]</span>
<span id="cb94-7"><a href="#cb94-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(caption)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="advanced-topics" class="level3">
<h3 class="anchored" data-anchor-id="advanced-topics">Advanced Topics</h3>
<ol type="1">
<li><p><strong>Multimodal Fusion Methods:</strong></p>
<ul>
<li><strong>Early Fusion:</strong> Combines modalities at the input stage (e.g., channel concatenation).
<ul>
<li>Advantages: Can capture low-level interactions between modalities.</li>
<li>Disadvantages: Dimensionality can become very large, and unique characteristics of each modality may be diluted.</li>
</ul></li>
<li><strong>Late Fusion:</strong> Processes each modality independently and then combines the results at the final stage (e.g., averaging, voting).
<ul>
<li>Advantages: Utilizes the characteristics of each modality to the fullest, simple to implement.</li>
<li>Disadvantages: Difficult to capture low-level interactions between modalities.</li>
</ul></li>
<li><strong>Hybrid Fusion:</strong> A combination of Early Fusion and Late Fusion, performing fusion at various levels.
<ul>
<li>Advantages: Can take advantage of the strengths of both Early and Late Fusion.</li>
<li>Disadvantages: Model complexity increases.</li>
</ul></li>
<li><strong>Suitable Situations:</strong>
<ul>
<li><strong>Early Fusion:</strong> When close interaction between modalities is crucial (e.g., synchronization of video and audio).</li>
<li><strong>Late Fusion:</strong> When each modality has independent meaning (e.g., image tagging and text description).</li>
<li><strong>Hybrid Fusion:</strong> For complex tasks that require capturing interactions at various levels.</li>
</ul></li>
</ul></li>
<li><p><strong>Cross-Modal Attention:</strong></p>
<ul>
<li><strong>Operation Principle:</strong> Uses a query from one modality to calculate attention weights for the key of another modality, and then uses these weights to perform a weighted sum of the value of the other modality, generating a new representation.</li>
<li><strong>Role:</strong>
<ul>
<li><strong>VQA:</strong> Determines which region of the image (key, value) should be focused on based on each word (query) in the question text.</li>
<li><strong>Image Captioning:</strong> Determines which region of the image (key, value) is related to each generated word (query).</li>
</ul></li>
</ul></li>
<li><p><strong>Text-Based Image Generation Models (DALL-E, Stable Diffusion, etc.):</strong></p>
<ul>
<li><strong>Operation Principle (Simplified):</strong>
<ul>
<li><strong>DALL-E (Transformer-based):</strong> Tokenizes text and images and uses a Transformer to model the probability of an image token sequence given a text token sequence.</li>
<li><strong>Stable Diffusion (Diffusion Model-based):</strong> Learns a forward process that progressively adds noise to an image and a reverse process that restores the image from the noise. Text information is provided as a condition in the reverse process to control the generated image.</li>
</ul></li>
</ul></li>
</ol>
<ul>
<li><strong>Positive impacts:</strong>
<ul>
<li><strong>Enhancing creativity:</strong> Visualizing new ideas, supporting the creation of artworks.</li>
<li><strong>Improving content production efficiency:</strong> Automating design, advertising, and educational material creation.</li>
<li><strong>Improving accessibility:</strong> Generating image descriptions for the visually impaired.</li>
</ul></li>
<li><strong>Negative impacts:</strong>
<ul>
<li><strong>Spreading Deepfakes and misinformation:</strong> Distorting reality, damaging reputation.</li>
<li><strong>Copyright infringement:</strong> Using and modifying existing images without permission.</li>
<li><strong>Job reduction:</strong> Replacing designers, illustrators, and other professions.</li>
<li><strong>Bias and discrimination:</strong> Creating discriminatory images against certain groups by reflecting biases in the learning data.</li>
</ul></li>
</ul>
</section>
</section>
</div>
</div>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<ol type="1">
<li><p><strong>CLIP (Learning Transferable Visual Models From Natural Language Supervision):</strong> The original paper on CLIP, a multi-modal expression learning method that connects images and text. <a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://arxiv.org/abs/2103.00020">https://arxiv.org/abs/2103.00020</a></p></li>
<li><p><strong>ViT (An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale):</strong> The original paper on ViT, which shows excellent performance in image classification using only the Transformer structure without CNN. <a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://arxiv.org/abs/2010.11929">https://arxiv.org/abs/2010.11929</a></p></li>
<li><p><strong>DALL-E (Zero-Shot Text-to-Image Generation):</strong> The paper on the DALL-E model, which generates images based on text descriptions. <a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://arxiv.org/abs/2102.12092">https://arxiv.org/abs/2102.12092</a></p></li>
<li><p><strong>MAE (Masked Autoencoders Are Scalable Vision Learners):</strong> The paper on MAE, which learns visual representations by masking and restoring parts of images. <a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://arxiv.org/abs/2111.06377">https://arxiv.org/abs/2111.06377</a></p></li>
<li><p><strong>Visual Question Answering (VQA):</strong> One of the early VQA studies, presenting the VQA dataset and baseline model. <a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://arxiv.org/abs/1505.00468">https://arxiv.org/abs/1505.00468</a></p></li>
<li><p><strong>Show, Attend and Tell (Neural Image Caption Generation with Visual Attention):</strong> The paper that first introduced the attention mechanism to image captioning. <a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://arxiv.org/abs/1502.03044">https://arxiv.org/abs/1502.03044</a></p></li>
<li><p><strong>Multimodal Machine Learning: A Survey and Taxonomy:</strong> A comprehensive survey paper on multimodal machine learning. <a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://arxiv.org/abs/1705.09406">https://arxiv.org/abs/1705.09406</a></p></li>
<li><p><strong>A Tutorial on Multimodal Deep Learning, Jiquan Ngiam:</strong> A tutorial on multimodal deep learning from NeurIPS 2011 (video). <a href="https://www.google.com/search?q=https://www.youtube.com/watch%3Fv%3DcR_ACqfF-bY%26list%3DPL_45CaSOtPzL-HWxMcnr02KvmP9Gq-xdb">https://www.youtube.com/watch?v=cR_ACqfF-bY&amp;list=PL_45CaSOtPzL-HWxMcnr02KvmP9Gq-xdb</a></p></li>
<li><p><strong>CMU Multimodal Machine Learning Course (11-777, Spring 2023), Louis-Philippe Morency:</strong> Carnegie Mellon University’s multimodal machine learning course materials. <a href="https://cmu-multicomp-lab.github.io/mmml-course/spring2023/">https://cmu-multicomp-lab.github.io/mmml-course/spring2023/</a></p></li>
<li><p><strong>A Comprehensive Survey on Deep Multimodal Learning:</strong> 2022 survey paper on multimodal deep learning. <a href="https://arxiv.org/abs/2204.11984">https://arxiv.org/abs/2204.11984</a></p></li>
<li><p><strong>arXiv:</strong> Search for the latest multimodal learning research papers using keywords like “multimodal learning”, “vision-language”, etc. <a href="https://arxiv.org/">https://arxiv.org/</a></p></li>
<li><p><strong>Hugging Face Transformers Multimodal Documentation:</strong> Hugging Face Transformers library’s multimodal model documentation. <a href="https://huggingface.co/docs/transformers/main/en/model_doc/auto#multimodal-models">https://huggingface.co/docs/transformers/main/en/model_doc/auto#multimodal-models</a></p></li>
</ol>
<p>There is no original text to translate.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>