{"entries":[],"headings":["장-멀티모달-딥러닝-다중-감각-융합의-시작","멀티모달과-딥러닝","멀티모달-데이터와-딥러닝의-만남","멀티모달-딥러닝의-중요성과-응용-분야","멀티모달-딥러닝의-역사와-발전-과정","초기-단계-2010년대-초반","어텐션-메커니즘-도입-2010년대-중반","트랜스포머-등장과-멀티모달-혁신-2017년-이후","최근-동향-지능의-확장과-융합","초기-멀티모달-접근법","이미지-캡셔닝-멀티모달-융합의-첫걸음","초기-cnn-rnn-구조-2014년-이전","어텐션-메커니즘-도입-2015년-이후","bottom-up-and-top-down-attention-2017년-이후","딥러닝-dna-관점에서-본-이미지-캡셔닝의-진화","이미지-캡셔닝-모델-blip-예제","시각-질의응답vqa-이미지-이해와-추론","초기-vqa-모델-cnn-rnn-2015년-이전","멀티모달-어텐션-메커니즘-2016년-이후","외부-지식-통합-2018년-이후","딥러닝-dna-관점에서-본-vqa의-진화","vqa-모델-vilt-예제","멀티모달-융합fusion-이론-cmu-강의-기반-분류","joint-representations","coordinated-representations","encoder-decoder","멀티모달-융합fusion과-최신-연구-동향","멀티모달-융합의-다양한-분류","융합-시점에-따른-분류-early-late-hybrid-fusion","모델-구조에-따른-분류","기타-분류","최신-트렌드-어텐션-기반-융합과-자기지도-학습","cross-modal-attention","multi-head-attention","자기지도-학습과-멀티모달-융합","토큰-레벨-vs-인스턴스-레벨-융합-2025년-연구","결론","모달리티-통합-전략","초기-융합early-fusion","후기-융합late-fusion","혼합-융합hybrid-fusion","최신-모델의-정교한-통합-전략-2023년-이후","멀티모달-표현-학습-기법","모달리티-간-표현-학습","크로스모달-어텐션-구조","perceiver-아키텍처","크로스-어텐션-구현과-훈련-안정성","훈련의-구조","실험결과","실험-결과표","모델-구조와-훈련성패의-분석표","어텐션-구조별-설명","주요-결과-분석","비전-트랜스포머vit","cnn에서-vit로의-패러다임-전환","이미지-패치-임베딩의-원리","이미지-패치-분할","linear-projection","포지셔널-인코딩-메커니즘","vit의-구조와-주요-구성-요소","vit-훈련-예제","vit-22b-극한의-스케일","mae-v3-자기-지도-학습","clip-멀티모달-학습의-이정표","clip의-기본-구조-듀얼-인코더-dual-encoder","image-encoder","text-encoder","zero-shot-전이의-메커니즘","딥다이브-contrastive-learning과-clip","contrastive-learning의-기본-원리","contrastive-loss-함수","contrastive-learning-방법론","clip-contrastive-language-image-pre-training","clip의-학습","clip의-특징","clip의-활용","contrastive-learning과-clip의-한계-및-향후-연구-방향","결론-1","연습문제","연습문제-해답","기본-문제","응용-문제","심화-문제","참고-자료"]}